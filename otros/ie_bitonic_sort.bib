@comment{author = {Nicholas M. Gotts and George A.K. {van Voorn} and J. Gareth Polhill and Eline de Jong and Bruce Edmonds and Gert Jan Hofstede and Ruth Meyer},
keywords = {Socio-ecological system, Agent-based model, Complexity, Ontology},
abstract = {Socio-Ecological Systems (SESs) are the systems in which our everyday lives are embedded, so understanding them is important. The complex properties of such systems make modelling an indispensable tool for their description and analysis. Human actors play a pivotal role in SESs, but their interactions with each other and their environment are often underrepresented in SES modelling. We argue that more attention should be given to social aspects in models of SESs, but this entails additional kinds of complexity. Modelling choices need to be as transparent as possible, and to be based on analysis of the purposes and limitations of modelling. We recommend thinking in terms of modelling projects rather than single models. Such a project may involve multiple models adopting different modelling methods. We argue that agent-based models (ABMs) are an essential tool in an SES modelling project, but their expressivity, which is their major advantage, also produces problems with model transparency and validation. We propose the use of formal ontologies to make the structure and meaning of models as explicit as possible, facilitating model design, implementation, assessment, comparison and extension.}
}}

@article{1985174,
 doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
 issn = {0167-9236},
 journal = {Decision Support Systems},
 number = {2},
 pages = {174},
 title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
 url = {https://www.sciencedirect.com/science/article/pii/0167923685900715},
 volume = {1},
 year = {1985}
}

@article{ADHIKARI20121374,
 abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.},
 author = {R.S. Adhikari and N. Aste and M. Manfren},
 doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
 issn = {1876-6102},
 journal = {Energy Procedia},
 keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
 note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
 pages = {1374-1379},
 title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
 url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
 volume = {14},
 year = {2012}
}

@incollection{ASCHEID200711,
 abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.},
 address = {Burlington},
 author = {Gerd Ascheid and Heinrich Meyr},
 booktitle = {Customizable Embedded Processors},
 doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
 editor = {Paolo Lenne and Rainer Leupers},
 issn = {18759661},
 pages = {11-37},
 publisher = {Morgan Kaufmann},
 series = {Systems on Silicon},
 title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
 url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
 year = {2007}
}

@incollection{ASCHEID200711,
 abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.},
 address = {Burlington},
 author = {Gerd Ascheid and Heinrich Meyr},
 booktitle = {Customizable Embedded Processors},
 doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
 editor = {Paolo Lenne and Rainer Leupers},
 issn = {18759661},
 pages = {11-37},
 publisher = {Morgan Kaufmann},
 series = {Systems on Silicon},
 title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
 url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
 year = {2007}
}

@article{DING2025121721,
 abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.},
 author = {Haixin Ding},
 doi = {https://doi.org/10.1016/j.ins.2024.121721},
 issn = {0020-0255},
 journal = {Information Sciences},
 keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
 pages = {121721},
 title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
 url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
 volume = {694},
 year = {2025}
}

@article{EVANS2008100,
 abstract = {In this study, we focus on the conditions which permit people to assert a conditional statement of the form ‘if p then q’ with conversational relevance. In a broadly decision-theoretic approach, also drawing on hypothetical thinking theory [Evans, J. St. B. T. (2007). Hypothetical thinking: Dual processes in reasoning and judgement. Hove, UK: Psychology Press.], we predicted that conditional tips and promises would appear more useful and persuasive and be more likely to encourage an action p when (a) the conditional link from p to q was stronger, (b) the cost of the action p was lower and (c) the benefit of the consequence q was higher. Similarly, we predicted that conditional warnings and threats would be seen as more useful and persuasive and more likely to discourage an action p when (a) the conditional link from p to q was stronger, (b) the benefit of the action p was lower and (c) the cost of the consequence q was higher. All predictions were strongly confirmed, suggesting that such conditionals may best be asserted when they are of high relevance to the goals of the listener.},
 author = {Jonathan St.B.T. Evans and Helen Neilens and Simon J. Handley and David E. Over},
 doi = {https://doi.org/10.1016/j.cognition.2008.02.001},
 issn = {0010-0277},
 journal = {Cognition},
 keywords = {Conditionals, Reasoning, Decision making, Language comprehension},
 number = {1},
 pages = {100-116},
 title = {When can we say ‘if’?},
 url = {https://www.sciencedirect.com/science/article/pii/S0010027708000310},
 volume = {108},
 year = {2008}
}

@article{FALZON2006629,
 abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.},
 author = {Lucia Falzon},
 doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
 issn = {0377-2217},
 journal = {European Journal of Operational Research},
 keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
 number = {2},
 pages = {629-643},
 title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
 url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
 volume = {170},
 year = {2006}
}

@article{FALZON2006629,
 abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.},
 author = {Lucia Falzon},
 doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
 issn = {0377-2217},
 journal = {European Journal of Operational Research},
 keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
 number = {2},
 pages = {629-643},
 title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
 url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
 volume = {170},
 year = {2006}
}

@article{FALZON2006629,
 abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.},
 author = {Lucia Falzon},
 doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
 issn = {0377-2217},
 journal = {European Journal of Operational Research},
 keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
 number = {2},
 pages = {629-643},
 title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
 url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
 volume = {170},
 year = {2006}
}

@article{FALZON2006629,
 abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.},
 author = {Lucia Falzon},
 doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
 issn = {0377-2217},
 journal = {European Journal of Operational Research},
 keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
 number = {2},
 pages = {629-643},
 title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
 url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
 volume = {170},
 year = {2006}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@article{GOVIL2022103125,
 abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.},
 author = {Nikhil Govil and Ashish Sharma},
 doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
 issn = {0965-9978},
 journal = {Advances in Engineering Software},
 keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
 pages = {103125},
 title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
 url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
 volume = {168},
 year = {2022}
}

@article{JONES2013122,
 abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.},
 author = {Steven R. Jones},
 doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
 issn = {0732-3123},
 journal = {The Journal of Mathematical Behavior},
 keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
 number = {2},
 pages = {122-141},
 title = {Understanding the integral: Students’ symbolic forms},
 url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
 volume = {32},
 year = {2013}
}

@article{KORDAKI2017122,
 abstract = {This paper presents a 10-year review study that focuses on the investigation of the use of Digital Card Games (DCGs) as learning tools in education. Specific search terms keyed into 10 large scientific electronic databases identified 50 papers referring to the use of non-commercial DCGs in education during the last decade (2003–2013). The findings revealed that the DCGs reported in the reviewed papers: (a) were used for the learning of diverse subject disciplines across all educational levels and leaning towards the school curriculum, in two ways: game-construction and game-play, (b) were mainly proposed by their designers as meaningful, familiar and appealing learning contexts, in order to motivate and engage players/students and also to promote social, rich and constructivist educational experiences while at the same time integrating modern technologies and innovative gamed-based approaches, (c) were implemented using a plethora of digital tools, (d) mainly adopted social and constructivist views of learning during their design and use, although the views were explicitly reported in only a few of these, (e) were evaluated – in more than half of the studies – with positive results in terms of: student learning, attitudes towards DCGs and enrichment of social interaction and collaboration, (f) appeared to support students to acquire essential thinking skills through DCG-play. However, despite the rich DCG-game experiences reported in the reviewed papers, some essential but under-researched topics were also specified.},
 author = {Maria Kordaki and Anthi Gousiou},
 doi = {https://doi.org/10.1016/j.compedu.2017.02.011},
 issn = {0360-1315},
 journal = {Computers & Education},
 keywords = {Applications in subject areas, Interactive learning environments, Pedagogical issues, Review, Digital card games},
 pages = {122-161},
 title = {Digital card games in education: A ten year systematic review},
 url = {https://www.sciencedirect.com/science/article/pii/S036013151730043X},
 volume = {109},
 year = {2017}
}

@article{LUO2023101957,
 abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.},
 author = {Yuhan Luo and Minna Ni and Feng Zhang},
 doi = {https://doi.org/10.1016/j.aei.2023.101957},
 issn = {1474-0346},
 journal = {Advanced Engineering Informatics},
 keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
 pages = {101957},
 title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
 url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
 volume = {56},
 year = {2023}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@incollection{RENNE2022147,
 abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.},
 author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 booktitle = {Creating Resilient Transportation Systems},
 doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
 editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 isbn = {978-0-12-816820-2},
 keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
 pages = {147-192},
 publisher = {Elsevier},
 title = {Chapter 8 - Measuring and assessing resilience},
 url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
 year = {2022}
}

@incollection{RENNE2022147,
 abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.},
 author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 booktitle = {Creating Resilient Transportation Systems},
 doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
 editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 isbn = {978-0-12-816820-2},
 keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
 pages = {147-192},
 publisher = {Elsevier},
 title = {Chapter 8 - Measuring and assessing resilience},
 url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
 year = {2022}
}

@incollection{RENNE2022147,
 abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.},
 author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 booktitle = {Creating Resilient Transportation Systems},
 doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
 editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 isbn = {978-0-12-816820-2},
 keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
 pages = {147-192},
 publisher = {Elsevier},
 title = {Chapter 8 - Measuring and assessing resilience},
 url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
 year = {2022}
}

@article{RONGHUA2024e27753,
 abstract = {In order to address the autonomous underwater vehicle navigation challenge for dam inspections, with the goal of enabling safe inspections and reliable obstacle avoidance, an improved smooth Ant Colony Optimization algorithm is proposed for path planning. This improved algorithm would optimize the smoothness of the path besides the robustness, avoidance of local optima, and fast computation speed. To achieve the goal of reducing turning time and improving the directional effect of path selection, a corner-turning heuristic function is introduced. Experimental simulation results show that the improved algorithm performs best than other algorithms in terms of path smoothness and iteration stability in path planning.},
 author = {Meng Ronghua and Cheng Xinhao and Wu Zhengjia and  {Du xuan}},
 doi = {https://doi.org/10.1016/j.heliyon.2024.e27753},
 issn = {2405-8440},
 journal = {Heliyon},
 keywords = {Improved ant colony optimization, Safety factors, Dam inspections},
 number = {7},
 pages = {e27753},
 title = {Improved ant colony optimization for safe path planning of AUV},
 url = {https://www.sciencedirect.com/science/article/pii/S2405844024037848},
 volume = {10},
 year = {2024}
}

@article{SALINGER1994139,
 abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.},
 author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
 doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
 issn = {0045-7825},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 number = {1},
 pages = {139-156},
 title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
 url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
 volume = {119},
 year = {1994}
}

@incollection{SRIPRASADH202545,
 abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.},
 author = {K. Sriprasadh},
 booktitle = {Primer to Neuromorphic Computing},
 doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
 editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
 isbn = {978-0-443-21480-6},
 keywords = {Deep learning, Neural network, Machine learning, Expert systems},
 pages = {45-66},
 publisher = {Academic Press},
 title = {Chapter 3 - Review of existing neuromorphic systems},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
 year = {2025}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{SZYMANSKI2021,
 abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.},
 author = {Erika Szymanski},
 doi = {https://doi.org/10.1128/msystems.00769-21},
 issn = {2379-5077},
 journal = {mSystems},
 keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
 number = {4},
 title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
 url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
 volume = {6},
 year = {2021}
}

@article{VISWAN2023102808,
 abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.},
 author = {Nisha Ann Viswan and Upinder Singh Bhalla},
 doi = {https://doi.org/10.1016/j.conb.2023.102808},
 issn = {0959-4388},
 journal = {Current Opinion in Neurobiology},
 pages = {102808},
 title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
 url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
 volume = {83},
 year = {2023}
}

@article{WANG2025111994,
 abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.},
 author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
 doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
 issn = {0888-3270},
 journal = {Mechanical Systems and Signal Processing},
 keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
 pages = {111994},
 title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
 url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
 volume = {224},
 year = {2025}
}

@article{WANG2025111994,
 abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.},
 author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
 doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
 issn = {0888-3270},
 journal = {Mechanical Systems and Signal Processing},
 keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
 pages = {111994},
 title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
 url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
 volume = {224},
 year = {2025}
}

@article{XHAXHIU2024270,
 abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.},
 author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
 doi = {https://doi.org/10.1016/j.enss.2024.09.001},
 issn = {2772-6835},
 journal = {Energy Storage and Saving},
 keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
 number = {4},
 pages = {270-277},
 title = {Seaweed boards as value-added natural waste product for insulation and building materials},
 url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
 volume = {3},
 year = {2024}
}

@article{XHAXHIU2024270,
 abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.},
 author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
 doi = {https://doi.org/10.1016/j.enss.2024.09.001},
 issn = {2772-6835},
 journal = {Energy Storage and Saving},
 keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
 number = {4},
 pages = {270-277},
 title = {Seaweed boards as value-added natural waste product for insulation and building materials},
 url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
 volume = {3},
 year = {2024}
}

@article{XIAO1995169,
 abstract = {Three-dimensional, time-dependent features of melt flows which occur during the Czochralski growth of oxide crystals are analyzed using a theoretical bulk-flow model. The transition from a steady, axisymmetric flow to a time-dependent, three-dimensional state characterized by an annular wave structure is found to strongly affect the temperature distribution and heat transfer through the melt. The results are obtained using a novel, massively parallel implementation of the Galerkin finite element method which affords high spatial resolution of the computed flows.},
 author = {Qiang Xiao and Jeffrey J. Derby},
 doi = {https://doi.org/10.1016/0022-0248(95)00090-9},
 issn = {0022-0248},
 journal = {Journal of Crystal Growth},
 number = {3},
 pages = {169-181},
 title = {Three-dimensional melt flows in Czochralski oxide growth: high-resolution, massively parallel, finite element computations},
 url = {https://www.sciencedirect.com/science/article/pii/0022024895000909},
 volume = {152},
 year = {1995}
}

@article{ZHAO2024102465,
 abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.},
 author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
 doi = {https://doi.org/10.1016/j.inffus.2024.102465},
 issn = {1566-2535},
 journal = {Information Fusion},
 keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
 pages = {102465},
 title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
 url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
 volume = {110},
 year = {2024}
}

@article{ZHAO2024102465,
 abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.},
 author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
 doi = {https://doi.org/10.1016/j.inffus.2024.102465},
 issn = {1566-2535},
 journal = {Information Fusion},
 keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
 pages = {102465},
 title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
 url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
 volume = {110},
 year = {2024}
}
