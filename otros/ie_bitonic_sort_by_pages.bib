@comment{author = {Nicholas M. Gotts and George A.K. {van Voorn} and J. Gareth Polhill and Eline de Jong and Bruce Edmonds and Gert Jan Hofstede and Ruth Meyer},
keywords = {Socio-ecological system, Agent-based model, Complexity, Ontology},
abstract = {Socio-Ecological Systems (SESs) are the systems in which our everyday lives are embedded, so understanding them is important. The complex properties of such systems make modelling an indispensable tool for their description and analysis. Human actors play a pivotal role in SESs, but their interactions with each other and their environment are often underrepresented in SES modelling. We argue that more attention should be given to social aspects in models of SESs, but this entails additional kinds of complexity. Modelling choices need to be as transparent as possible, and to be based on analysis of the purposes and limitations of modelling. We recommend thinking in terms of modelling projects rather than single models. Such a project may involve multiple models adopting different modelling methods. We argue that agent-based models (ABMs) are an essential tool in an SES modelling project, but their expressivity, which is their major advantage, also produces problems with model transparency and validation. We propose the use of formal ontologies to make the structure and meaning of models as explicit as possible, facilitating model design, implementation, assessment, comparison and extension.}
}}

@article{1985174,
 doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
 issn = {0167-9236},
 journal = {Decision Support Systems},
 number = {2},
 pages = {174},
 title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
 url = {https://www.sciencedirect.com/science/article/pii/0167923685900715},
 volume = {1},
 year = {1985}
}

@article{ADAMO2024109162,
 abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.},
 author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
 doi = {https://doi.org/10.1016/j.compag.2024.109162},
 issn = {0168-1699},
 journal = {Computers and Electronics in Agriculture},
 keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
 pages = {109162},
 title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
 url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
 volume = {224},
 year = {2024}
}

@article{ADHIKARI20121374,
 abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.},
 author = {R.S. Adhikari and N. Aste and M. Manfren},
 doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
 issn = {1876-6102},
 journal = {Energy Procedia},
 keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
 note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
 pages = {1374-1379},
 title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
 url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
 volume = {14},
 year = {2012}
}

@article{AGNOLI2020116385,
 abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.},
 author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
 doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
 issn = {1053-8119},
 journal = {NeuroImage},
 keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
 pages = {116385},
 title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
 url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
 volume = {207},
 year = {2020}
}

@incollection{ASCHEID200711,
 abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.},
 address = {Burlington},
 author = {Gerd Ascheid and Heinrich Meyr},
 booktitle = {Customizable Embedded Processors},
 doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
 editor = {Paolo Lenne and Rainer Leupers},
 issn = {18759661},
 pages = {11-37},
 publisher = {Morgan Kaufmann},
 series = {Systems on Silicon},
 title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
 url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
 year = {2007}
}

@article{AUGIER2001307,
 abstract = {This essay contains a study of some of Herbert Simon's ideas, with particular emphasis on the role of bounded rationality in Simon's thinking and his contributions to economics and psychology. I describe Simon's visions for challenging rational choice theory, through limited rationality, and for bringing psychology into economics, putting this in perspective by describing the evolution of some of this thoughts, focusing on the continuity in his work.},
 author = {Mie Augier},
 doi = {https://doi.org/10.1016/S0167-4870(01)00036-8},
 issn = {0167-4870},
 journal = {Journal of Economic Psychology},
 keywords = {Herbert Simon, Bounded rationality, Carnegie Mellon University, Economics and psychology},
 number = {3},
 pages = {307-334},
 title = {Sublime Simon: The consistent vision of economic psychology's Nobel laureate},
 url = {https://www.sciencedirect.com/science/article/pii/S0167487001000368},
 volume = {22},
 year = {2001}
}

@article{BELLANTE2025104341,
 abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.},
 author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
 doi = {https://doi.org/10.1016/j.cose.2025.104341},
 issn = {0167-4048},
 journal = {Computers & Security},
 keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
 pages = {104341},
 title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
 url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
 year = {2025}
}

@article{BEYNON2008476,
 abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.},
 author = {Meurig Beynon and Steve Russ},
 doi = {https://doi.org/10.1016/j.jal.2008.09.008},
 issn = {1570-8683},
 journal = {Journal of Applied Logic},
 keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
 note = {The Philosophy of Computer Science},
 number = {4},
 pages = {476-489},
 title = {Experimenting with computing},
 url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
 volume = {6},
 year = {2008}
}

@incollection{BILLEN2023385,
 abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.},
 author = {Magali I. Billen},
 booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
 doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
 editor = {João C. Duarte},
 isbn = {978-0-323-85733-8},
 keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
 pages = {385-405},
 publisher = {Elsevier},
 title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
 url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
 year = {2023}
}

@incollection{CALVERT201369,
 abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.},
 author = {Jane Calvert and Emma Frow},
 booktitle = {Microbial Synthetic Biology},
 doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
 editor = {Colin Harwood and Anil Wipat},
 issn = {0580-9517},
 keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
 pages = {69-86},
 publisher = {Academic Press},
 series = {Methods in Microbiology},
 title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
 url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
 volume = {40},
 year = {2013}
}

@article{COSMIDES198951,
 abstract = {Models of the various adaptive specializations that have evolved in the human psyche could become the building blocks of a scientific theory of culture. The first step in creating such models is the derivation of a so-called “computational theory” of the adaptive problem each psychological specialization has evolved to solve. In Part II, as a case study, a sketch of a computational theory of social exchange (cooperation for mutual benefit) is developed. The dynamics of natural selection in Pleistocene ecological conditions define adaptive information processing problems that humans must be able to solve in order to participate in social exchange: individual recognition, memory for one's history of interaction, value communication, value modeling, and a shared grammar of social contracts that specifies representational structure and inferential procedures. The nature of these adaptive information processing problems places constraints on the class of cognitive programs capable of solving them; this allows one to make empirical predictions about how the cognitive processes involved in attention, communication, memory, learning, and reasoning are mobilized in situations of social exchange. Once the cognitive programs specialized for regulating social exchange are mapped, the variation and invariances in social exchange within and between cultures can be meaningfully discussed.},
 author = {Leda Cosmides and John Tooby},
 doi = {https://doi.org/10.1016/0162-3095(89)90013-7},
 issn = {0162-3095},
 journal = {Ethology and Sociobiology},
 keywords = {Reciprocal Altruism, Cooperation, Tit for tat, Cognition, Reasoning, Evolution, Learning, Culture},
 number = {1},
 pages = {51-97},
 title = {Evolutionary psychology and the generation of culture, part II: Case study: A computational theory of social exchange},
 url = {https://www.sciencedirect.com/science/article/pii/0162309589900137},
 volume = {10},
 year = {1989}
}

@article{CRANFORD2022100638,
 abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.},
 author = {Steve Cranford},
 doi = {https://doi.org/10.1016/j.patter.2022.100638},
 issn = {2666-3899},
 journal = {Patterns},
 number = {11},
 pages = {100638},
 title = {Navigating the “Kessel Run” of digital materials acceleration},
 url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
 volume = {3},
 year = {2022}
}

@article{DEVGUN2023141,
 author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
 doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
 issn = {1877-9182},
 journal = {Cardiac Electrophysiology Clinics},
 keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease},
 note = {Left Atrial Appendage Occlusion},
 number = {2},
 pages = {141-150},
 title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
 url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
 volume = {15},
 year = {2023}
}

@article{DING2025121721,
 abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.},
 author = {Haixin Ding},
 doi = {https://doi.org/10.1016/j.ins.2024.121721},
 issn = {0020-0255},
 journal = {Information Sciences},
 keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
 pages = {121721},
 title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
 url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
 volume = {694},
 year = {2025}
}

@article{EBBY200573,
 abstract = {This study examines one child's use of computational procedures over a period of 3 years in an urban elementary school where teachers were using a standards-based curriculum. From a sociocultural perspective, the use of standard algorithms to solve mathematical problems is viewed as a cultural tool that both enables and constrains particular practices. As this student appropriated and mastered procedures for addition, subtraction, multiplication and division, she could solve problems that involved fairly straightforward computations or where she could easily model the action to determine an appropriate computation. At the same time, her use of these algorithms, along with other readily available tools, such as her fingers or multiplication tables, constrained her ability to reflect on the tens-structure of the number system, an effect that had serious consequences for her overall mathematical achievement. The results of this study suggest that even when not directly introduced, algorithms have such strong currency that they can mediate more reform-oriented instruction.},
 author = {Caroline Brayer Ebby},
 doi = {https://doi.org/10.1016/j.jmathb.2004.12.002},
 issn = {0732-3123},
 journal = {The Journal of Mathematical Behavior},
 keywords = {Algorithms, Computation, Learning, Student understanding, Sociocultural perspective, Reform curriculum},
 number = {1},
 pages = {73-87},
 title = {The powers and pitfalls of algorithmic knowledge: a case study},
 url = {https://www.sciencedirect.com/science/article/pii/S0732312304000768},
 volume = {24},
 year = {2005}
}

@article{EVANS2008100,
 abstract = {In this study, we focus on the conditions which permit people to assert a conditional statement of the form ‘if p then q’ with conversational relevance. In a broadly decision-theoretic approach, also drawing on hypothetical thinking theory [Evans, J. St. B. T. (2007). Hypothetical thinking: Dual processes in reasoning and judgement. Hove, UK: Psychology Press.], we predicted that conditional tips and promises would appear more useful and persuasive and be more likely to encourage an action p when (a) the conditional link from p to q was stronger, (b) the cost of the action p was lower and (c) the benefit of the consequence q was higher. Similarly, we predicted that conditional warnings and threats would be seen as more useful and persuasive and more likely to discourage an action p when (a) the conditional link from p to q was stronger, (b) the benefit of the action p was lower and (c) the cost of the consequence q was higher. All predictions were strongly confirmed, suggesting that such conditionals may best be asserted when they are of high relevance to the goals of the listener.},
 author = {Jonathan St.B.T. Evans and Helen Neilens and Simon J. Handley and David E. Over},
 doi = {https://doi.org/10.1016/j.cognition.2008.02.001},
 issn = {0010-0277},
 journal = {Cognition},
 keywords = {Conditionals, Reasoning, Decision making, Language comprehension},
 number = {1},
 pages = {100-116},
 title = {When can we say ‘if’?},
 url = {https://www.sciencedirect.com/science/article/pii/S0010027708000310},
 volume = {108},
 year = {2008}
}

@article{EVANS2008100,
 abstract = {In this study, we focus on the conditions which permit people to assert a conditional statement of the form ‘if p then q’ with conversational relevance. In a broadly decision-theoretic approach, also drawing on hypothetical thinking theory [Evans, J. St. B. T. (2007). Hypothetical thinking: Dual processes in reasoning and judgement. Hove, UK: Psychology Press.], we predicted that conditional tips and promises would appear more useful and persuasive and be more likely to encourage an action p when (a) the conditional link from p to q was stronger, (b) the cost of the action p was lower and (c) the benefit of the consequence q was higher. Similarly, we predicted that conditional warnings and threats would be seen as more useful and persuasive and more likely to discourage an action p when (a) the conditional link from p to q was stronger, (b) the benefit of the action p was lower and (c) the cost of the consequence q was higher. All predictions were strongly confirmed, suggesting that such conditionals may best be asserted when they are of high relevance to the goals of the listener.},
 author = {Jonathan St.B.T. Evans and Helen Neilens and Simon J. Handley and David E. Over},
 doi = {https://doi.org/10.1016/j.cognition.2008.02.001},
 issn = {0010-0277},
 journal = {Cognition},
 keywords = {Conditionals, Reasoning, Decision making, Language comprehension},
 number = {1},
 pages = {100-116},
 title = {When can we say ‘if’?},
 url = {https://www.sciencedirect.com/science/article/pii/S0010027708000310},
 volume = {108},
 year = {2008}
}

@article{FALZON2006629,
 abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.},
 author = {Lucia Falzon},
 doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
 issn = {0377-2217},
 journal = {European Journal of Operational Research},
 keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
 number = {2},
 pages = {629-643},
 title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
 url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
 volume = {170},
 year = {2006}
}

@article{FOSGERAU2021109911,
 abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.},
 author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
 doi = {https://doi.org/10.1016/j.econlet.2021.109911},
 issn = {0165-1765},
 journal = {Economics Letters},
 keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
 pages = {109911},
 title = {Some remarks on CCP-based estimators of dynamic models},
 url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
 volume = {204},
 year = {2021}
}

@article{GARDECKI2018138,
 abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.},
 author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
 doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
 issn = {2405-8963},
 journal = {IFAC-PapersOnLine},
 keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
 note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
 number = {6},
 pages = {138-143},
 title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
 url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
 volume = {51},
 year = {2018}
}

@incollection{GARDNER2024103,
 abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.},
 author = {Nicole Gardner},
 booktitle = {Scaling the Smart City},
 doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
 editor = {Nicole Gardner},
 isbn = {978-0-443-18452-9},
 keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
 pages = {103-128},
 publisher = {Elsevier},
 series = {Smart Cities},
 title = {Chapter 5 - Smart design for socially engaging environments},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
 year = {2024}
}

@article{GOVIL2022103125,
 abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.},
 author = {Nikhil Govil and Ashish Sharma},
 doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
 issn = {0965-9978},
 journal = {Advances in Engineering Software},
 keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
 pages = {103125},
 title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
 url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
 volume = {168},
 year = {2022}
}

@article{GRANJO202021,
 abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.},
 author = {José F.O. Granjo and Maria G. Rasteiro},
 doi = {https://doi.org/10.1016/j.ece.2020.03.002},
 issn = {1749-7728},
 journal = {Education for Chemical Engineers},
 keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
 pages = {21-28},
 title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
 url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
 volume = {31},
 year = {2020}
}

@article{HASSABIS2007299,
 abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.},
 author = {Demis Hassabis and Eleanor A. Maguire},
 doi = {https://doi.org/10.1016/j.tics.2007.05.001},
 issn = {1364-6613},
 journal = {Trends in Cognitive Sciences},
 number = {7},
 pages = {299-306},
 title = {Deconstructing episodic memory with construction},
 url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
 volume = {11},
 year = {2007}
}

@article{JONES2013122,
 abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.},
 author = {Steven R. Jones},
 doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
 issn = {0732-3123},
 journal = {The Journal of Mathematical Behavior},
 keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
 number = {2},
 pages = {122-141},
 title = {Understanding the integral: Students’ symbolic forms},
 url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
 volume = {32},
 year = {2013}
}

@article{KONDINSKI20241071,
 abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.},
 author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
 doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
 issn = {2451-9294},
 journal = {Chem},
 keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
 number = {4},
 pages = {1071-1083},
 title = {Hacking decarbonization with a community-operated CreatorSpace},
 url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
 volume = {10},
 year = {2024}
}

@article{KRUSKOPF2024104574,
 abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.},
 author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
 doi = {https://doi.org/10.1016/j.tate.2024.104574},
 issn = {0742-051X},
 journal = {Teaching and Teacher Education},
 keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
 pages = {104574},
 title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
 url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
 volume = {144},
 year = {2024}
}

@incollection{KURGANSKAYA2024760,
 abstract = {We describe theoretical and conceptual approaches to treat crystal-fluid interactions across the scales in the communities studying mineral-fluid interactions for a variety of purposes, from understanding fundamental principles to geological reservoir characterization and environmental mitigation. We delineate basics of theory, recent breakthroughs, and challenges in modeling approaches from the atomistic scale to the mesoscale. Quantum Mechanics, Molecular Dynamics, Kinetic Monte Carlo and Voronoi computational geometry are covered. We discuss possible theoretical and conceptual developments to overcome those challenges toward more reliable predictive models. A special attention is given to the development of interfaces between the techniques addressing different scales.},
 address = {Oxford},
 author = {I. Kurganskaya and R.D. Rohlfs and A. Luttge},
 booktitle = {Encyclopedia of Solid-Liquid Interfaces (First Edition)},
 doi = {https://doi.org/10.1016/B978-0-323-85669-0.00034-9},
 edition = {First Edition},
 editor = {Klaus Wandelt and Gianlorenzo Bussetti},
 isbn = {978-0-323-85670-6},
 keywords = {Crystal-water interface, Electric double layer, Grand canonical Monte Carlo, Kinetic Monte Carlo, Kinetics, Mineral–water interface, Parameterization, Reaction pathways, Reaction probability, Reaction rates, Statistical mechanics of interfaces, Stepwave, Stochastic model, Upscaling, Voronoi},
 pages = {760-792},
 publisher = {Elsevier},
 title = {Multi-scale modeling of crystal-fluid interactions: State-of-the-art, challenges and prospects},
 url = {https://www.sciencedirect.com/science/article/pii/B9780323856690000349},
 year = {2024}
}

@article{LUCKRING2024100998,
 abstract = {Concentrated vortex flows contribute to the aerodynamic performance of aircraft at elevated load conditions. For military interests, the vortex flows are exploited at maneuver conditions of combat aircraft and missiles. For transport interests, the vortex flows are exploited at takeoff and landing conditions as well as at select transonic conditions. Aircraft applications of these vortex flows are reviewed with a historical perspective followed by a discussion of the underlying physics of a concentrated vortex flow. A hierarchy of computational fluid dynamics simulation technology is then presented followed by findings from a capability survey for predicting concentrated vortex flows with computational fluid dynamics. Results are focused on military and civil fixed-wing aircraft; only limited results are included for missiles, and rotary-wing applications are not assessed. Opportunities for predictive capability advancement are then reported with comments related to digital transformation interests. A hierarchical approach that merges a physics-based perspective of the concentrated vortex flows with a systems engineering viewpoint of the air vehicle is also used to frame much of the discussion.},
 author = {James M. Luckring and Arthur Rizzi},
 doi = {https://doi.org/10.1016/j.paerosci.2024.100998},
 issn = {0376-0421},
 journal = {Progress in Aerospace Sciences},
 pages = {100998},
 title = {Prediction of concentrated vortex aerodynamics: Current CFD capability survey},
 url = {https://www.sciencedirect.com/science/article/pii/S0376042124000241},
 volume = {147},
 year = {2024}
}

@article{LUO2023101957,
 abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.},
 author = {Yuhan Luo and Minna Ni and Feng Zhang},
 doi = {https://doi.org/10.1016/j.aei.2023.101957},
 issn = {1474-0346},
 journal = {Advanced Engineering Informatics},
 keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
 pages = {101957},
 title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
 url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
 volume = {56},
 year = {2023}
}

@article{MONNAHAN2024107024,
 abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.},
 author = {Cole C. Monnahan},
 doi = {https://doi.org/10.1016/j.fishres.2024.107024},
 issn = {0165-7836},
 journal = {Fisheries Research},
 keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
 pages = {107024},
 title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
 url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
 volume = {275},
 year = {2024}
}

@article{NAZI2025100124,
 abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.},
 author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
 doi = {https://doi.org/10.1016/j.nlp.2024.100124},
 issn = {2949-7191},
 journal = {Natural Language Processing Journal},
 keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
 pages = {100124},
 title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
 url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
 volume = {10},
 year = {2025}
}

@article{NORGAARD2023105308,
 abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.},
 author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
 doi = {https://doi.org/10.1016/j.cognition.2022.105308},
 issn = {0010-0277},
 journal = {Cognition},
 keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
 pages = {105308},
 title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
 url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
 volume = {230},
 year = {2023}
}

@article{OLADEJO2024111880,
 abstract = {In this paper, a novel metaheuristic called ‘The Hiking Optimization Algorithm’ (HOA) is proposed. HOA is inspired by hiking, a popular recreational activity, in recognition of the similarity between the search landscapes of optimization problems and the mountainous terrains traversed by hikers. HOA’s mathematical model is premised on Tobler’s Hiking Function (THF), which determines the walking velocity of hikers (i.e. agents) by considering the elevation of the terrain and the distance covered. THF is employed in determining hikers’ positions in the course of solving an optimization problem. HOA’s performance is demonstrated by benchmarking with 29 well-known test functions (including unimodal, multimodal, fixed-dimension multimodal, and composite functions), three engineering design problems (EDPs), (including I-beam, tension/compression spring, and gear train problems) and two N-P Hard problems (i.e. Traveling Salesman’s and Knapsack Problems). Moreover, HOA’s results are verified by comparison to 14 other metaheuristics, including Teaching Learning Based Optimization (TLBO), Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization, Grey Wolf Optimizer (GWO) as well as newly introduced algorithms such as Komodo Mlipir Algorithm (KMA), Quadratic Interpolation Optimization (QIO), and Coronavirus Optimization Algorithm (COVIDOA). In this study, we employ statistical tests such as the Wilcoxon rank sum, Friedman test, and Dunn’s post hoc test for the performance evaluation. HOA’s results are competitive and, in many instances, outperform the aforementioned well-known metaheuristics. The source codes of HOA and related metaheuristics can be accessed publicly via this link: https://github.com/DayoSun/The-Hiking-Optimization-Algorithm.},
 author = {Sunday O. Oladejo and Stephen O. Ekwe and Seyedali Mirjalili},
 doi = {https://doi.org/10.1016/j.knosys.2024.111880},
 issn = {0950-7051},
 journal = {Knowledge-Based Systems},
 keywords = {Optimization, Metaheuristics, Hiking, Tobler’s Hiking function, Algorithm, Benchmark, Problem solving},
 pages = {111880},
 title = {The Hiking Optimization Algorithm: A novel human-based metaheuristic approach},
 url = {https://www.sciencedirect.com/science/article/pii/S0950705124005148},
 volume = {296},
 year = {2024}
}

@article{PANULAONTTO2019292,
 abstract = {Expert informants can be used as the principal information source in the modeling of socio-techno-economic systems or problems to support planning, foresight and decision-making. Such modeling is theory-driven, grounded in expert judgment and understanding, and can be contrasted with data-driven modeling approaches. Several families of approaches exist to enable expert elicited systems modeling with varying input information requirements and analytical ambitions. This paper proposes a novel modeling language and computational process, which combines aspects from various other approaches in an attempt to create a flexible and practical systems modeling approach based on expert elicitation. It is intended to have high fitness in modeling of systems that lack statistical data and exhibit low quantifiability of important system characteristics. AXIOM is positioned against Bayesian networks, cross-impact analysis, structural analysis, and morphological analysis. The modeling language and computational process are illustrated with a small example model. A software implementation is also presented.},
 author = {Juha Panula-Ontto},
 doi = {https://doi.org/10.1016/j.techfore.2018.10.006},
 issn = {0040-1625},
 journal = {Technological Forecasting and Social Change},
 keywords = {Systems modeling, Modeling techniques, Decision support, Cross-impact analysis, Belief networks, Expert elicitation},
 pages = {292-308},
 title = {The AXIOM approach for probabilistic and causal modeling with expert elicited inputs},
 url = {https://www.sciencedirect.com/science/article/pii/S0040162518305870},
 volume = {138},
 year = {2019}
}

@article{RANDALL1991219,
 author = {John H. Randall},
 doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
 issn = {0024-3795},
 journal = {Linear Algebra and its Applications},
 pages = {219-223},
 title = {Review of linear least squares computations: by R.W. Farebrother},
 url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
 volume = {153},
 year = {1991}
}

@incollection{RENNE2022147,
 abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.},
 author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 booktitle = {Creating Resilient Transportation Systems},
 doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
 editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 isbn = {978-0-12-816820-2},
 keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
 pages = {147-192},
 publisher = {Elsevier},
 title = {Chapter 8 - Measuring and assessing resilience},
 url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
 year = {2022}
}

@incollection{RENNE2022147,
 abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.},
 author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 booktitle = {Creating Resilient Transportation Systems},
 doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
 editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
 isbn = {978-0-12-816820-2},
 keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
 pages = {147-192},
 publisher = {Elsevier},
 title = {Chapter 8 - Measuring and assessing resilience},
 url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
 year = {2022}
}

@article{SALINGER1994139,
 abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.},
 author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
 doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
 issn = {0045-7825},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 number = {1},
 pages = {139-156},
 title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
 url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
 volume = {119},
 year = {1994}
}

@incollection{SRIPRASADH202545,
 abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.},
 author = {K. Sriprasadh},
 booktitle = {Primer to Neuromorphic Computing},
 doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
 editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
 isbn = {978-0-443-21480-6},
 keywords = {Deep learning, Neural network, Machine learning, Expert systems},
 pages = {45-66},
 publisher = {Academic Press},
 title = {Chapter 3 - Review of existing neuromorphic systems},
 url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
 year = {2025}
}

@article{UDDIN2021106,
 abstract = {Introduction
Chronic pain has multiple aetiological factors and complexity. Pain theory helps us to guide and organize our thinking to deal with this complexity. The objective of this paper is to critically review the most influential theory in pain science history (the gate control theory of pain) and focus on its implications in chronic pain rehabilitation to minimize disability.
Methods
In this narrative review, all the published studies that focused upon pain theory were retrieved from Ovoid Medline (from 1946 till present), EMBAS, AMED and PsycINFO data bases.
Results
Chronic pain is considered a disease or dysfunction of the nervous system. In chronic pain conditions, hypersensitivity is thought to develop from changes to the physiological top-down control (inhibitory) mechanism of pain modulation according to the pain theory. Pain hypersensitivity manifestation is considered as abnormal central inhibitory control at the gate controlling mechanism. On the other hand, pain hypersensitivity is a prognostic factor in pain rehabilitation. It is clinically important to detect and manage hypersensitivity responses and their mechanisms.
Conclusion
Since somatosensory perception and integration are recognized as a contributor to the pain perception under the theory, then we can use the model to direct interventions aimed at pain relief. The pain theory should be leveraged to develop and refine measurement tools with clinical utility for detecting and monitoring hypersensitivity linked to chronic pain mechanisms.},
 author = {Zakir Uddin and Joy C. MacDermid and Fatma A. Hegazy and Tara L. Packham},
 doi = {https://doi.org/10.2174/1875399X02114010106},
 issn = {1875-399X},
 journal = {The Open Sports Sciences Journal},
 keywords = {Chronic pain , Hypersensitivity , Theory , Rehabilitation , Disability , T-cell},
 pages = {106-113},
 title = {Application of Theory in Chronic Pain Rehabilitation Research and Clinical Practice},
 url = {https://www.sciencedirect.com/science/article/pii/S1875399X2100013X},
 volume = {14},
 year = {2021}
}

@article{VASILE201177,
 abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.},
 author = {Cristian Vasile},
 doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
 issn = {1877-0428},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {multiple intelligence, entry points, personality, interests},
 note = {Teachers for the Knowledge Society},
 pages = {77-81},
 title = {Entry points, interests and attitudes. An integrative approach of learning},
 url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
 volume = {11},
 year = {2011}
}

@article{VISWAN2023102808,
 abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.},
 author = {Nisha Ann Viswan and Upinder Singh Bhalla},
 doi = {https://doi.org/10.1016/j.conb.2023.102808},
 issn = {0959-4388},
 journal = {Current Opinion in Neurobiology},
 pages = {102808},
 title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
 url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
 volume = {83},
 year = {2023}
}

@article{WANG2025111994,
 abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.},
 author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
 doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
 issn = {0888-3270},
 journal = {Mechanical Systems and Signal Processing},
 keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
 pages = {111994},
 title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
 url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
 volume = {224},
 year = {2025}
}

@article{XHAXHIU2024270,
 abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.},
 author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
 doi = {https://doi.org/10.1016/j.enss.2024.09.001},
 issn = {2772-6835},
 journal = {Energy Storage and Saving},
 keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
 number = {4},
 pages = {270-277},
 title = {Seaweed boards as value-added natural waste product for insulation and building materials},
 url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
 volume = {3},
 year = {2024}
}

@article{XIAO1995169,
 abstract = {Three-dimensional, time-dependent features of melt flows which occur during the Czochralski growth of oxide crystals are analyzed using a theoretical bulk-flow model. The transition from a steady, axisymmetric flow to a time-dependent, three-dimensional state characterized by an annular wave structure is found to strongly affect the temperature distribution and heat transfer through the melt. The results are obtained using a novel, massively parallel implementation of the Galerkin finite element method which affords high spatial resolution of the computed flows.},
 author = {Qiang Xiao and Jeffrey J. Derby},
 doi = {https://doi.org/10.1016/0022-0248(95)00090-9},
 issn = {0022-0248},
 journal = {Journal of Crystal Growth},
 number = {3},
 pages = {169-181},
 title = {Three-dimensional melt flows in Czochralski oxide growth: high-resolution, massively parallel, finite element computations},
 url = {https://www.sciencedirect.com/science/article/pii/0022024895000909},
 volume = {152},
 year = {1995}
}

@article{YANG2022101239,
 abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.},
 author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
 doi = {https://doi.org/10.1016/j.joi.2021.101239},
 issn = {1751-1577},
 journal = {Journal of Informetrics},
 keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
 number = {1},
 pages = {101239},
 title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
 url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
 volume = {16},
 year = {2022}
}

@article{YONG2023e13529,
 abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.},
 author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
 doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
 issn = {2405-8440},
 journal = {Heliyon},
 keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
 number = {2},
 pages = {e13529},
 title = {Structure bionic topology design method based on biological unit cell},
 url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
 volume = {9},
 year = {2023}
}

@article{YU2023114721,
 abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.},
 author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
 doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
 issn = {0029-8018},
 journal = {Ocean Engineering},
 keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
 pages = {114721},
 title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
 url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
 volume = {281},
 year = {2023}
}

@article{ZHAO2024102465,
 abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.},
 author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
 doi = {https://doi.org/10.1016/j.inffus.2024.102465},
 issn = {1566-2535},
 journal = {Information Fusion},
 keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
 pages = {102465},
 title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
 url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
 volume = {110},
 year = {2024}
}
