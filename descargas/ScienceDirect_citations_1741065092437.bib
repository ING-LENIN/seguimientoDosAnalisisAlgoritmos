@article{FEDORENKO2014120,
title = {Reworking the language network},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {3},
pages = {120-126},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S136466131300288X},
author = {Evelina Fedorenko and Sharon L. Thompson-Schill},
keywords = {domain specificity, domain generality, language network, cognitive control, fMRI},
abstract = {Prior investigations of functional specialization have focused on the response profiles of particular brain regions. Given the growing emphasis on regional covariation, we propose to reframe these questions in terms of brain ‘networks’ (collections of regions jointly engaged by some mental process). Despite the challenges that investigations of the language network face, a network approach may prove useful in understanding the cognitive architecture of language. We propose that a language network plausibly includes a functionally specialized ‘core’ (brain regions that coactivate with each other during language processing) and a domain-general ‘periphery’ (a set of brain regions that may coactivate with the language core regions at some times but with other specialized systems at other times, depending on task demands). Framing the debate around network properties such as this may prove to be a more fruitful way to advance our understanding of the neurobiology of language.}
}
@article{RIVAS20011369,
title = {Computational identification of noncoding RNAs in E. coli by comparative genomics},
journal = {Current Biology},
volume = {11},
number = {17},
pages = {1369-1373},
year = {2001},
issn = {0960-9822},
doi = {https://doi.org/10.1016/S0960-9822(01)00401-8},
url = {https://www.sciencedirect.com/science/article/pii/S0960982201004018},
author = {Elena Rivas and Robert J. Klein and Thomas A. Jones and Sean R. Eddy},
abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs 1, 2. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{ZAHRAH2024100481,
title = {Unmasking hate in the pandemic: A cross-platform study of the COVID-19 infodemic},
journal = {Big Data Research},
volume = {37},
pages = {100481},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2214579624000558},
author = {Fatima Zahrah and Jason R.C. Nurse and Michael Goldsmith},
keywords = {Social media analysis, Cross-platform analysis, Online hate, COVID-19},
abstract = {The past few decades have established how digital technologies and platforms have provided an effective medium for spreading hateful content, which has been linked to several catastrophic consequences. Recent academic studies have also highlighted how online hate is a phenomenon that strategically makes use of multiple online platforms. In this article, we seek to advance the current research landscape by harnessing a cross-platform approach to computationally analyse content relating to the 2020 COVID-19 pandemic. More specifically, we analyse content on hate-specific environments from Twitter, Reddit, 4chan and Stormfront. Our findings show how content and posting activity can change across platforms, and how the psychological components of online content can differ depending on the platform being used. Through this, we provide unique insight into the cross-platform behaviours of online hate. We further define several avenues for future research within this field so as to gain a more comprehensive understanding of the global hate ecosystem.}
}
@article{ROHLFS2025128701,
title = {Generalization in neural networks: A broad survey},
journal = {Neurocomputing},
volume = {611},
pages = {128701},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128701},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014723},
author = {Chris Rohlfs},
keywords = {Literature review, Deep learning, Overfitting, Causality, Domain generalization, Transfer learning, Foundation models, Multimodal, Semantic knowledge, Abstraction, Biologically-inspired},
abstract = {This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from training to test data are discussed, with suggestive evidence presented that, at least for the ImageNet dataset, popular classification models show substantial overfitting. An empirical example and perspectives from statistics highlight how models’ (2) distribution generalization can benefit from consideration of causal relationships and counterfactual scenarios. Transfer learning approaches and results for (3) domain generalization are summarized, as is the wealth of domain generalization benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the emergence of transformer-based foundation models such as those used for language processing. Studies performing (5) modality generalization are reviewed, including those that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Higher-level (6) scope generalization results are surveyed, including graph-based approaches to represent symbolic knowledge in networks and attribution strategies for improving networks’ explainability. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.}
}
@article{NIKZAINAL2024101739,
title = {Prof. Serena Nik-Zainal},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101739},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004695},
author = {Serena Nik-Zainal},
abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.}
}
@incollection{OBRIEN2014141,
title = {7 - Reasoning with graphs},
editor = {Jamie O’Brien},
booktitle = {Shaping Knowledge},
publisher = {Chandos Publishing},
pages = {141-174},
year = {2014},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-751-4},
doi = {https://doi.org/10.1533/9781780634326.141},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347514500078},
author = {Jamie O’Brien},
keywords = {graph databases, logic and computing, reasoning, spatial data structures, visualization},
abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.}
}
@incollection{WARE2021425,
title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {425-456},
year = {2021},
series = {Interactive Technologies},
isbn = {978-0-12-812875-6},
doi = {https://doi.org/10.1016/B978-0-12-812875-6.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128756000128},
author = {Colin Ware},
keywords = {Interactive visualization design, Visual thinking, Visual thinking design patterns, Visual working memory, Visualization design, Visualization types},
abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.}
}
@article{CARBONARO20101098,
title = {Computer-game construction: A gender-neutral attractor to Computing Science},
journal = {Computers & Education},
volume = {55},
number = {3},
pages = {1098-1111},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510001399},
author = {Mike Carbonaro and Duane Szafron and Maria Cutumisu and Jonathan Schaeffer},
keywords = {Computing Science, Females in Science, Computer game construction},
abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.}
}
@article{NEWMAN20031668,
title = {Frontal and parietal participation in problem solving in the Tower of London: fMRI and computational modeling of planning and high-level perception},
journal = {Neuropsychologia},
volume = {41},
number = {12},
pages = {1668-1682},
year = {2003},
issn = {0028-3932},
doi = {https://doi.org/10.1016/S0028-3932(03)00091-5},
url = {https://www.sciencedirect.com/science/article/pii/S0028393203000915},
author = {Sharlene D Newman and Patricia A Carpenter and Sashank Varma and Marcel Adam Just},
keywords = {Planning, fMRI, Spatial working memory, Problem solving, Tower of London, Computational modeling, 4CAPS},
abstract = {This study triangulates executive planning and visuo-spatial reasoning in the context of the Tower of London (TOL) task by using a variety of methodological approaches. These approaches include functional magnetic resonance imaging (fMRI), functional connectivity analysis, individual difference analysis, and computational modeling. A graded fMRI paradigm compared the brain activation during the solution of problems with varying path lengths: easy (1 and 2 moves), moderate (3 and 4 moves) and difficult (5 and 6 moves). There were three central findings regarding the prefrontal cortex: (1) while both the left and right prefrontal cortices were equally involved during the solution of moderate and difficult problems, the activation on the right was differentially attenuated during the solution of the easy problems; (2) the activation observed in the right prefrontal cortex was highly correlated with individual differences in working memory (measured independently by the reading span task); and (3) different patterns of functional connectivity were observed in the left and right prefrontal cortices. Results obtained from the superior parietal region also revealed left/right differences; only the left superior parietal region revealed an effect of difficulty. These fMRI results converged upon two hypotheses: (1) the right prefrontal area may be more involved in the generation of a plan, whereas the left prefrontal area may be more involved in plan execution; and (2) the right superior parietal region is more involved in attention processes while the left homologue is more of a visuo-spatial workspace. A 4CAPS computational model of the cognitive processes and brain activation in the TOL task integrated these hypothesized mechanisms, and provided a reasonably good fit to the observed behavioral and brain activation data. The multiple research approaches presented here converge on a deepening understanding of the combination of perceptual and conceptual processes in this type of visual problem solving.}
}
@incollection{MAYER2010273,
title = {Problem Solving and Reasoning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {273-278},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00487-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004875},
author = {R.E. Mayer},
keywords = {Convergent thinking, Creativity, Deductive reasoning, Directed thinking, Divergent thinking, Einstellung, Everyday thinking, Expert problem solving, Functional fixedness, Ill-defined problem, Inductive reasoning, Insight, Means-ends analysis, Nonroutine problem, Problem solving, Problem space, Productive thinking, Reasoning, Reproductive thinking, Routine problem, Thinking, Transfer, Well-defined problem},
abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.}
}
@article{WEYDMANN2025111173,
title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {136},
pages = {111173},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624002410},
author = {Gibson Weydmann and Igor Palmieri and Reinaldo A.G. Simões and Samara Buchmann and Eduardo Schmidt and Paulina Alves and Lisiane Bizarro},
keywords = {Overweight, Reinforcement Learning, Working Memory, Computational Modelling},
abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.}
}
@article{ROSS2021100069,
title = {Kinenoetic analysis: Unveiling the material traces of insight},
journal = {Methods in Psychology},
volume = {5},
pages = {100069},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000266},
author = {Wendy Ross and Frédéric Vallée-Tourangeau},
keywords = {Insight, Case study, Observation},
abstract = {Research on insight problem solving sets itself a challenging goal: How to explain the origin of a new idea. It compounds the difficulty of this challenge by traditionally seeking to explain the phenomenon in strictly mental terms. Rather, we suggest that thoughts and actions are bound to objects, inviting a granular description of the world within which thinking proceeds. As the reasoner transforms the world, the physical traces of these changes can be mapped in space and time. Not only can the reasoner see these changes, and act upon them, the researcher can develop new inscription devices that captures the trajectory of the creative arc along spatial and temporal coordinates. Kinenoetic is a term we employ to capture the idea that knowledge comes from the movement of objects and that this knowledge is both at the level of the problem-solver and at the level of the researcher. This form of knowledge can only be constructed in problem solving environments where reasoners can manipulate physical elements. A kinenoetic analysis tracks and maps the changes to the object-qua-models of proto solutions, and in the process unveils the physical genesis of new ideas and creativity. Our aim here is to lay out a method for using the objects commonly employed in interactive problem-solving research, tracing the process of thought to elucidate underlying cognitive mechanisms. Thus, the focus turns from the effects of objects on thoughts, to tracing object-thought mutualities as they are enacted and made visible.}
}
@article{KITTAS2010401,
title = {Evolution of the rate of biological aging using a phenotype based computational model},
journal = {Journal of Theoretical Biology},
volume = {266},
number = {3},
pages = {401-407},
year = {2010},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2010.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022519310003619},
author = {Aristotelis Kittas},
keywords = {Evolution, Aging, Computer simulations, Age-structured populations, Modelling},
abstract = {In this work I introduce a simple model to study how natural selection acts upon aging, which focuses on the viability of each individual. It is able to reproduce the Gompertz law of mortality and can make predictions about the relation between the level of mutation rates (beneficial/deleterious/neutral), age at reproductive maturity and the degree of biological aging. With no mutations, a population with low age at reproductive maturity R stabilizes at higher density values, while with mutations it reaches its maximum density, because even for large pre-reproductive periods each individual evolves to survive to maturity. Species with very short pre-reproductive periods can only tolerate a small number of detrimental mutations. The probabilities of detrimental (Pd) or beneficial (Pb) mutations are demonstrated to greatly affect the process. High absolute values produce peaks in the viability of the population over time. Mutations combined with low selection pressure move the system towards weaker phenotypes. For low values in the ratio Pd/Pb, the speed at which aging occurs is almost independent of R, while higher values favor significantly species with high R. The value of R is critical to whether the population survives or dies out. The aging rate is controlled by Pd and Pb and the amount of the viability of each individual is modified, with neutral mutations allowing the system more “room” to evolve. The process of aging in this simple model is revealed to be fairly complex, yielding a rich variety of results.}
}
@article{ROSSITER20083713,
title = {Compromises between feasibility and performance within linear MPC},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3713-3718},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00627},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639526X},
author = {J.A. Rossiter and Yihang Ding},
keywords = {Constraints, Feasibility, Performance, Computational Efficiency, Contours},
abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.}
}
@article{BELVEDERE201218,
title = {A computational index derived from whole-genome copy number analysis is a novel tool for prognosis in early stage lung squamous cell carcinoma},
journal = {Genomics},
volume = {99},
number = {1},
pages = {18-24},
year = {2012},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888754311002424},
author = {Ornella Belvedere and Stefano Berri and Rebecca Chalkley and Caroline Conway and Fabio Barbone and Federica Pisa and Kenneth MacLennan and Catherine Daly and Melissa Alsop and Joanne Morgan and Jessica Menis and Peter Tcherveniakov and Kostas Papagiannopoulos and Pamela Rabbitts and Henry M. Wood},
keywords = {Lung cancer, Copy number, Survival, Next-generation sequencing},
abstract = {Squamous cell carcinoma of the lung is remarkable for the extent to which the same chromosomal abnormalities are detected in individual tumours. We have used next generation sequencing at low coverage to produce high resolution copy number karyograms of a series of 89 non-small cell lung tumours specifically of the squamous cell subtype. Because this methodology is able to create karyograms from formalin-fixed paraffin-embedded material, we were able to use archival stored samples for which survival data were available and correlate frequently occurring copy number changes with disease outcome. No single region of genomic change showed significant correlation with survival. However, adopting a whole-genome approach, we devised an algorithm that relates to total genomic damage, specifically the relative ratios of copy number states across the genome. This algorithm generated a novel index, which is an independent prognostic indicator in early stage squamous cell carcinoma of the lung.}
}
@article{YANG202075,
title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
journal = {Information Sciences},
volume = {541},
pages = {75-97},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306009},
author = {Xin Yang and Yingying Zhang and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way granular computing, Sequential three-way decision, Local neighborhood, Temporal-spatial, Multi-granularity},
abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.}
}
@article{ARCHAMBAULT2024102865,
title = {Ethical dimensions of algorithmic literacy for college students: Case studies and cross-disciplinary connections},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {3},
pages = {102865},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102865},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000260},
author = {Susan Gardner Archambault and Shalini Ramachandran and Elisa Acosta and Sheree Fu},
keywords = {Algorithmic literacy, Information literacy, Algorithmic bias, AI ethics, Algorithmic fairness, Computer science education},
abstract = {This article addresses three key questions related to the ethical facets of algorithmic literacy. First, it synthesizes existing literature to identify six core ethical components, including bias, privacy, transparency, accountability, accuracy, and non-maleficence. Second, a crosswalk maps the intersections of these principles across the Association of College and Research Libraries' Framework for Information Literacy for Higher Education and the Association of Computing Machinery's Code of Ethics and Professional Conduct and Joint Statement on Principles for Responsible Algorithmic Systems. This analysis reveals significant overlap on issues like unfairness and transparency, helping prioritize topics for instruction. Finally, case studies showcase pedagogical strategies for teaching ethical considerations, informed by the crosswalk. Workshops for diverse undergraduates and computer science students employed reallife instances of algorithmic bias to prompt reflection on unintended harm, contestability, and responsible development. Pre-post surveys indicated expanded critical perspectives after the interventions. By systematically examining shared values and testing instructional approaches, this study provides practical tools to shape ethical thinking on algorithms. It also demonstrates promising practices for responsibly advancing algorithmic literacy across disciplines. Ultimately, fostering interdisciplinary awareness and multipronged educational initiatives can empower students to question algorithmic authority and biases.}
}
@article{RONAYNE2021318,
title = {Evaluating the sunk cost effect},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {318-327},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001293},
author = {David Ronayne and Daniel Sgroi and Anthony Tuckwell},
keywords = {Sunk cost effect, Sunk cost fallacy, Endowment effect, Cognitive ability, Psychological scales, Scale validation},
abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.}
}
@article{OMHOLT2003107,
title = {Eberhard O. Voit, Computational Analysis of Biochemical Systems. A Practical Guide for Biochemists and Molecular Biologists, Cambridge University Press, 2000, 531 pages (ISBN 0-521-78579-0; paperback)},
journal = {Mathematical Biosciences},
volume = {181},
number = {1},
pages = {107-109},
year = {2003},
issn = {0025-5564},
doi = {https://doi.org/10.1016/S0025-5564(02)00153-0},
url = {https://www.sciencedirect.com/science/article/pii/S0025556402001530},
author = {Stig W Omholt}
}
@article{MUSSO2015267,
title = {A single dual-stream framework for syntactic computations in music and language},
journal = {NeuroImage},
volume = {117},
pages = {267-283},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915004000},
author = {Mariacristina Musso and Cornelius Weiller and Andreas Horn and Volkmer Glauche and Roza Umarova and Jürgen Hennig and Albrecht Schneider and Michel Rijntjes},
abstract = {This study is the first to compare in the same subjects the specific spatial distribution and the functional and anatomical connectivity of the neuronal resources that activate and integrate syntactic representations during music and language processing. Combining functional magnetic resonance imaging with functional connectivity and diffusion tensor imaging-based probabilistic tractography, we examined the brain network involved in the recognition and integration of words and chords that were not hierarchically related to the preceding syntax; that is, those deviating from the universal principles of grammar and tonal relatedness. This kind of syntactic processing in both domains was found to rely on a shared network in the left hemisphere centered on the inferior part of the inferior frontal gyrus (IFG), including pars opercularis and pars triangularis, and on dorsal and ventral long association tracts connecting this brain area with temporo-parietal regions. Language processing utilized some adjacent left hemispheric IFG and middle temporal regions more than music processing, and music processing also involved right hemisphere regions not activated in language processing. Our data indicate that a dual-stream system with dorsal and ventral long association tracts centered on a functionally and structurally highly differentiated left IFG is pivotal for domain–general syntactic competence over a broad range of elements including words and chords.}
}
@article{LOWE20219898316,
title = {In-Depth Computational Analysis of Natural and Artificial Carbon Fixation Pathways},
journal = {BioDesign Research},
volume = {2021},
pages = {9898316},
year = {2021},
issn = {2693-1257},
doi = {https://doi.org/10.34133/2021/9898316},
url = {https://www.sciencedirect.com/science/article/pii/S2693125724000566},
author = {Hannes Löwe and Andreas Kremling},
abstract = {In the recent years, engineering new-to-nature CO2- and C1-fixing metabolic pathways made a leap forward. New, artificial pathways promise higher yields and activity than natural ones like the Calvin-Benson-Bassham (CBB) cycle. The question remains how to best predict their in vivo performance and what actually makes one pathway “better” than another. In this context, we explore aerobic carbon fixation pathways by a computational approach and compare them based on their specific activity and yield on methanol, formate, and CO2/H2 considering the kinetics and thermodynamics of the reactions. Besides pathways found in nature or implemented in the laboratory, this included two completely new cycles with favorable features: the reductive citramalyl-CoA cycle and the 2-hydroxyglutarate-reverse tricarboxylic acid cycle. A comprehensive kinetic data set was collected for all enzymes of all pathways, and missing kinetic data were sampled with the Parameter Balancing algorithm. Kinetic and thermodynamic data were fed to the Enzyme Cost Minimization algorithm to check for respective inconsistencies and calculate pathway-specific activities. The specific activities of the reductive glycine pathway, the CETCH cycle, and the new reductive citramalyl-CoA cycle were predicted to match the best natural cycles with superior product-substrate yield. However, the CBB cycle performed better in terms of activity compared to the alternative pathways than previously thought. We make an argument that stoichiometric yield is likely not the most important design criterion of the CBB cycle. Still, alternative carbon fixation pathways were paretooptimal for specific activity and product-substrate yield in simulations with C1 substrates and CO2/H2 and therefore hold great potential for future applications in Industrial Biotechnology and Synthetic Biology.}
}
@article{SU2024108233,
title = {Musical protein: Mapping the time sequence of music onto the spatial architecture of proteins},
journal = {Computer Methods and Programs in Biomedicine},
volume = {252},
pages = {108233},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724002281},
author = {Jun Su and Peng Zhou},
keywords = {Musical protein, Life of music, Bioinformatics, Stave, Note, Piano, Conversion of music to protein},
abstract = {Background and objective
Music, the ubiquitous language across human cultures, is traditionally considered as a form of art but has been linked to biomolecules in recent years. However, previous efforts have only been addressed on sonification of nucleic acids and proteins to produce so-called life music, the soundscape from the basic building blocks of life. In this study, we attempted to, for the first time, conduct a reverse operation of this process, i.e. conversion of music to protein (CoMtP).
Methods
A novel notion termed musical protein (MP) –– the protein defined by music –– was proposed and, on this basis, we described a computational strategy to map the time sequence of music onto the spatial architecture of proteins, which considered that each note in the stave of a music (target) can be simply characterized by two acoustical quantities and that each residue in the primary sequence of a protein (hit) was represented by amino acid descriptors.
Results
A simulated annealing (SA) algorithm was applied to iteratively generate the best matched MP hit for a music target and structural bioinformatics was then used to model spatial advanced structure for the resulting MP. We also demonstrated that some small MPs derived from music segments may have potential biological functions, which, for example, can serve as antimicrobial peptides (AMPs) to inhibit clinical bacterial strains with moderate or high antibacterial potency.
Conclusions
This work may benefit many aspects; for example, it would open a door for the hearing-impaired persons to ‘listen’ music in a biological vision and could be a mean of exposing students to the concepts of biomolecules at an earlier age through the use of auditory characteristics. The CoMtP would also facilitate the rational design of proteins with biological and medicinal significance.}
}
@article{ARUN2009S1116,
title = {P03-117 A bedside schizophrenia thought disorder scale},
journal = {European Psychiatry},
volume = {24},
pages = {S1116},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71349-5},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713495},
author = {C.P. Arun},
abstract = {Present classification systems for thought disorder lack consistency and require one to remember long-winded definitions limiting their use to research settings. As an extension of recent work in this area (World Congress, 2008), we classify the characteristic thought disorder patterns seen in schizophrenia according to the location of the lesion in notional "threads" of mental computational processes that string speech together. These threads must take both semantics and syntax into consideration in performing their function. When we speak - just as when we write - there is a natural hierarchy topic thread (the topic of the ‘essay’) and multiples of paragraph threads, sentence threads, clause threads, word threads and phoneme threads. Intuitively, we grade the severity of thought disorder depending upon whether a particular thread gets stuck (S), reconnects abnormally (R) or is absent altogether: I.paragraph thread R: Disjointed sentences S: Circumstantiality;II.topic threadR: Tangentiality S: Preoccupatory thinking;III.sentence threads R: Knight's move thinking S: Clause perseveration;IV.clause threads R: Word salad S: Word perseveration, fusion;V.word threads R: Incoherent sounds/ neologisms/ paraphasias S: Phoneme/syllable perseveration;VI.phoneme threads - Failure of production: Mutism.Of course, one must record all the lesions that are present at any given time. This scale incorporates a intuitive progression from mild to severe thought disorder in Schizophrenia. Using the STDS would allow the straightforward ‘bedside’ quantification of the severity of thought disorder and enforce discipline into the thought assessment section of the Mental State Examination.}
}
@article{BLACK202010653,
title = {A revolution in biochemistry and molecular biology education informed by basic research to meet the demands of 21st century career paths},
journal = {Journal of Biological Chemistry},
volume = {295},
number = {31},
pages = {10653-10661},
year = {2020},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.AW120.011104},
url = {https://www.sciencedirect.com/science/article/pii/S0021925817501040},
author = {Paul N. Black},
keywords = {biochemistry, molecular biology, teaching, learning, primary research, leadership, environment, inclusive excellence, STEM education, biochemistry and molecular biology teaching and learning},
abstract = {The National Science Foundation estimates that 80% of the jobs available during the next decade will require math and science skills, dictating that programs in biochemistry and molecular biology must be transformative and use new pedagogical approaches and experiential learning for careers in industry, research, education, engineering, health-care professions, and other interdisciplinary fields. These efforts require an environment that values the individual student and integrates recent advances from the primary literature in the discipline, experimentally directed research, data collection and analysis, and scientific writing. Current trends shaping these efforts must include critical thinking, experimental testing, computational modeling, and inferential logic. In essence, modern biochemistry and molecular biology education must be informed by, and integrated with, cutting-edge research. This environment relies on sustained research support, commitment to providing the requisite mentoring, access to instrumentation, and state-of-the-art facilities. The academic environment must establish a culture of excellence and faculty engagement, leading to innovation in the classroom and laboratory. These efforts must not lose sight of the importance of multidimensional programs that enrich science literacy in all facets of the population, students and teachers in K-12 schools, nonbiochemistry and molecular biology students, and other stakeholders. As biochemistry and molecular biology educators, we have an obligation to provide students with the skills that allow them to be innovative and self-reliant. The next generation of biochemistry and molecular biology students must be taught proficiencies in scientific and technological literacy, the importance of the scientific discourse, and skills required for problem solvers of the 21st century.}
}
@article{LIU2006207,
title = {Evolutionary design in a multi-agent design environment},
journal = {Applied Soft Computing},
volume = {6},
number = {2},
pages = {207-220},
year = {2006},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S156849460500013X},
author = {Hong Liu and Mingxi Tang},
keywords = {Multi-agent system, Evolutionary computing, Generic algorithm, Computer-aided design, Creative design},
abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.}
}
@article{WILLIAMS2023145,
title = {Stabilizing expectations when shifting from analytical to intuitive reasoning: The role of prediction errors in reasoning},
journal = {Cortex},
volume = {161},
pages = {145-153},
year = {2023},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223000412},
author = {Chad C. Williams and Cameron D. Hassall and Olave E. Krigolson},
keywords = {Reasoning, Prediction errors, Cognitive control, Theta, EEG},
abstract = {As humans, we rely on intuitive reasoning for most of our decisions. However, when there is a novel or atypical decision to be made, we must rely on a slower and more deliberative thought process—analytical reasoning. As we gain experience with these novel or atypical decisions, our reasoning shifts from analytical to intuitive, which parallels a reduction in the need for cognitive control. Here, we sought to confirm this claim by employing electroencephalographic (EEG) measures of cognitive control as participants performed a simple perceptual decision-making task. Specifically, we had participants categorize “blobs” into families based on their visual attributes so we could examine how their reasoning changed with learning. In a key manipulation, halfway through the experiment we introduced novel blob families to categorize, thus temporarily increasing the need for analytical reasoning (i.e., cognitive control). Congruent with past research, we focused our EEG analyses on frontal theta activity as it has been linked to cognitive control and analytical thinking. As hypothesized, we found a transition from analytical to intuitive decision-making systems with learning as indexed by a decrease in frontal theta power. Further, when the novel blobs were introduced at the midpoint of the experiment, we found that decisions about these stimuli recruited analytical reasoning as indicated by increased theta power in comparison to decisions about well-practiced stimuli. We propose our findings to reflect prediction errors to decision demands—a monitoring process that determines whether our expectations of demands are met. Shifting from analytical to intuitive reasoning thus reflects the stabilization of our expectations of decision demands, which can be violated with unexpected demands when encountering novel stimuli.}
}
@article{TAY20211,
title = {Modelability across time as a signature of identity construction on YouTube},
journal = {Journal of Pragmatics},
volume = {182},
pages = {1-15},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621002307},
author = {Dennis Tay},
keywords = {Identity construction, Social media, LIWC, Modelability, ARIMA, Time series analysis},
abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.}
}
@article{LORE2024105149,
title = {Using multiple, dynamically linked representations to develop representational competency and conceptual understanding of the earthquake cycle},
journal = {Computers & Education},
volume = {222},
pages = {105149},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105149},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001635},
author = {Christopher Lore and Hee-Sun Lee and Amy Pallant and Jie Chao},
keywords = {Teaching/learning strategies, Simulations, Pedagogical issues, Applications in subject areas},
abstract = {Using computational methods to produce and interpret multiple scientific representations is now a common practice in many science disciplines. Research has shown students have difficulty in moving across, connecting, and sensemaking from multiple representations. There is a need to develop task-specific representational competencies for students to reason and conduct scientific investigations using multiple representations. In this study, we focus on three representational competencies: 1) linking between representations, 2) disciplinary sensemaking from multiple representations, and 3) conceptualizing domain-relevant content derived from multiple representations. We developed a block code-based computational modeling environment with three different representations and embedded it within an online activity for students to carry out investigations around the earthquake cycle. The three representations include a procedural representation of block codes, a geometric representation of land deformation build-up, and a graphical representation of deformation build-up over time. We examined the extent of students' representational competencies and which competencies are most correlated with students’ future performance in a computationally supported geoscience investigation. Results indicate that a majority of the 431 students showed at least some form of representational competence. However, a relatively small number of students showed sophisticated levels of linking, sensemaking, and conceptualizing from the representations. Five of seven representational competencies, the most prominent being code sensemaking (η2 = 0.053, p < 0.001), were significantly correlated to student performance on a summative geoscience investigation.}
}
@article{ANDERSON2023102027,
title = {Nurse scholars of the Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program},
journal = {Nursing Outlook},
volume = {71},
number = {5},
pages = {102027},
year = {2023},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2023.102027},
url = {https://www.sciencedirect.com/science/article/pii/S002965542300132X},
author = {Cindy M. Anderson and Nina Ardery and Daniel Pesut and Carmen Alvarez and Tamryn F. Gray and Karen M. Rose and Jasmine L. Travers and Janiece Taylor and Kathy D. Wright},
keywords = {Health professions diversity, Equity, Inclusive excellence, Robert Wood Johnson Foundation, Nurse faculty scholars, Harold Amos, Medical faculty development, Legacy leadership},
abstract = {Background
The challenge to increase the diversity, inclusivity, and equity of nurse scientists is a critical issue to enhance nursing knowledge development, health care, health equity, and health outcomes in the United States.
Purpose
The purpose of this paper is to highlight the current nurse scholars in the Robert Wood Johnson Foundation (RWJF) Harold Amos Medical Faculty Development Program (AMFDP).
Discussion
Profiles and the programs of research and scholarship of the current AMFDP nurse scholars are described and discussed. Scholars share lessons learned, and how the AMFDP program has influenced their thinking and commitments to future action in service of nursing science, diversity efforts, legacy leadership, issues of health equity.
Conclusion
RWJF has a history of supporting the development of nursing scholars. AMFDP is an example of legacy leadership program that contributes to a culture of health and the development of next-generation nursing science scholars.}
}
@article{ECONOMOU1994131,
title = {Activities, issues and perspectives in computational physics: a view from Greece},
journal = {Computational Materials Science},
volume = {2},
number = {1},
pages = {131-136},
year = {1994},
issn = {0927-0256},
doi = {https://doi.org/10.1016/0927-0256(94)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0927025694900558},
author = {E.N. Economou}
}
@article{GOLDBERG2012261,
title = {An efficient tree-based computation of a metric comparable to a natural diffusion distance},
journal = {Applied and Computational Harmonic Analysis},
volume = {33},
number = {2},
pages = {261-281},
year = {2012},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520311001266},
author = {Maxim J. Goldberg and Seonja Kim},
keywords = {Tree, Diffusion, Distance, Metric},
abstract = {Using diffusion to define distances between points on a manifold (or a sampled data set) has been successfully employed in various applications such as data organization and approximately isometric embedding of high dimensional data in low dimensional Euclidean space. Recently, P. Jones has proposed a diffusion distance which is both intuitively appealing and scales appropriately with increasing time. In the first part of our paper, we present an efficient tree-based approach to computing an approximation to Jonesʼs diffusion distance. We also show our approximation is comparable to Jonesʼs distance. Neither Jonesʼs distance, nor our approximation, satisfies the triangle inequality; in particular, in the case of heat flow on Rn, Jonesʼs separation distance gives a scaled square of the Euclidean distance. In the second part of our paper, we present a general construction to obtain an “almost” metric from a general distance. We also discuss a numerical procedure to implement our construction. Additionally, we show that in the case of heat flow on Rn, we recover (scaled) Euclidean distance from Jonesʼs distance.}
}
@article{PARK2023101271,
title = {The impact of research and representation of site analysis for creative design approach in architectural design studio},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101271},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101271},
url = {https://www.sciencedirect.com/science/article/pii/S187118712300041X},
author = {Eun Joo Park and Keunhye Lee and Eunki Kang},
keywords = {Architectural design studio, Creative thinking, Design education, Design methodology, Site analysis},
abstract = {In the context of architectural education, design studio projects generally begin with the research of design themes and contexts; however, few attempts have been made to appreciate site analysis as reliable architectural design research. This study aims to explore strategies that link site analysis and design application to bridge the gap between research and representation as a framework for applying architectural design education. To bridge this knowledge gap, this study describes four phases of site analysis—(1) site selection, (2) site survey, (3) problem identification, and (4) suggestion for the design approach —in which visual expression of the representation technique was explored to develop a creative design approach through observation and analysis of students’ work. A curriculum that adopts the four phases of site analysis was developed based on the SPC and expanded for second-year architecture students in the university. The results showed that there are differences between architecture students in schematizing specific ideas and analysis methods, and that a substantial change in the process occurs when including visual expression in site analysis. In addition, the combination of group-based work and site analysis led to problem-solving that showed co-evolution. Finally, the study describes how research shapes site analysis and explains how representation can contribute to understanding the research. Site analysis consists of an initial attempt to explore research related to creative approaches, and may benefit both architecture educators and students.}
}
@incollection{BUNGE1980155,
title = {CHAPTER 7 - Thinking and Knowing},
editor = {MARIO BUNGE},
booktitle = {The Mind–Body Problem},
publisher = {Pergamon},
pages = {155-173},
year = {1980},
isbn = {978-0-08-024720-5},
doi = {https://doi.org/10.1016/B978-0-08-024720-5.50012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080247205500128},
author = {MARIO BUNGE},
abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.}
}
@incollection{MARON1965118,
title = {On Cybernetics, Information Processing, and Thinking},
editor = {Norbert Wiener and J.P. Schadé},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {17},
pages = {118-138},
year = {1965},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(08)60158-2},
url = {https://www.sciencedirect.com/science/article/pii/S0079612308601582},
author = {M.E. Maron},
abstract = {Publisher Summary
It is the purpose of this chapter to examine the origins, development, and present status of those key cybernetic notions that provide an information-flow framework within which to attack one aspect of the question of how a person thinks— that is,.the question of the information mechanisms and processes that underlie and are correlated with thinking. After an introductory survey of the scope and ramifications of the information sciences, the cybernetic way of looking at the information processing in the nervous system is examined, so as to see in what sense it provides new and sharp tools of analysis for the neurophysiologist. With this as background, the problem of artificial intelligence is considered and with that the logical and linguistic difficulties in talking about the relationship between thinking and brain activity. An information-flow model of an artificial brain mechanism is described whose activity; it is argued is the correlate to activity, such as perceiving, learning, thinking, knowing, etc. This leads finally to a consideration of the impact of these notions on theoretical neurophysiology and its attempt to frame suitable hypotheses and on epistemology that is concerned with the logical analysis of measures, methods, and techniques, which can justify the activity of knowing.}
}
@article{MACHADO2023101290,
title = {A multiple criteria framework to assess learning methodologies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101290},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101290},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000603},
author = {Rafaela Heloisa Carvalho Machado and Samuel Vieira Conceição and Renata Pelissari and Sarah Ben Amor and Thiago Lombardi Resende},
keywords = {Active learning methodologies, Skills, Multiple criteria decision making, MCDA, MCDM},
abstract = {New job skills required by the professional market have been causing significant changes in the learning process of undergraduate students. Different learning methodologies can be adopted to assist in the development of those skills, and the process of choosing the most suitable learning methodology for each situation may be complex, involving multiple and conflicting criteria. In order to support the choice of learning methodologies for the development of the “4C skills”, i.e, collaboration, communication, creativity and critical thinking, we propose a new framework based on the multiple criteria decision-making approach PROMETHEE II (Preference Ranking Organization Method for Enrichment of Evaluations), considering as criteria the “4C skills”, student motivation, level of learning, student comfort, decision-making capacity and time required for class preparation. Passive methods and active learning methodologies such as Guided Reciprocal Peer Questioning (GRPQ), Think-Pair-Share (TPS), and Problem Based Learning (PBL) are compared. Each methodology was applied to three groups of students of Industrial Engineering of a Brazilian University, totaling 138 students. As a result, PBL obtained the best assessment in the three groups, followed by GRPQ. The proposed framework validates the assessment of learning methodologies, providing a structure and guideline for its replication in other educational institutions.}
}
@article{CEKIRGE199465,
title = {An appropriate algorithm in parallel computations for three-dimensional hydrodynamics},
journal = {Mathematical and Computer Modelling},
volume = {20},
number = {1},
pages = {65-84},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90219-4},
url = {https://www.sciencedirect.com/science/article/pii/0895717794902194},
author = {H.M. Cekirge and J. Berlin and R.A. Bernatz and M. Koch},
keywords = {Methods of characteristics, Tidal currents, Parallel computations, Three-dimensional hydrodynamics, Tidal currents in the Arabian Gulf},
abstract = {There are a number of numerical methods for solving three-dimensional hydrodynamical models. An important aspect of any method is its efficient use of parallel computer architectures in an effort to minimize the clock time requirements in certain simulations such as oil spill modeling which uses three-dimensional hydrodynamics. The vertical-horizontal splitting (VHS) algorithm, using the method of characteristics for the two-dimensional horizontal plane and a generalization of the Crank-Nicholson method for vertical integration, is well-suited for the parallel architecture of the CM-2 machine.}
}
@article{BEAR2020104057,
title = {What comes to mind?},
journal = {Cognition},
volume = {194},
pages = {104057},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719302306},
author = {Adam Bear and Samantha Bensinger and Julian Jara-Ettinger and Joshua Knobe and Fiery Cushman},
keywords = {Sampling, Decision-making, Consciousness, Computation},
abstract = {When solving problems, like making predictions or choices, people often “sample” possibilities into mind. Here, we consider whether there is structure to the kinds of thoughts people sample by default—that is, without an explicit goal. Across three experiments we found that what comes to mind by default are samples from a probability distribution that combines what people think is likely and what they think is good. Experiment 1 found that the first quantities that come to mind for everyday behaviors and events are quantities that combine what is average and ideal. Experiment 2 found, in a manipulated context, that the distribution of numbers that come to mind resemble the mathematical product of the presented statistical distribution and a (softmax-transformed) prescriptive distribution. Experiment 3 replicated these findings in a visual domain. These results provide insight into the process generating people’s conscious thoughts and invite new questions about the value of thinking about things that are both likely and good.}
}
@article{BERRUTO2024858,
title = {Engineering agricultural soil microbiomes and predicting plant phenotypes},
journal = {Trends in Microbiology},
volume = {32},
number = {9},
pages = {858-873},
year = {2024},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2024.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X2400043X},
author = {Chiara A. Berruto and Gozde S. Demirer},
keywords = {rhizosphere engineering, plant microbiome, machine learning, community modeling, host–microbe interactions, microbiome-associated phenotype},
abstract = {Plant growth-promoting rhizobacteria (PGPR) can improve crop yields, nutrient use efficiency, plant tolerance to stressors, and confer benefits to future generations of crops grown in the same soil. Unlocking the potential of microbial communities in the rhizosphere and endosphere is therefore of great interest for sustainable agriculture advancements. Before plant microbiomes can be engineered to confer desirable phenotypic effects on their plant hosts, a deeper understanding of the interacting factors influencing rhizosphere community structure and function is needed. Dealing with this complexity is becoming more feasible using computational approaches. In this review, we discuss recent advances at the intersection of experimental and computational strategies for the investigation of plant–microbiome interactions and the engineering of desirable soil microbiomes.}
}
@article{VIERTEL2019109,
title = {A Computational model of the mammalian external tufted cell},
journal = {Journal of Theoretical Biology},
volume = {462},
pages = {109-121},
year = {2019},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318304752},
author = {Ryan Viertel and Alla Borisyuk},
keywords = {External tufted cell, Bursting, Glomerulus, Olfactory bulb, Hodgkin Huxley model},
abstract = {We introduce a novel detailed conductance-based model of the bursting activity in external tufted (ET) cells of the olfactory bulb. We investigate the mechanisms underlying their bursting, and make experimentally-testable predictions. The ionic currents included in the model are specific to ET cells, and their kinetic and other parameters are based on experimental recordings. We validate the model by showing that its bursting characteristics under various conditions (e.g. blocking various currents) are consistent with experimental observations. Further, we identify the bifurcation structure and dynamics that explain bursting behavior. This analysis allows us to make predictions of the response of the cell to current pulses at different burst phases. We find that depolarizing (but not hyperpolarizing) inputs received during the interburst interval can advance burst timing, creating the substrate for synchronization by excitatory connections. It has been hypothesized that such synchronization among the ET cells within one glomerulus might help coordinate the glomerular output. Next we investigate model parameter sensitivity and identify parameters that play the most prominent role in controlling each burst characteristic, such as the burst frequency and duration. Finally, the response of the cell to periodic inputs is examined, reflecting the sniffing-modulated input that these cell receive in vivo. We find that individual cells can be better entrained by inputs with higher, rather than lower, frequencies than the intrinsic bursting frequency of the cell. Nevertheless, a heterogeneous population of ET cells (as may be found in a glomerulus) is able to produce reliable periodic population responses even at lower input frequencies.}
}
@article{JIANG2024109208,
title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
journal = {Aerospace Science and Technology},
volume = {150},
pages = {109208},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.109208},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824003390},
author = {Lulu Jiang and Guanhua Chen and Xiaopeng Xue and Xin Pan and Gang Chen},
keywords = {Supersonic Parachute, Mars atmosphere, Aerodynamic Optimization, Shape design, Wide Speed Range},
abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{NUNEZV2024101173,
title = {Recommendation system using bio-inspired algorithms for urban orchards},
journal = {Internet of Things},
volume = {26},
pages = {101173},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001148},
author = {Juan M. {Núñez V.} and Juan M. Corchado and Diana M. Giraldo and Sara Rodríguez-González and Fernando {De la Prieta}},
keywords = {Internet of Things, Bio-inspired algorithms, Urban orchards, Lettuce crops, Social design},
abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.}
}
@article{DIACOPOULOS2020103911,
title = {A systematic review of mobile learning in social studies},
journal = {Computers & Education},
volume = {154},
pages = {103911},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103911},
url = {https://www.sciencedirect.com/science/article/pii/S036013152030110X},
author = {Mark Michael Diacopoulos and Helen Crompton}
}
@article{RICHARDSON1991305,
title = {Computational physics on the CM-2 supercomputer},
journal = {Physics Reports},
volume = {207},
number = {3},
pages = {305-320},
year = {1991},
issn = {0370-1573},
doi = {https://doi.org/10.1016/0370-1573(91)90149-G},
url = {https://www.sciencedirect.com/science/article/pii/037015739190149G},
author = {John L. Richardson},
abstract = {The Connection Machine Supercomputer system is described with emphasis on the solution to large scale physics problems. Numerous parallel algorithms as well as their implementation are given that demonstrate the use of the Connection Machine for physical simulations. Applications discussed include classical mechanics, quantum mechanics, electromagnetism, fluid flow, statistical physics and quantum field theories. The visualization of physical phenomena is also discussed and in the lectures video tapes demonstrating this capability are shown. Connection Machine performance and I/O characteristics are also described as well as the CM-2 software.}
}
@article{TEO2024102655,
title = {Age-appropriate adaptation of creativity tasks for infants aged 12–24 months},
journal = {MethodsX},
volume = {12},
pages = {102655},
year = {2024},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2024.102655},
url = {https://www.sciencedirect.com/science/article/pii/S2215016124001092},
author = {Ling Zheng Teo and Victoria Leong},
keywords = {Precursors of creativity, Infancy, Measurements of creativity},
abstract = {Creativity is an important skill that relates to innovation, problem-solving and artistic achievement. However, relatively little is known about the early development of creative potential in very young children, in part due to a paucity of tasks suitable for use during infancy. Current measures of creativity in early childhood include the Unusual Box Test, Torrance's Thinking Creatively in Action and Movement (TCAM) task and the Toca Kitchen Monsters task. These tasks are designed for children aged above 12, 36 and 18 months respectively, but very few measures of creativity can be used for infants aged below 2. Accordingly, here we report age-appropriate adaptations of TCAM and Toca Kitchen Monsters tasks for infants as young as 12 to 24 months. Considerations taken into account include (1) infants’ cognitive capacities (i.e., attention span, language comprehension skills, motor skills, and approach to play), and (2) practicality of the stimuli, including suitability for use amid the COVID-19 pandemic. The modified creativity battery for infants includes three tasks: Music Play, Object Play and Exploratory Play tasks. The task protocols elaborated in this paper are intended to facilitate studies on the early development of creativity in infants aged between 12 and 24 months. Primary highlights include:•Age-appropriate adaptation of creativity tasks for use with infants aged between 12 and 24 months.•Consideration of infants’ cognitive capacities and stimulus practicality.•Innovative use of movement as expression of infants’ creative behaviour.}
}
@incollection{GALLICCHIO201127,
title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
editor = {Christo Christov},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {85},
pages = {27-80},
year = {2011},
booktitle = {Computational chemistry methods in structural biology},
issn = {1876-1623},
doi = {https://doi.org/10.1016/B978-0-12-386485-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123864857000028},
author = {Emilio Gallicchio and Ronald M. Levy},
keywords = {Quasi-chemical description, Statistical mechanics, Potential of mean force, PDT, MM/PBSA, free energy perturbation, BEDAM, double decoupling},
abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.}
}
@article{PAN2025125506,
title = {CISL-PD: A deep learning framework of clinical intervention strategies for Parkinson’s disease based on directional counterfactual Dual GANs},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125506},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125506},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402373X},
author = {Changrong Pan and Yu Tian and Lingyan Ma and Tianshu Zhou and Shuyu Ouyang and Jingsong Li},
keywords = {Parkinson’s disease, Intervention strategies, Counterfactual generation, Generative Adversarial Network},
abstract = {Parkinson’s disease (PD) is a prevalent chronic neurodegenerative disorder characterized by both motor and non-motor symptoms. The significant heterogeneity among PD patients poses a major challenge for treatment interventions. Current clinical interventions for PD primarily target motor symptoms, often neglecting non-motor symptoms, which can lead to unnecessary complications in non-motor symptoms while treating motor symptoms. Therefore, it is crucial to provide comprehensive and precise intervention strategies that encompass both symptom types. To address this issue, we develop a deep learning framework of clinical intervention strategies for PD (CISL-PD) based on counterfactual thinking. This framework introduces Directional Counterfactual Dual Generative Adversarial Networks (DCD-GANs), which apply various counterfactual constraints to longitudinal data to generate practical and plausible counterfactual instances aligned with clinical reality. By analyzing these counterfactual instances and their differences from the original instances, we explore PD intervention strategies with duration-specific fine regulation of multidimensional features. Experiments conducted on 374 PD patients from the Parkinson’s Progression Markers Initiative (PPMI) demonstrate that the counterfactual instances generated by DCD-GANs surpass other state-of-the-art models in terms of similarity (0.307 ± 0.246), sparsity (0.513 ± 0.161), smoothness (0.238 ± 0.135), and trend consistency (0.100 ± 0.089). From these generated counterfactual instances, we develop three clinically feasible intervention strategies that address both motor and non-motor symptoms and identify corresponding patterns of PD with distinct progression differences. Validation on an independent cohort of 351 patients from the National Institute of Neurological Disorders and Stroke Parkinson’s Disease Biomarkers Program (PDBP) confirmed the framework’s robustness and generalizability. By offering precise, multidimensional intervention strategies that can address both motor and non-motor symptoms, the CISL-PD framework has the potential to enhance patient outcomes, reduce complications, improve overall quality of life, and guide clinical decision-making.}
}
@article{IYER2024e32546,
title = {Inspiring a convergent engineering approach to measure and model the tissue microenvironment},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32546},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32546},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085773},
author = {Rishyashring R. Iyer and Catherine C. Applegate and Opeyemi H. Arogundade and Sushant Bangru and Ian C. Berg and Bashar Emon and Marilyn Porras-Gomez and Pei-Hsuan Hsieh and Yoon Jeong and Yongdeok Kim and Hailey J. Knox and Amir Ostadi Moghaddam and Carlos A. Renteria and Craig Richard and Ashlie Santaliz-Casiano and Sourya Sengupta and Jason Wang and Samantha G. Zambuto and Maria A. Zeballos and Marcia Pool and Rohit Bhargava and H. Rex Gaskins},
keywords = {Bioengineering, Interdisciplinary research, Bioimaging, Biomaterials, Biosensing, Computational biology, Biomedical devices, Biotechnology},
abstract = {Understanding the molecular and physical complexity of the tissue microenvironment (TiME) in the context of its spatiotemporal organization has remained an enduring challenge. Recent advances in engineering and data science are now promising the ability to study the structure, functions, and dynamics of the TiME in unprecedented detail; however, many advances still occur in silos that rarely integrate information to study the TiME in its full detail. This review provides an integrative overview of the engineering principles underlying chemical, optical, electrical, mechanical, and computational science to probe, sense, model, and fabricate the TiME. In individual sections, we first summarize the underlying principles, capabilities, and scope of emerging technologies, the breakthrough discoveries enabled by each technology and recent, promising innovations. We provide perspectives on the potential of these advances in answering critical questions about the TiME and its role in various disease and developmental processes. Finally, we present an integrative view that appreciates the major scientific and educational aspects in the study of the TiME.}
}
@article{ZHANG2024100667,
title = {Research on the application value of Multimedia-Based virtual reality technology in drama education activities},
journal = {Entertainment Computing},
volume = {50},
pages = {100667},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100667},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000351},
author = {Bingyu Zhang and Wenwen Jiang},
keywords = {Multimedia, Virtual reality, Academic technology, Drama education, and ANOVA},
abstract = {The research and moral use of academic technology focuses on developing, implementing, and overseeing the use of suitable technical resources and procedures to enhance learning and achievement. Multimedia has found its position in some form as an educational technology platform in the contemporary environment of academic universities. The use of virtual reality software as an intellectual tool and learning provider allows students to perform cognitive rehabilitation of preexisting information frameworks. People are paying more and more attention to how preschoolers' holistic skills develop as education reform progresses. Drama education is incorporated into the school curriculum to enhance young children's artistic, intellectual, and linguistic skills. Therefore, this study aims to examine the potential of multimedia-based virtual reality technology (MVRT) in drama education. The participants in this study were students from different universities in China. Students were exposed to multimedia-based virtual reality technology, and its efficacy was assessed using a statistical analytic approach called Analysis of variance (ANOVA). Drama understanding rate, educational improvement ratio, teaching quality rate, student achievement ratio, computation time, and parental support rate are among the performance metrics used to assess performance. Multimedia-based virtual reality technology (MVRT) for drama education showed outstanding success, with a 98% improvement ratio in educational outcomes and higher teaching quality. Students exhibited improved performance, supported by solid parental approval, demonstrating the effectiveness of MVRT in enhancing educational experiences.}
}
@article{MORABIA2020164,
title = {Pandemics and methodological developments in epidemiology history},
journal = {Journal of Clinical Epidemiology},
volume = {125},
pages = {164-169},
year = {2020},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620306454},
author = {Alfredo Morabia},
keywords = {Epidemiology, History, Pandemics, Covid-19, Plague, Cholera, Tuberculosis, Influenza, HIV/AIDS},
abstract = {The crisis spurred by the pandemic of COVID-19 has revealed weaknesses in our epidemiologic methodologic corpus, which scientists are struggling to compensate. This article explores whether this phenomenon is characteristic of pandemics or not. Since the emergence of population-based sciences in the 17th century, we can observe close temporal correlations between the plague and the discovery of population thinking, cholera and population-based group comparisons, tuberculosis and the formalization of cohort studies, the 1918 Great Influenza and the creation of an academic epidemiologic counterpart to the public health service, the HIV/AIDS epidemic, and the formalization of causal inference concepts. The COVID-19 pandemic seems to have promoted the widespread understanding of population thinking both with respect to ways of flattening an epidemic curve and the societal bases of health inequities. If the latter proves true, it will support my hypothesis that pandemics did accelerate profound changes in epidemiologic methods and concepts.}
}
@article{GROSBERG2009359,
title = {Computational models of heart pumping efficiencies based on contraction waves in spiral elastic bands},
journal = {Journal of Theoretical Biology},
volume = {257},
number = {3},
pages = {359-370},
year = {2009},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2008.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022519308006103},
author = {Anna Grosberg and Morteza Gharib},
keywords = {Cardiac modeling, Left ventricular twist, Myocardium macro-structure, Finite element simulations},
abstract = {We present a framework for modeling biological pumping organs based on coupled spiral elastic band geometries and active wave-propagating excitation mechanisms. Two pumping mechanisms are considered in detail by way of example: one of a simple tube, which represents a embryonic fish heart and another more complicated structure with the potential to model the adult human heart. Through finite element modeling different elastic contractions are induced in the band. For each version the pumping efficiency is measured and the dynamics are evaluated. We show that by combining helical shapes of muscle bands with a contraction wave it is possible not only to achieve efficient pumping, but also to create desired dynamics of the structure. As a result we match the function of the model pumps and their dynamics to physiological observations.}
}
@article{CANIZARES2024100020,
title = {Taming the Rhinoceros: A brief history of a ubiquitous tool},
journal = {Perspectives in Architecture and Urbanism},
volume = {1},
number = {2},
pages = {100020},
year = {2024},
issn = {2950-2675},
doi = {https://doi.org/10.1016/j.pau.2024.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2950267524000241},
author = {Galo Canizares},
keywords = {Software, History, Parametric design, Digital fabrication, Theory},
abstract = {At the turn of the millennium, architects and educators, propelled by the demise of critical theory, found a space to speculate about technology’s role in the future of architecture. As Michael Speaks wrote in 2002, “if philosophy was the intellectual dominant of early twentieth century vanguards and theory the intellectual dominant of late twentieth century vanguards, then intelligence has become the intellectual dominant of twenty-first century post-vanguards” (Speaks, 2010, p. 211). This emphasis on intelligence fostered a progressive narrative around the increasing reliance on software in design processes. This paper examines architectural practice during this period, with a specific focus on the rise of a new set of values, priorities, and factors that transformed architectural thinking and making. In contrast to existing accounts of digital design’s history, this paper places less importance on outputs and more on the shifts in modes of working and enacting design labor. More specifically, it narrows in on a software application that, as will be argued, drastically changed both cultural values and design knowledge: Rhinoceros. Beyond simply facilitating the production of geometrically intricate and complex architectural assemblies, Rhinoceros helped shape the discourse on parametric, computational, and algorithmic design, redefining the role of the designer as a creative technologist. In doing so, it also engendered a specific community of practice, which in turn produced its own culture and folklore. The spread of this software greatly contributed to the rise of two new kinds of architectural technologists: the “parametric designer” and the “digital fabricator,” two actors who would significantly impact how architecture was imagined and produced from the mid-2000s through the 2010s.}
}
@article{DUAN2021107596,
title = {Equidistant k-layer multi-granularity knowledge space},
journal = {Knowledge-Based Systems},
volume = {234},
pages = {107596},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107596},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008583},
author = {Jiangli Duan and Guoyin Wang and Xin Hu},
keywords = {Granular computing, Multi-granularity knowledge space, Knowledge space distance, Hierarchical quotient space},
abstract = {A multi-granularity knowledge space is a computational model that simulates human thinking and solves complex problems. However, as the amount of data increases, the multi-granularity knowledge space will have a larger number of layers, which will reduce its problem-solving ability. Therefore, we define a knowledge space distance measurement and propose two algorithms to select k representative layers from the multi-granularity knowledge space, where k is specified by the user according to the needs in problem solving, and k representative layers are approximately equidistant. First, we propose a knowledge space distance to measure the distance between any two layers in a multi-granularity knowledge space with superset-subset relationships, and the rationality of the knowledge space distance is verified by theory and experiment. Second, relying on the knowledge space distance and knowledge space distance variance, we propose two algorithms (i.e., a deterministic algorithm and a heuristic algorithm) to select an approximate equidistant k-layer multi-granularity knowledge space. Third, in addition to verifying the effectiveness of the knowledge space distance, the knowledge space distance variance, the deterministic algorithm and the heuristic algorithm, we verify that the equidistant k-layer multi-granularity knowledge space is more efficient than the original multi-granularity knowledge space.}
}
@article{LOPEZBRAU2023105524,
title = {People can use the placement of objects to infer communicative goals},
journal = {Cognition},
volume = {239},
pages = {105524},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105524},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001580},
author = {Michael Lopez-Brau and Julian Jara-Ettinger},
keywords = {Computational modeling, Social objects, Theory of Mind},
abstract = {Beyond words and gestures, people have a remarkable capacity to communicate indirectly through everyday objects: A hat on a chair can mean it is occupied, rope hanging across an entrance can mean we should not cross, and objects placed in a closed box can imply they are not ours to take. How do people generate and interpret the communicative meaning of objects? We hypothesized that this capacity is supported by social goal inference, where observers recover what social goal explains an object being placed in a particular location. To test this idea, we study a category of common ad-hoc communicative objects where a small cost is used to signal avoidance. Using computational modeling, we first show that goal inference from indirect physical evidence can give rise to the ability to use object placement to communicate. We then show that people from the U.S. and the Tsimane’—a farming-foraging group native to the Bolivian Amazon—can infer the communicative meaning of object placement in the absence of a pre-existing convention, and that people’s inferences are quantitatively predicted by our model. Finally, we show evidence that people can store and retrieve this meaning for use in subsequent encounters, revealing a potential mechanism for how ad-hoc communicative objects become quickly conventionalized. Our model helps shed light on how humans use their ability to interpret other people’s behavior to embed social meaning into the physical world.}
}
@article{FRITSCH2024100297,
title = {Teaching advanced topics in econometrics using introductory textbooks: The case of dynamic panel data methods},
journal = {International Review of Economics Education},
volume = {47},
pages = {100297},
year = {2024},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2024.100297},
url = {https://www.sciencedirect.com/science/article/pii/S147738802400015X},
author = {Markus Fritsch and Andrew Adrian Yu Pua and Joachim Schnurbus},
keywords = {Teaching econometrics, instrumental variables, linear dynamic panel data methods, cigarette demand, lagged variables},
abstract = {We show how to use the introductory econometrics textbook by Stock and Watson (2019) as a starting point for teaching and studying dynamic panel data methods. The materials are intended for undergraduate students taking their second econometrics course, undergraduate students in seminar-type courses, independent study courses, capstone, or thesis projects, and beginning graduate students in a research methods course. First, we distill the methodological core necessary to understand dynamic panel data methods. Second, we design an empirical and a theoretical case study to highlight the capabilities, downsides, and hazards of the method. The empirical case study is based on the cigarette demand example in Stock and Watson (2019) and illustrates that economic and methodological issues are interrelated. The theoretical case study shows how to evaluate current empirical practices from a theoretical standpoint. We designed both case studies to boost students’ confidence in working with technical material and to provide instructors with more opportunities to let students develop econometric thinking and to actively communicate with applied economists. Although we focus on Stock and Watson (2019) and the statistical software R, we also show how to modify the material for use with another introductory textbook by Wooldridge (2020) and Stata, and highlight some possible further pathways for instructors and students to reuse and extend our materials.}
}
@incollection{KOZA2002275,
title = {Chapter 10 - Genetic Programming: Biologically Inspired Computation That Exhibits Creativity in Producing Human-Competitive Results},
editor = {Peter J Bentley and David W. Corne},
booktitle = {Creative Evolutionary Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {275-298},
year = {2002},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-673-9},
doi = {https://doi.org/10.1016/B978-155860673-9/50048-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606739500483},
author = {John R. Koza and Forrest H. Bennett and David Andre and Martin A. Keane},
abstract = {Publisher Summary
One of the central challenges of computer science is to get a computer to solve a problem without programming it explicitly. The challenge is to create an automatic system whose input is a high-level statement of a problem's requirements and whose output is a satisfactory solution to the given problem. This challenge is the common goal of such fields of research as artificial intelligence and machine learning. Paraphrasing Arthur Samuel, this challenge addresses the question: How can computers are made to do what needs to be done, without being told exactly how to do it? As Samuel further explained: “The aim is to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.” This chapter provides an affirmative answer to the following two questions: Starting only with a high-level statement of the problem's requirements, can computers automatically discover the solution to nontrivial problems? And, can automatically created solutions be competitive with the products of human creativity and inventiveness? In answering these questions, this chapter focuses on a biologically inspired domain-independent problem-solving technique of evolutionary computation, called genetic programming. For each problem, genetic programming automatically creates entities that improve on previously patented inventions, or duplicate the functionality of previously patented inventions or duplicate the functionality of previously patented inventions. The chapter also discusses the importance of illogic in achieving creativity and inventiveness.}
}
@article{EBERBACH2007200,
title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
journal = {Theoretical Computer Science},
volume = {383},
number = {2},
pages = {200-243},
year = {2007},
note = {Complexity of Algorithms and Computations},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507003192},
author = {Eugene Eberbach},
keywords = {Problem solving, Process algebras, Anytime algorithms, SuperTuring models of computation, Bounded rational agents, $-calculus, Intractability, Undecidability, Completeness, Optimality, Search optimality, Total optimality},
abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.}
}
@article{BIBRI2018758,
title = {A foundational framework for smart sustainable city development: Theoretical, disciplinary, and discursive dimensions and their synergies},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {758-794},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717313069},
author = {Simon Elias Bibri},
keywords = {Smart sustainable cities, Theories, Academic disciplines, Academic discourses, Multidimensional framework, Interdisciplinarity and transdisciplinarity, Systems thinking, Complexity science, Sustainability, Computing and ICT},
abstract = {In the subject of smart sustainable cities, the underlying theories are a foundation for practice. Moreover, scholarly research in the field of smart sustainable cities operates out of the understanding that advances in the underlying knowledge necessitate pursuing multifaceted questions that can only be resolved from the vantage point of interdisciplinarity or transdisciplinarity. Indeed, research problems in this field are inherently too complex to be addressed by single disciplines. The PhD study addressing the topic of smart sustainable city development falls within the broad research field of sustainability transition and sustainability science where ICT is seen as a salient factor given its transformational, disruptive, and synergetic effects as an enabling, integrative, and constitutive technology. In light of this, the approach to the PhD study is of an applied theoretical kind, and its aim is to investigate and analyze how to advance and sustain the contribution of sustainable urban forms to the goals of sustainable development with support of ICT of pervasive computing. This is to primarily create a framework for strategic smart sustainable city development based on scientific principles, theories, and academic disciplines and discourses used to guide urban actors in their practice towards sustainability and analyze its impact. This involves the application of a set of integrative foundational elements drawn from urban planning, urban design, sustainability, sustainable development, sustainability science, data science, computer science, complexity science, systems theory, systems thinking, and ICT. Accordingly, it is deemed of high significance to devise a multidimensional framework consisting of relevant theories and academic disciplines and discourses that underpin the development of smart sustainable cities as a set of future practices. This framework in turn emphasizes the interdisciplinary and transdisciplinary nature and orientation of the topic of smart sustainable cities and thus the relevance of pursuing an interdisciplinary and transdisciplinary approach into studying this topic. Therefore, this paper endeavors to systematize the very complex and dense scientific area of smart sustainable cities in terms of identifying, distilling, and structuring the core dimensions of a foundational framework for smart sustainable city development as a set of future practices. In doing so, it focuses on a number of fundamental theories along with academic disciplines and discourses, with the aim of setting a framework that analytically relates city development, sustainability, and ICT, while emphasizing how and to what extent sustainability and ICT have particularly become influential in city development in modern society. In addition, this paper offers an in–depth interdisciplinary and transdisciplinary discussion covering topics of high relevance to the PhD study and at the heart of the very synergic relationship between the theoretical, disciplinary, and discursive dimensions of the foundational framework underpinning smart sustainable city development. These dimensions thus form the basis for the framework for strategic smart sustainable city development that is under investigation and will be developed based on a backcasting approach to strategic planning. This study provides an important lens through which to understand a set of influential theories and established academic disciplines and discourses with high potential for integration, fusion, and practicality in relation to the practice of smart sustainable city development.}
}
@article{ALVARADO20043,
title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
journal = {Expert Systems with Applications},
volume = {26},
number = {1},
pages = {3-8},
year = {2004},
note = {Intelligent Computing in the Petroleum Industry, ICPI-02},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403001039},
author = {Matı&#x0301;as Alvarado and Leonid Cheremetov and Francisco Cantú}
}
@incollection{RZESZEWSKI2024219,
title = {Chapter 10 - Augmented reality content and relations of power in smart spaces},
editor = {Zhihan Lyu},
booktitle = {Smart Spaces},
publisher = {Academic Press},
pages = {219-234},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-13462-3},
doi = {https://doi.org/10.1016/B978-0-443-13462-3.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313462300008X},
author = {Michal Rzeszewski and Leighton Evans},
keywords = {Augmented reality, Smart space, Agency of place, Power relations},
abstract = {This chapter’s main aim is to interrogate how augmented reality (AR) content can change the relations of power within a place and transform the perception of place from being a material background for social interactions into a living agent that can have its own agency. We use the concept of sociotechnical imaginary and thematic analysis of AR-related media content to explore main narrations in the current discourse on AR and smart urban spaces. We identify two dominant themes: “smart information in place” and “subversion of meaning” that combine two ways of thinking about relation between place and AR. In the first one, AR acts not as an augmentation of place, but as a reducer of the experience of place down to seeing the world as information and data for the achievement of efficiencies in late capitalism. In the second one, AR can be seen as transcending the traditional constellations of power relations that shape places and spaces of modern cities, skewing them toward nonhuman actors, of which AR may be the most visible and influential. Through the visible differentiation of users, alteration of perception and the forcing of presence and behavior, AR is a technology that makes visible the systems and processes of control and mediation of space and place. As such, the smart space itself will become visible through the presence, use, and functioning of AR in a manner that has not been the case previously. We posit, therefore, that AR can be seen as a physical manifestation of the agency of place. This position can have consequences for the practical development of smart spaces and for theoretical consideration of the human-technology interaction in urban space in its material and digital dimensions.}
}
@article{LOCK2023102310,
title = {Conserving complexity: A complex systems paradigm and framework to study public relations’ contribution to grand challenges},
journal = {Public Relations Review},
volume = {49},
number = {2},
pages = {102310},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123000255},
author = {Irina Lock},
keywords = {Grand challenges, Public issues, Public relations, Strategic communication, Complex adaptive systems, Digital communication, Complexity},
abstract = {Sustainable development poses a grand challenge for society, addressed by organisations through their public relations activities. Grand challenges are complex by nature and call for nontrivial solutions whose effects show at the level of society. That is why studying public relations’ contribution to grand challenges requires a macro perspective that accounts for the dynamic interaction between individual, organisational, and system levels in a digital communication environment. This paper offers a new paradigm to analyse organisations’ significant and at times undue impact on grand challenges through public relations. It develops a framework inspired by complex adaptive systems thinking and adopts its ten properties for public relations: emergence, adaptivity, heterogeneous actors, nonlinear effects, feedback mechanisms, self-organisation, phase transitions, networks, scaling, and cooperation. The paper applies the framework to the example of sustainable development. It shows why research on grand challenges requires a holistic perspective and how it can help study digitally born communication phenomena. The proposed complex systems paradigm provides space for critical, social scientific, and interpretative research lines in public relations. Inquiries start from the grand challenge and study the communicative interactions between organisations and other actors from existing theory while accounting for the ten properties of complex adaptive systems. The paper outlines how future research can enrich the study of public relations and discusses its limits.}
}
@article{SARTON195551,
title = {The astral religion of antiquity and the “thinking machines” of to-day},
journal = {Vistas in Astronomy},
volume = {1},
pages = {51-60},
year = {1955},
issn = {0083-6656},
doi = {https://doi.org/10.1016/0083-6656(55)90012-X},
url = {https://www.sciencedirect.com/science/article/pii/008366565590012X},
author = {George Sarton}
}
@article{JU2022101062,
title = {Proposal for a STEAM education program for creativity exploring the roofline of a hanok using GeoGebra and 4Dframe},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101062},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101062},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000657},
author = {Hyunshik Ju and Hogul Park and Eun Young Jung and Seoung-Hey Paik},
keywords = {STEAM education program, Creativity, Catenary curve, Korean traditional architecture, Modelling, GeoGebra, 4Dframe},
abstract = {This research was conducted to confirm the feasibility of a STEAM education program in which the mathematics, physics, and Korean traditional arts underlying the hanok roofline are investigated using educational tools of GeoGebra and 4Dframe. This paper contends that this program has the potential to engage students in knowledge restructuring regarding the perception of the hanok’s architectural beauty, Newton's concept of gravity, and mathematical functions. The Octagonal Pavilion in Tapgol Park in Seoul, South Korea, a representative hanok, was used as an educational resource. GeoGebra is used to determine that the roofline of the Octagonal Pavilion generally follows the formula of the catenary curve and then the roofline is modelled using 4Dframe. The catenary form of the roofline of the hanok is linked to the Korean sense of beauty in the pursuit of naturalness under the influence of gravity and organically harmonizes with the environment. The class described in this study, in which the curve of the roofline of the Octagonal Pavilion is explored using GeoGebra and 4Dframe, can help develop creative and critical thinking in students in the context of STEAM education. The findings of this study have the potential to expand the scope of STEAM education to include content for creative education.}
}
@article{PAPADOPOULOS2019210,
title = {Using mobile puzzles to exhibit certain algebraic habits of mind and demonstrate symbol-sense in primary school students},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {210-227},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231730189X},
author = {Ioannis Papadopoulos},
keywords = {Algebraic habits of mind, mobile puzzles, Symbol sense},
abstract = {Given the growing concern for developing students’ algebraic ideas and thinking in earlier grades (NCTM, 2000) it is important for students to have experiences that better prepare them for their formal introduction to algebra. Mobile puzzles seem to be an opportunity for exhibiting certain algebraic habits of mind as well as for demonstrating symbol-sense which might support students in their transition from arithmetic to algebra. These puzzles include multiple balanced collections of objects whose weights must be determined by the solver. The arms/beams must be perfectly balanced for it to hang properly. Therefore, they represent, in a pictorial way, systems of equations. Each arm/beam that balances two sets of objects (representing variables as unknown “weights”) represents an equation. The data derived from Grade-6 students who were asked to solve a collection of tasks reflect the presence of the “Puzzling and Persevering” and “Seeking and Using Structure” habits of mind. At the same time these data incorporate instances of some main components of symbol-sense such as “friendliness with symbols”, “manipulating and ‘reading through’ symbolic expressions”, and “choice of symbols”. Also discussed is the way this experience contributes to an intuitive application of the conventional rules for solving equations that will be later introduced to the students as the standard algebraic “moves”.}
}
@article{LEBARON2000679,
title = {Agent-based computational finance: Suggested readings and early research},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {5},
pages = {679-702},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00022-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000226},
author = {Blake LeBaron},
keywords = {Agents, Heterogeneous information, Simulated markets, Learning, Evolution},
abstract = {The use of computer simulated markets with individual adaptive agents in finance is a new, but growing field. This paper explores some of the early works in the area concentrating on a set of some of the earliest papers. Six papers are summarized in detail, along with references to many other pieces of this wide ranging research area. It also covers many of the questions that new researchers will face when getting into the field, and hopefully can serve as a kind of minitutorial for those interested in getting started.}
}
@incollection{KAMAREDDINE2012801,
title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
editor = {Dov M. Gabbay and Akihiro Kanamori and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {6},
pages = {801-845},
year = {2012},
booktitle = {Sets and Extensions in the Twentieth Century},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-51621-3.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444516213500116},
author = {Fairouz Kamareddine and Twan Laan and Robert Constable}
}
@incollection{CORMACK2005325,
title = {4.1 - Computational Models of Early Human Vision},
editor = {AL BOVIK},
booktitle = {Handbook of Image and Video Processing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Burlington},
pages = {325-IX},
year = {2005},
series = {Communications, Networking and Multimedia},
isbn = {978-0-12-119792-6},
doi = {https://doi.org/10.1016/B978-012119792-6/50083-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780121197926500838},
author = {Lawrence K. Cormack}
}
@article{KESIC2024101072,
title = {Complexity and biocomplexity: Overview of some historical aspects and philosophical basis},
journal = {Ecological Complexity},
volume = {57},
pages = {101072},
year = {2024},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2023.101072},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X23000442},
author = {Srdjan Kesić},
keywords = {Cybernetics, General systems theory, Complexity, Modeling, Biocomplexity, Emergence, Autopoiesis},
abstract = {Complexity has radically changed human understanding of the world environment and continues challenging our best scientific theories. In a rapidly changing research landscape, historical and philosophical insights into Complexity can heighten awareness of the proper theoretical perspectives scientists should adopt to advance the study of biocomplexity, including ecological complexity. The present work aims to deepen this awareness and disclose how researchers should generally approach, scientifically and philosophically, the question of what Complexity is, which is of great importance not only to the scientific community but also far beyond. First, this article reviews some critical historical turning points that led to Complexity. Second, the paper discusses philosophical-scientific approaches to the emergence as one of the most critical features of complex systems. The critical ideas behind attempts to understand the generators of complexity in nature are then presented, focusing on the living world. Finally, the review focuses on understanding the ecosystem- and organism-oriented perspectives of biocomplexity. We conclude that the genuine problem of the origin of complexity theory and biocomplexity will continue to inspire generations of researchers to search for new, more comprehensive mathematical and computational frameworks to explain biological hierarchies in order to further advance the scientific understanding of life.}
}
@article{WANG201854,
title = {Studying cognitive development in cultural context: A multi-level analysis approach},
journal = {Developmental Review},
volume = {50},
pages = {54-64},
year = {2018},
note = {Towards a Cultural Developmental Science},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0273229717301041},
author = {Qi Wang},
keywords = {Culture, Cognition, Episodic thinking, Memory, Multiple levels of analysis},
abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.}
}
@article{PEARSON1994203,
title = {Report on University of Wales Institute of non-Newtonian Fluid Mechanics Mini-Symposium on “Continuum and Microstructural Modelling in Computational Rheology” Seiont Manor, Gwynedd, 11–12 April 1994},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {55},
number = {2},
pages = {203-205},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)80006-5},
url = {https://www.sciencedirect.com/science/article/pii/0377025794800065},
author = {J.R.A. Pearson}
}
@article{CAO2024102160,
title = {Self-assembly of peptides: The acceleration by molecular dynamics simulations and machine learning},
journal = {Nano Today},
volume = {55},
pages = {102160},
year = {2024},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2024.102160},
url = {https://www.sciencedirect.com/science/article/pii/S174801322400015X},
author = {Nana Cao and Kang Huang and Jianjun Xie and Hui Wang and Xinghua Shi},
keywords = {Peptides, Self-assembly, Molecular dynamics, Machine learning},
abstract = {Peptides, biopolymeric compounds connected by peptide bonds, have garnered significant attention in recent years as their potential wide applications in fields such as drug delivery, tissue engineering, and antibiotics. Peptides exhibit excellent biocompatibility and stability due to their structural similarities to many bioactive substances found in human bodies. The self-assembly of peptides has piqued considerable interest with groundbreaking advancements achieved in experimental research. However, it is still a big challenge to establish comprehensive theoretical model to accurately describe the behavior of peptide self-assembly. Current peptide self-assembly designs primarily rely on experimental outcomes and general rules, which is inefficient and susceptible to human errors. In recent years, thanks to rapid advancements in computer techniques and theoretical methods, computational research has become a vital tool in complementing experimental research with rapid development witted in this field. This review delves into the description of peptide self-assembly, covering relevant sequences, structures, morphologies, rules, and application areas. It places particular emphasis on the recent progress in computational methods such as molecular dynamics (MD) simulations and machine learning (ML) techniques in the study. Finally, we provide a perspective on the application of computational methods to expedite exploration in the realm of multi-peptide self-assembly.}
}
@article{FUXJAGER2023105340,
title = {Systems biology as a framework to understand the physiological and endocrine bases of behavior and its evolution—From concepts to a case study in birds},
journal = {Hormones and Behavior},
volume = {151},
pages = {105340},
year = {2023},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2023.105340},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X23000387},
author = {Matthew J. Fuxjager and T. Brandt Ryder and Nicole M. Moody and Camilo Alfonso and Christopher N. Balakrishnan and Julia Barske and Mariane Bosholn and W. Alice Boyle and Edward L. Braun and Ioana Chiver and Roslyn Dakin and Lainy B. Day and Robert Driver and Leonida Fusani and Brent M. Horton and Rebecca T. Kimball and Sara Lipshutz and Claudio V. Mello and Eliot T. Miller and Michael S. Webster and Morgan Wirthlin and Roy Wollman and Ignacio T. Moore and Barney A. Schlinger},
keywords = {Systems biology, Animal behavior, Organismal physiology, Adaptive evolution, Manakin birds, Androgenic hormones, Robustness},
abstract = {Organismal behavior, with its tremendous complexity and diversity, is generated by numerous physiological systems acting in coordination. Understanding how these systems evolve to support differences in behavior within and among species is a longstanding goal in biology that has captured the imagination of researchers who work on a multitude of taxa, including humans. Of particular importance are the physiological determinants of behavioral evolution, which are sometimes overlooked because we lack a robust conceptual framework to study mechanisms underlying adaptation and diversification of behavior. Here, we discuss a framework for such an analysis that applies a “systems view” to our understanding of behavioral control. This approach involves linking separate models that consider behavior and physiology as their own networks into a singular vertically integrated behavioral control system. In doing so, hormones commonly stand out as the links, or edges, among nodes within this system. To ground our discussion, we focus on studies of manakins (Pipridae), a family of Neotropical birds. These species have numerous physiological and endocrine specializations that support their elaborate reproductive displays. As a result, manakins provide a useful example to help imagine and visualize the way systems concepts can inform our appreciation of behavioral evolution. In particular, manakins help clarify how connectedness among physiological systems—which is maintained through endocrine signaling—potentiate and/or constrain the evolution of complex behavior to yield behavioral differences across taxa. Ultimately, we hope this review will continue to stimulate thought, discussion, and the emergence of research focused on integrated phenotypes in behavioral ecology and endocrinology.}
}
@article{NOH2006554,
title = {Computational tools for isotopically instationary 13C labeling experiments under metabolic steady state conditions},
journal = {Metabolic Engineering},
volume = {8},
number = {6},
pages = {554-577},
year = {2006},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2006.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717606000449},
author = {Katharina Nöh and Aljoscha Wahl and Wolfgang Wiechert},
keywords = {Instationary C metabolic flux analysis, C labeling experiment, C labeling dynamics, Parameter identifiability, Optimal experimental design, },
abstract = {13C metabolic flux analysis (MFA) has become an important and powerful tool for the quantitative analysis of metabolic networks in the framework of metabolic engineering. Isotopically instationary 13C MFA under metabolic stationary conditions is a promising refinement of classical stationary MFA. It accounts for the experimental requirements of non-steady-state cultures as well as for the shortening of the experimental duration. This contribution extends all computational methods developed for classical stationary 13C MFA to the instationary situation by using high-performance computing methods. The developed tools allow for the simulation of instationary carbon labeling experiments (CLEs), sensitivity calculation with respect to unknown parameters, fitting of the model to the measured data, statistical identifiability analysis and an optimal experimental design facility. To explore the potential of the new approach all these tools are applied to the central metabolism of Escherichia coli. The achieved results are compared to the outcome of the stationary counterpart, especially focusing on statistical properties. This demonstrates the specific strengths of the instationary method. A new ranking method is proposed making both an a priori and an a posteriori design of the sampling times available. It will be shown that although still not all fluxes are identifiable, the quality of flux estimates can be strongly improved in the instationary case. Moreover, statements about the size of some immeasurable pool sizes can be made.}
}
@article{MASOUD201293,
title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {90},
pages = {93-108},
year = {2012},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2012.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1386142512000418},
author = {Mamdouh S. Masoud and Alaa E. Ali and Medhat A. Shaker and Gehan S. Elasala},
keywords = {Uric, Complexes, Synthesis, Spectroscopy, Thermal analysis, Computational},
abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.}
}
@article{VANDENHURK2023106030,
title = {Consideration of compound drivers and impacts in the disaster risk reduction cycle},
journal = {iScience},
volume = {26},
number = {3},
pages = {106030},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106030},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223001074},
author = {Bart J.J.M. {van den Hurk} and Christopher J. White and Alexandre M. Ramos and Philip J. Ward and Olivia Martius and Indiana Olbert and Kathryn Roscoe and Henrique M.D. Goulart and Jakob Zscheischler},
keywords = {Earth sciences, Social sciences, Decision science},
abstract = {Summary
Consideration of compound drivers and impacts are often missing from applications within the Disaster Risk Reduction (DRR) cycle, leading to poorer understanding of risk and benefits of actions. The need to include compound considerations is known, but lack of guidance is prohibiting practitioners from including these considerations. This article makes a step toward practitioner guidance by providing examples where consideration of compound drivers, hazards, and impacts may affect different application domains within disaster risk management. We discern five DRR categories and provide illustrative examples of studies that highlight the role of “compound thinking” in early warning, emergency response, infrastructure management, long-term planning, and capacity building. We conclude with a number of common elements that may contribute to the development of practical guidelines to develop appropriate applications for risk management.}
}
@article{NISHI2022314,
title = {Health and landscape approaches: A comparative review of integrated approaches to health and landscape management},
journal = {Environmental Science & Policy},
volume = {136},
pages = {314-325},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122002027},
author = {Maiko Nishi and Shizuka Hashimoto},
keywords = {Landscape approaches, One Health, Ecohealth, Planetary Health, Social-ecological systems, Sustainability transformation},
abstract = {Landscape approaches are integrated place-based approaches and provide cross-sectoral opportunities to facilitate sustainability transformations. The COVID-19 outbreak has profound ramifications for multiple dimensions of landscapes, ranging from mobility and lifestyle to value to environment and society. Therefore, integrated approaches to “health” have been more vigorously promoted in the policy arena dealing with human–nature interactions. The ecosystem principles of the Convention on Biological Diversity, which resonate with landscape approaches, are generally aligned with integrated approaches to health. However, commonalities and distinctions between these integrated approaches in both political and scientific domains have not been clarified. Drawing on a narrative review of the literature on “One Health,” “Ecohealth,” and “Planetary Health” as major health-oriented approaches in comparison with landscape approaches, the aspects of landscape approaches to be complemented in addressing health-related challenges were examined in this study. In addition to the review on the intellectual roots and evolutionary pathways, a comparative analysis of these relevant approaches was conducted in terms of three realms including theoretical assumptions, knowledge bases, and research paradigms. The results of the comparative review show that all approaches share systems thinking, interdisciplinarity, cross-sectoral collaboration, and holistic paradigm but differ with respect to their focused management problems, disciplines, and sectors as well as ontological and epistemological underpinnings. Pointing to the recent theoretical and methodological development in integrating health in placemaking, the results of this study suggest that pragmatic landscape approaches could be strengthened by using health-related research paradigms to achieve better constructivism–positivism meeting grounds regarding health–landscape intersections.}
}
@article{OLIVER2014289,
title = {Crack-path field and strain-injection techniques in computational modeling of propagating material failure},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {274},
pages = {289-348},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514000139},
author = {J. Oliver and I.F. Dias and A.E. Huespe},
keywords = {Fracture, Computational material failure, Strong discontinuities, Crack-path field, Strain injection, Finite elements with embedded discontinuities},
abstract = {The work presents two new numerical techniques devised for modeling propagating material failure, i.e. cracks in fracture mechanics or slip-lines in soil mechanics. The first one is termed crack-path-field technique and is conceived for the identification of the path of those cracks, or slip-lines, represented by strain-localization based solutions of the material failure problem. The second one is termed strain-injection, and consists of a procedure to insert, during specific stages of the simulation and in selected areas of the domain of analysis, goal oriented specific strain fields via mixed finite element formulations. In the approach, a first injection, of elemental constant strain modes (CSM) in quadrilaterals, is used, in combination of the crack-path-field technique, for obtaining reliable information that anticipates the position of the crack-path. Based on this information, in a subsequent stage, a discontinuous displacement mode (DDM) is efficiently injected, ensuring the required continuity of the crack-path across sides of contiguous elements. Combination of both techniques results in an efficient and robust procedure based on the staggered resolution of the crack-path-field and the mechanical failure problems. It provides the classical advantages of the “intra-elemental” methods for capturing complex propagating displacement discontinuities in coarse meshes, as E-FEM or X-FEM methods, with the non-code-invasive character of the crack-path-field technique. Numerical representative simulations of a wide range of benchmarks, in terms of the type of material and the failure problem, show the broad applicability, accuracy and robustness of the proposed methodology. The finite element code used for the simulations is open-source and available at http://www.cimne.com/compdesmat/.}
}
@article{VALLESPERIS2024102448,
title = {Digital citizenship at school: Democracy, pragmatism and RRI},
journal = {Technology in Society},
volume = {76},
pages = {102448},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102448},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002531},
author = {Núria Vallès-Peris and Miquel Domènech},
keywords = {Science and technology studies, Digital citizenship, Responsible research and innovation, Democracy, Pragmatism},
abstract = {This paper presents a strategy for fostering digital citizenship at school that transcends the mere use of digital devices or instructional methods focused solely on their use. The core premise of this proposal rests on the need for an ethical-political debate concerning digitization in education. In addition, it emphasizes the need to cultivate a form of digital literacy that blends science and technology with the humanities, and erases the traditional boundaries between making and thinking. The proposed approach encapsulates two primary concerns: firstly, it asserts that digital literacy serves as a foundation for meaningful participation in digital societies; secondly, it underscores the importance of democratizing digital technologies by incorporating the perspectives, needs, and concerns of children. Drawing inspiration from the theories of pragmatism and responsible research and innovation (RRI), we present a conceptual framework for digital citizenship. To operationalize this approach, we adapt John Dewey's pragmatic model of inquiry as a method that can be applied within the school setting. This pragmatic methodology serves as a conduit for developing hands-on experience geared towards developing digital citizenship. The practical implementation of this methodology is illustrated through an actualized experience with 10- and 11-year-old children in a public primary school, regarding the issue of care robots. This paper advocates for a symbiotic relationship between theoretical understanding and practical application, and puts forward a concrete proposal for the integration of digital citizenship in schools in the form of a four-phase procedural model, based on the creation of what we term ‘the encounter’ between the educational community and the research and development community.}
}
@incollection{SHI2021117,
title = {Chapter 4 - Mind model},
editor = {Zhongzhi Shi},
booktitle = {Intelligence Science},
publisher = {Elsevier},
pages = {117-149},
year = {2021},
isbn = {978-0-323-85380-4},
doi = {https://doi.org/10.1016/B978-0-323-85380-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385380400004X},
author = {Zhongzhi Shi},
keywords = {Mind model, Turing machine, physical symbol system, SOAR model, ACT-R model, CAM model, cognitive cycle, PMJ model},
abstract = {The technology of building mind model is often called mind modeling, which aims to explore and study the human thinking mechanism.}
}
@article{INTRONE201479,
title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
journal = {Decision Support Systems},
volume = {64},
pages = {79-89},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001262},
author = {Joshua Introne and Luca Iandoli},
keywords = {Computer-supported argumentation, Evidence-based reasoning, Dempster–Shafer belief aggregation, Housing market prediction},
abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.}
}
@article{SPINU2022100205,
title = {A matter of trust: Learning lessons about causality will make qAOPs credible},
journal = {Computational Toxicology},
volume = {21},
pages = {100205},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000517},
author = {Nicoleta Spînu and Mark T.D. Cronin and Judith C. Madden and Andrew P. Worth},
keywords = {Model credibility, Adverse Outcome Pathway, qAOP, Causality, Next Generation Risk Assessment},
abstract = {Toxicology in the 21st Century has seen a shift from chemical risk assessment based on traditional animal tests, identifying apical endpoints and doses that are “safe”, to the prospect of Next Generation Risk Assessment based on non-animal methods. Increasingly, large and high throughput in vitro datasets are being generated and exploited to develop computational models. This is accompanied by an increased use of machine learning approaches in the model building process. A potential problem, however, is that such models, while robust and predictive, may still lack credibility from the perspective of the end-user. In this commentary, we argue that the science of causal inference and reasoning, as proposed by Judea Pearl, will facilitate the development, use and acceptance of quantitative AOP models. Our hope is that by importing established concepts of causality from outside the field of toxicology, we can be “constructively disruptive” to the current toxicological paradigm, using the “Causal Revolution” to bring about a “Toxicological Revolution” more rapidly.}
}
@article{SUJAN2023105994,
title = {Operationalising FRAM in Healthcare: A critical reflection on practice},
journal = {Safety Science},
volume = {158},
pages = {105994},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105994},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522003332},
author = {M. Sujan and L. Pickup and M.S. {de Vos} and R. Patriarca and L. Konwinski and A. Ross and P. McCulloch},
keywords = {Patient Safety, FRAM, Resilience Engineering, System Safety},
abstract = {Resilience Engineering principles are becoming increasingly popular in healthcare to improve patient safety. FRAM is the best-known Resilience Engineering method with several examples of its application in healthcare available. However, the guidance on how to apply FRAM leaves gaps, and this can be a potential barrier to its adoption and potentially lead to misuse and disappointing results. The article provides a self-reflective analysis of FRAM use cases to provide further methodological guidance for successful application of FRAM to improve patient safety. Five FRAM use cases in a range of healthcare settings are described in a structured way including critical reflection by the original authors of those studies. Individual reflections are synthesised through group discussion to identify lessons for the operationalisation of FRAM in healthcare. Four themes are developed: (1) core characteristics of a FRAM study, (2) flexibility regarding the underlying epistemological paradigm, (3) diversity with respect to the development of interventions, and (4) model complexity. FRAM is a systems analysis method that offers considerable flexibility to accommodate different epistemological positions, ranging from realism to phenomenology. We refer to these as computational FRAM and reflexive FRAM, respectively. Practitioners need to be clear about their analysis aims and their analysis position. Further guidance is needed to support practitioners to tell a convincing and meaningful “system story” through the lens of FRAM.}
}
@article{CAMPAGNOLO2007387,
title = {The Methods of Approximation and Lifting in Real Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {167},
pages = {387-423},
year = {2007},
note = {Proceedings of the Third International Conference on Computability and Complexity in Analysis (CCA 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000229},
author = {Manuel L. Campagnolo and Kerry Ojakian},
keywords = {Computable Analysis, Real Recursive Functions, Elementary Computable},
abstract = {The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [M.L. Campagnolo, C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002) 977–100]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [O. Bournez and E. Hainry, Elementarily computable functions over the real numbers and R-sub-recursive functions, Theoretical Computer Science 348 (2005) 130–147], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.}
}
@article{YANG2001167,
title = {Computational verb systems: computing with perceptions of dynamics},
journal = {Information Sciences},
volume = {134},
number = {1},
pages = {167-248},
year = {2001},
note = {Computing with Words},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(01)00096-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020025501000962},
author = {Tao Yang},
abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.}
}
@article{DEBRIGARD2021104574,
title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
journal = {Cognition},
volume = {209},
pages = {104574},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303930},
author = {Felipe {De Brigard} and Paul Henne and Matthew L. Stanley},
keywords = {Imagination, Counterfactual thinking, Plausibility, Similarity, Possible worlds},
abstract = {People frequently entertain counterfactual thoughts, or mental simulations about alternative ways the world could have been. But the perceived plausibility of those counterfactual thoughts varies widely. The current article interfaces research in the philosophy and semantics of counterfactual statements with the psychology of mental simulations, and it explores the role of perceived similarity in judgments of counterfactual plausibility. We report results from seven studies (N = 6405) jointly supporting three interconnected claims. First, the perceived plausibility of a counterfactual event is predicted by the perceived similarity between the possible world in which the imagined situation is thought to occur and the actual world. Second, when people attend to differences between imagined possible worlds and the actual world, they think of the imagined possible worlds as less similar to the actual world and tend to judge counterfactuals in such worlds as less plausible. Lastly, when people attend to what is identical between imagined possible worlds and the actual world, they think of the imagined possible worlds as more similar to the actual world and tend to judge counterfactuals in such worlds as more plausible. We discuss these results in light of philosophical, semantic, and psychological theories of counterfactual thinking.}
}
@article{LEE2005261,
title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
journal = {International Journal of Mass Spectrometry},
volume = {240},
number = {3},
pages = {261-272},
year = {2005},
note = {Mass Spectrometry of Biopolymers: From Model Systems to Ribosomes},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2004.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1387380604003975},
author = {Jeehiun K. Lee},
keywords = {Nucleic acids, RNA, DNA, Enzyme nucleobase, Acidity, Proton affinity},
abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.}
}
@article{ZHAO2023100891,
title = {Meet the authors: Yuxuan Zhao, Enmeng Lu, and Yi Zeng},
journal = {Patterns},
volume = {4},
number = {12},
pages = {100891},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100891},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002933},
author = {Yuxuan Zhao and Enmeng Lu and Yi Zeng},
abstract = {Yuxuan Zhao, associate professor, Enmeng Lu, research engineer, and Yi Zeng, professor and lab director, have proposed a brain-inspired bodily self-perception model based on biological findings on monkeys and humans. This model can reproduce various rubber hand illusion (RHI) experiments, which helps reveal the RHI’s computational and biological mechanisms. They talk about their view of data science and research plans for brain-inspired robot self-modeling and ethical robots.}
}
@article{JIANG2024102530,
title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102530},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102530},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001782},
author = {Shaofei Jiang and Jingwei Yang and Jing Xie and Xuesong Xu and Yubo Dou and Liting Jing},
keywords = {Product innovation design, Patent text, Knowledge graph, RFSB ontology model, Implicit relationship completion},
abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.}
}
@article{HUBERMAN19981169,
title = {Computation as economics},
journal = {Journal of Economic Dynamics and Control},
volume = {22},
number = {8},
pages = {1169-1186},
year = {1998},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(98)00008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188998000086},
author = {Bernardo A. Huberman},
abstract = {We use computers to study economics, but few people realize that we can use economics to study and design computational systems. The reason is that computer networks can be regarded as a community of processes that in their interactions, strategies and lack of perfect knowledge face the same issues as people in markets. This paper describes how computers have evolved to a point where economics approaches are useful for designing them and understanding their dynamics. Examples are given of existing computer systems that use market mechanisms and of novel phenomena, such as clustered volatility, that we uncovered when studying their evolution.}
}
@article{ATREIDES202135,
title = {E-governance with ethical living democracy},
journal = {Procedia Computer Science},
volume = {190},
pages = {35-39},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012461},
author = {Kyrtin Atreides},
keywords = {mASI, AGI, e-governance, mediated artificial superintelligence, collective superintelligence, direct digital democracy, liquid democracy, EAP, Effective Altruistic Principles, Ethical Living Democracy, ELD},
abstract = {A new form of e-governance is proposed based on systems seen in biological life at all scales. This model of e-governance offers the performance of collective superintelligence, equally high ethical quality, and a substantial reduction in resource requirements for government functions. In addition, the problems seen in modern forms of government such as misrepresentation, corruption, lack of expertise, short-term thinking, political squabbling, and popularity contests may be rendered virtually obsolete by this approach. Lastly, this model of government generates a digital ecosystem of intelligent life which mirrors physical citizens, serving to bridge the emotional divide between physical and digital life, while also producing the first form of government able to keep pace with accelerating technological progress.}
}
@article{MORGAN20052564,
title = {The visual computation of 2-D area by human observers},
journal = {Vision Research},
volume = {45},
number = {19},
pages = {2564-2570},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905002075},
author = {M.J. Morgan},
keywords = {Psychophysics, Shape, Weber fraction},
abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.}
}
@article{MALLETT20241105,
title = {New strategies for the cognitive science of dreaming},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {12},
pages = {1105-1117},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400264X},
author = {Remington Mallett and Karen R. Konkoly and Tore Nielsen and Michelle Carr and Ken A. Paller},
keywords = {sleep, dreams, memory, neuroscience, natural language processing},
abstract = {Dreams have long captivated human curiosity, but empirical research in this area has faced significant methodological challenges. Recent interdisciplinary advances have now opened up new opportunities for studying dreams. This review synthesizes these advances into three methodological frameworks and describes how they overcome historical barriers in dream research. First, with observable dreaming, neural decoding and real-time reporting offer more direct measures of dream content. Second, with dream engineering, targeted stimulation and lucidity provide routes to experimentally manipulate dream content. Third, with computational dream analysis, the generation and exploration of large dream-report databases offer powerful avenues to identify patterns in dream content. By enabling researchers to systematically observe, engineer, and analyze dreams, these innovations herald a new era in dream science.}
}
@article{OLIVEIRA2022102347,
title = {Beyond energy services: A multidimensional and cross-disciplinary agenda for home energy management research},
journal = {Energy Research & Social Science},
volume = {85},
pages = {102347},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004382},
author = {Sonja Oliveira and Lidia Badarnah and Merate Barakat and Anna Chatzimichali and Ed Atkins},
keywords = {Architecture, Biomimetics, Computational design, Cross-disciplinary methods, Home energy management},
abstract = {Home Energy Management (HEM) has a significantly growing impact on strategic energy policy, digital equity, as well as housing development and transport issues. With the proliferation of home working, reliance on electricity for heating and cooling and the increasing needs for electric charging for transportation, there is an urgent need to develop novel ways for efficient management of home energy use. Current efforts focus on HEM technologies at individual household levels, without considering the social or spatial context or their collective community-wide interrelated dependencies. We propose a multifaceted agenda at the intersection of disciplinary domains to tackle this problem by using a multidimensional lens that draws on energy behaviour, architectural research, biomimetics, and computational design, simultaneously. Optimal and effective behavioural patterns can be extracted and abstracted from nature, informing a more collective and interrelated behavioural dependencies approach that considers the complex multidimensional energy use patterns of different housing typologies. This paper discusses the analytical benefits of this new research approach through a study of home energy management behaviour. The approach though could be expanded to consider other similar empirical contexts whereby sustainable multidimensional resource management is sought such as water use, food distribution as well as transport and mobility.}
}
@article{CAGLAYAN2015131,
title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {131-153},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315000504},
author = {Günhan Caglayan},
keywords = {Undergraduate mathematics education, Dynamic geometry software, Visualization, Representation, Connection, Linear algebra, Eigenvectors, Eigenvalues, Matrices},
abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.}
}
@incollection{GRIFFIN2020303,
title = {Information Graphics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-314},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10563-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105633},
author = {Amy L. Griffin},
keywords = {Big data, Cognition, Exploratory data analysis, Geovisualization, Graphics, Information dashboard, Information visualization, Interactivity, Perception, Semiotics, Storytelling, Visual analytics},
abstract = {Information graphics include a wide variety of static and dynamic visual representations of information such as diagrams, statistical graphics, and maps. These displays take different forms depending on the purpose for which the graphic is being constructed (e.g., for thinking or communication), characteristics of the data, potential visual display forms, and whether or not the information graphic is interactive. Choosing an appropriate method for representing data requires a basic understanding of the perceptual and cognitive processing that occurs in the human visual system. Information graphics can be constructed using a wide array of tools, but are increasingly constructed using computer code and distributed through the internet.}
}
@article{ASSIOURAS2025104063,
title = {The evolution of artificial empathy in the hospitality metaverse era},
journal = {International Journal of Hospitality Management},
volume = {126},
pages = {104063},
year = {2025},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.104063},
url = {https://www.sciencedirect.com/science/article/pii/S027843192400375X},
author = {Ioannis Assiouras and Cornelia Laserer and Dimitrios Buhalis},
keywords = {Empathy, Artificial empathy, Artificial intelligence, Metaverse, Hospitality, Artificial intelligence agents},
abstract = {As hospitality enters the metaverse era, artificial empathy becomes essential for developing of artificial intelligence (AI) agents. Using the empathy cycle model, computational empathy frameworks and interdisciplinary research, this conceptual paper proposes a model explaining how artificial empathy will evolve in the hospitality metaverse era. The paper also addresses customer empathy and responses towards AI agents and other human actors with in the hospitality context. It explores how metaverse characteristics such as immersiveness, sociability, experiential nature, interoperability, blended virtual and physical environments as well as environmental fidelity will shape computational models and evolution of artificial empathy. Findings suggests that metaverse enables AI agents to form a seamless cycle of detection, resonation, and response to consumers’ affective states, facilitating the evolution of artificial empathy. Additionally, the paper outlines conditions under which the artificial empathy cycle may be disrupted and proposes future research questions that can advance our understanding of artificial empathy.}
}
@article{LAKAMSANI1995993,
title = {Mapping molecular dynamics computations on to hypercubes},
journal = {Parallel Computing},
volume = {21},
number = {6},
pages = {993-1013},
year = {1995},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)00006-A},
url = {https://www.sciencedirect.com/science/article/pii/016781919500006A},
author = {Vamsee Lakamsani and Laxmi N. Bhuyan and D.Scott Linthicum},
keywords = {Mapping problem, Recursive mincut, Molecular dynamics, Compact MD graph, Hypercube},
abstract = {We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules. We model the application as a task graph which we call a compact MD graph. Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.}
}
@article{BLOCK20191003,
title = {What Is Wrong with the No-Report Paradigm and How to Fix It},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {12},
pages = {1003-1013},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319302360},
author = {Ned Block},
keywords = {consciousness, perception, rivalry, frontal, global workspace, higher order},
abstract = {Is consciousness based in prefrontal circuits involved in cognitive processes like thought, reasoning, and memory or is it based in sensory areas in the back of the neocortex? The no-report paradigm has been crucial to this debate because it aims to separate the neural basis of the cognitive processes underlying post-perceptual decision and report from the neural basis of conscious perception itself. However, the no-report paradigm is problematic because, even in the absence of report, subjects might engage in post-perceptual cognitive processing. Therefore, to isolate the neural basis of consciousness, a no-cognition paradigm is needed. Here, I describe a no-cognition approach to binocular rivalry and outline how this approach can help to resolve debates about the neural basis of consciousness.}
}
@incollection{WU2012223,
title = {10 - Computational modeling and ab initio calculations in MAX phases – II},
editor = {I.M. Low},
booktitle = {Advances in Science and Technology of Mn+1AXn Phases},
publisher = {Woodhead Publishing},
pages = {223-270},
year = {2012},
isbn = {978-1-84569-991-8},
doi = {https://doi.org/10.1533/9780857096012.223},
url = {https://www.sciencedirect.com/science/article/pii/B9781845699918500102},
author = {E. Wu},
keywords = {computational modeling,  calculations, density function theory, energy band, electronic properties, density of states},
abstract = {Abstract:
This chapter reviews the latest researches and advances in the uses of the computational modeling and ab initio calculations on the study of the MAX phases and their properties. The fundamentals and approaches of the density functional theory in the ab initio quantum mechanical calculations and the importance of the theory in the study of the MAX phases are introduced. The studies of the electronic structures and properties, in particular, the energy band structures and total and/or partial density of states of the MAX phases, by using the means of the density function theory are illustrated and discussed. The stability and occurrence of the MAX phases predicted and confirmed by the density functional theory based energetic calculations are addressed. The ab initio calculated elastic and other physical properties of the MAX phases, and the effects of pressure, defects and impurities on the various structural and physical properties are also discussed.}
}