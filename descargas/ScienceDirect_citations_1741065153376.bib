@article{HUANG1993717,
title = {An investigation of gender differences in cognitive abilities among Chinese high school students},
journal = {Personality and Individual Differences},
volume = {15},
number = {6},
pages = {717-719},
year = {1993},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(93)90012-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699390012R},
author = {Jiafen Huang},
abstract = {This study investigated gender differences in 11 cognitive tests from a sample of grade 11 students in Shanghai, China. Research found that the girls outperformed boys significantly on Word Knowledge and Word Span tasks, and also on a Computational Speed and Accuracy test. Boys outperformed girls only on the Paper Folding test. Factor based scores showed that girls were superior to boys on memory, and verbal composites, whereas boys were superior to girls on the spatial composites. No gender differences were found on the Mathematical Thinking test and other reasoning tests. The research findings seemed to suggest that where the social conditions were more uniform the gender differences on visual-spatial and mathematical reasoning skills would be smaller.}
}
@article{HAMZI2023133853,
title = {Learning dynamical systems from data: A simple cross-validation perspective, part IV: Case with partial observations},
journal = {Physica D: Nonlinear Phenomena},
volume = {454},
pages = {133853},
year = {2023},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2023.133853},
url = {https://www.sciencedirect.com/science/article/pii/S0167278923002075},
author = {Boumediene Hamzi and Houman Owhadi and Yannis Kevrekidis},
keywords = {Learning dynamical systems, Kernel flows, Partial observations, Computational graph completion},
abstract = {A simple and interpretable way to learn a dynamical system from data is to interpolate its governing equations with a kernel. In particular, this strategy is highly efficient (both in terms of accuracy and complexity) when the kernel is data-adapted using Kernel Flows (KF) (Owhadi and Yoo, 2019), (which uses gradient-based optimization to learn a kernel based on the premise that a kernel is good if there is no significant loss in accuracy if half of the data is used for interpolation). In this work, we extend previous work on learning dynamical systems using Kernel Flows (Hamzi and Owhadi, 2021; Darcy et al. 2021; Lee et al. 2023; Darcy et al. 2023; Owhadi and Romit Maulik, 2021) to the case of learning vector-valued dynamical systems from time-series observations that are partial/incomplete in the state space. The method combines Kernel Flows with Computational Graph Completion.}
}
@article{NAVLAKHA201864,
title = {Network Design and the Brain},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {1},
pages = {64-78},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317302000},
author = {Saket Navlakha and Ziv Bar-Joseph and Alison L. Barth},
abstract = {Neural circuits have evolved to accommodate similar information processing challenges as those faced by engineered systems. Here, we compare neural versus engineering strategies for constructing networks. During circuit development, synapses are overproduced and then pruned back over time, whereas in engineered networks, connections are initially sparse and are then added over time. We provide a computational perspective on these two different approaches, including discussion of how and why they are used, insights that one can provide the other, and areas for future joint investigation. By thinking algorithmically about the goals, constraints, and optimization principles used by neural circuits, we can develop brain-derived strategies for enhancing network design, while also stimulating experimental hypotheses about circuit development and function.}
}
@incollection{WANG2001297,
title = {Computational Intelligence in Agile Manufacturing Engineering},
editor = {A. Gunasekaran},
booktitle = {Agile Manufacturing: The 21st Century Competitive Strategy},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {297-315},
year = {2001},
isbn = {978-0-08-043567-1},
doi = {https://doi.org/10.1016/B978-008043567-1/50016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080435671500164},
author = {Kesheng Wang}
}
@article{GEMMELL201720,
title = {Establishing the structures within populations of models},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {20-24},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300128},
author = {Philip M. Gemmell},
abstract = {As computational biology matures as a field, increasing attention is being paid to the relation of computational models to their target. One aspect of this is addressing how computational models can appropriately reproduce the variation seen in experimental data, with one solution being to use populations of models united by a common set of equations (the framework), with each individual member of the population (each model) possessing its own unique set of equation parameters. These model populations are then calibrated and validated against experimental data, and as a whole reproduce the experimentally observed variation. The primary focus of validation thus becomes the population, with the individual models' validation seemingly deriving from their membership of this population. The role of individual models within the population is not clear, with uncertainty regarding the relationship between individual models and the population they make up. This work examines the role of models within the population, how they relate to the population they make up, and how both can be said to be validated in this context.}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{HIPOLITO2023103510,
title = {Breaking boundaries: The Bayesian Brain Hypothesis for perception and prediction},
journal = {Consciousness and Cognition},
volume = {111},
pages = {103510},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023000478},
author = {Inês Hipólito and Michael Kirchhoff},
keywords = {Bayesian Brain Hypothesis, Modularity of the Mind, Cognitive processes, Informational boundaries},
abstract = {This special issue aims to provide a comprehensive overview of the current state of the Bayesian Brain Hypothesis and its standing across neuroscience, cognitive science and the philosophy of cognitive science. By gathering cutting-edge research from leading experts, this issue seeks to showcase the latest advancements in our understanding of the Bayesian brain, as well as its potential implications for future research in perception, cognition, and motor control. A special focus to achieve this aim is adopted in this special issue, as it seeks to explore the relation between two seemingly incompatible frameworks for the understanding of cognitive structure and function: the Bayesian Brain Hypothesis and the Modularity Theory of the Mind. In assessing the compatibility between these theories, the contributors to this special issue open up new pathways of thinking and advance our understanding of cognitive processes.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{NKONGOLO2022182,
title = {Using Deep Packet Inspection Data to Examine Subscribers on the Network},
journal = {Procedia Computer Science},
volume = {215},
pages = {182-191},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020920},
author = {Mike Nkongolo and Jacobus Phillipus {van Deventer} and Sydney Mambwe Kasongo},
keywords = {Deep packet inspection, machine learning, UGRansome, telecommunication, data science},
abstract = {This article proposes the creation of the deep packet inspection (DPI) dataset to study subscribers’ behavior on the network, applying ensemble learning to this dataset, and comparing it with the UGRansome dataset. The subscriber can be thought of as a person or a group of users using a network service or connectivity. The DPI features represent the subscriber network usage, and the ensemble learning approach is implemented on the DPI dataset to predict the subscriber's service category on the network. The classification and prediction problem addressed on the DPI dataset reached a precision of 100%. The paper predicts that the web and streaming categories with Netflix, Facebook, and YouTube services will be the most utilized in the next few years. This study will lead to a better understanding of the idiosyncratic behavior of active subscribers on the network, exposing novel network anomalies and facilitating the development of novel DPI systems.}
}
@article{BRYANSMITH2023105405,
title = {Real-time social media sentiment analysis for rapid impact assessment of floods},
journal = {Computers & Geosciences},
volume = {178},
pages = {105405},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001097},
author = {Lydia Bryan-Smith and Jake Godsall and Franky George and Kelly Egode and Nina Dethlefs and Dan Parsons},
keywords = {Social media, Sentiment analysis, Flooding, Artificial Intelligence},
abstract = {Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.}
}
@article{BINKOWSKA201435,
title = {Computational and experimental study of charge distribution in the α-disulfonyl carbanions},
journal = {Journal of Molecular Structure},
volume = {1062},
pages = {35-43},
year = {2014},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022286014000258},
author = {Iwona Binkowska and Jacek Koput and Arnold Jarczewski},
keywords = {Proton transfer, Carbon acids, Charge distribution,  computation},
abstract = {The electron densities of the disulfonyl carbanions were determined using experimental 13C chemical shifts. The 13C NMR spectra and electron densities for the disulfonyl, nitro, and cyano carbon acids were calculated at the MP2/cc-pVDZ level of theory. The calculated chemical shifts for disulfonyl carbanions show satisfying correlation with our own experimental data. The calculated π electron densities at the Cα atom correspond roughly to the “experimental” π electron densities estimated from the 13C chemical shifts. The natural charges at Cα in disulfonyl stabilized carbanions are significantly more negative than with other types of carbanions, partly because of the significant negative natural charge of the α carbon in parent carbon acids. The calculated increase of the negative charge caused by ionization is larger for sulfonyl carbon acids than for cyano- and nitroalkanes. The 13C chemical shifts δ of Cα in disulfonyl stabilized carbanions decrease with more negative calculated negative natural charge at Cα, with a slope of 220ppm/electron. The influence of phenyl ring para-substitution on the charge distribution in carbanions and relationship between the 13C chemical shifts and charge density have been discussed. It appears that the π electron density in these planar or nearly planar carbanions has a decisive impact on the chemical shifts.}
}
@article{DEALMEIDA2022478,
title = {Assisting in the choice to fill a vacancy to compose the PROANTAR team: Applying VFT and the CRITIC-GRA-3N methodology},
journal = {Procedia Computer Science},
volume = {214},
pages = {478-486},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019123},
author = {Isaque David Pereira {de Almeida} and Lucas Ramon dos Santos Hermogenes and Igor Pinheiro de Araújo Costa and Miguel Ângelo Lellis Moreira and Carlos Francisco Simões Gomes and Marcos {dos Santos} and David de Oliveira Costa and Ian José Agra Gomes},
keywords = {CRITIC-GRA-3N method, Brazilian Navy, COVID-19},
abstract = {Antarctica is the southernmost continent of our planet, and it has been verified as the coldest region on earth. The Brazilian Antarctic Program (PROANTAR) has as its main objective the promotion of high-quality scientific research in the Antarctic region, seeking to understand the events that occur there. PROANTAR, coordinated by the Navy Commander, has some sectors that are based in Brazil and others that are located in the Antarctic continent. The military that volunteers to occupy any vacancy that is allocated to that continent needs, besides passing through several pre-established criteria, to pass the selection process. The purpose of this article is to help the Naval Administration in the selection of volunteer officers to occupy a vacancy in the Antarctic continent. To obtain the alternatives, the officers that best fit the established vacancy, and the criteria to be evaluated, Value-Focused Thinking (VFT) was applied. Next, with all the necessary data, the CRITIC-GRA-3N method was used as a Multicriteria Decision Support (MDS) technique, the CRITIC-GRA-3N method, the CRITIC Importance Through Intercriteria Correlation (CRITIC) method to obtain the criteria weights and the Grey Relational Analysis (GRA) method, with three normalizations, to order the alternatives. At the end of the application of the methods, the article can generate five ordinations of the volunteer officers to occupy the vacancy offered in PROANTAR.}
}
@article{HABTEMARIAM1990653,
title = {Research in computational epidemiology},
journal = {Mathematical and Computer Modelling},
volume = {14},
pages = {653-658},
year = {1990},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(90)90263-M},
url = {https://www.sciencedirect.com/science/article/pii/089571779090263M},
author = {T. Habtemariam and D. Oryang and F. Gabreab and V. Robnett and G. Trammell},
abstract = {The emerging new area referred to as computational science or science done on a computer adds a third dimension to the traditional methods of theoretical and experimental approaches. Counterparts to computational science such as computational linguistice, computational engineering and others arc beginning to take roots. Naturally, new research paths and opportunities in computational epidemiology must also be explored. One of the major challenges in epidemiologic research is the issue of how to realistically and effectively handle complex bioepidemiologic dynamics involving interactions between humans or animals, etiological agents and the multiple array of environmental and socioeconomic determinants which affect these populations. To understand the behavior of such complex biological systems, it is useful to devise computer based simulation models. Computational epidemiologic approaches now provide alternative avenues to classical laboratory and/or field experimental methods. Systems which may be impractical because they are too large, or, not feasible because the cost is too prohibitive can now be simulated realistically. In the past obtaining solutions to biomathematical equations with any degree of complexity was impossible. However, the availability of powerful computers now makes the quantitative analysis of such systems feasible and indeed practical. With this in mind our research at Tuskegee University has focused on: a) Epidemiologic modelling and expert systems, and, b) Hypertext/hypermedia based epidemiologic knowledge management. The case studies for our research involve the bioepidemiologic dynamics of two complex host-parasite systems of trypanosoma and schistosoma. The ultimate goal is to develop resources and methodologies based on computational technology to advance epidemiologic research. The paper will address the methodological issues and findings as well as questions related to configuring an appropriate research workstation for computational epidemiology.}
}
@article{EDLA2015254,
title = {Is heart rate variability better than routine vital signs for prehospital identification of major hemorrhage?},
journal = {The American Journal of Emergency Medicine},
volume = {33},
number = {2},
pages = {254-261},
year = {2015},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2014.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S073567571400881X},
author = {Shwetha Edla and Andrew T. Reisner and Jianbo Liu and Victor A. Convertino and Robert Carter and Jaques Reifman},
abstract = {Objective
During initial assessment of trauma patients, metrics of heart rate variability (HRV) have been associated with high-risk clinical conditions. Yet, despite numerous studies, the potential of HRV to improve clinical outcomes remains unclear. Our objective was to evaluate whether HRV metrics provide additional diagnostic information, beyond routine vital signs, for making a specific clinical assessment: identification of hemorrhaging patients who receive packed red blood cell (PRBC) transfusion.
Methods
Adult prehospital trauma patients were analyzed retrospectively, excluding those who lacked a complete set of reliable vital signs and a clean electrocardiogram for computation of HRV metrics. We also excluded patients who did not survive to admission. The primary outcome was hemorrhagic injury plus different PRBC transfusion volumes. We performed multivariate regression analysis using HRV metrics and routine vital signs to test the hypothesis that HRV metrics could improve the diagnosis of hemorrhagic injury plus PRBC transfusion vs routine vital signs alone.
Results
As univariate predictors, HRV metrics in a data set of 402 subjects had comparable areas under receiver operating characteristic curves compared with routine vital signs. In multivariate regression models containing routine vital signs, HRV parameters were significant (P < .05) but yielded areas under receiver operating characteristic curves with minimal, nonsignificant improvements (+0.00 to +0.05).
Conclusions
A novel diagnostic test should improve diagnostic thinking and allow for better decision making in a significant fraction of cases. Our findings do not support that HRV metrics add value over routine vital signs in terms of prehospital identification of hemorrhaging patients who receive PRBC transfusion.}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{KNIGHT20061084,
title = {‘When I first came here, I thought medicine was black and white’: Making sense of medical students’ ways of knowing},
journal = {Social Science & Medicine},
volume = {63},
number = {4},
pages = {1084-1096},
year = {2006},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606000451},
author = {Lynn Valerie Knight and Karen Mattick},
keywords = {Medical training, Professional knowledge, Epistemology, Evidence-based medicine, United Kingdom},
abstract = {Personal beliefs about what knowledge is and how we understand, integrate and apply knowledge (known as personal epistemologies) are entrenched in the process of decision-making. Evidence-based medicine in all its forms brings with it the need for an ever more sophisticated appreciation of individual patients’ perspectives and ‘scientific’ perspectives within the clinical encounter. However, current theoretical perspectives on personal epistemology focus more on scientific ways of knowing where knowledge is abstracted and logical. We conducted semi-structured interviews to investigate medical students’ personal epistemological thinking towards the end of their second year of training at a new medical school in the South West of England. Whilst responses were varied, students appeared to express predominantly simplistic levels of epistemological thinking according to current developmental models of personal epistemology. However, the process of professional identity formation together with epistemological thinking brought together both scientific and experiential ways of knowing in a way that has largely been ignored by current theorists in the domain of personal epistemology.}
}
@article{SUN2025e01027,
title = {First-principles calculations of electronic and mechanical properties of magnesium indium intermetallic compounds},
journal = {Computational Condensed Matter},
volume = {43},
pages = {e01027},
year = {2025},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2025.e01027},
url = {https://www.sciencedirect.com/science/article/pii/S2352214325000267},
author = {Liang Sun and Yidan Huang and Kaifeng Zhao and Zuoming Chen and Xiongtao Shang and Wenzhen Xu and wenyan Zhai and Pengyue Han and Jin Jia and Jianhong Peng},
keywords = {Mg-In intermetallic compounds, First-principles calculations, Phonon spectra, Anisotropy, Mechanical properties, Electronic properties},
abstract = {In the search for innovative alternatives to aluminum-magnesium alloys, this study takes a unique approach by focusing on magnesium-indium binary alloys, with an emphasis on the intermetallic compounds Mg2In, MgIn3, Mg5In2, and Mg3In. With the help of cutting-edge first-principles computational techniques, the four compounds are comprehensively and thoroughly analyzed in terms of crystal structure, anisotropy, phonon spectra, electronic properties, and mechanical properties. The charge transfer phenomenon from magnesium to indium is found for the first time, and the s-orbital density of indium is at its peak in the Mg-In phase. In terms of mechanical properties, Mg2In, Mg5In2, and MgIn3 exhibit similar bulk moduli, while the shear modulus, Young's modulus, and hardness of MgIn3 are significantly lower than those of the other phases, emphasizing its unique deformability. Taking the results together, MgIn3 shows great potential for application in cutting-edge fields such as biomedical materials due to its compact size, corrosion resistance, low hardness, and high plasticity, which opens up a new way of thinking for the development of Mg-In alloy-based advanced materials.}
}
@article{FU2022107,
title = {Everyday Creativity is Associated with Increased Frontal Electroencephalography Alpha Activity During Creative Ideation},
journal = {Neuroscience},
volume = {503},
pages = {107-117},
year = {2022},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306452222004596},
author = {Lei Fu and Jia Zhao and Jiangzhou Sun and Yuchi Yan and Mujie Ma and Qunlin Chen and Jiang Qiu and Wenjing Yang},
keywords = {Everyday creativity, Alpha power, Alpha coherence, Creative ideation, Frontal cortex},
abstract = {Everyday creativity is the basic ability of human survival and penetrates every aspect of life. Nevertheless, the neural mechanisms underlying everyday creativity was largely unexplored. In this study, seventy-five participants completed the creative behaviour inventory, a tool for assessing creative behaviour in daily life. The participants also completed the alternate uses task (AUT) during an electroencephalography (EEG) assessment to evaluate creative thinking. Alpha power was used to quantify neural oscillations during the creative process, while alpha coherence was used to quantify information communication between frontal regions and other sites during creative ideation. Moreover, these two task-related quantitative measures were combined to investigate the relationship between individual differences in everyday creativity and EEG alpha activity during creative idea generation. Compared with the reference period, increased alpha power was observed in the frontal cortex of the right hemisphere and increased functional coupling was observed between frontal and parietal/temporal regions during the activation period. Interestingly, individual differences in everyday creativity were associated with distinct patterns of EEG alpha activity. Specifically, individuals with higher everyday creativity had increased alpha power in the frontal cortex, and increased changes in coherence in frontal-temporal regions of the right hemisphere while performing the AUT. It might indicate that individuals with higher everyday creativity had an enhanced ability to focus on internal information processing and control bottom-up stimuli, as well as better selection of novel semantic information when performing creative ideation tasks.}
}
@article{KARI2022102843,
title = {The Sabatier principle as a tool for discovery and engineering of industrial enzymes},
journal = {Current Opinion in Biotechnology},
volume = {78},
pages = {102843},
year = {2022},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2022.102843},
url = {https://www.sciencedirect.com/science/article/pii/S095816692200177X},
author = {Jeppe Kari and Kay Schaller and Gustavo A Molina and Kim Borch and Peter Westh},
abstract = {The recent breakthrough in all-atom, protein structure prediction opens new avenues for a range of computational approaches in enzyme design. These new approaches could become instrumental for the development of technical biocatalysts, and hence our transition toward more sustainable industries. Here, we discuss one approach, which is well-known within inorganic catalysis, but essentially unexploited in biotechnology. Specifically, we review examples of linear free-energy relationships (LFERs) for enzyme reactions and discuss how LFERs and the associated Sabatier Principle may be implemented in algorithms that estimate kinetic parameters and enzyme performance based on model structures.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{GENTILI2024150060,
title = {Living cells and biological mechanisms as prototypes for developing chemical artificial intelligence},
journal = {Biochemical and Biophysical Research Communications},
volume = {720},
pages = {150060},
year = {2024},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2024.150060},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X24005965},
author = {Pier Luigi Gentili and Pasquale Stano},
keywords = {Chemical AI, Synthetic cell, Chemical neural networks, Neuromorphic engineering, Molecular fuzzy sets, Molecular computing},
abstract = {Artificial Intelligence (AI) is having a revolutionary impact on our societies. It is helping humans in facing the global challenges of this century. Traditionally, AI is developed in software or through neuromorphic engineering in hardware. More recently, a brand-new strategy has been proposed. It is the so-called Chemical AI (CAI), which exploits molecular, supramolecular, and systems chemistry in wetware to mimic human intelligence. In this work, two promising approaches for boosting CAI are described. One regards designing and implementing neural surrogates that can communicate through optical or chemical signals and give rise to networks for computational purposes and to develop micro/nanorobotics. The other approach concerns “bottom-up synthetic cells” that can be exploited for applications in various scenarios, including future nano-medicine. Both topics are presented at a basic level, mainly to inform the broader audience of non-specialists, and so favour the rise of interest in these frontier subjects.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{HANNA2025100705,
title = {Future of Artificial Intelligence—Machine Learning Trends in Pathology and Medicine},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100705},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2025.100705},
url = {https://www.sciencedirect.com/science/article/pii/S0893395225000018},
author = {Matthew G. Hanna and Liron Pantanowitz and Rajesh Dash and James H. Harrison and Mustafa Deebajah and Joshua Pantanowitz and Hooman H. Rashidi},
keywords = {artificial intelligence, computational pathology, machine learning, operations},
abstract = {Artificial intelligence (AI) and machine learning (ML) are transforming the field of medicine. Health care organizations are now starting to establish management strategies for integrating such platforms (AI-ML toolsets) that leverage the computational power of advanced algorithms to analyze data and to provide better insights that ultimately translate to enhanced clinical decision-making and improved patient outcomes. Emerging AI-ML platforms and trends in pathology and medicine are reshaping the field by offering innovative solutions to enhance diagnostic accuracy, operational workflows, clinical decision support, and clinical outcomes. These tools are also increasingly valuable in pathology research in which they contribute to automated image analysis, biomarker discovery, drug development, clinical trials, and productive analytics. Other related trends include the adoption of ML operations for managing models in clinical settings, the application of multimodal and multiagent AI to utilize diverse data sources, expedited translational research, and virtualized education for training and simulation. As the final chapter of our AI educational series, this review article delves into the current adoption, future directions, and transformative potential of AI-ML platforms in pathology and medicine, discussing their applications, benefits, challenges, and future perspectives.}
}
@article{MANDAVE2023100276,
title = {Bio-inspired computing algorithms in dementia diagnosis – a application-oriented review},
journal = {Results in Control and Optimization},
volume = {12},
pages = {100276},
year = {2023},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666720723000784},
author = {Deepa D. Mandave and Lalit V. Patil},
keywords = {Dementia, Biomotivated algorithms, Image segmentation, Meta-heuristic, Alzheimer, Optimization, Feature selection},
abstract = {Dementia is a major neurocognitive disease which affects memory, thinking skills, attitudes, and social behavior, extremely causing disturbances in daily routine activities and social activities. Alzheimer is the most general form of dementia in the elderly. Recently, biomotivated techniques have become famous in the domain of healthcare and have obtained appreciable success. This review shows that these techniques are mostly utilized to resolve various problems such as image segmentation, feature selection, classification, and optimization in the detection of various disorders like cancer, anemia, Alzheimer, kidney and skin diseases. It is observed that the dementia diagnosis was performed using classical approaches which led to reduced performance (accuracy, precision). This performance parameter can be enhanced by using biomotivated techniques. This paper presents a comprehensive analysis of the different role of biomotivated metaheuristics in the domain of dementia diagnosis with a detailed analysis of published work. The results showed that a biomotivated technique plays an important role in dementia diagnosis.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{FURLAN2022163,
title = {The earth vibrates with analogies: The Dirac sea and the geology of the vacuum},
journal = {Studies in History and Philosophy of Science},
volume = {93},
pages = {163-174},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368122000590},
author = {Stefano Furlan and Rocco Gaudenzi},
keywords = {Analogies, Analogical thinking, Heuristics, History of quantum physics, Vacuum, Spontaneous symmetry breaking},
abstract = {The debate around analogy in modern physics that focuses on its role as a logical inference often correspondingly overlooks its historical dimension and the other equally important functions and aspects that are intertwined with this dimension. Inspired by a close investigation of the primary sources and archival material of a few historical actors, this paper lays out a framework on analogy-making which preserves as much as possible its historical complexity. While not losing sight of the logical role, our framework puts a special emphasis on the heuristic process, and aims at offering to the historian and philosopher of science as well as the physicist some tools to capture the subtle functions of analogical reasoning involved in such a process. After having traced it out theoretically, we make use of this framework to interpret the growth of the ideas of two remarkable physicists dealing with the multifaceted notion of vacuum in 20th century physics. We first consider the trajectory followed by John A. Wheeler, between the 1960s and 1970s, towards (in his own words) a “geology of the vacuum”; and then examine, starting from the hitherto neglected Japanese reception of the idea of Dirac sea in the early 1930s, the pathway that led Yoichiro Nambu to the discovery of spontaneous symmetry breaking.}
}
@article{BEECH2023105401,
title = {Consequences of phonological variation for algorithmic word segmentation},
journal = {Cognition},
volume = {235},
pages = {105401},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000355},
author = {Caroline Beech and Daniel Swingley},
keywords = {Language acquisition, Computational modeling, Word segmentation, Phonological variation},
abstract = {Over the first year, infants begin to learn the words of their language. Previous work suggests that certain statistical regularities in speech could help infants segment the speech stream into words, thereby forming a proto-lexicon that could support learning of the eventual vocabulary. However, computational models of word segmentation have typically been tested using language input that is much less variable than actual speech is. We show that using actual, transcribed pronunciations rather than dictionary pronunciations of the same speech leads to worse segmentation performance across models. We also find that phonologically variable input poses serious problems for lexicon building, because even correctly segmented word forms exhibit a complex, many-to-many relationship with speakers' intended words. Many phonologically distinct word forms were actually the same intended word, and many identical transcriptions came from different intended words. The fact that previous models appear to have substantially overestimated the utility of simple statistical heuristics suggests a need to consider the formation of the lexicon in infancy differently.}
}
@article{YANG2023414,
title = {A review of sequential three-way decision and multi-granularity learning},
journal = {International Journal of Approximate Reasoning},
volume = {152},
pages = {414-433},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2200192X},
author = {Xin Yang and Yanhua Li and Tianrui Li},
keywords = {Three-way decision, Granular computing, Sequential three-way decision, Three-way multi-granularity learning},
abstract = {The concept of three-way decision, interpreted and described as thinking, problem solving, and information processing in “threes”, has been widely studied and applied in machine learning and data engineering in recent years. In open-world environment, the connection and interaction of dynamic and uncertainty by multi-granularity learning gives more vitality to three-way decision. In this paper, we investigate and summarize the initial and development models of three-way decision. Then we revisit the historical line of sequential three-way decision from rough set to granular computing. Besides, we focus on exploring a unified framework of three-way multi-granularity learning with four crucial problems on mining uncertain region continually. Finally, we give some proposals on three-way decision associated with open-continual learning.}
}
@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}
@article{HILLERT2021103158,
title = {How did language evolve in the lineage of higher primates?},
journal = {Lingua},
volume = {264},
pages = {103158},
year = {2021},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S0024384121001303},
author = {Dieter Hillert},
keywords = {Broca’s area, Comparative studies, Homo erectus, Language capacity, Neural circuits, Prehistoric artefacts},
abstract = {Speech components emerged in the hominin lineage before the rise of modern human behavior and were already in place in monkey species. Evidence from genetics to archaeological records points to an accumulative increase of those computational properties required for modern language. At about 2.4 mya, the polytypical species Homo erectus sensu lato (s.l.) appeared with significant cortical growth indicated by neural migration factors and fossil skulls. The evidence suggests that early Homo erectus s.l. was equipped with a computational capacity for premodern language. The same species developed Acheulean toolmaking and showed signs of a symbolic and aesthetic mind at about half a mya. We conclude that the modern language capacity evolved at around 1 mya in the merging species late Homo erectus s.l. and pre-archaic Homo sapiens.}
}
@article{SCHACTER1999403,
title = {Computer-based performance assessments: a solution to the narrow measurement and reporting of problem-solving☆☆The findings and opinions expressed in this report do not reflect the position or policies of ISX, Advanced Research Projects Agency, the Department of the Navy, or the Department of Defense; nor do they reflect the positions or policies of the National Institute on Student Achievement, Curriculum, and Assessment, the Office of Educational Research and Improvement, or the US Department of Education.},
journal = {Computers in Human Behavior},
volume = {15},
number = {3},
pages = {403-418},
year = {1999},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(99)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563299000291},
author = {J. Schacter and H.E. Herl and G.K.W.K. Chung and R.A. Dennis and H.F. O'Neil},
keywords = {Assessment, Problem solving, Computers, Internet, Technology, Education},
abstract = {Although performance assessments test for higher order thinking and problem solving, they rarely report students' thinking process data back to teachers, students, or the public. Web-based database-backed performance assessments provide a viable means for concurrently reporting both performance and thinking process data. In the research conducted here, we report our findings from a study that assessed student problem solving using networked computers. Both performance and process data could be reported back to teachers and students such that they could diagnose and understand how they performed and what problem-solving processes contributed to or detracted from their performance.}
}
@article{DALLAT2019266,
title = {Risky systems versus risky people: To what extent do risk assessment methods consider the systems approach to accident causation? A review of the literature},
journal = {Safety Science},
volume = {119},
pages = {266-279},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517305295},
author = {Clare Dallat and Paul M. Salmon and Natassia Goode},
keywords = {Risk, Risk assessment, Risk assessment methods, Systems thinking},
abstract = {Accidents are now widely acknowledged to be a systems phenomenon. As part of a proactive approach to safety management, organisations use risk assessment methods to identify the hazards and associated risks that may lead to accidents. Although there is an extensive body of literature on the need for a systems thinking approach in accident analysis, little has been said regarding the theoretical underpinnings of risk assessment methods. The aim of this paper was to systematically review the risk assessment methods presented in the literature and evaluate the extent to which they are underpinned by a systems thinking approach. A total of 342 methods spanning a range of safety-critical domains were evaluated using Rasmussen’s tenets of accident causation. A key finding is that the majority of existing risk assessment methods are not consistent with Rasmussen’s model of accident causation (arguably the most popular model in safety science circles). Instead, the majority of risk assessment methods focus on risks at the so called sharp-end and largely view accidents as emerging from a linear, or chain-of-events process. This overlooks emergent risks at other levels of the system, including supervisory, managerial, regulatory and government levels. The findings therefore suggest that the majority of existing risk assessment methods may be inadequate for identifying hazards and analysing risks within complex sociotechnical systems. The implications for risk assessment practice are discussed.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{MOLINARO20231150,
title = {A goal-centric outlook on learning},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1150-1164},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002073},
author = {Gaia Molinaro and Anne G.E. Collins},
keywords = {goals, learning, decision-making, reinforcement learning, rewards, abstraction, motivation, computational modeling},
abstract = {Goals play a central role in human cognition. However, computational theories of learning and decision-making often take goals as given. Here, we review key empirical findings showing that goals shape the representations of inputs, responses, and outcomes, such that setting a goal crucially influences the central aspects of any learning process: states, actions, and rewards. We thus argue that studying goal selection is essential to advance our understanding of learning. By following existing literature in framing goal selection within a hierarchy of decision-making problems, we synthesize important findings on the principles underlying goal value attribution and exploration strategies. Ultimately, we propose that a goal-centric perspective will help develop more complete accounts of learning in both biological and artificial agents.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{WANG2022e09982,
title = {Applying the post-digital strategy of anexact architecture to non-standard design practices within the challenging construction contexts},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e09982},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09982},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012701},
author = {Sining Wang and Dandan Lin},
keywords = {Design practice strategy, Post-digital architecture, Parametric design, Developing region, Non-standard architecture},
abstract = {New architectural forms offered by digital design approaches often appear incompatible with the prescribed precision and control in construction, especially in developing regions where advanced implementation means are limited. In response, this paper suggests working with design practice indeterminacy. Named ‘anexact architecture’, the post-digital design practice strategy presents a convergent diagram of seeking the feasible design solution space. It relies on the procedural parametric modelling to constantly integrate computation and humanisation, so that a rigorous built outcome is capable of accommodating project-specific idiosyncrasies and constraints. The demonstrator projects are discussed based on the combination of the Participatory Action Research method and the idea of anexact architecture. This paper aims to illustrate the peculiarity of anexact architecture and its ideology of treating design delivery uncertainties as essentials rather than negatives when practicing in a volatile construction context.}
}
@article{SURYARAJ2024124407,
title = {Block based motion estimation model using CNN with representative point matching algorithm for object tracking in videos},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124407},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124407},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424012739},
author = {C.K. Suryaraj and M.R. Geetha},
keywords = {Motion Estimation, Object Tracking, CNN, RPM, SSIM, Video Sequence, Computation Time},
abstract = {Motion estimation is considered significant for tracking the movement of an object in video sequences, and it is widely used in various video processing applications. Traditionally, many researchers focus on pixel-based motion estimation for object tracking, but it experienced increased computation time and cost. To reduce computation time, the utilization of a block-based motion estimation approach for object tracking is a recent trend. The existing block-based approach faces difficulty in finding representative points within the intensity domain. Therefore, this current research merged the deep learning approach with a block-matching algorithm for achieving efficient object tracking. In this proposed work, initially, video sequences are collected from a benchmark video dataset. Then, the acquired video sequences are segmented into frames. From the segmented frames, current and previous frames are considered for motion estimation. Frames are sent for the data augmentation process in which the process of flipping, cropping, and rotation is carried out. Then, the augmented frames are sent into Convolutional Neural Network (CNN) for feature extraction. Representative Point Matching (RPM) is used to estimate the motion vector based on the extracted features. After estimating the motion vector, the similarity between two consecutive frames is found using Structural Similarity Index (SSIM) technique. Finally, based on the similarity score, the movement of an object in the video is tracked effectively. Simulation analysis of the proposed block-based motion estimation model is done by evaluating some performance metrics. RMSE, PSNR, Execution Time, SSIM, and accuracy obtained for the proposed model are 27.5, 26.5 db, 31 sec, 0.91, and 94 %. This analysis suggested that the proposed CNN-RPM motion estimation model performs better in tracking the movement of the object.}
}
@article{GREENSPAN1990490,
title = {A counterexample of the use of energy as a measure of computational accuracy},
journal = {Journal of Computational Physics},
volume = {91},
number = {2},
pages = {490-494},
year = {1990},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(90)90051-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999190900512},
author = {Donald Greenspan}
}
@article{BOVE20031040,
title = {Computational fluid dynamics in the evaluation of hemodynamic performance of cavopulmonary connections after the norwood procedure for hypoplastic left heart syndrome},
journal = {The Journal of Thoracic and Cardiovascular Surgery},
volume = {126},
number = {4},
pages = {1040-1047},
year = {2003},
issn = {0022-5223},
doi = {https://doi.org/10.1016/S0022-5223(03)00698-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022522303006986},
author = {Edward L. Bove and Marc R. {de Leval} and Francesco Migliavacca and Gualtiero Guadagni and Gabriele Dubini},
keywords = {17, 21},
abstract = {Objective
Computational fluid dynamics have been used to study the hemodynamic performance of surgical operations, resulting in improved design. Efficient designs with minimal energy losses are especially important for cavopulmonary connections. The purpose of this study was to compare hydraulic performance between the hemi-Fontan and bidirectional Glenn procedures, as well as the various types of completion Fontan operations.
Methods
Three-dimensional models were constructed of typical hemi-Fontan and bidirectional Glenn operations according to anatomic data derived from magnetic resonance scans, angiocardiograms, and echocardiograms. Boundary conditions were imposed, and fluid dynamics were calculated from a mathematic code. Power losses, flow distribution to each lung, and pressures were measured at three predetermined levels of pulmonary arteriolar resistance. Models of the lateral tunnel, total cavopulmonary connection, and extracardiac conduit completion Fontan operations were constructed, and power losses, total flow distribution, vena caval and pulmonary arterial pressures, and flow distribution of inferior vena caval return were calculated.
Results
The hemi-Fontan and bidirectional Glenn procedures performed nearly identically, with similar power losses and nearly equal flow distributions to each lung at all levels of pulmonary arteriolar resistance. However, the lateral tunnel Fontan procedure as performed after the hemi-Fontan operation had lower power losses (6.9 mW, pulmonary arteriolar resistance 3 units) than the total cavopulmonary connection (40.5 mW) or the extracardiac conduit (42.9 mW), although the inclusion of an enlargement patch toward the right in the total cavopulmonary connection was effective in reducing the difference (10.0 mW). Inferior vena caval flow to the right lung was 52% for the lateral tunnel, compared with 19%, 30%, 19%, and 15% for the total cavopulmonary connection, total cavopulmonary connection with right-sided enlargement patch, extracardiac conduit, and extracardiac conduit with a bevel to the left lung, respectively.
Conclusions
According to these methods, the hemi-Fontan and bidirectional Glenn procedures performed equally well, but important differences in energy losses and flow distribution were found after the completion Fontan procedures. The superior hydraulic performance of the lateral tunnel Fontan operation after the hemi-Fontan procedure relative to any other method may be due to closer to optimal caval offset achieved in the surgical reconstruction.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@article{SENVAR20161140,
title = {Hospital Site Selection via Hesitant Fuzzy TOPSIS},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1140-1145},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.656},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309296},
author = {Ozlem Senvar and Irem Otay and Eda Bolturk},
keywords = {Facility Layout, Location Selection, Multi criteria decision making (MCDM), TOPSIS, Hesitant fuzzy set (HFS)},
abstract = {This study handles the problem of establishing a well-organized and distributed network of a hospital that delivers its services to the target population. We propose a new multi criteria decision making (MCDM) process that integrates hesitant fuzzy sets (HFSs) to Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The MCDM process defined under uncertainties are perfectly defined by HFSs reflecting comprehensively hesitant thinking of decision makers. Our proposed methodology is implemented to select the optimum site for a new hospital in Istanbul.}
}
@article{BAUSO201776,
title = {Consensus via multi-population robust mean-field games},
journal = {Systems & Control Letters},
volume = {107},
pages = {76-83},
year = {2017},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167691117301287},
author = {D. Bauso},
keywords = {Synchronization, Consensus, Mean-field games},
abstract = {In less prescriptive environments where individuals are told ‘what to do’ but not ‘how to do’, synchronization can be a byproduct of strategic thinking, prediction, and local interactions. We prove this in the context of multi-population robust mean-field games. The model sheds light on a multi-scale phenomenon involving fast synchronization within the same population and slow inter-cluster oscillation between different populations.}
}
@article{BERNUS201583,
title = {Enterprise architecture: Twenty years of the GERAM framework},
journal = {Annual Reviews in Control},
volume = {39},
pages = {83-93},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000097},
author = {Peter Bernus and Ovidiu Noran and Arturo Molina},
abstract = {Apart from the 20-year anniversary in 2014 of the first publication of the GERAM (‘Generalised Enterprise Reference Architecture and Methodology’) Enterprise Architecture Framework, the timeliness of this paper lies in the new interest in the use of systems theory in enterprise architecture (EA), and consequently, ‘light-weight’ architecture frameworks (AFs). Thus, this paper is about the use of systems thinking and systems theory in EA and about how it is possible to reconcile and understand, based on a single overarching framework, the interplay of two major enterprise change endeavours: on one hand enterprise engineering (i.e. deliberate change) and on the other hand evolutionary, organic change. The paper also demonstrates how such change processes can be illustrated by employing systems thinking to construct dynamic business models; the evolution of these concepts is exemplified using past applications in networked enterprise building, and more recent proposals in environmental-, disaster- and healthcare management. Finally, the paper attempts to plot the way GERAM, as a framework to think about the creation and evolution of complex socio-technical systems, will continue to contribute to the society in the context of future challenges and emerging opportunities.}
}
@article{ZHANG2022104545,
title = {Watching a hands-on activity improves students’ understanding of randomness},
journal = {Computers & Education},
volume = {186},
pages = {104545},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001166},
author = {Icy (Yunyi) Zhang and Mary C. Tucker and James W. Stigler},
keywords = {Hands-on demonstration, Computer simulation, Statistics education, Multimedia learning, Online instruction, Instructional sequence, Embodied cognition},
abstract = {Introductory statistics students struggle to understand randomness as a data generating process, and especially its application to the practice of data analysis. Although modern computational techniques for data analysis such as simulation, randomization, and bootstrapping have the potential to make the idea of randomness more concrete, representing such random processes with R code is not as easy for students to understand as is something like a coin-flip, which is both concrete and embodied. In this study, in the context of multimedia learning, we designed and tested the efficacy of an instructional sequence that preceded computational simulations with embodied demonstrations. We investigated the role that embodied hands-on movement might play in facilitating students’ understanding of the shuffle function in R. Our findings showed that students who watched a video of hands shuffling data written on pieces of paper learned more from a subsequent live-coding demonstration of randomization using R than did students only introduced to the concept using R. Although others have found an advantage of students themselves engaging in hands-on activities, this study showed that merely watching someone else engage can benefit learning. Implications for online and remote instruction are discussed.}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{BELLA2023100509,
title = {Circular dichroism simulations of chiral buckybowls by means curvature analyses},
journal = {FlatChem},
volume = {40},
pages = {100509},
year = {2023},
issn = {2452-2627},
doi = {https://doi.org/10.1016/j.flatc.2023.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2452262723000417},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Buckybowl, Chirality, Curvature, TD-DFT, Circular dichroism},
abstract = {A detailed understanding and interpretation of chiral properties of molecular systems, especially in condensed phase, often requires computational models that allow their structural and electronic features to be connected to the observed experimental spectra. The present paper is focused on modelling the circular dichroism spectra of chiral buckybowls, combining topological aspects and the density functional theory. For the first time Ball Pivoting Algorithm was proposed to hook up the chemical topology to the DFT through the surface reconstruction. Particularly, the gaussian curvature of a constructed probe set of corannulene and sumanene derivatives was used as discriminant parameter to benchmark a list of 10 functionals (B3LYP, B97D, M06-2X, HSEH1PBE, wB97XD, CAM-B3LYP, LC-wPBE, TPSSTPSS, mPW1PW91 and APFD). The latter provide to be noticeably accurate to reproduce the curvature effect of the considered molecules. A TD-DFT/BOMD mixed approach provided a comprehensive overview of the spectral chiral pattern prediction trends when multiple DFT functionals are scanned. The preliminary topological analysis efforts were then recompensed with the very precise computed CD spectra, again APFD confirmed as the leader functional, this time for TD-DFT vertical transition calculations. Therefore, we strongly recommend the use of the of dispersion embedded APFD functional coupled with the 6–311++G(2d,2p) basis set for the computation of the functionalized chiral buckybowls ECD spectra. © 2017 Elsevier Inc. All rights reserved.}
}
@article{VIGNAPIANO201999,
title = {Disorganization and cognitive impairment in schizophrenia: New insights from electrophysiological findings},
journal = {International Journal of Psychophysiology},
volume = {145},
pages = {99-108},
year = {2019},
note = {The Neurophysiology of Schizophrenia: A Critical Update},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S016787601830984X},
author = {Annarita Vignapiano and Thomas Koenig and Armida Mucci and Giulia M. Giordano and Antonella Amodio and Mario Altamura and Antonello Bellomo and Roberto Brugnoli and Giulio Corrivetti and Giorgio {Di Lorenzo} and Paolo Girardi and Palmiero Monteleone and Cinzia Niolu and Silvana Galderisi and Mario Maj},
keywords = {Disorganization dimension, Difficulty in abstract thinking, Neurocognitive domains, Alpha rhythm, Spectral power, Topographic analysis},
abstract = {In subjects with schizophrenia (SCZ), the disorganization dimension is a strong predictor of real-life functioning. “Conceptual disorganization” (P2), “Difficulty in abstract thinking” (N5) and “Poor attention” (G11) are core features of the disorganization factor, evaluated using the Positive and Negative Syndrome Scale. The heterogeneity of this dimension and its overlap with neurocognitive deficits are still debated. Within the multicenter study of the Italian Network for Research on Psychoses, we investigated electrophysiological and neurocognitive correlates of disorganization and its component items to assess the heterogeneity of this dimension and its possible overlap with neurocognitive deficits. Resting state EEG was recorded in 145 stabilized SCZ and 69 matched healthy controls (HC). Spectral amplitude was averaged in ten frequency bands. Neurocognitive domains were assessed by MATRICS Consensus Cognitive Battery (MCCB). RAndomization Graphical User software explored band spectral amplitude differences between groups and correlations with disorganization and MCCB scores in SCZ. Correlations between disorganization and MCCB scores were also investigated. Compared to HC, SCZ showed increased delta, theta, and beta 1 and decreased alpha 2 activity. A negative correlation between alpha 1 and disorganization was observed in SCZ. At the item level, only “N5” showed the same correlation. MCCB neurocognitive composite score was associated with disorganization, “P2” and “N5”. Our findings suggest only a partial overlap between disorganization and neurocognitive impairment. The association of alpha 1 with the “N5” item suggests that some aspects of disorganization could be underpinned by the impairment of basic neurobiological functions that are only partially evaluated using MCCB.}
}
@article{CALEFFI2024110672,
title = {Distributed quantum computing: A survey},
journal = {Computer Networks},
volume = {254},
pages = {110672},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110672},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624005048},
author = {Marcello Caleffi and Michele Amoretti and Davide Ferrari and Jessica Illiano and Antonio Manzalini and Angela Sara Cacciapuoti},
keywords = {Quantum internet, Quantum networks, Quantum communications, Quantum computing, Quantum computation, Distributed quantum computing, Quantum algorithms, Quantum compiler, Quantum compiling, Simulator},
abstract = {Nowadays, quantum computing has reached the engineering phase, with fully-functional quantum processors integrating hundreds of noisy qubits. Yet – to fully unveil the potential of quantum computing out of the labs into the business reality – the challenge ahead is to substantially scale the qubit number, reaching orders of magnitude exceeding thousands of fault-tolerant qubits. To this aim, the distributed quantum computing paradigm is recognized as the key solution for scaling the number of qubits. Indeed, accordingly to such a paradigm, multiple small-to-moderate-scale quantum processors communicate and cooperate for executing computational tasks exceeding the computational power of single processing devices. The aim of this survey is to provide the reader with an overview about the main challenges and open problems arising with distributed quantum computing from a computer and communications engineering perspective. Furthermore, this survey provides an easy access and guide towards the relevant literature and the prominent results in the field.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{TRAGER20241555,
title = {The human touch: Utilizing AlphaFold 3 to analyze structures of endogenous metabolons},
journal = {Structure},
volume = {32},
number = {10},
pages = {1555-1562},
year = {2024},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0969212624003356},
author = {Toni K. Träger and Christian Tüting and Panagiotis L. Kastritis},
abstract = {Summary
Computational structural biology aims to accurately predict biomolecular complexes with AlphaFold 3 spearheading the field. However, challenges loom for structural analysis, especially when complex assemblies such as the pyruvate dehydrogenase complex (PDHc), which catalyzes the link reaction in cellular respiration, are studied. PDHc subcomplexes are challenging to predict, particularly interactions involving weaker, lower-affinity subcomplexes. Supervised modeling, i.e., integrative structural biology, will continue to play a role in fine-tuning this type of prediction (e.g., removing clashes, rebuilding loops/disordered regions, and redocking interfaces). 3D analysis of endogenous metabolic complexes continues to require, in addition to AI, precise and multi-faceted interrogation methods.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{HALLOWELL2023100240,
title = {Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100240},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100240},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000240},
author = {Nina Hallowell and Shirlene Badger and Francis McKay and Angeliki Kerasidou and Christoffer Nellåker},
keywords = {Computational phenotyping, Rare disease, Diagnosis, AI, Qualitative interviews},
abstract = {Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic.}
}
@article{HEGG201856,
title = {Preservice teacher proficiency with transformations-based congruence proofs after a college proof-based geometry class},
journal = {The Journal of Mathematical Behavior},
volume = {51},
pages = {56-70},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301153},
author = {Meredith Hegg and Dimitri Papadopoulos and Brian Katz and Timothy Fukawa-Connelly},
keywords = {Mathematics teacher education, Transformational geometry, Proof},
abstract = {This report explores pre-service teachers’ proficiency with concepts of transformational geometry at the end of a semester-long advanced geometry course. In the course, the instructor incorporated transformational geometry content, including congruence proofs, in an attempt to prepare the pre-service teachers to teach high school geometry in alignment with the Common Core State Standards for Mathematics. At the conclusion of the course, students expressed a preference for using traditional triangle congruence criteria (SAS, ASA, SSS, and AAS) over using transformations to complete proofs, but were nevertheless generally successful in completing proofs using transformations. Similarly, while the students often described thinking of transformations in terms of analytic forms, they were successfully able to prove triangle congruences in synthetic contexts. Finally, some evidence indicates that students may have motion or process conceptions of transformations, but not map or object conceptions, but this evidence is not conclusive.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@article{IONESCU2014275,
title = {Embodied Cognition: Challenges for Psychology and Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {128},
pages = {275-280},
year = {2014},
note = {International Conference: EDUCATION AND PSYCHOLOGY CHALLENGES - TEACHERS FOR THE KNOWLEDGE SOCIETY – 2nd EDITION EPC – TKS 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.03.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814022472},
author = {Thea Ionescu and Dermina Vasc},
keywords = {embodiment, cognition, sensory-motor processes, action, education.},
abstract = {Embodied cognition considers that human cognition is fundamentally grounded in sensory-motor processes and in our body's morphology and internal states. In this paper, we discuss some of the features of this post-cognitivist approach and the challenges that follow for psychology and education. These challenges point to the need to reconsider cognition and the way we pursue education today. If we want to have an efficient educational system we have to look at fundamental research in cognitive science to have an accurate description of what cognition is. Only then can we design optimal educational settings for the development of thinking.}
}
@article{GUO2024324,
title = {Optimization of robot manipulator configuration calibration by using Zhang neural network for repetitive motion},
journal = {Applied Mathematical Modelling},
volume = {134},
pages = {324-348},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X24002853},
author = {Pengfei Guo and Yunong Zhang and Shuai Li and Ning Tan},
keywords = {Temporally dependent quadratic programming, Filtered reciprocal-kind Zhang neural network, Lyapunov stability, Robot manipulator configuration calibration},
abstract = {High precision and low complexity control algorithm plays an important role in the developing of the end-effector instrumentation of different robot manipulators. In order to reduce the kinetic energy and the high-speed drift phenomenon of the repetitive motion tracking task, the robot manipulator needs to calibrate its configuration. In this paper, we formulate the configuration calibration of the robot manipulator for the repetitive motion task as a future quadratic programming optimization problem constrained with equality constraints, which is also regarded as a fundamental problem in artificial intelligence and modern control engineering. Zhang neural network, which is a canonical method, can be adopted to deal with the continuous form of the future optimization problem, named as temporally dependent quadratic programming problem with equality constraints. In order to overcome the issue of temporally dependent inverse computing, a novel Zhang neural network model and its uncertain disturbance tolerant model, which are termed as filtered reciprocal-kind Zhang neural network model and uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network model, respectively, are proposed by integrating the energy-type cost function and Zhang neural network design formula for solving the temporally dependent quadratic programming problem with equality constraints in this paper. Based on the Euler discrete formula and the models, the discrete filtered reciprocal-kind Zhang neural network and the discrete uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network algorithms are proposed for solving the future quadratic programming problem with equality constraints and the robot manipulator configuration calibration problem of repetitive motion. The convergence properties of the reciprocal-kind Zhang neural network model and its corresponding uncertain disturbance tolerant model are obtained by Lyapunov stability theory of nonlinear system and its corresponding perturbed system, while the convergence property of the filtered reciprocal-kind Zhang neural network model is analyzed by the limit thinking. For the repetitive motion task, three experiments for solving the configuration calibration problem of PUMA560, Kinova Jaco2, and Franka Emika Panda robot manipulators are performed to illustrate the effectiveness, robustness and superiority of our proposed discrete filtered reciprocal-kind Zhang neural network algorithms.}
}
@article{FINGER2025101535,
title = {When kids juggle it all: Biliteracy instruction and the development of discourse connectedness in L1 and L2 writing},
journal = {Cognitive Development},
volume = {73},
pages = {101535},
year = {2025},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2024.101535},
url = {https://www.sciencedirect.com/science/article/pii/S0885201424001205},
author = {Ingrid Finger and Cristiane Ely Lemke and Larissa da Silva Cury and Natália Bezerra Mota and Janaina Weissheimer},
keywords = {Bilingual writing development, Discourse connectedness, Graph analysis, Narrative writing},
abstract = {The present longitudinal study explored how bilingual educational contexts shape children's cognitive and linguistic development. Its main goal was to investigate the development of discourse connectedness (measured by long-range connectedness - LSC) in written narratives in Portuguese (L1) and English (L2) by 78 children of a bilingual school in Brazil within a year span (from 2021 to 2022). Participants created a narrative in their L1 or L2 based on a sequence of five images, which were analyzed with the computational tool SpeechGraphs (Mota et al., 2014). Connectedness scores were expected to vary as a function of Language (L1, L2) and of Year of data collection (Time 1, Time 2), favoring, respectively, the L1 and Time 2. The results confirmed our hypotheses, with long-range recurrence (LSC) scores in the L1 narratives higher than in the L2 at both times of data collection. In addition, the longitudinal analysis revealed higher connectedness scores for narratives written in Time 2 in both languages. Overall, our findings indicate that the children's performance in terms of connectedness progressed in a parallel way in the two languages during the school years, with an expected advantage for the narratives written in their dominant language. In addition, they highlight the potential of using SpeechGraphs - a cost-effective, non-invasive computational tool - to analyze children's use of two prestige languages in a particular bilingual educational context.}
}
@article{RUTTEN20211,
title = {50 Years of Russian Literature: Mapping, Mixing, and Queering Slavic Literary Studies},
journal = {Russian Literature},
volume = {125-126},
pages = {1-8},
year = {2021},
note = {50 Years of Russian Literature & Teffi’s Theatrical & Cinematic Work},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000673},
author = {Ellen Rutten},
keywords = {Russian Literature, Editorial, Transdisciplinarity, Slavic Literary Studies, Transnational Academic Communication},
abstract = {Russian Literature turned fifty this year. In this editorial contribution, editor-in-chief Ellen Rutten reflects on the journal’s past, its current profile, and future editorial plans. As Rutten argues, Russian Literature has three distinguishing features. First, the journal has always generously invited other disciplines on board – and its transdisciplinary inclusivity has increased in recent years – while maintaining a steady gaze on Slavic literary studies. Second, the journal acts as a transnational and transcontinental scholarly contact zone – a status that cannot be isolated from our choice to publish both Anglophone and Russophone analyses. And third, Russian Literature brings together a range of scholarly voices and genres that is unusually broad for a scholarly periodical, through a strategy of active editorial outreach to young talents and leading experts in the field. Rutten concludes with a few words on upcoming volumes and plans, including new archival publications and volumes-in-the-making inspired by recent shifts in thinking about geopolitics, gender, and health and environment.}
}
@article{KOSCHINSKY2013172,
title = {The case for spatial analysis in evaluation to reduce health inequities},
journal = {Evaluation and Program Planning},
volume = {36},
number = {1},
pages = {172-176},
year = {2013},
note = {Special Section: Rethinking Evaluation of Health Equity Initiatives},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0149718912000237},
author = {Julia Koschinsky},
keywords = {Spatial analysis, Spatial perspective, Program evaluation, Evaluation, Health inequities, Realist evaluation, Randomized control trials (RCTs)},
abstract = {The article begins by giving an overview of spatial thinking concepts that are relevant to evaluation. The article relates the spatial perspective to both a realist evaluation and a randomized control trial perspective in evaluation to demonstrate the benefits of a spatialized program and evaluation perspective. The article mainly suggests that the adoption of a spatial perspective can add new insights to the theory and practice of evaluation in ways that helps evaluation move closer to reducing health inequities.}
}
@article{XIA2023100730,
title = {Understanding common human driving semantics for autonomous vehicles},
journal = {Patterns},
volume = {4},
number = {7},
pages = {100730},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100730},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000703},
author = {Yingji Xia and Maosi Geng and Yong Chen and Sudan Sun and Chenlei Liao and Zheng Zhu and Zhihui Li and Washington Yotto Ochieng and Panagiotis Angeloudis and Mireille Elhajj and Lei Zhang and Zhenyu Zeng and Bing Zhang and Ziyou Gao and Xiqun (Michael) Chen},
keywords = {human-machine interaction, neuroscience, hierarchical understanding abstraction, electroencephalography, neural-informed model, driving behavior perception, driving semantics, autonomous vehicle},
abstract = {Summary
Autonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomous transport systems is complete. The critical challenge of improving mutual understanding between both vehicle types cannot be addressed only by feeding extensive driving data into data-driven models but by enabling autonomous vehicles to understand and apply common driving behaviors analogous to human drivers. Therefore, we designed and conducted two electroencephalography experiments for comparing the cerebral activities of human linguistics and driving understanding. The results showed that driving activates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic understanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understand common human driving behavior in a brain-inspired manner. This study highlights the pathway of fusing neuroscience into complex human behavior understanding tasks and provides a computational neural model to understand human driving behaviors, which will enable autonomous vehicles to perceive and think like human drivers.}
}
@incollection{FROEHLICH2023685,
title = {Mixed methods and social network analysis},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {685-692},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110590},
author = {Dominik E. Froehlich},
keywords = {Data collection, Education research, Ethics, Mixed methods social network analysis, Mixed methods, Relational methods, Research design, Social network analysis, Structure},
abstract = {In this chapter, we discuss the application of mixed methods thinking to social network analysis, a methodological approach that focuses on social relationships and structures. For that purpose, we first define mixed methods and social network analysis and their intersection, which we call Mixed Methods Social Network Analysis (MMSNA). We then summarize the historical developments in social network analysis, which also explain the reason for the increasing application of MMSNA in educational research. The majority of the chapter then focuses on how MMSNA is applied in educational research and what the main topics of the current academic debates are.}
}
@article{THOMPSON2011107,
title = {Intuition, reason, and metacognition},
journal = {Cognitive Psychology},
volume = {63},
number = {3},
pages = {107-140},
year = {2011},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010028511000454},
author = {Valerie A. Thompson and Jamie A. {Prowse Turner} and Gordon Pennycook},
keywords = {Metacognition, Reasoning, Dual Process Theories, Intuition, Analytic thinking, Retrospective confidence},
abstract = {Dual Process Theories (DPT) of reasoning posit that judgments are mediated by both fast, automatic processes and more deliberate, analytic ones. A critical, but unanswered question concerns the issue of monitoring and control: When do reasoners rely on the first, intuitive output and when do they engage more effortful thinking? We hypothesised that initial, intuitive answers are accompanied by a metacognitive experience, called the Feeling of Rightness (FOR), which can signal when additional analysis is needed. In separate experiments, reasoners completed one of four tasks: conditional reasoning (N=60), a three-term variant of conditional reasoning (N=48), problems used to measure base rate neglect (N=128), or a syllogistic reasoning task (N=64). For each task, participants were instructed to provide an initial, intuitive response to the problem along with an assessment of the rightness of that answer (FOR). They were then allowed as much time as needed to reconsider their initial answer and provide a final answer. In each experiment, we observed a robust relationship between the FOR and two measures of analytic thinking: low FOR was associated with longer rethinking times and an increased probability of answer change. In turn, FOR judgments were consistently predicted by the fluency with which the initial answer was produced, providing a link to the wider literature on metamemory. These data support a model in which a metacognitive judgment about a first, initial model determines the extent of analytic engagement.}
}
@article{KIRIMTAY2025111785,
title = {Tau and MAP6 establish labile and stable domains on microtubules},
journal = {iScience},
volume = {28},
number = {3},
pages = {111785},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111785},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225000446},
author = {Koray Kirimtay and Wenqiang Huang and Xiaohuan Sun and Liang Qiang and Dong V. Wang and Calvin T. Sprouse and Erin M. Craig and Peter W. Baas},
keywords = {Cell Biology, Cellular neuroscience},
abstract = {Summary
We previously documented that individual microtubules in the axons of cultured juvenile rodent neurons consist of a labile domain and a stable domain and that experimental depletion of tau results in selective shortening and partial stabilization of the labile domain. After first confirming these findings in adult axons, we sought to understand the mechanism that accounts for the formation and maintenance of these microtubule domains. We found that fluorescent tau and MAP6 ectopically expressed in RFL-6 fibroblasts predominantly segregate on different microtubules or different domains on the same microtubule, with the tau-rich ones becoming more labile than in control cells and the MAP6-rich ones being more stable than in control cells. These and other experimental findings, which we studied further using computational modeling with tunable parameters, indicate that these two MAPs do not merely bind to pre-existing stable and labile domains but actually create stable and labile domains on microtubules.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{NAGLE2019100684,
title = {Using APOS theory as a framework for considering slope understanding},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100684},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318301469},
author = {Courtney Nagle and Rafael Martínez-Planell and Deborah Moore-Russo},
keywords = {APOS, Slope, APOS levels between stages, Precalculus, Rate of change, Totality},
abstract = {In this paper a framework for slope is proposed using APOS (Action-Process-Object-Schema) Theory and conceptualizations of slope previously identified in research. The proposed APOS-slope framework allows for discussion of students’ cognitive development in relation to different conceptualizations of slope. As such, it may be adopted as a means to advance future research or as a way to plan instruction. In particular, the framework uses specific examples to consider interrelations between the ways of thinking about slope that have been reported to provide additional insight on how individuals understand this concept. The proposed framework contributes to the field by bringing together a number of past studies related to slope and providing a common ground under which these works might be interpreted.}
}
@article{SENAPATI202449,
title = {Oxymoron: An Automatic Detection from the Corpus},
journal = {Procedia Computer Science},
volume = {244},
pages = {49-56},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029788},
author = {Apurbalal Senapati},
keywords = {Oxymoron, Antonymy, Corpus, Computational linguistic, Natural Language Processing, Bengali language},
abstract = {An oxymoron is a linguistic phenomenon in which a pair of opposite or antonymous words are combined to convey a new meaning. Sometimes, it is used to express figurative, irony, or rhetoric within the text. This issue has received relatively less attention in the realms of linguistics and computational disciplines. Oxymorons play a significant role in various language-processing applications. This study represents a pioneering effort in the exploration of oxymorons in the Bengali language. A corpus-based study of oxymoron is a fundamental issue that has not been explored so far. A system has been proposed for the automated recognition of oxymorons from a given corpus. Frequency analysis, semantic similarity, and an antonym dictionary have been employed to discern oxymorons within the corpus. The system achieved promising results when tested on a Bengali corpus, and found 308 distinct oxymorons. A corpus-based descriptive statistics is measured in two different corpora. The most common oxymorons are ranked based on their frequency. Their notable presence underscores the importance of the Bengali language. This study aimed to explore fundamental questions concerning oxymorons, such as the automated detection of oxymorons within a corpus, descriptive statistics regarding oxymorons across languages, and the process of their construction and creation. Additionally, efforts were made to extract oxymorons from large language models using zero-shot prompts, but the results were not as promising compared to our proposed system.}
}
@article{CADART2025113107,
title = {An optimal penalty method for the joint stiffening in beam models of additively manufactured lattice structures},
journal = {International Journal of Solids and Structures},
volume = {306},
pages = {113107},
year = {2025},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2024.113107},
url = {https://www.sciencedirect.com/science/article/pii/S0020768324004669},
author = {T. Cadart and T. Hirschler and S. Bahi and S. Roth and F. Demoly and N. Lebaal},
keywords = {Lattice structure, Beam formulation, Penalty method, Joint stiffening, Optimization, Additive manufacturing, Material jetting},
abstract = {Additive manufacturing is revolutionizing structural design, with lattice structures becoming increasingly prominent due to their superior mechanical properties. However, simulating these structures quickly and accurately using the finite element method (FEM) remains challenging. Recent research has highlighted beam element simulation within FEM as a more efficient alternative to traditional solid FE simulations, achieving similar accuracy with reduced computational resources. However, a significant challenge is managing the lack of rigidity at nodes and the prevalence of low aspect ratio beams. While various methodologies have been proposed to address these issues, there is still a gap in the comprehensive evaluation of their limitations. An optimal node penalization methodology is required to expand the limited range of accurately represented lattice behavior. A preliminary study investigates lattice geometries through comparative analysis of solid and beam FE simulations. Built on this, we developed a methodology suitable to linear, dynamics and nonlinear beam FE simulations, contributing to enhanced computational speed and accuracy. Several lattice structures were printed using material jetting and quasi-static compressive tests were conducted to validate the methodology’s accuracy. The numerical results reveal a good accuracy between the proposed beam FE methodology and the experimental data, offering a better alternative to conventional FEM for energy absorption in terms of computing time.}
}
@article{LU2022100056,
title = {Nonlinear EEG signatures of mind wandering during breath focus meditation},
journal = {Current Research in Neurobiology},
volume = {3},
pages = {100056},
year = {2022},
issn = {2665-945X},
doi = {https://doi.org/10.1016/j.crneur.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2665945X22000298},
author = {Yiqing Lu and Julio Rodriguez-Larios},
keywords = {EEG, Mind wandering, Meditation, Complexity, Nonlinear analysis},
abstract = {In meditation practices that involve focused attention to a specific object, novice practitioners often experience moments of distraction (i.e., mind wandering). Previous studies have investigated the neural correlates of mind wandering during meditation practice through Electroencephalography (EEG) using linear metrics (e.g., oscillatory power). However, their results are not fully consistent. Since the brain is known to be a chaotic/nonlinear system, it is possible that linear metrics cannot fully capture complex dynamics present in the EEG signal. In this study, we assess whether nonlinear EEG signatures can be used to characterize mind wandering during breath focus meditation in novice practitioners. For that purpose, we adopted an experience sampling paradigm in which 25 participants were iteratively interrupted during meditation practice to report whether they were focusing on the breath or thinking about something else. We compared the complexity of EEG signals during mind wandering and breath focus states using three different algorithms: Higuchi's fractal dimension (HFD), Lempel-Ziv complexity (LZC), and Sample entropy (SampEn). Our results showed that EEG complexity was generally reduced during mind wandering relative to breath focus states. We conclude that EEG complexity metrics are appropriate to disentangle mind wandering from breath focus states in novice meditation practitioners, and therefore, they could be used in future EEG neurofeedback protocols to facilitate meditation practice.}
}
@article{KORN2023102578,
title = {Navigating large chemical spaces in early-phase drug discovery},
journal = {Current Opinion in Structural Biology},
volume = {80},
pages = {102578},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X23000520},
author = {Malte Korn and Christiane Ehrt and Fiorella Ruggiu and Marcus Gastreich and Matthias Rarey},
abstract = {The size of actionable chemical spaces is surging, owing to a variety of novel techniques, both computational and experimental. As a consequence, novel molecular matter is now at our fingertips that cannot and should not be neglected in early-phase drug discovery. Huge, combinatorial, make-on-demand chemical spaces with high probability of synthetic success rise exponentially in content, generative machine learning models go hand in hand with synthesis prediction, and DNA-encoded libraries offer new ways of hit structure discovery. These technologies enable to search for new chemical matter in a much broader and deeper manner with less effort and fewer financial resources. These transformational developments require new cheminformatics approaches to make huge chemical spaces searchable and analyzable with low resources, and with as little energy consumption as possible. Substantial progress has been made in the past years with respect to computation as well as organic synthesis. First examples of bioactive compounds resulting from the successful use of these novel technologies demonstrate their power to contribute to tomorrow's drug discovery programs. This article gives a compact overview of the state-of-the-art.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{NISSEL2024105856,
title = {Why wearing a yellow hat is impossible: Chinese and U.S. children's possibility judgments},
journal = {Cognition},
volume = {251},
pages = {105856},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105856},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001422},
author = {Jenny Nissel and Jiaying Xu and Lihanjing Wu and Zachary Bricken and Jennifer M. Clegg and Hui Li and Jacqueline D. Woolley},
keywords = {Cognitive development, Social development, Possibility, Intuitive theories, Cross-cultural, LIWC},
abstract = {When thinking about possibility, one can consider both epistemic and deontic principles (i.e., physical possibility and permissibility). Cultural influences may lead individuals to weigh epistemic and deontic obligations differently; developing possibility conceptions are therefore positioned to be affected by cultural surroundings. Across two studies, 251 U.S. and Chinese 4-, 6-, and 8-year-olds sampled from major metropolitan areas in Texas and the Hubei, Sichuan, Gansu, and Guangdong Provinces judged the possibility of impossible, improbable, and ordinary events. Across cultures and ages, children judged ordinary events as possible and impossible events as impossible; cultural differences emerged in developing conceptions of improbable events. Whereas U.S. children became more likely to judge these events possible with age, Chinese children's judgments remained consistent with age: Chinese 4- to 8-year-olds judged these events to be possible ∼25% of the time. In Study 2, to test whether this difference was attributable to differential prioritization of epistemic versus deontic constraints, children also judged whether each event was an epistemic violation (i.e., required magic to happen) and a deontic violation (i.e., would result in someone getting in trouble). With age, epistemic judgments were increasingly predictive of possibility judgments for improbable events for U.S. children, and decreasingly so for Chinese children. Contrary to our predictions, deontic judgments were not predictive. We propose that cultural valuation of norms might shape children's developing intuitions about possibility. We discuss our findings in light of three accounts of possibility conceptions, suggesting ways to integrate cultural context into each.}
}
@article{SINGH2024101269,
title = {An empirical approach to understand the role of emotions in code comprehension},
journal = {Journal of Computer Languages},
volume = {79},
pages = {101269},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101269},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000121},
author = {Divjot Singh and Ashutosh Mishra and Ashutosh Aggarwal},
keywords = {Code comprehension, Systematic literature review, Emotions, Cognitive skills},
abstract = {Programming and cognitive skills are two pivotal abilities of programmers to maintain software products. First, this study included a systematic literature review on code comprehension, emotions, cognitive psychology, and belief-desire-intention domains to analyse various code comprehension monitoring techniques, performance metrics, and computational methodologies. Second, a case study is conducted to examine the influence of various emotional stages on programmers’ programming and cognitive skills while comprehending the software code. The categorization of the participants is done empirically based on their expertism level, and the same results are verified using various machine learning models and performance metrics.}
}
@article{RAHARINIRINA2021100332,
title = {Inferring gene regulatory networks from single-cell RNA-seq temporal snapshot data requires higher-order moments},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100332},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100332},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100180X},
author = {N. Alexia Raharinirina and Felix Peppert and Max {von Kleist} and Christof Schütte and Vikram Sunkara},
keywords = {single cell, RNA sequencing, time-course snapshots, Markov chains, chemical master equation, moment equations},
abstract = {Summary
Single-cell RNA sequencing (scRNA-seq) has become ubiquitous in biology. Recently, there has been a push for using scRNA-seq snapshot data to infer the underlying gene regulatory networks (GRNs) steering cellular function. To date, this aspiration remains unrealized due to technical and computational challenges. In this work we focus on the latter, which is under-represented in the literature. We took a systemic approach by subdividing the GRN inference into three fundamental components: data pre-processing, feature extraction, and inference. We observed that the regulatory signature is captured in the statistical moments of scRNA-seq data and requires computationally intensive minimization solvers to extract it. Furthermore, current data pre-processing might not conserve these statistical moments. Although our moment-based approach is a didactic tool for understanding the different compartments of GRN inference, this line of thinking—finding computationally feasible multi-dimensional statistics of data—is imperative for designing GRN inference methods.}
}
@article{ENGLAND2008163,
title = {Rattling the cage: computational models of chaperonin-mediated protein folding},
journal = {Current Opinion in Structural Biology},
volume = {18},
number = {2},
pages = {163-169},
year = {2008},
note = {Theory and simulation / Macromolecular assemblages},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2007.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X08000067},
author = {Jeremy England and Del Lucent and Vijay Pande},
abstract = {Chaperonins are known to maintain the stability of the proteome by facilitating the productive folding of numerous misfolded or aggregation-prone proteins and are thus essential for cell viability. Despite their established importance, the mechanism by which chaperonins facilitate protein folding remains unknown. Computer simulation techniques are now being employed to complement experimental ones in order to shed light on this mystery. Here we review previous computational models of chaperonin-mediated protein folding in the context of the two main hypotheses for chaperonin function: iterative annealing and landscape modulation. We then discuss new results pointing to the importance of solvent (a previously neglected factor) in chaperonin activity. We conclude with our views on the future role of simulation in studying chaperonin activity as well as protein folding in other biologically relevant confined contexts.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@article{NA2023105139,
title = {Towards a neurocomputational account of social controllability: From models to mental health},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {148},
pages = {105139},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105139},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423001082},
author = {Soojung Na and Shawn A. Rhoads and Alessandra N.C. Yu and Vincenzo G. Fiore and Xiaosi Gu},
keywords = {Social controllability, Computational psychiatry, Reinforcement learning, Model-based learning, Model-free learning, Cognitive map},
abstract = {Controllability, or the influence one has over their surroundings, is crucial for decision-making and mental health. Traditionally, controllability is operationalized in sensorimotor terms as one’s ability to exercise their actions to achieve an intended outcome (also termed “agency”). However, recent social neuroscience research suggests that humans also assess if and how they can exert influence over other people (i.e., their actions, outcomes, beliefs) to achieve desired outcomes ("social controllability”). In this review, we will synthesize empirical findings and neurocomputational frameworks related to social controllability. We first introduce the concepts of contextual and perceived controllability and their respective relevance for decision-making. Then, we outline neurocomputational frameworks that can be used to model social controllability, with a focus on behavioral economic paradigms and reinforcement learning approaches. Finally, we discuss the implications of social controllability for computational psychiatry research, using delusion and obsession-compulsion as examples. Taken together, we propose that social controllability could be a key area of investigation in future social neuroscience and computational psychiatry research.}
}
@article{LEUNG2020345,
title = {Limited cognitive ability and selective information processing},
journal = {Games and Economic Behavior},
volume = {120},
pages = {345-369},
year = {2020},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825620300063},
author = {Benson Tsz Kin Leung},
keywords = {Limited ability, Information overload, Information avoidance, Confirmation bias, Wishful thinking, Polarization},
abstract = {This paper studies the information processing behavior of a decision maker (DM) who can only process a subset of all information he receives: before taking an action, the DM receives sequentially a number of signals and decides whether to process or ignore each of them as it is received. The model generates an information processing behavior consistent with that documented in the psychological literature: first, the DM chooses to process signals that are strong; second, his processing strategy exhibits confirmation bias if he has a strong prior belief; third, he tends to process signals that suggest favorable outcomes (wishful thinking). As an application I analyze how the Internet and the induced change in information availability affects the processing behavior of the DM. I show that providing more/better information to the DM could strengthen his confirming bias.}
}
@article{ORJI2022100626,
title = {Assessing the pre-conditions for the pedagogical use of digital tools in the Nigerian higher education sector},
journal = {The International Journal of Management Education},
volume = {20},
number = {2},
pages = {100626},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100626},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000283},
author = {Ifeyinwa Juliet Orji and Frank Ojadi and Ukoha Kalu Okwara},
keywords = {Digitalization, Higher education, TOE theory, Social media, Learning outcomes, Nigeria},
abstract = {Currently, there is a burgeoning interest in digitalization as evidenced in extant literature. Nevertheless, the effect, based on teachers’ own perspectives, of the pedagogical use of digital technologies on learning outcomes in the higher education sector has been under-investigated. Thus, this paper aims to investigate the pre-conditions for the effective adoption of social media tools in the Nigerian higher education sector and to assess the impact of the adoption on specific learning outcomes. A multi-criteria decision-making (MCDM) methodology was proposed for study analysis, aided by views of experts with sufficient teaching experience in Nigerian business school programs. The results indicate that adequate budgetary allocations, technical competence, a sufficient level of privacy, and an effective government regulatory framework are the most important of the investigated pre-conditions. Additionally, the pedagogical use of social media in business school programs is more strongly associated with learning outcomes such as professionalism and strategic thinking, emotional intelligence, and social maturity. Hence, the article offers guidance to decision-makers in the higher education sector on how to actualize the successful adoption of social media for pedagogical use and build effective business strategies at various levels of the digitalization process.}
}
@article{XU2023108916,
title = {Joint optimization task offloading and trajectory control for unmanned-aerial-vehicle-assisted mobile edge computing},
journal = {Computers and Electrical Engineering},
volume = {111},
pages = {108916},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623003403},
author = {Fei Xu and Sen Wang and Weiya Su and Lin Zhang},
keywords = {Edge computing, Computation offloading, Deep reinforcement learning, Unmanned Aerial Vehicle, Trajectory control},
abstract = {The appearance of Mobile Edge Computing (MEC) and Unmanned Aerial Vehicle (UAV) is significant for the future progress of the Internet of Things (IoT). Since the system model with a continuous action space and high-dimensional state space, the joint optimization of UAV trajectory and the computational offloading problem is non-convex, and traditional algorithms for instance ant colony algorithm, genetic algorithm, Actor Critic (AC) algorithm, and Deep Deterministic Policy Gradient (DDPG) algorithm are difficult to cope with. Reasonably formulating the computational task offloading strategy and the trajectory control of the UAV is crucial for the high-efficiency completion of the task. In this paper, a computational offloading and trajectory control system model for UAV-assisted MEC is proposed. We seek to maximize the user ratio of coverage by jointly optimizing computing offload scheduling and UAV trajectories. We propose an improved DDPG algorithm to optimize the objective function and achieve the optimal solution. Meanwhile, our algorithm can achieve an improvement in the user rate of coverage while avoiding obstacles as compared with baseline algorithms, AC, and DDPG.}
}
@article{ZHOU2023126996,
title = {Coal consumption prediction in thermal power units: A feature construction and selection method},
journal = {Energy},
volume = {273},
pages = {126996},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126996},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003900},
author = {Jian Zhou and Wei Zhang},
keywords = {Thermal power units, Coal consumption prediction, Regression analysis, K-means algorithm, Genetic algorithm},
abstract = {Digitization and related facilities have enabled the thermal power generation enterprises to record real-time data of thermal power units. There are many data-driven applications based on real-time monitoring and operational data in power units, while limited studies lay on the operational improvements, especially on coal consumption prediction under all working conditions. We build an intelligent prediction model of coal consumption based on key features selection, working condition clustering, and regression analysis. We combine feature construction and feature selection methods to cope with the problem caused by directly specifying feature subset for model building of traditional prediction method, which may fall into the thinking pattern and miss potentially better feature subset. Besides, to cope with the different coal consumption under different working conditions, we apply cluster analysis to construct a sub-coal consumption prediction model for each cluster category. Numerical results show that compared with other methods, it has the advantages of lower regression error and moderate model complexity, which can provide efficient decision support for operational improvement in thermal power generation.}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{LEONIDOV2022112279,
title = {Strategic stiffening/cooling in the Ising game},
journal = {Chaos, Solitons & Fractals},
volume = {160},
pages = {112279},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112279},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922004891},
author = {Andrey Leonidov and Ekaterina Vasilyeva},
keywords = {Binary choice game, Ising game, Graph, Forward-looking, Myopic, Noise},
abstract = {The dynamic noisy binary choice (Ising) game of forward-looking agents on a complete graph is analysed. It is shown that strategic considerations lead to effective interaction strengthening (noise reduction) as compared to the myopic game. We show that strategic agents are able to come to consensus in the wider range of noise values than myopic ones. Effective population dynamics with time-dependent probabilities reflecting this strategic stiffening/cooling effect is described.}
}
@article{LU2024103920,
title = {The integrated multi-performance fast optimization strategy for battery thermal management system},
journal = {Case Studies in Thermal Engineering},
volume = {54},
pages = {103920},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103920},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X23012261},
author = {Hao Lu and Xiaole Tang and Hongchang Li and Wenjun Zhao and Xiqiang Chang and Weifang Lin},
keywords = {Short-cut computation, Computational fluid dynamics, Weighted average, Optimization algorithm},
abstract = {Increased battery energy density is required to boost electric vehicle endurance; however, this also raises the possibility of thermal runaway and power battery explosion. Improving the cooling system performance requires optimization and enhancement of classical systems. Traditional design approaches struggle to simultaneously enhance multiple aspects of performance, while an optimization based on Computational Fluid Dynamics (CFD) methods is often inefficient. Therefore, by integrating a flow resistance network model (FRNM) with a weighted average optimization algorithm (INFO), an efficient optimization for the comprehensive performance of the system can be achieved. Five optimized systems under different airflow rates were obtained through optimization. A comparison with two existing systems validated the effectiveness of the optimized system. The results demonstrate that, compared to the two reference systems, the optimized system decreases the maximum temperature difference by 65.51 % and 39.07 %, respectively. Furthermore, the improvement in temperature uniformity is more significant, increasing by 63.76 % and 34.40 %, respectively.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{LIANG2019341,
title = {Is ecoregional scale precise enough for lake nutrient criteria? Insights from a novel relationship-based clustering approach},
journal = {Ecological Indicators},
volume = {97},
pages = {341-349},
year = {2019},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18308094},
author = {Zhongyao Liang and Yong Liu and Huili Chen and Yao Ji},
keywords = {Spatial scale, Nutrient criteria, Relationship-based clustering approach, Relationship mapping, Hierarchical clustering, Leave-one-out cross-validation},
abstract = {While the ecoregional lake nutrient criteria have been widely used in the past two decades, the overconfidence on their applicability may mislead the pollution management decisions, considering the spatial heterogeneity within the ecoregion. The exploration of applicability is thereby important, but is hindered by the difficulty in recognizing reliable relationship patterns between the nutrient and management endpoint. We propose a novel relationship-based clustering approach (RCA) to explore whether the ecoregional scale is precise enough for nutrient criteria. The approach (a) simulates relationships using Bayesian Linear Models, (b) clusters lakes according to relationship similarities via relationship mapping and hierarchical clustering, and (c) identifies reliable relationship patterns based on the leave-one-out cross-validation. The RCA is then employed to explore Chlorophyll a-total phosphorus relationships of 34 lakes in four Ecological Drainage Units (EDUs) in the U.S. Long-term water quality data is from a newly established database (LAGOS-NE). The results show that multiple relationship patterns exist in all the EDUs. The ecoregional relationships misestimate the nutrient effect in over a half of lakes. Therefore, we determine that the ecoregional scale is not precise enough for nutrient criteria and the sub-ecoregional scale is then recommended. Besides, the RCA provides a backward thinking for determining the spatial scale and can be used in some other fields where relationship-based clustering is needed.}
}