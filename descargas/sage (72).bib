@article{doi:10.1177/1532708617750178,
author = {Nicole Marie Brown},
title = {Methodological Cyborg as Black Feminist Technology: Constructing the Social Self Using Computational Digital Autoethnography and Social Media},
journal = {Cultural Studies ↔ Critical Methodologies},
volume = {19},
number = {1},
pages = {55–67},
year = {2019a},
doi = {10.1177/1532708617750178},
URL = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
abstract = {This article reimagines the quantified self within the context of Black feminist technologies. Bringing computation and autoethnographic methods together using a methodology I call computational digital autoethnography, I harvest my social media data to create a corpus for analysis. I apply topic modeling to these data to uncover themes that are connected with broader societal issues affecting African American women. Applying a computational autoethnographic approach to a researcher’s own digitized data allows for yet another dimension of mixed-methods research. This radical intervention has the potential to transform the social sciences by bringing together two seemingly divergent methodological approaches in service to Black feminist ways of knowing.}
}

@article{doi:10.1177/0391398818792757,
author = {Bruce Garvey and Liuqing Chen and Feng Shi and Ji Han and Peter RN Childs},
title = {New directions in computational, combinational and structural creativity},
journal = {The International Journal of Artificial Organs},
volume = {233},
number = {2},
pages = {738–751},
year = {2019b},
doi = {10.1177/0391398818792757},
note = {PMID:30141359},
URL = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
abstract = {Despite the evolution of ventricular assist devices, ventricular assist device patients still suffer from complications due to the damage to blood by fluid dynamic stress. Since rotary ventricular assist devices are assumed to exert mainly shear stress, studies of blood damage are based on shear flow experiments. However, measurements and simulations of cell and protein deformation show normal and shear stresses deform, and potentially damage, cells and proteins differently. The aim was to use computational fluid dynamics to assess the prevalence of normal stress, in comparison with shear stress, in rotary ventricular assist devices. Our calculations showed normal stresses do occur in rotary ventricular assist devices: the fluid volumes experiencing normal stress above 10 Pa were 0.011 mL (0.092%) and 0.027 mL (0.39%) for the HeartWare HVAD and HeartMate II (HMII), and normal stresses over 100 Pa were present. However, the shear stress volumes were up to two orders of magnitude larger than the normal stress volumes. Considering thresholds for red blood cell and von Willebrand factor deformation by normal and shear stresses, the fluid volumes causing deformation by normal stress were between 2.5 and 5 times the size of those causing deformation by shear stress. The exposure times to the individual normal stress deformation regions were around 1 ms. The results clearly show, for the first time, that while blood within rotary ventricular assist devices experiences more shear stress at much higher magnitudes as compared with normal stress, there is sufficient normal stress exposure present to cause deformation of, and potentially damage to, the blood components. This study is the first to quantify the fluid stress components in real blood contacting devices.}
}

@article{doi:10.1177/1475090216642467,
author = {Max Haase and Gary Davidson and Jonathan Binns and Giles Thomas and Neil Bose},
title = {Full-scale resistance prediction in finite waters: A study using computational fluid dynamics simulations, model test experiments and sea trial measurements},
journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
volume = {231},
number = {1},
pages = {316–328},
year = {2017c},
doi = {10.1177/1475090216642467},
URL = {https://doi-org.crai.referencistas.com/10.1177/1475090216642467},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090216642467},
abstract = {The development of large medium-speed catamarans aims increasing economic viability and reducing the possible negative influence on the environment of fast sea transportation. These vessels are likely to operate at hump speed where wave-making can be the dominating component of the total resistance. Shallow water may considerably amplify the wave-making and hence the overall drag force. Computational fluid dynamics is used to predict the drag force of medium-speed catamarans at model and full scale in infinite and restricted water to study the impact on the resistance. Steady and unsteady shallow-water effects that occur in model testing or full-scale operation are taken into account using computational fluid dynamics as they are inherently included in the mathematical formulations. Unsteady effects in the ship-model response were recorded in model test experiments, computational fluid dynamics simulations and full-scale measurements and found to agree with each other. For a medium-speed catamaran in water that is restricted in width and depth, it was found that computational fluid dynamics is capable of accurately predicting the drag with a maximum deviation of no more than 6% when compared to experimental results in model scale. The influences of restricted depth and width were studied using computational fluid dynamics where steady finite width effects in shallow water and finite depth effects at finite width were quantified. Full-scale drag from computational fluid dynamics predictions in shallow water (h/L = 0.12 – 0.17) was found to be between full-scale measurements and extrapolated model test results. Finally, it is shown that current extrapolation procedures for shallow-water model tests over-estimate residuary resistance by up to 12% and underestimate frictional forces by up to 35% when compared to validated computational fluid dynamics results. This study concludes that computational fluid dynamics is a versatile tool to predict the full-scale ship resistance to a more accurate extent than extrapolation model test data and can also be utilised to estimate model sizes that keep finite-water effects to an agreed minimum.}
}

@article{doi:10.1177/14780771211040169,
author = {Matthias H Haeusler and Nicole Gardner and Daniel K Yu and Claire Oh and Blair Huang},
title = {(Computationally) designing out waste: Developing a computational design workflow for minimising construction and demolition waste in early-stage architectural design},
journal = {International Journal of Architectural Computing},
volume = {19},
number = {4},
pages = {594–611},
year = {2021d},
doi = {10.1177/14780771211040169},
URL = {https://doi-org.crai.referencistas.com/10.1177/14780771211040169},
eprint = {https://doi-org.crai.referencistas.com/10.1177/14780771211040169},
abstract = {In the architecture, engineering and construction (AEC) industry, waste is oft framed as an economic problem typically addressed in a building’s construction and demolition phase. Yet, architectural design decision-making can significantly determine construction waste outcomes. Following the logic of zero waste, this research addresses waste minimisation ‘at the source’. By resituating the problem of construction waste within the architectural design process, the research explores waste as a data and informational problem in a design system. Accordingly, this article outlines the creation of an integrated computational design decision support waste tool that employs a novel data structure combining HTML-scraped material data and historic building information modelling (BIM) data to generate waste evaluations in a browser-based 3D modelling platform. Designing an accessible construction waste tool for use by architects and designers aims to heighten awareness of the waste implications of design decisions towards challenging the systems of consumption and production that generate construction and demolition waste.}
}

@article{doi:10.1177/0391398818792757,
author = {Dominica PY Khoo and Andrew N Cookson and Harinderjit S Gill and Katharine H Fraser},
title = {Normal fluid stresses are prevalent in rotary ventricular assist devices: A computational fluid dynamics analysis},
journal = {The International Journal of Artificial Organs},
volume = {41},
number = {11},
pages = {738–751},
year = {2018e},
doi = {10.1177/0391398818792757},
note = {PMID:30141359},
URL = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0391398818792757},
abstract = {Despite the evolution of ventricular assist devices, ventricular assist device patients still suffer from complications due to the damage to blood by fluid dynamic stress. Since rotary ventricular assist devices are assumed to exert mainly shear stress, studies of blood damage are based on shear flow experiments. However, measurements and simulations of cell and protein deformation show normal and shear stresses deform, and potentially damage, cells and proteins differently. The aim was to use computational fluid dynamics to assess the prevalence of normal stress, in comparison with shear stress, in rotary ventricular assist devices. Our calculations showed normal stresses do occur in rotary ventricular assist devices: the fluid volumes experiencing normal stress above 10 Pa were 0.011 mL (0.092%) and 0.027 mL (0.39%) for the HeartWare HVAD and HeartMate II (HMII), and normal stresses over 100 Pa were present. However, the shear stress volumes were up to two orders of magnitude larger than the normal stress volumes. Considering thresholds for red blood cell and von Willebrand factor deformation by normal and shear stresses, the fluid volumes causing deformation by normal stress were between 2.5 and 5 times the size of those causing deformation by shear stress. The exposure times to the individual normal stress deformation regions were around 1 ms. The results clearly show, for the first time, that while blood within rotary ventricular assist devices experiences more shear stress at much higher magnitudes as compared with normal stress, there is sufficient normal stress exposure present to cause deformation of, and potentially damage to, the blood components. This study is the first to quantify the fluid stress components in real blood contacting devices.}
}

@article{doi:10.1177/1934578X221096966,
author = {Ying Liu and Han Yan and Hui-bin Jia and Li Pan and Jia-zheng Liu and Ya-wen Zhang and Jing Wang and Dao-gang Qin and Lei Ma and Ting Wang},
title = {Jiedu Huoxue Decoction for Cytokine Storm and Thrombosis in Severe COVID-19: A Combined Bioinformatics and Computational Chemistry Approach},
journal = {Natural Product Communications},
volume = {17},
number = {4},
pages = {1934578X221096966},
year = {2022f},
doi = {10.1177/1934578X221096966},
URL = {https://doi-org.crai.referencistas.com/10.1177/1934578X221096966},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1934578X221096966},
abstract = {Jiedu Huoxue Decoction (JHD), a recommended traditional prescription for patients with severe COVID-19, has appeared in the treatment protocols in China. Based on bioinformatics and computational chemistry methods, including molecular docking, molecular dynamics (MD) simulation, and Molecular Mechanics Generalized Born Surface Area (MM/GBSA) calculation, we aimed to reveal the mechanism of JHD in treating severe COVID-19. The compounds in JHD were obtained and screened on TCMSP, SwissADME, and ADMETLab platforms. The compound targets were obtained from TCMSP and STITCH, while COVID-19 targets were obtained from Genecards and NCBI. The protein-protein interaction network was constructed by using STRING. Gene Ontology (GO) and KEGG enrichment were performed with ClueGO and R language. AutoDock vina was employed for molecular docking. 100 ns MD simulation of the optimal docking complex was carried out with AmberTools 20. A total of 84 compounds and 29 potential targets of JHD for COVID-19 were collected. The key phytochemicals included quercetin, luteolin, β-sitosterol, puerarin, stigmasterol, kaempferol, and wogonin, which could regulate the immune system. The hub genes included IL6, IL10, VEGFA, IL1B, CCL2, HMOX1, DPP4, and ACE2. ACE2 and DPP4 were related to SARS-CoV-2 entering cells. GO and KEGG analysis showed that JHD could intervene in cytokine storm and endothelial proliferation and migration related to thrombosis. The molecular docking, 100 ns MD simulation, and MM/GBSA calculation confirmed that targets enriched in the COVID-19 pathway had high affinities with related compounds, and the conformations of the puerarin-ACE2, quercetin-EGFR, luteolin-EGFR, and quercetin-IL1B complexes were stable. In a word, JHD could treat COVID-19 by intervening in cytokine storm, thrombosis, and the entry of SARS-CoV-2, while regulating the immune system. These mechanisms were consistent with JHD’s therapeutic concept of “detoxification” and “promoting blood circulation and removing blood stasis” in treating COVID-19. The research provides a theoretical basis for the development and application of JHD.}
}

@article{doi:10.1080/17470218.2015.1134602,
author = {Pavel Logačev and Shravan Vasishth},
title = {Understanding underspecification: A comparison of two computational implementations},
journal = {Quarterly Journal of Experimental Psychology},
volume = {69},
number = {5},
pages = {996–1012},
year = {2016g},
doi = {10.1080/17470218.2015.1134602},
note = {PMID:26960441},
URL = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
eprint = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201–216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of lexical ambiguity resolution. Journal of Memory and Language, 39(4), 558–592], which has been explained in terms of the Unrestricted Race Model, can equally well be explained by assuming underspecification in ambiguous conditions driven by task-demands. Specifically, if comprehension questions require that ambiguities be resolved, the parser tends to make an attachment: when questions are about superficial aspects of the target sentence, readers tend to pursue an underspecification strategy. It is reasonable to assume that individual differences in strategy will play a significant role in the application of such strategies, so that studying average behaviour may not be informative. In order to study the predictions of the good-enough processing theory, we implemented two versions of underspecification: the partial specification model (PSM), which is an implementation of the Swets et al. proposal, and a more parsimonious version, the non-specification model (NSM). We evaluate the relative fit of these two kinds of underspecification to Swets et al.’s data; as a baseline, we also fitted three models that assume no underspecification. We find that a model without underspecification provides a somewhat better fit than both underspecification models, while the NSM model provides a better fit than the PSM. We interpret the results as lack of unambiguous evidence in favour of underspecification; however, given that there is considerable existing evidence for good-enough processing in the literature, it is reasonable to assume that some underspecification might occur. Under this assumption, the results can be interpreted as tentative evidence for NSM over PSM. More generally, our work provides a method for choosing between models of real-time processes in sentence comprehension that make qualitative predictions about the relationship between several dependent variables. We believe that sentence processing research will greatly benefit from a wider use of such methods.}
}

@article{doi:10.1177/1094342004048534,
author = {D. E. Post and R. P. Kendall},
title = {Software Project Management and Quality Engineering Practices for Complex,                 Coupled Multiphysics, Massively Parallel Computational Simulations: Lessons Learned                 From ASCI},
journal = {The International Journal of High Performance Computing Applications},
volume = {18},
number = {4},
pages = {399–416},
year = {2004h},
doi = {10.1177/1094342004048534},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342004048534},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342004048534},
abstract = {Many institutions are now developing large-scale, complex, coupled multiphysics computational simulations for massively parallel platforms for the simulation of the performance of nuclear weapons and certification of the stockpile, and for research in climate and weather prediction, magnetic and inertial fusion energy, environmental systems, astrophysics, aerodynamic design, combustion, biological and biochemical systems, and other areas. The successful development of these simulations is aided by attention to sound software project management and software engineering. We have developed “lessons learned” from a set of code projects that the Department of Energy National Nuclear Security Agency has sponsored to develop nuclear weapons simulations over the last 50 years. We find that some, but not all, of the software project management and development practices (rather than processes) commonly employed for non-technical software add value to the development of scientific software and we identify those that we judge add value. Another key finding, consistent with general software industry experience, is that the optimal project schedule and resource level are solely determined by the requirements once the requirements are fixed.}
}

@article{doi:10.3181/0704-MR-97,
author = {Harlan Robins and Michael Krasnitz and Arnold J. Levine},
title = {The Computational Detection of Functional Nucleotide Sequence Motifs in the Coding Regions of Organisms},
journal = {Experimental Biology and Medicine},
volume = {233},
number = {6},
pages = {665–673},
year = {2008i},
doi = {10.3181/0704-MR-97},
URL = {https://doi-org.crai.referencistas.com/10.3181/0704-MR-97},
eprint = {https://doi-org.crai.referencistas.com/10.3181/0704-MR-97},
abstract = {A new algorithm has been constructed for finding under- and overrepresented oligonucleotide motifs in the protein coding regions of genomes that have been normalized for G/C content, codon usage, and amino acid order. This Robins-Krasnitz algorithm has been employed to compare the oligonucleotide frequencies between many different prokaryotic genomes. Evidence is presented demonstrating that at least some of these sequence motifs are functionally important and selected for or against during the evolution of these prokaryotes. The applications of this method include the optimization of protein expression for synthetic genes in foreign organisms, identification of novel oligonucleotide signals used by the organism and the examination of evolutionary relationships not dependent upon different gene sequence trees.}
}

@article{doi:10.1177/0361198196155000101,
author = {Hέlène Tattegrain-Veste and Thierry Bellet and Annie Pauziέ and Andrέ Chapon},
title = {Computational Driver Model in Transport Engineering: COSMODRIVE},
journal = {Transportation Research Record},
volume = {1550},
number = {1},
pages = {1–7},
year = {1996j},
doi = {10.1177/0361198196155000101},
URL = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
abstract = {With regard to road safety issues, a deep understanding of the driver as a logic system is crucial to predict the most probable behavior according to the contextual elements. Knowledge and data about human functional abilities exist. But the problem is to organize and structure them. The development of a computational approach in driver modelization is addressed. In the first part, a brief historical overview is presented of available driver models in ergonomics and psychological areas, and the distinction between predictive and explicative models in an implementation perspective is the focus. In the second part, the computational aspect of the work is described, along with the software concepts, the cognitive modeling needs, and the implementation choices. Object-oriented techniques were chosen because they provide a modular overview of the general system and offer a convenient representation of cognitive processes. Object-oriented formalism, in particular object modeling technique diagrams, acts as a bridge between the two domains of computer science and the human sciences. The objective is to determine whether it is possible to implement reliably a driver model using the techniques from artificial intelligence and based on the theoretical knowledge from cognitive sciences research. This attempt to establish links between different scientific domains, requiring a common tool, is a challenge. A first step of a work that will have to be developed in a long-term time scale, taking into account its quite ambitious objective, is described.}
}

