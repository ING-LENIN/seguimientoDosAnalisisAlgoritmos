@article{BEJAR2005117,
title = {Sensor networks and distributed CSP: communication, computation and complexity},
journal = {Artificial Intelligence},
volume = {161},
number = {1},
pages = {117-147},
year = {2005},
note = {Distributed Constraint Satisfaction},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400150X},
author = {Ramón Béjar and Carmel Domshlak and Cèsar Fernández and Carla Gomes and Bhaskar Krishnamachari and Bart Selman and Magda Valls},
keywords = {Distributed CSP benchmark, Phase transitions, Randomized combinatorial search, Communication network delays, NP-completeness},
abstract = {We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC), and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of SensorDCSP. We found that random delays (due to network traffic or in some cases actively introduced by the agents) combined with a dynamic decentralized restart strategy can improve the performance of DisCSP algorithms. In addition, we introduce GSensorDCSP, a plain-embedded version of SensorDCSP that is closely related to various real-life dynamic tracking systems. We perform both analytical and empirical study of this benchmark domain. In particular, this benchmark allows us to study the attractiveness of solution repairing for solving a sequence of DisCSPs that represent the dynamic tracking of a set of moving objects.}
}
@article{IANDOLI2014298,
title = {Socially augmented argumentation tools: Rationale, design and evaluation of a debate dashboard},
journal = {International Journal of Human-Computer Studies},
volume = {72},
number = {3},
pages = {298-319},
year = {2014},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2013.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581913001043},
author = {Luca Iandoli and Ivana Quinto and Anna {De Liddo} and Simon {Buckingham Shum}},
keywords = {Computer-supported argument visualization, Grounding process, Common ground, Debate dashboard, Collective deliberation, Visual feedback},
abstract = {Collaborative Computer-Supported Argument Visualization (CCSAV) is a technical methodology that offers support for online collective deliberation over complex dilemmas. As compared with more traditional conversational technologies, like wikis and forums, CCSAV is designed to promote more critical thinking and evidence-based reasoning, by using representations that highlight conceptual relationships between contributions, and through computational analytics that assess the structural integrity of the network. However, to date, CCSAV tools have achieved adoption primarily in small-scale educational contexts, and only to a limited degree in real world applications. We hypothesise that by reifying conversations as logical maps to address the shortcomings of chronological streams, CCSAV tools underestimate the importance of participation and interaction in enhancing collaborative knowledge-building. We argue, therefore, that CCSAV platforms should be socially augmented in order to improve their mediation capability. Drawing on Clark and Brennan influential Common Ground theory, we designed a Debate Dashboard, which augmented a CCSAV tool with a set of widgets that deliver meta-information about participants and the interaction process. An empirical study simulating a moderately sized collective deliberation scenario provides evidence that this experimental version outperformed the control version on a range of indicators, including usability, mutual understanding, quality of perceived collaboration, and accuracy of individual decisions. No evidence was found that the addition of the Debate Dashboard impeded the quality of the argumentation or the richness of content.}
}
@article{GERPOTT2024101783,
title = {New ways of seeing: Four ways you have not thought about Registered Reports yet},
journal = {The Leadership Quarterly},
volume = {35},
number = {2},
pages = {101783},
year = {2024},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2024.101783},
url = {https://www.sciencedirect.com/science/article/pii/S1048984324000122},
author = {Fabiola H. Gerpott and Roman Briker and George Banks},
keywords = {Registered Reports, Open Science, Transparency, Quantitative, Qualitative, Leadership},
abstract = {The Leadership Quarterly has helped as a pioneer in accepting Registered Reports (RRs), a submission format where authors provide the introduction, theory section, and methods of their paper for peer review before data collection. Proud but never satisfied, we aim to further boost the number of suitable RR submissions due to our firm belief in their potential for fostering transparent, high-impact research. To inspire authors to explore diverse data collection strategies and methods beyond experiments and survey-based (replication) studies, this work presents four distinct but equally suitable research formats for RRs: meta-analyses, qualitative research, computational approaches, and field intervention studies. Expanding prior research that has explored and promoted general practices and methodological standards for RRs, we offer unique recommendations for preparing an adequate RR proposal along each of these four RR avenues. Additionally, we provide a table of summary resources for authors, reviewers, and editors looking to engage more with RR. In conclusion, we envision a future where other top-tier journals and funding agencies follow The Leadership Quarterly by embracing the incorporation of RRs as a critical component of their strategic approach.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{WISTEN199777,
title = {Distributed computation of dynamic traffic equilibria},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {5},
number = {2},
pages = {77-93},
year = {1997},
note = {Parallel Computing in Transport Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(97)00003-X},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X9700003X},
author = {M.B. Wisten and M.J. Smith},
abstract = {The dynamic traffic assignment problem is formulated in the space of splitting rates rather than link and route flows. A distributed algorithm for computation of dynamic user-equilibria is specified. The algorithm has been implemented on a Meiko Computing Surface with 32 T800 processors and some numerical results are given. We do not yet have a general proof of convergence for the algorithm but we have been able to demonstrate convergence with all test networks used.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{CHEN20171,
title = {Heterogeneity in generalized reinforcement learning and its relation to cognitive ability},
journal = {Cognitive Systems Research},
volume = {42},
pages = {1-22},
year = {2017},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300559},
author = {Shu-Heng Chen and Ye-Rong Du},
keywords = {Generalized reinforcement learning, Experience-weighted attraction learning, Cognitive ability, Granularity},
abstract = {In this paper, we study the connections between working memory capacity (WMC) and learning in the context of economic guessing games. We apply a generalized version of reinforcement learning, popularly known as the experience-weighted attraction (EWA) learning model, which has a connection to specific cognitive constructs, such as memory decay, the depreciation of past experience, counterfactual thinking, and choice intensity. Through the estimates of the model, we examine behavioral differences among individuals due to different levels of WMC. In accordance with ‘Miller’s magic number’, which is the constraint of working memory capacity, we consider two different sizes (granularities) of strategy space: one is larger (finer) and one is smaller (coarser). We find that constraining the EWA models by using levels (granules) within the limits of working memory allows for a better characterization of the data based on individual differences in WMC. Using this level-reinforcement version of EWA learning, also referred to as the EWA rule learning model, we find that working memory capacity can significantly affect learning behavior. Our likelihood ratio test rejects the null that subjects with high WMC and subjects with low WMC follow the same EWA learning model. In addition, the parameter corresponding to ‘counterfactual thinking ability’ is found to be reduced when working memory capacity is low.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{GRIFFITHS2020873,
title = {Understanding Human Intelligence through Human Limitations},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {873-883},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320302151},
author = {Thomas L. Griffiths},
keywords = {artificial intelligence, inductive bias, meta-learning, rational meta-reasoning, cultural evolution},
abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.}
}
@article{PITOWSKY1996161,
title = {Laplace's demon consults an oracle: The computational complexity of prediction},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {27},
number = {2},
pages = {161-180},
year = {1996},
issn = {1355-2198},
doi = {https://doi.org/10.1016/1355-2198(96)85115-X},
url = {https://www.sciencedirect.com/science/article/pii/135521989685115X},
author = {Itamar Pitowsky}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{MUEHLENSIEPEN2022,
title = {Factors Associated With Telemedicine Use Among German General Practitioners and Rheumatologists: Secondary Analysis of Data From a Nationwide Survey},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40304},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122007373},
author = {Felix Muehlensiepen and Pascal Petit and Johannes Knitza and Martin Welcker and Nicolas Vuillerme},
keywords = {telemedicine, rheumatology, primary care, secondary analysis, health services research},
abstract = {Background
Previous studies have demonstrated telemedicine (TM) to be an effective tool to complement rheumatology care and address workforce shortage. With the outbreak of the SARS-CoV-2 pandemic, TM experienced a massive upswing. However, in rheumatology care, the use of TM stagnated again shortly thereafter. Consequently, the factors associated with physicians’ willingness to use TM (TM willingness) and actual use of TM (TM use) need to be thoroughly investigated.
Objective
This study aimed to identify the factors that determine TM use and TM willingness among German general practitioners and rheumatologists.
Methods
We conducted a secondary analysis of data from a German nationwide cross-sectional survey with general practitioners and rheumatologists. Bayesian univariate and multivariate logistic regression analyses were applied to the data to determine which factors were associated with TM use and TM willingness. The predictor variables (covariates) that were studied individually included sociodemographic factors (eg, age and sex), work characteristics (eg, practice location and medical specialty), and self-assessed knowledge of TM. All the variables positively and negatively associated with TM use and TM willingness in the univariate analysis were then considered for Bayesian model averaging analysis after a selection based on the variance inflation factor (≤2.5). All analyses were stratified by sex.
Results
Univariate analysis revealed that out of 83 variables, 36 (43%) and 34 (41%) variables were positively or negatively associated (region of practical equivalence≤5%) with TM use and TM willingness, respectively. The Bayesian model averaging analysis allowed us to identify 13 and 17 factors of TM use and TM willingness, respectively. Among these factors, being female, having very poor knowledge of TM, treating <500 patients per quarter, and not being willing to use TM were negatively associated with TM use, whereas having good knowledge of TM and treating >1000 patients per quarter were positively associated with TM use. In addition, being aged 51 to 60 years, thinking that TM is not important for current and future work, and not currently using TM were negatively associated with TM willingness, whereas owning a smart device and working in an urban area were positively associated with TM willingness.
Conclusions
The results point to the close connection between health care professionals’ knowledge of TM and actual TM use. These results lend support to the integration of digital competencies into medical education as well as hands-on training for health care professionals. Incentive programs for physicians aged >50 years and practicing in rural areas could further encourage TM willingness.}
}
@article{SAJID2023103174,
title = {Thermal case classification of solar-powered cars for binary tetra hybridity nanofluid using Cash and Carp method with Hamilton-Crosser model},
journal = {Case Studies in Thermal Engineering},
volume = {49},
pages = {103174},
year = {2023},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103174},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X2300480X},
author = {Tanveer Sajid and Wasim Jamshed and Nek Muhammad Katbar and Mohamed R. Eid and Assmaa Abd-Elmonem and Nesreen Sirelkhtam Elmki Abdalla and Sayed M. {El Din} and Gilder Cieza Altamirano},
keywords = {Solar sports car, Solar sheet, Reiner-Philippoff tetrhybrid nanofluid, Thermal radiation, Heat generation},
abstract = {Solar energy is the most important source of thermal energy that comes from the sun. This kind of energy has enormous potential applications in fields of technology such as photovoltaic panels, renewable power, solar light poles, and solar pumps used for water extraction. The era in which we are living is all about the applications of solar energy in industrial sectors most importantly in solar sports car manufacturing. This article presents a new way of thinking about the heat transport analyses of photovoltaic hybrid vehicles, by factoring Casson-Sutterby liquid with the inclusion of various effects like variable thermal conduction, thermal radiation, heat generation, and tetrahybrid nanoparticles. To solve the modelled equations in regards to both momentum and energy, another well-computational approach known as the Cash and Carp method was used. The effects of a wide variety of factors on temperature, shear stress, and velocity fields, as well as the surface drag coefficient and Nusselt number, are briefly described and illustrated in the form of tables and figures. It then found that the thermal radiation, heat production, and thermal conductivity parameters and insertion of agglomerative tetrhybrid nanoparticles in the base fluid amplify heat transfer rate, it has been shown that the performance of the solar car increases in terms of heat transition. In comparison to standard nanofluid, tetrahybrid nanofluid is the most effective medium for the transmission of heat. From the regression analysis, it is observed that the error in terms of Nusselt number is smaller 0.0151 for the case ε=1.5, and increases to 0.0151 in the case of ε=2.5. Relative percentage error is smaller 4.62% in the case of heat generation Q=0.7 but a maximum of 15.8% in the case of thermal radiation Rd=2.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{ZHANG2024100479,
title = {Open source implementations of numerical algorithms for computing the complete elliptic integral of the first kind},
journal = {Results in Applied Mathematics},
volume = {23},
pages = {100479},
year = {2024},
issn = {2590-0374},
doi = {https://doi.org/10.1016/j.rinam.2024.100479},
url = {https://www.sciencedirect.com/science/article/pii/S2590037424000499},
author = {Hong-Yan Zhang and Wen-Juan Jiang},
keywords = {Complete elliptic integral of the first kind (CEI-1), Algorithm design, Orthogonal polynomials, Verification-validation-testing (VVT), STEM education, Computer programming},
abstract = {The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions ellipticK and EllipticK provided by MATLAB and Mathematica respectively, are based on Kcs(m) instead of the usual form K(k) such that Kcs(k2)=K(k) and m=k2. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss–Chebyshev method and Gauss–Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing the CEI-1 are designed, verified, validated and tested, which can be utilized in R& D and be reused properly. Numerical results show that our open source implementations based on K(k) are equivalent to the commercial implementation based on Kcs(m). The general algorithms for computing orthogonal polynomials developed are valuable for the STEM education and scientific computation.}
}
@article{LIU2024111728,
title = {DuaPIN: Auxiliary task enhanced dual path interaction network for civil court view generation},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111728},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003630},
author = {Nayu Liu and Luyao Ma and Yiquan Wu and Kaiwen Wei and Cunhang Fan and Yating Zhang},
keywords = {Dual path interaction network, Auxiliary task, Civil court view generation, Natural language processing},
abstract = {Civil court view generation (CCVG) is a novel but important task for legal intelligence that aims to automatically generate a judge’s opinion based on the plaintiff’s claims and fact descriptions to interpret the judgment result. The task is more challenging than criminal court view generation as the latter generates views based only on criminal facts as input, whereas the CCVG must consider both the plaintiff’s claims and civil facts under the principle of “no claim, no trial.” However, current approaches still follow criminal domain practices to solve problems in civil cases. Moreover, the explicit modeling of the potential correspondence between claims and facts has often been neglected, as court views are required to respond to each corresponding claim based on factual evidence. To address the issues, we propose a dual path interaction network augmented by two self-supervised auxiliary tasks (named DuaPIN), which follows a bionic design by simulating the thinking logic of judges when writing opinions. Specifically, we construct a structurally symmetric Transformer-based dual path multi-encoder–decoder model such that the two inputs, claim and fact, contribute equally to the generation of civil court views. Moreover, an auxiliary task enhanced (ATE) training paradigm using multiple DuaPIN decoders is proposed to explicitly model the potential correspondence between claims and facts. Extensive experiments on public legal document dataset demonstrated that DuaPIN achieves competitive performance compared with previous methods and offers certain performance improvements to popular pre-trained language models via the ATE training method.}
}
@article{ZAHEDI2024103730,
title = {How hypnotic suggestions work – A systematic review of prominent theories of hypnosis},
journal = {Consciousness and Cognition},
volume = {123},
pages = {103730},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000977},
author = {Anoushiravan Zahedi and Steven {Jay Lynn} and Werner Sommer},
keywords = {Hypnosis, Theory, Suggestibility, Hypnotizability, Hypnotic Suggestions (HS), Posthypnotic Suggestions (PHS), Direct Verbal Suggestions},
abstract = {In recent decades, hypnosis has increasingly moved into the mainstream of scientific inquiry. Hypnotic suggestions are frequently implemented in behavioral, neurocognitive, and clinical investigations and interventions. Despite abundant reports about the effectiveness of suggestions in altering behavior, perception, cognition, and agency, no consensus exists regarding the mechanisms driving these changes. This article reviews competing theoretical accounts that address the genesis of subjective, behavioral, and neurophysiological responses to hypnotic suggestions. We systematically analyze the broad landscape of hypnosis theories that best represent our estimation of the current status and future avenues of scientific thinking. We start with procedural descriptions of hypnosis, suggestions, and hypnotizability, followed by a comparative analysis of systematically selected theories. Considering that prominent theoretical perspectives emphasize different aspects of hypnosis, our review reveals that each perspective possesses salient strengths, limitations, and heuristic values. We highlight the necessity of revisiting extant theories and formulating novel evidence-based accounts of hypnosis.}
}
@incollection{DORFLER202057,
title = {Artificial Intelligence},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {57-64},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23863-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245238637},
author = {Viktor Dörfler},
keywords = {AI, AI paradigms, Artificial intelligence, Artificial neural networks, Cognition, Creativity, Intuition, Learning, Machine learning, Mind, Mind and machine, Narrow AI, Thinking, Thinking machines, Wide AI},
abstract = {ARTIFICIAL INTELLIGENCE is a label coined to describe machines that can perform something humans would perform through thinking. In this chapter, I am looking at artificial intelligence (AI) specifically in the context of creativity. My view is inevitably a personal one, as for the time being, the answers to the tough questions on AI entail working with beliefs more so than with facts, opinions more so than hard evidence. What matters most in “AI Creativity” is how we define creativity, as this definition will determine whether we can consider AI to be creative, now or in the future. It is also important to explore what kind of impact AI has or may have on human creativity.}
}
@article{WORBOYS199885,
title = {Computation with imprecise geospatial data},
journal = {Computers, Environment and Urban Systems},
volume = {22},
number = {2},
pages = {85-106},
year = {1998},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(98)00023-4},
url = {https://www.sciencedirect.com/science/article/pii/S0198971598000234},
author = {Michael Worboys},
abstract = {Imprecision in spatial data arises from the granularity or resolution at which observations of phenomena are made, and from the limitations imposed by computational representations, processing and presentational media. Precision is an important component of spatial data quality, and a key to appropriate integration of collections of data sets. Previous work of the author provides a theoretical foundation for imprecision of spatial data resulting from finite granularities, and gives the beginnings of an approach to reasoning with such data using methods similar to rough set theory. This paper develops the theory further, and extends the work to a model that includes both spatial and semantic components. Notions such as observation, schema, frame of discernment and vagueness are examined and formalised.}
}
@article{DECARVALHO2025111158,
title = {A game-inspired algorithm for marginal and global clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111158},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009099},
author = {Miguel {de Carvalho} and Gabriel Martos and Andrej Svetlošák},
keywords = {Cluster analysis, Mixture models, Model-based clustering, Similarity-based clustering, Unsupervised learning},
abstract = {An often overlooked pitfall of model-based clustering is that it typically results in the same number of clusters per margin, an assumption that may not be natural in practice. We develop a clustering method that takes advantage of the sturdiness of model-based clustering, while attempting to mitigate this issue. The proposed approach allows each margin to have a varying number of clusters and employs a strategy game-inspired algorithm, named ‘Reign-and-Conquer’, to cluster the data. Since the proposed clustering approach only specifies a model for the margins, but leaves the joint unspecified, it has the advantage of being partially parallelizable; hence, the proposed approach is computationally appealing as well as more tractable for moderate to high dimensions than a ‘full’ (joint) model-based clustering approach. A battery of numerical experiments on simulated data indicates an overall good performance of the proposed methods in a variety of scenarios, and real datasets are used to showcase their usefulness in practice.}
}
@article{ZHAI2023101373,
title = {Can reflective interventions improve students’ academic achievement? A meta-analysis},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101373},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101373},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001414},
author = {Na Zhai and Yong Huang and Xiaomei Ma and Jingchun Chen},
keywords = {Reflection, Reflective intervention, Academic achievement, Meta-analysis},
abstract = {Reflection is widely acknowledged as a crucial skill for successful learning and decision-making. Recent evidence has shown that reflection can enhance motivation for in-depth learning, improve cognitive and metacognitive strategies, and promote self-regulated learning. While some studies have reported the positive effects of reflective interventions on student academic outcomes, conflicting findings exist. To provide a comprehensive understanding of the effectiveness of reflective interventions on academic achievement, this meta-analysis synthesized data from 25 quantitative studies (comprising 29 effect sizes) conducted between 2012 and 2022, with a total of 2,111 participants. The results revealed a significant overall effect of reflective interventions on academic achievement (g = 0.793, p < 0.001). Further moderator analyses indicated that the effectiveness of reflective interventions was influenced by factors such as learning mode, intervention duration, the role of reflective writing, and culture. However, education level, discipline, teacher or expert feedback, peer interaction, and technological scaffolding did not significantly affect the impact of reflective interventions across studies. These findings highlight the importance of fostering reflective thinking and refining the detailed design of reflective interventions to enhance students’ academic achievement.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@incollection{COXON2019179,
title = {Chapter 7 - Transforming Future Mobility},
editor = {Selby Coxon and Robbie Napper and Mark Richardson},
booktitle = {Urban Mobility Design},
publisher = {Elsevier},
pages = {179-214},
year = {2019},
isbn = {978-0-12-815038-2},
doi = {https://doi.org/10.1016/B978-0-12-815038-2.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150382000074},
author = {Selby Coxon and Robbie Napper and Mark Richardson},
keywords = {Innovation methodology, Design thinking, Future mobility},
abstract = {The book having built a picture of a driverless, accessible, positive experience and inventively built mobility landscape, now leverages the techniques of design thinking to demonstrate the tools of a future economy and how they might be applied to a range of future mobility speculations. This chapter demonstrates that a combination of technology developments and design thinking skills can generate inventive compelling solutions to mobility problems. The chapter is illustrated with examples of these research speculations.}
}
@article{YECKEL1998206,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate: II. Spin down},
journal = {Journal of Crystal Growth},
volume = {191},
number = {1},
pages = {206-224},
year = {1998},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(98)00102-X},
url = {https://www.sciencedirect.com/science/article/pii/S002202489800102X},
author = {Andrew Yeckel and Yuming Zhou and Michael Dennis and Jeffrey J. Derby},
keywords = {Fluid flow, Solution growth, Finite element model},
abstract = {Three-dimensional, time-dependent flows that occur in the Lawrence Livermore National Laboratory system for rapid growth of potassium dihydrogen phosphate (KDP) crystals from solution are studied using massively parallel finite element computations. The simulation reveals that excellent global mixing occurs during the spin-down phase of a time-dependent stirring cycle. The large scale fluid motions in the radial and axial directions that promote mixing are caused primarily by effects of platform geometry, but are augmented to some degree by the intrinsic tendency of a decelerating rotational flow to reverse direction within Ekman layers that form at the boundaries. Along with Part I of this work [Y. Zhou and J.J. Derby, J. Crystal Growth 180 (1997) 497], which emphasized spin up and steady rotation, significant advances have been made in our understanding of hydrodynamic phenomena in this system.}
}
@article{THIBODEAU2017852,
title = {How Linguistic Metaphor Scaffolds Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {11},
pages = {852-863},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301535},
author = {Paul H. Thibodeau and Rose K. Hendricks and Lera Boroditsky},
keywords = {analogy, decision making, framing, language and thought, metaphor, reasoning},
abstract = {Language helps people communicate and think. Precise and accurate language would seem best suited to achieve these goals. But a close look at the way people actually talk reveals an abundance of apparent imprecision in the form of metaphor: ideas are ‘light bulbs’, crime is a ‘virus’, and cancer is an ‘enemy’ in a ‘war’. In this article, we review recent evidence that metaphoric language can facilitate communication and shape thinking even though it is literally false. We first discuss recent experiments showing that linguistic metaphor can guide thought and behavior. Then we explore the conditions under which metaphors are most influential. Throughout, we highlight theoretical and practical implications, as well as key challenges and opportunities for future research.}
}
@article{LIU2023109530,
title = {Quantum computing for power systems: Tutorial, review, challenges, and prospects},
journal = {Electric Power Systems Research},
volume = {223},
pages = {109530},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623004194},
author = {Hualong Liu and Wenyuan Tang},
keywords = {Quantum computing, Optimization, Power systems, Renewable energy, Climate neutrality},
abstract = {As a large number of renewable energy resources are connected to power systems, the operation, planning, and optimization of power systems have been becoming more and more complex. Power flow calculation, unit commitment, economic dispatch, energy pricing, and power system planning are essentially computation problems. A lot of computing resources are required for these problems, which are non-trivial, especially for large-scale power systems with the high penetration of renewable energy. Traditionally, the calculation and optimization of power systems are completed by classical computers based on the classical computing theory and the von Neumann architecture. However, with Moore’s law getting closer and closer to the limit, the importance of quantum computing has become increasingly prominent. Quantum computing has been applied to some fields to a certain extent, yet the applications of quantum computing in power systems are rare. As the power industry is the foundation of the national economy, introducing quantum computing into the power system has far-reaching and crucial significance, such as improving the penetration of renewable energy, enhancing the computing efficiency, and helping in achieving the goal of net zero and climate neutrality by 2050. This paper first introduces the core concepts, essential ideas and theories of quantum computing, and then reviews the existing literature on the applications of quantum computing in power systems, and puts forward our critical thinking about the applications of quantum computing in power systems. In brief, this paper is dedicated to a tutorial on quantum computing targeting power system professionals and a review of its applications in power systems. The main contributions of this paper are: (1) introduce quantum computing into the field of power engineering in a thoroughly detailed way and delineate the analysis methodologies of quantum circuits systematically without losing mathematical rigor; (2) based on Dirac’s notation, the related formulae are derived meticulously with sophisticated schematic diagrams; (3) elaborate and derive some critical quantum algorithms in depth, which play an important role in the applications of quantum computing in power systems; (4) critically summarize and comment on the existing literature on the applications of quantum computing in power systems; (5) the future applications and challenges of quantum computing in power systems are prospected and remarked.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{LI2023101752,
title = {The role of inhibition in overcoming arithmetic natural number bias in the Chinese context: Evidence from behavioral and ERP experiments},
journal = {Learning and Instruction},
volume = {86},
pages = {101752},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101752},
url = {https://www.sciencedirect.com/science/article/pii/S095947522300021X},
author = {Xiaodong Li and Ping Xu and Ronghuan Jiang and Shuang Chen},
keywords = {Inhibitory control, Negative priming, Natural number bias, Arithmetic operation, Event-related potential},
abstract = {The natural number bias (NNB) in arithmetic operations refers to the application of natural number properties to reasoning about rational numbers. Previous studies found the NNB interferes with students’ problem-solving. However, few studies have examined it in the Chinese context or the underlying mechanism by which it can be overcome. Addressing these gaps, in Experiments 1a (n = 31) and 1b (n = 30), we found that Chinese students demonstrate the NNB despite linguistic differences between Chinese and western languages. Experiment 2 (n = 38) adopted a negative priming paradigm and found that inhibitory control was necessary to overcome the NNB. Experiment 3 (n = 34) employed the event-related potential technique; we observed increased P2 amplitude when students solved congruent problems, and increased N2 and decreased P3 amplitude when they solved incongruent problems. These results indicated that the NNB is rooted in intuitive thinking, and overcoming this bias relies on inhibition.}
}
@article{HAYES201739,
title = {Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
journal = {Behaviour Research and Therapy},
volume = {98},
pages = {39-57},
year = {2017},
note = {Best Practice Guidelines for Modern Statistical Methods in Applied Clinical Research},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005796716301887},
author = {Andrew F. Hayes and Nicholas J. Rockwood},
keywords = {Mediation analysis, Moderation, Interaction, Regression analysis, Mechanisms},
abstract = {There have been numerous treatments in the clinical research literature about various design, analysis, and interpretation considerations when testing hypotheses about mechanisms and contingencies of effects, popularly known as mediation and moderation analysis. In this paper we address the practice of mediation and moderation analysis using linear regression in the pages of Behaviour Research and Therapy and offer some observations and recommendations, debunk some popular myths, describe some new advances, and provide an example of mediation, moderation, and their integration as conditional process analysis using the PROCESS macro for SPSS and SAS. Our goal is to nudge clinical researchers away from historically significant but increasingly old school approaches toward modifications, revisions, and extensions that characterize more modern thinking about the analysis of the mechanisms and contingencies of effects.}
}
@article{DAS2024100104,
title = {AI and data-driven urbanism: The Singapore experience},
journal = {Digital Geography and Society},
volume = {7},
pages = {100104},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000266},
author = {Diganta Das and Berwyn Kwek},
keywords = {Singapore, Smart cities, Smart nation, Artificial intelligence (AI), Digital urbanism},
abstract = {This paper presents a deep and critical analysis of Singapore's new wave of state-built digital tools and services and how it connects to its larger smart urbanism project, also known as Smart Nation. The COVID-19 pandemic, and particularly Singapore's response, served as a real-world testing ground for smart urbanist strategies. In particular, we analysed the logic that emanates from these novel digital interventions, how they operate on the complex urban built environment and the population, and their effects on urban and citizenry morphologies. Next, we examined a series of state-led technological implementations that have emerged since the Covid-19 pandemic, providing digital solutions that assist citizens with the changing rhythms of everyday living, data-capturing sensors and gantries to aid authorities in contract tracing efforts and enforce vaccination differentiation measures, geospatial digital mapping of demographic data, in withal robotics for automated policing and cleaning activities; and the use of AI and automated data-driven tools in public health to improve service delivery and care to patients. While we are unable to exhaust every piece of technology for the purpose of this paper, these developments, along with their design thinking and operations, we argue, are helpful in revealing the contemporary conjectures of Singaporean digital urban idealism and the governing strategies of the state. By examining Singapore's response, this study aims to contribute to the ongoing discourse on smart urbanism, offering insights into how cities can leverage technology effectively while balancing technological innovation with privacy and public trust.}
}
@article{BARILE2022467,
title = {Platform-based innovation ecosystems: Entering new markets through holographic strategies},
journal = {Industrial Marketing Management},
volume = {105},
pages = {467-477},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122001614},
author = {Sergio Barile and Cristina Simone and Francesca Iandolo and Antonio Laudando},
keywords = {Platforms, Innovation ecosystems, Platform-based innovation ecosystems, Holographic strategies, Digital algorithms, Platform envelopment},
abstract = {The platformization seems to be a demiurgic force, increasingly (re)shaping this millennium and its socio-economic, technological and physical structures, institutions, and human lives. Innovation ecosystems are experiencing this platformization, leading to the rise of platform-based innovation ecosystems. However, the industrial and managerial literature still lacks a shared definition, a consistent theoretical and strategic framework to explain how platform-based innovation ecosystems emerge and replicate from market to market. This conceptual work attempts to fill those gaps by integrating the extant literature on innovation ecosystems in two ways. First, moving from the literature on innovation ecosystems and industry platforms, using systems thinking framing, it explains the platformization of innovation ecosystems through the double lens structure-system. Second, it identifies the holographic strategy as one of the typical patterns featuring platform-based innovation ecosystem envelopment beyond extant market boundaries. These conceptualizations have insightful theoretical, managerial, and policy implications. In particular, the work discusses the ecosystem as a valid unit of analysis for understanding such an unprecedented shaped-by-platform landscape. Then, it describes the growth strategies of the platform-based innovation ecosystem supporting the platform sponsor in mastering multipoint competition. Eventually, the study pinpoints crucial issues for policymakers in regulating the impact that platformization is having on society.}
}
@article{KELTNER2021216,
title = {A taxonomy of positive emotions},
journal = {Current Opinion in Behavioral Sciences},
volume = {39},
pages = {216-221},
year = {2021},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621000991},
author = {Dacher Keltner and Alan Cowen},
abstract = {Within social functionalist theory (SFT), emotions structure attachment relations, cooperative alliances, hierarchies, and collectives. Within this line of thinking, a rich array of positive emotions enable the formation and negotiation of these relationships. Guided by these arguments, we synthesize how top-down confirmatory studies and data-driven, computational studies converge on evidence for 11 positive emotions with distinct experience, expression, and physiology. This taxonomy includes amusement, awe, compassion, contentment, desire, love, joy, interest, pride, relief, and triumph. We conclude by considering how recent taxonomic efforts will advance emotion science in mapping the distinct forms and functions of the positive emotions.}
}
@article{MAHONY2020104668,
title = {New ideas for non-animal approaches to predict repeated-dose systemic toxicity: Report from an EPAA Blue Sky Workshop},
journal = {Regulatory Toxicology and Pharmacology},
volume = {114},
pages = {104668},
year = {2020},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2020.104668},
url = {https://www.sciencedirect.com/science/article/pii/S0273230020300945},
author = {Catherine Mahony and Randolph S. Ashton and Barbara Birk and Alan R. Boobis and Tom Cull and George P. Daston and Lorna Ewart and Thomas B. Knudsen and Irene Manou and Sebastian Maurer-Stroh and Luigi Margiotta-Casaluci and Boris P. Müller and Pär Nordlund and Ruth A. Roberts and Thomas Steger-Hartmann and Evita Vandenbossche and Mark R. Viant and Mathieu Vinken and Maurice Whelan and Zvonar Zvonimir and Mark T.D. Cronin},
keywords = {Repeated dose toxicity testing, Alternatives, Safety assessment, Chemical legislation, , , Read-across, },
abstract = {The European Partnership for Alternative Approaches to Animal Testing (EPAA) convened a ‘Blue Sky Workshop’ on new ideas for non-animal approaches to predict repeated-dose systemic toxicity. The aim of the Workshop was to formulate strategic ideas to improve and increase the applicability, implementation and acceptance of modern non-animal methods to determine systemic toxicity. The Workshop concluded that good progress is being made to assess repeated dose toxicity without animals taking advantage of existing knowledge in toxicology, thresholds of toxicological concern, adverse outcome pathways and read-across workflows. These approaches can be supported by New Approach Methodologies (NAMs) utilising modern molecular technologies and computational methods. Recommendations from the Workshop were based around the needs for better chemical safety assessment: how to strengthen the evidence base for decision making; to develop, standardise and harmonise NAMs for human toxicity; and the improvement in the applicability and acceptance of novel techniques. “Disruptive thinking” is required to reconsider chemical legislation, validation of NAMs and the opportunities to move away from reliance on animal tests. Case study practices and data sharing, ensuring reproducibility of NAMs, were viewed as crucial to the improvement of non-animal test approaches for systemic toxicity.}
}
@article{GARGALO2024108504,
title = {A process systems engineering view of environmental impact assessment in renewable and sustainable energy production: Status and perspectives},
journal = {Computers & Chemical Engineering},
volume = {180},
pages = {108504},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003745},
author = {Carina L. Gargalo and Haoshui Yu and Nikolaus Vollmer and Ahmad Arabkoohsar and Krist V. Gernaey and Gürkan Sin},
keywords = {Renewable and sustainable energy systems, Environmental impact assessment, Process systems engineering, Life cycle assessment, Sustainability},
abstract = {With the increasing concern for climate change, renewable and sustainable energy production has attracted considerable attention from the scientific community, industrial practitioners, and policy and decision-makers. There are many technological alternatives for each sub-category of complex sustainable energy systems. Life cycle assessment (LCA) can be an effective tool to compare the environmental impacts of each pathway and identify the most promising alternatives from an environmental impact perspective. This contribution first reviews the environmental assessment methods and tools developed over the years. Secondly, a comprehensive review of the contribution of the PSE community to the environmental impact analysis of renewable energy systems is performed. It is observed that while LCA is the preferred method, these studies differed widely concerning the choice of impact assessment method used, the level of details shared concerning the underlying LCA calculations, and whether or not sensitivity and uncertainty analyses were carried out, among many others. This makes the comparison of results from different studies difficult and often impossible. It is clear that the PSE community, with its emphasis on systems thinking and holistic approaches, plays a critical role in the design, integration, and operation of complex sustainable energy systems. However, the thorough calculations necessary to ensure a robust and transparent LCA analysis require a shared methodology and a detailed description of the rules. Such explicit, systematic, and transparent methods will set the bar for a minimum requirement for thorough LCA calculations, ensuring fair comparison and discussions of different technical solutions developed in the wider PSE community for sustainable renewables.}
}
@article{ABEYSEKERA2024100213,
title = {ChatGPT and academia on accounting assessments},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {1},
pages = {100213},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124000076},
author = {Indra Abeysekera},
keywords = {Academia, Accounting, Assessments, ChatGPT, Multiple Choice Questions, Sustainable Development Goals of the United Nations},
abstract = {ChatGPT is considered a risk and an opportunity for academia. An area of threat in contemporary settings is whether it can become a student agent for assessments in academia. This study determines how ChatGPT can become a human agent for students on two financial accounting course units, multiple choice question assessments. The study provided five numerical-based and five narrative-based multiple choice questions. There were ten questions for the Introductory Financial Accounting and 10 for the Advanced Financial Accounting course units. ChatGPT received one question at a time requesting a solution. In the Introductory Financial Accounting section, ChatGPT produced incorrect answers because it incorrectly assumed the underlying assumptions contained in those questions. In Advanced Financial Accounting, ChatGPT presented incorrect answers because of the complexity of the task contained in those questions. ChatGPT demonstrated similar competencies in providing solutions to numerical-based and narrative-based questions. ChatGPT obtained the correct answers to sit in the 80th percentile in the Introductory Financial Accounting course unit assessment and the 50th percentile in the Advanced Financial course unit assessment. ChatGPT4 showed improved performance, with the 90th percentile for Introductory Financial Accounting and the 70th percentile for Advanced Financial Accounting. The findings indicate that the knowledge construct requires reflective thinking with ChatGPT in the ecosystem, and what is assumed and assessable knowledge must be revisited.}
}
@article{LI2023113687,
title = {Twins transformer: Cross-attention based two-branch transformer network for rotating bearing fault diagnosis},
journal = {Measurement},
volume = {223},
pages = {113687},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113687},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123012514},
author = {Jie Li and Yu Bao and WenXin Liu and PengXiang Ji and LeKang Wang and Zhongbing Wang},
keywords = {Attention mechanisms, Cross-attention, Fault diagnosis, Transformer},
abstract = {Due to the inherent shortcomings of traditional depth models, the Transformer model based on the self-attention mechanism has become popular in the field of fault diagnosis. The current Transformer's self-attentive mechanism provides an alternative way of thinking, which can make direct association between each signal. However, it can only focus on the association information within a sequence, and it is difficult to understand the information gap between samples. Therefore, this paper proposes the two-branch Twins attention, which for the first time uses cross-attention to focus on information associations between samples. Twins attention uses cross-attention to learn information associations between samples in addition to retaining the information associations within sequences learned by self-attention. The performance of the proposed model was validated on four popular bearing datasets. Compared to the original transformer structure, the average accuracy of each dataset improved by 1.73% to 99.42%, leading the noise experiments.}
}
@article{RAIKOV2018492,
title = {Cognitive Modelling Quality Rising by Applying Quantum and Optical Semantic Approaches},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {492-497},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.309},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318329823},
author = {A. Raikov},
keywords = {big data, quantum semantic, cognitive modelling, deep learning, decision-making, optical computing},
abstract = {Advanced decision support systems require significant acceleration of decision-making under conditions when factors describing the situation are ill-defined and non-metric. As a rule, such conditions arise in the field of politics, culture and in the social sphere. Cognitive modelling technology is applied in these cases. The cognitive models are created by people, experts from different subjects’ fields. The modelling processes take a great deal of time. Furthermore, the result of the modelling has to be verified when the model’s creators cannot get complete information and have to understand the problem very quickly. The factors and their mutual relationships in cognitive models could be verified with Big Data analysis technology. But this approach takes into account only denotational semantics that are based on the mapping of the model on formalised logical constructions, words, objects, schemes. This paper addresses the issue of creating cognitive semantics that take into consideration thinking, feeling and transcendental factors. It is shown that the classical computer or quantum computer cannot ensure cognitive semantics because they are based on discrete representation of data. An optical computing and Optical Semantic approach could be applied. The architecture of the special optical processor is represented.}
}
@article{SELVAKKUMARAN2020111053,
title = {Review of the use of system dynamics (SD) in scrutinizing local energy transitions},
journal = {Journal of Environmental Management},
volume = {272},
pages = {111053},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720309816},
author = {Sujeetha Selvakkumaran and Erik O. Ahlgren},
keywords = {System dynamics, Modelling, Local, Energy transitions, Multi-level perspective},
abstract = {Local energy transition processes are complex socio-technical transitions requiring careful study. The use of System Dynamics (SD) in modelling and analyzing local energy transitions is especially suitable given the characteristics of SD. Our aim is to systematically categorize the different ways SD is used and useful to scrutinize local energy transitions, and to see if we can discern any common themes that can be useful to researchers looking to scrutinize local energy transitions, using SD. The study is exploratory in nature, with peer-reviewed journal and conference articles analyzed using content analysis. The six categories on which the articles are analyzed are: the sector the article studies; the transition that is studied in the article; the modelling depth in the article; the objective of the article; the justification for using SD provided in the article and the levels of interaction with ‘local’. Our findings show most of the local energy transitions have been studied using simulatable Stock and Flow Diagrams in SD methodology. The important sectors in the energy field are represented in terms of SD modelling of local energy transitions, including electricity, transport, district heating etc. Most of the local energy transitions scrutinized by SD in the articles have descriptive objectives, with some prescriptive, and just one evaluative objective. In terms of justification for using SD provided by the articles analyzed in this study, we found four major themes along which the justifications that were provided. They are dynamics, feedbacks, delays and complexity, systematic thinking, bridging disciplines and actor interactions and behaviour. The ‘dynamics, feedbacks, delays and complexity’ theme is the most cited justification for the use of SD in scrutinizing local energy transitions, followed by systematic thinking.}
}
@article{SLANKSTERSCHMIERER202538,
title = {Modernizing toxicology: The importance of accessible NAM training},
journal = {Toxicology Letters},
volume = {406},
pages = {38-39},
year = {2025},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425000402},
author = {Eryn Slankster-Schmierer},
keywords = {Training, Professional development, Education and outreach, New approach methodologies, In vitro toxicology, Computational toxicology, Regulatory toxicology, Alternative approaches, Nonanimal, NAM},
abstract = {Current toxicology curricula and certifications are heavily reliant on animal-based research and lack mandatory education and training in New Approach Methodologies (NAMs). Traditional animal-based toxicological methods come with many concerns, including translatability and reproducibility, which NAMs are aptly positioned to address. The NAM Use for Regulatory Application (NURA) program aims to bridge this educational gap by providing training to toxicologists, method developers, regulators, and legislators on the use of NAMs, helping to build confidence in NAM use and facilitate the shift to more human-based methods.}
}
@incollection{BARDINI202537,
title = {Chapter 4 - From sketch to landscape: Transforming neuronal concepts across technological change},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {37-52},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000043},
author = {Roberta Bardini},
keywords = {Artificial intelligence, Brain complexity, Breakthroughs in neuroscience, Cell theory, Computational biology, Electrophysiology, Emerging technologies in neuroscience, History of neuroscience, History of technology, Interdisciplinary approaches in neuroscience, Machine learning, Multiomics, Nervous system, Neural networks (biological sciences), Neurology, Neuron, Neuron doctrine, Neuroscience, Neuroscience research advances, Patch-clamp, Patch-seq, Systems biology, Techniques in neuroscience},
abstract = {This chapter delves into the evolution of neuronal concepts, tracing their journey through technological change, from the early days of hand-drawn sketches to the advanced techniques of computational modeling in systems neuroscience. Covering a timeline from the 17th century's scientific revolution to the 21st century's technological boom, this analysis highlights key milestones in the history of neurology and the intricate understanding of neuronal functions. It presents a narrative that emphasizes the significant role technological innovations have played in shaping our perception of neuronal cells. With each scientific breakthrough, we gain deeper insights into the neuron's critical role within the brain's complex network. This chapter not only provides a historical perspective but also sets the stage for future discoveries that will continue to revolutionize our understanding of the neuroscience landscape.}
}
@article{ALTAY20111111,
title = {Fuzzy cognitive mapping in factor elimination: A case study for innovative power and risks},
journal = {Procedia Computer Science},
volume = {3},
pages = {1111-1119},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005569},
author = {Ayca Altay and Gülgün Kayakutlu},
keywords = {Fuzzy cognitive maps, Innovation, Factor prioritization, Factor elimination},
abstract = {Factor or criteria prioritization is essential for decision making and planning. In most areas in decision making, integrating the related literature yields an exuberance of criteria which leads a robust decision. Yet, an excess number of criteria may handicap decision making or evaluations in terms of computational time and complexity. In these circumstances, decreasing the number of factors in exchange for a negligible amount of knowledge can emancipate the decision maker yet does not affect the quality of the decision. This elimination can be conducted through qualitative methods such as interviews or quantitative methods. However, quantitative methods are more trustworthy since qualitative methods can be deceptive due to the perceptions of the interviewee. Furthermore, working with larger groups is more prone to neutrality in terms of group thinking. On the subject of innovative power and risks, the literature offers 48 criteria depending on the industry, size or demographics of related companies. Prioritizing and working with these criteria for their decision making applications becomes computationally expensive, especially when embedded in more complex algorithms. In this study, 48 criteria will be reduced using Fuzzy Cognitive Maps and it is believed to provide a sufficient number of criteria with a negligible loss of information and comparisons will be conducted.}
}
@article{JONES2001325,
title = {NMR quantum computation},
journal = {Progress in Nuclear Magnetic Resonance Spectroscopy},
volume = {38},
number = {4},
pages = {325-360},
year = {2001},
issn = {0079-6565},
doi = {https://doi.org/10.1016/S0079-6565(00)00033-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079656500000339},
author = {J.A. Jones}
}
@article{YOON2021100865,
title = {United States and South Korean citizens’ interpretation and assessment of COVID-19 quantitative data},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100865},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100865},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000262},
author = {Hyunkyoung Yoon and Cameron O’Neill Byerley and Surani Joshua and Kevin Moore and Min Sook Park and Stacy Musgrave and Laura Valaas and James Drimalla},
keywords = {COVID-19, Graphs, Representations of quantitative data, Rate of change, Exponential growth},
abstract = {We investigate United States and South Korean citizens’ mathematical schemes and how these schemes supported or hindered their attempts to assess the severity of COVID-19. We selected web and media-based COVID-19 data representations that we hypothesized citizens would interpret differently depending on their mathematical schemes. We included items that we conjectured would be easier or more difficult to interpret with schemes that prior research had reported were more or less productive, respectively. We used the representations during clinical interviews with 25 United States and seven South Korean citizens. We illustrate that citizens’ mathematical schemes (as well as their beliefs) impacted how they assessed the severity of COVID-19. We present vignettes of citizens’ schemes that inhibited interpreting representations of COVID-19 in ways compatible with the displayed quantitative data, schemes that aided them in assessing the severity of COVID-19, and beliefs about the reliability of scientific data that overrode their mathematical conclusions.}
}
@article{SAWLEY1994363,
title = {A comparative study of the use of the data-parallel approach for compressible flow calculations},
journal = {Parallel Computing},
volume = {20},
number = {3},
pages = {363-373},
year = {1994},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(06)80019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106800190},
author = {M.L. Sawley and C.M. Bergman},
keywords = {Computational fluid dynamics, Euler equations, Data-parallel programming, Portability, Performance results},
abstract = {The results are presented of an investigation into the use of the data-parallel programming approach on four different massively-parallel computers: the MasPar MP-1 and MP-2 and the Thinking Machines CM-200 and CM-5. A code to calculate inviscid compressible flow, originally written in FORTRAN 77 for a traditional vector computer, has been re-written entirely in Fortran 90 to take advantage of the compilers available on the massively-parallel computers. It is shown that the discretization of the governing equations on a regular mesh is well adapted to data parallelism. For a typical test problem of supersonic flow through a ramped duct, computational speeds have been achieved using these massively-parallel computers that are superior to those obtained using a single processor of a Cray Y-MP. In addition, this study has enabled the question of code portability between the different computers to be assessed.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{GOSAK2024103888,
title = {The ChatGPT effect and transforming nursing education with generative AI: Discussion paper},
journal = {Nurse Education in Practice},
volume = {75},
pages = {103888},
year = {2024},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2024.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1471595324000179},
author = {Lucija Gosak and Lisiane Pruinelli and Maxim Topaz and Gregor Štiglic},
keywords = {Artificial Intelligence, ChatGPT, Documentation, Education, Nursing, Nursing Diagnosis},
abstract = {Aim
The aim of this study is to present the possibilities of nurse education in the use of the Chat Generative Pre-training Transformer (ChatGPT) tool to support the documentation process.
Background
The success of the nursing process is based on the accuracy of nursing diagnoses, which also determine nursing interventions and nursing outcomes. Educating nurses in the use of artificial intelligence in the nursing process can significantly reduce the time nurses spend on documentation.
Design
Discussion paper.
Methods
We used a case study from Train4Health in the field of preventive care to demonstrate the potential of using Generative Pre-training Transformer (ChatGPT) to educate nurses in documenting the nursing process using generative artificial intelligence. Based on the case study, we entered a description of the patient's condition into Generative Pre-training Transformer (ChatGPT) and asked questions about nursing diagnoses, nursing interventions and nursing outcomes. We further synthesized these results.
Results
In the process of educating nurses about the nursing process and nursing diagnosis, Generative Pre-training Transformer (ChatGPT) can present potential patient problems to nurses and guide them through the process from taking a medical history, setting nursing diagnoses and planning goals and interventions. Generative Pre-training Transformer (ChatGPT) returned appropriate nursing diagnoses, but these were not in line with the North American Nursing Diagnosis Association – International (NANDA-I) classification as requested. Of all the nursing diagnoses provided, only one was consistent with the most recent version of the North American Nursing Diagnosis Association – International (NANDA-I). Generative Pre-training Transformer (ChatGPT) is still not specific enough for nursing diagnoses, resulting in incorrect answers in several cases.
Conclusions
Using Generative Pre-training Transformer (ChatGPT) to educate nurses and support the documentation process is time-efficient, but it still requires a certain level of human critical-thinking and fact-checking.}
}
@article{RANGEL2012970,
title = {Value normalization in decision making: theory and evidence},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {970-981},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812001201},
author = {Antonio Rangel and John A Clithero},
abstract = {A sizable body of evidence has shown that the brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence.}
}
@article{TAKANO201922,
title = {Difficulty in updating positive beliefs about negative cognition is associated with increased depressed mood},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {64},
pages = {22-30},
year = {2019},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005791617302926},
author = {Keisuke Takano and Julie {Van Grieken} and Filip Raes},
keywords = {depression, Rumination, Memory, Reinforcement learning, Q-learning},
abstract = {Background and objectives
Depressed people hold positive beliefs about negative cognition (e.g., rumination is useful to find a solution), which may motivate those individuals to engage in sustained negative thinking. However, in reality, rumination often leads to unfavorable outcomes. Thus, such beliefs create a large discrepancy between one's expectations and the actual outcome. Therefore, we hypothesized that this prediction error would be associated with increased depressed mood.
Methods
We observed how people update their positive beliefs about negative cognition within a volatile environment, in which negative cognition does not always result in a beneficial outcome. Forty-six participants were offered two response options (retrieving a negative or positive personal memory) and subsequently provided either an economic reward or punishment. Retrieving a negative (rather than positive) memory was initially reinforced, although this action-outcome contingency was reversed during the task. In the control condition, positive memory retrieval was initially reinforced, although a contingency reversal was employed to encourage negative memory retrieval.
Results
Model-based computational modeling revealed that participants who showed a delay in switching from negative to positive (but not from positive to negative) responses experienced increased levels of depressed mood. This delay in switching was also found to be associated with depressive symptoms and trait rumination.
Limitations
The non-clinical nature of the sample may limit the clinical implications of the results.
Conclusions
Difficulty in updating positive beliefs (or outcome predictions) for negative cognition may play an important role in depressive symptomatology.}
}
@article{LANGE2024101191,
title = {What are explanatory proofs in mathematics and how can they contribute to teaching and learning?},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101191},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101191},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000683},
author = {Marc Lange},
keywords = {Explanation, Proof, Generalization, Pedagogy, Unification, Coincidence},
abstract = {This paper will examine several simple examples (drawn from the mathematics literature) where there are multiple proofs of the same theorem, but only some of these proofs are widely regarded by mathematicians as explanatory. These examples will motivate an account of explanatory proofs in mathematics. Along the way, the paper will discuss why deus ex machina proofs are not explanatory, what a mathematical coincidence is, and how a theorem's proper setting reflects the naturalness of various mathematical kinds. The paper will also investigate how context influences which features of a theorem are salient and consequently which proofs are explanatory. The paper will discuss several ways in which explanatory proofs can contribute to teaching and learning, including how shifts in context (and hence in a proof’s explanatory power) can be exploited in a classroom setting, leading students to dig more deeply into why some theorem holds. More generally, the paper will examine how “Why?” questions operate in mathematical thinking, teaching, and learning.}
}
@incollection{PESCE2024123,
title = {Chapter Seven - Creativity and consciousness in motion: The roundtrip of “mindful” and “mindless” processes in embodied creativity},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {287},
pages = {123-151},
year = {2024},
booktitle = {The Neurophysiology of Silence (C): Creativity, Aesthetic Experience and Time},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2024.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612324000785},
author = {Caterina Pesce and Nicoletta Tocci},
keywords = {Creative thinking, Motor creativity, Embodiment, Hypofrontality, Flow, Incubation, Mind wandering, Nature, Green exercise, Mindful movements},
abstract = {In this opinion paper, we make a journey across different accounts of creativity that emphasize either the mindful, conscious and cognitive expression of creativity, or its mindless, unconscious and sensorimotor expression. We try to go beyond dichotomy, putting creativity in motion and outlining its embodied and enactive features. Based on the assumption that no creative act is purely conscious or purely unconscious, our discussion on creativity relies on the distinction of three types of creativity that complementarily contribute to the creative process through shifts in the activation of their substrates in the brain: the deliberate, spontaneous and flow types of creativity. The latter is a hybrid and embodied type, in which movement and physical activity meet creativity. We then focus on the most fascinating contribution of unconscious processes and mind wandering to spontaneous and flow modes of creativity, exploring what happens when the individual apparently takes a break from a deliberate and effortful search for solutions and the creative process progresses through an incubation phase. This phase and the overall creative process can be facilitated by physical activity which, depending on its features and context, can disengage the cognitive control network and free the mind from filters that constrain cognitive processes or, conversely, can engage attentional control on sensorimotor and cognitive task components in a mindful way. Lastly, we focus on the unique features of the outer natural environment of physical activity and of the inner environment during mindful movements that can restore capacities and boost creativity.}
}
@article{COOPER2022100755,
title = {Balboa security v. M&M systems: Forensic accounting for determining commercial damages},
journal = {Journal of Accounting Education},
volume = {58},
pages = {100755},
year = {2022},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100755},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000427},
author = {John R. Cooper and Brett S. Kawada},
keywords = {Forensic accounting, Commercial damages, Litigation, Supplier-customer relationship},
abstract = {The ability of accounting students to apply skills beyond traditional accounting in a thoughtful and analytical way is becoming increasingly important, especially in fraud detection and forensic accounting. This case provides an opportunity for students to use critical thinking and problem-solving skills in applying accounting knowledge to a supplier-customer commercial damages litigation matter. Students are provided with a fact pattern of a supplier-customer relationship where they analyze issues related to commercial damages stemming from sources common in real world forensic accounting cases. Students evaluate the facts, which include not only financial data but also interviews with key personnel of parties to the legal action, and demonstrate an understanding of the issues involved in the case through responses of questions regarding overriding forensic accounting and professional practice issues. Students will also prepare a written commercial damages report demonstrating the ability to effectively communicate their analyses.}
}
@article{ZHURAVLEV2023104934,
title = {Three levels of information processing in the brain},
journal = {Biosystems},
volume = {229},
pages = {104934},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104934},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001090},
author = {Aleksandr V. Zhuravlev},
keywords = {Brain, Information, Consciousness, The hard problem of consciousness, Qualia, Entropy},
abstract = {Information, the measure of order in a complex system, is the opposite of entropy, the measure of chaos and disorder. We can distinguish several levels at which information is processed in the brain. The first one is the level of serial molecular genetic processes, similar in some aspects to digital computations (DC). At the same time, higher cognitive activity is probably based on parallel neural network computations (NNC). The advantage of neural networks is their intrinsic ability to learn, adapting their parameters to specific tasks and to external data. However, there seems to be a third level of information processing as well, which involves subjective consciousness and its units, so called qualia. They are difficult to study experimentally, and the very fact of their existence is hard to explain within the framework of modern physical theory. Here I propose a way to consider consciousness as the extension of basic physical laws – namely, total entropy dissipation leading to a system simplification. At the level of subjective consciousness, the brain seems to convert information embodied by neural activity to a more simple and compact form, internally observed as qualia. Whereas physical implementations of both DC and NNC are essentially approximate and probabilistic, qualia-associated computations (QAC) make the brain capable of recognizing general laws and relationships. While elaborating a behavioral program, the conscious brain does not act blindly or gropingly but according to the very meaning of such general laws, which gives it an advantage compared to any artificial intelligence system.}
}
@article{INDLEKOFER20021035,
title = {Number theory—probabilistic, heuristic, and computational approaches},
journal = {Computers & Mathematics with Applications},
volume = {43},
number = {8},
pages = {1035-1061},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)80012-8},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102800128},
author = {K.-H Indlekofer},
keywords = {Probabilistic number theory, Asymptotic results on arithmetic function, Computational number theory, Stone-Cech compactification, Measure and integration on },
abstract = {After the description of the models of Kubilius, Novoselov and Schwarz, and Spilker, respectively, a probability theory for finitely additive probability measures is developed by use of the Stone-Cech compactification of N. The new model is applied to the result of Erdős and Wintner about the limit distribution of additive functions and to the famous result of Szemerédi in combinatorial number theory. Further, it is explained how conjectures on prime values of irreducible polynomials are used in the search for large prime twins and Sophie Germain primes.}
}
@article{ESCOLAGASCON2023111893,
title = {Who falls for fake news? Psychological and clinical profiling evidence of fake news consumers},
journal = {Personality and Individual Differences},
volume = {200},
pages = {111893},
year = {2023},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2022.111893},
url = {https://www.sciencedirect.com/science/article/pii/S0191886922003981},
author = {Álex Escolà-Gascón and Neil Dagnall and Andrew Denovan and Kenneth Drinkwater and Miriam Diez-Bosch},
keywords = {Fake news, Pseudoscientific information, Cognitive biases, Individual differences, Clinical prevention},
abstract = {Awareness of the potential psychological significance of false news increased during the coronavirus pandemic, however, its impact on psychopathology and individual differences remains unclear. Acknowledging this, the authors investigated the psychological and psychopathological profiles that characterize fake news consumption. A total of 1452 volunteers from the general population with no previous psychiatric history participated. They responded to clinical psychopathology assessment tests. Respondents solved a fake news screening test, which allowed them to be allocated to a quasi-experimental condition: group 1 (non-fake news consumers) or group 2 (fake news consumers). Mean comparison, Bayesian inference, and multiple regression analyses were applied. Participants with a schizotypal, paranoid, and histrionic personality were ineffective at detecting fake news. They were also more vulnerable to suffer its negative effects. Specifically, they displayed higher levels of anxiety and committed more cognitive biases based on suggestibility and the Barnum Effect. No significant effects on psychotic symptomatology or affective mood states were observed. Corresponding to these outcomes, two clinical and therapeutic recommendations related to the reduction of the Barnum Effect and the reinterpretation of digital media sensationalism were made. The impact of fake news and possible ways of prevention are discussed.}
}
@article{MANCHO200655,
title = {A tutorial on dynamical systems concepts applied to Lagrangian transport in oceanic flows defined as finite time data sets: Theoretical and computational issues},
journal = {Physics Reports},
volume = {437},
number = {3},
pages = {55-124},
year = {2006},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2006.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0370157306003401},
author = {Ana M. Mancho and Des Small and Stephen Wiggins},
keywords = {Lagrangian transport, Geophysical fluid flows, Finite time hyperbolicity, Finite time Lyapunov exponents, Stable and unstable manifolds, Transport barriers},
abstract = {In the past 15 years the framework and ideas from dynamical systems theory have been applied to a variety of transport and mixing problems in oceanic flows. The motivation for this approach comes directly from advances in observational capabilities in oceanography (e.g., drifter deployments, remote sensing capabilities, satellite imagery, etc.) which reveal space–time structures that are highly suggestive of the structures one visualizes in the global, geometrical study of dynamical systems theory. In this tutorial, we motivate this approach by showing the relationship between fluid transport in two-dimensional time-periodic incompressible flows and the geometrical structures that exist for two-dimensional area-preserving maps, such as hyperbolic periodic orbits, their stable and unstable manifolds and KAM (Kolmogorov–Arnold–Moser) tori. This serves to set the stage for the attempt to “transfer” this approach to more realistic flows modelling the ocean. However, in order to accomplish this several difficulties must be overcome. The first difficulty that confronts us that any attempt to carry out a dynamical systems approach to transport requires us to obtain the appropriate “dynamical system”, which is the velocity field describing the fluid flow. In general, adequate model velocity fields are obtained by numerical solution of appropriate partial differential equations describing the dynamical evolution of the velocity field. Numerical solution of the partial differential equations can only be done for a finite time interval, and since the ocean is generally not time-periodic, this leads to a new type of dynamical system: a finite-time, aperiodically time-dependent velocity field defined as a data set on a space–time grid. The global, geometrical analysis of transport in such dynamical systems requires both new concepts and new analytical and computational tools, as well as the necessity to discard some of the standard ideas and results from dynamical systems theory. The purpose of this tutorial is to describe these new concepts and analytical tools first using simple dynamical systems where quantities can be computed exactly. We then discuss their computational implications and implementation in the context of a model geophysical flow: a turbulent wind-driven double-gyre in the quasigeostrophic approximation.}
}
@article{MASROURI2025102428,
title = {Animal-skin-pattern-inspired multifunctional composites by generative AI},
journal = {Cell Reports Physical Science},
volume = {6},
number = {2},
pages = {102428},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2025.102428},
url = {https://www.sciencedirect.com/science/article/pii/S266638642500027X},
author = {Milad Masrouri and Akshay Vilas Jadhav and Zhao Qin},
keywords = {composite design, bioinspiration, generative AI, molecular dynamics, elastic network, animal pattern, biomimicry, 3D printing, toughness modulus, multifunctional materials},
abstract = {Summary
Bioinspired composite materials offer several advantages by mimicking the structure of natural counterparts. However, their complex hierarchical structure, compared to the limited number of observations, makes it difficult to extract all the structural features and vary the structure to optimize the materials’ functions without losing their natural features. We applied generative artificial intelligence (GenAI) to design composites inspired by animal skin patterns, leveraging a small dataset to generate diverse configurations that closely emulate natural designs. Our computational simulations investigated the structure-mechanics relationship in these materials, revealing significant variations in mechanical functions and identifying patterns that exhibited superior mechanical properties. We validated these outstanding configurations’ performance through tensile tests on specimens produced by a multimaterial printer. We showcase GenAI’s role in structural augmentation that can yield rational bioinspired designs, complemented by an educational web page with interactive games for public access.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{GOULETCOULOMBE2025,
title = {Time-varying parameters as ridge regressions},
journal = {International Journal of Forecasting},
year = {2025},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169207024000931},
author = {Philippe {Goulet Coulombe}},
abstract = {Time-varying parameter (TVP) models are frequently used in economics to capture structural change. I highlight a rather underutilized fact—that these are actually ridge regressions. Instantly, this makes computations, tuning, and implementation much easier than in the state-space paradigm. Among other things, solving the equivalent dual ridge problem is computationally very fast even in high dimensions, and the crucial ‘amount of time variation’ is tuned by cross-validation. Evolving volatility is dealt with using a two-step ridge regression. I consider extensions that incorporate sparsity (the algorithm selects which parameters vary and which do not) and reduced-rank restrictions (variation is tied to a factor model). To demonstrate the usefulness of the approach, I use it to study the evolution of monetary policy in Canada using large time-varying local projections and TVP-VARs with demanding lag lengths. The applications require the estimation of up to 4600 TVPs, a task within the reach of the new method.}
}
@article{WOODSIDE2011153,
title = {Responding to the severe limitations of cross-sectional surveys: Commenting on Rong and Wilkinson’s perspectives},
journal = {Australasian Marketing Journal (AMJ)},
volume = {19},
number = {3},
pages = {153-156},
year = {2011},
note = {Special Section: Marketing and Public Policy},
issn = {1441-3582},
doi = {https://doi.org/10.1016/j.ausmj.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1441358211000231},
author = {Arch G. Woodside},
keywords = {Direct research, Executives, Folk theory-of-mind, fs/QCA.com, Sensemaking, Surveys, Thinking},
abstract = {While a meta-analysis is necessary to test the claim that the logic dominates the majority of studies, most studies by academic scholars on thinking and actions by executives appear to rely on cross-sectional surveys that use self-reports by executives via scaled (e.g., strongly disagree to strongly agree) instruments whereby one executive per firm completes the instrument and data are collected for 50–500 firms. Useable response rates in these studies are almost always below 30% of the distributions of the surveys. While these studies are sometimes worthwhile for learning how respondents assess concepts and relationships among concepts, Rong and Wilkinson’s perspective on the severe limits to the value of such studies rings true: such surveys reveal more about executives’ sensemaking processes than the actual processes. The limitations of using one-shot, one-person-per-firm, self-reports as valid indicators of causal relationships of actual processes are so severe that academics should do more than think twice before using such surveys as the main method for collecting data – if scholars seek to understand and describe actual processes additional methods are necessary for data collection. The relevant literature includes several gems of exceptionally high quality, validity, and usefulness in the study of actual processes; identifying these studies is a useful step toward reducing the reliance on one-shot self-report surveys.}
}
@article{GUPTA19971,
title = {Future Challenges for Fuzzy-Neural Computing Systems},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {25},
pages = {1-6},
year = {1997},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC'97), Kuala Lumpur, Malaysia, 22-25 September 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41292-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017412924},
author = {Madan M. Gupta},
keywords = {Neural Systems, Fuzzy Systems, Fuzzy Logic, Neural Fuzzy Computing},
abstract = {Recently, several significant advances have been made in two distinct theoretical areas. These theoretical advances have created an innovative field of theoretical and applied interest: fuzzy neural systems. Researchers have provided a theoretical basis in the field while industry has used this theoretical basis to create a new class of machines using the innovative technology of fuzzy neural networks. The theory of fuzzy logic provides a mathematical framework for capturing the uncertainties associated with human cognitive processes, such as thinking and reasoning. It also provides a mathematical morphology for emulating certain perceptual and linguistic attributes associated with human cognition. On the other hand, computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. The integration of these two fields, fuzzy logic and neural networks, has the potential for combining the benefits of these two fascinating fields into a single capsule. The intent of this paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks.}
}
@incollection{MURMAN2012323,
title = {15 - Innovation in aeronautics through Lean Engineering},
editor = {Trevor M. Young and Mike Hirst},
booktitle = {Innovation in Aeronautics},
publisher = {Woodhead Publishing},
pages = {323-360},
year = {2012},
isbn = {978-1-84569-550-7},
doi = {https://doi.org/10.1533/9780857096098.3.323},
url = {https://www.sciencedirect.com/science/article/pii/B978184569550750015X},
author = {E.M. Murman},
keywords = {Lean Engineering, Lean Product Development},
abstract = {Abstract:
The dynamics of innovation theory indicate that, for products as mature as aircraft, process innovation is an important contributor to product success and innovation. Many aerospace companies have adopted Lean Thinking as an enterprise-wide continuous improvement strategy. This chapter extends Lean Thinking to the engineering domain with a Lean Engineering framework based upon observational findings from a decade of research in the aerospace domain, published works on Toyota and Southwest Airlines, and practitioner input. Examples illustrate how the framework maybe be applied. Lean Engineering is not totally new to aerospace, and it continues to evolve. Future challenges are briefly summarized.}
}
@article{BUSBY20161029,
title = {Agent-based computational modelling of social risk responses},
journal = {European Journal of Operational Research},
volume = {251},
number = {3},
pages = {1029-1042},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722171501173X},
author = {J.S. Busby and B.S.S. Onggo and Y. Liu},
keywords = {OR in societal problem analysis, Multiagent systems, Risk management},
abstract = {A characteristic aspect of risks in a complex, modern society is the nature and degree of the public response – sometimes significantly at variance with objective assessments of risk. A large part of the risk management task involves anticipating, explaining and reacting to this response. One of the main approaches we have for analysing the emergent public response, the social amplification of risk framework, has been the subject of little modelling. The purpose of this paper is to explore how social risk amplification can be represented and simulated. The importance of heterogeneity among risk perceivers, and the role of their social networks in shaping risk perceptions, makes it natural to take an agent-based approach. We look in particular at how to model some central aspects of many risk events: the way actors come to observe other actors more than external events in forming their risk perceptions; the way in which behaviour both follows risk perception and shapes it; and the way risk communications are fashioned in the light of responses to previous communications. We show how such aspects can be represented by availability cascades, but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements, and the differentiation of private and public risk beliefs. Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically – such as endogenously-produced peaks in risk concern – can be explained by this model.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{SAVIN202110,
title = {Main topics in EIST during its first decade: A computational-linguistic analysis},
journal = {Environmental Innovation and Societal Transitions},
volume = {41},
pages = {10-17},
year = {2021},
note = {Celebrating a decade of EIST: What’s next for transition studies?},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S221042242100037X},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Machine learning, Topic modelling, Literature review},
abstract = {We analyse 465 articles published in EIST from June 2011 until June 2021 to identify topics addressed in the journal. We find eight main topics and assess how their shares changed over time as well as how many citations they received. The topics with the largest shares in all publications are “Theory of socio-technical transitions” and “Urban regimes and niches”. The two most cited topics, “Theory of socio-technical transitions” and “Geography and diffusion of eco-innovations”, showed a rising share over time, while the share of topic “Finance, investment and growth” declined. We further assess the geographical coverage of topics, through affiliations of the corresponding authors. The resulting map indicates dominant topics for the 34 countries that contributed to publications in EIST.}
}
@article{SUN201859,
title = {An ecosystemic framework for business sustainability},
journal = {Business Horizons},
volume = {61},
number = {1},
pages = {59-72},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317301271},
author = {Jiazhe Sun and Shunan Wu and Kaizhong Yang},
keywords = {Business sustainability, Systems theory, Ecosystemic theory, Complexity science, Adaptive management, Corporate sustainability},
abstract = {This article introduces an ecosystemic framework to foster innovation for business sustainability. We emphasize the idea of systemic thinking in which the business operates as a system similar to a living organism. In this framework, businesses impact the environment in which they operate in a fluid, dynamic, and interdependent way. This approach contrasts with the linear approach commonly used in business and other disciplines, which tries to explain what might cause an action or reaction but ignores any feedback effect between the subsequent action and its cause. This article offers practical solutions and guidance for business leaders to incorporate complexity science into creating sustainable businesses.}
}
@article{ASIF202532,
title = {Machine learning-driven catalyst design, synthesis and performance prediction for CO2 hydrogenation},
journal = {Journal of Industrial and Engineering Chemistry},
volume = {144},
pages = {32-47},
year = {2025},
issn = {1226-086X},
doi = {https://doi.org/10.1016/j.jiec.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S1226086X24006269},
author = {Muhammad Asif and Chengxi Yao and Zitu Zuo and Muhammad Bilal and Hassan Zeb and Seungjae Lee and Ziyang Wang and Taesung Kim},
keywords = {Heterogeneous catalysis, DFT calculation, Machine learning, 3D printing, CO hydrogenation},
abstract = {Atmospheric concentrations of CO2 must be lowered to mitigate climate change and rising global temperatures. CO2 utilization is the most promising approach for the sustainable reduction of CO2 emissions. Interdisciplinary research is gaining increasing attention due to its broader application potential and the promising results of combining various fields. Computational approaches in catalytic research could be cost-effective and environmentally friendly. Machine Learning (ML) and 3D printing technologies may soon be able to produce nanoscale raw materials to synthesize the catalyst for commercial-scale applications. In this review article, recent advances in catalyst synthesis using 3D printing technologies and ML-based catalytic reactions, particularly those in CO2 hydrogenation, are critically analyzed, with a focus on the function of ML model prediction. ML approaches with high prediction accuracies are discussed comprehensively. Based on the literature Gray-box models can provide useful insights by revealing the essential catalytic traits, factors, and circumstances that affect the results. They can also provide a practical solution by fusing the benefits of black-box algorithms, such as ensemble models and NNs, with feature importance analysis. Finally, suggestions and recommendations for the potential applications of ML in chemical science, especially in heterogeneous catalysis, are provided along with future research directions.}
}
@article{WANG2024109589,
title = {Cellular gradient algorithm for solving complex mechanical optimization design problems},
journal = {International Journal of Mechanical Sciences},
volume = {282},
pages = {109589},
year = {2024},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2024.109589},
url = {https://www.sciencedirect.com/science/article/pii/S0020740324006301},
author = {Rugui Wang and Xinpeng Li and Haibo Huang and Zhipeng Fan and Fuqiang Huang and Ningjuan Zhao},
keywords = {Mechanical optimization, Optimization algorithm, Discrete integrable problem, Cellular automaton, Gradient descent},
abstract = {In mechanical optimization design problems, there are often some non-continuous or non-differentiable objective functions. For these non-continuous and non-differentiable optimization objectives, it is often difficult for existing optimal design algorithms to find the desired optimal solutions. In this paper, we incorporate the idea of gradient descent into cellular automata and propose a Cellular Gradient (CG) method. First, we have given the basic rules and algorithmic framework of CG and designed three kinds of growth and extinction rules respectively. Then, the three evolutionary rules for cellular within a single cycle are analyzed separately for form and ordering. The best expressions for the cellular jealous neighbor rule and the solitary regeneration rule are given, and the most appropriate order in which the rules are run is selected. Finally, the solution results of the cellular gradient algorithm and other classical optimization design algorithms are compared with a multi-objective multi-parameter mechanical optimization design problem as an example. The computational results show that the cellular gradient algorithm has an advantage over other algorithms in solving global and dynamic mechanical optimal design problems. The novelty of CG is to provide a new way of thinking for solving optimization problems with global discontinuities.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{WELLEK1961715,
title = {The contribution of the perception-typological approaches to the typology of character, and the role of sensation, imagination, and thinking in the organizational concept of personality},
journal = {Acta Psychologica},
volume = {19},
pages = {715-723},
year = {1961},
issn = {0001-6918},
doi = {https://doi.org/10.1016/S0001-6918(61)80321-1},
url = {https://www.sciencedirect.com/science/article/pii/S0001691861803211},
author = {Albert Wellek}
}
@article{ANURADHA2022100429,
title = {A RNN based offloading scheme to reduce latency and preserve energy using RNNBOS},
journal = {Measurement: Sensors},
volume = {24},
pages = {100429},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422000630},
author = {C. Anuradha and M. Ponnavaikko},
keywords = {Computational offloading, Mobile edge computing, Deep neural network, Energy consumption and mobile cloud computing},
abstract = {Mobile cloud computing is currently evolving quickly in today's trend and it provides infinite number of applications to the people those who are using regularly.MCC means the mobile gadgets are strongly tied up with cloud technology to execute various application for attaining many tasks. Mobile devices contain different application according to its own capacity to hold each application. In which many applications are in need of connecting with cloud storage. A new proposed technique named RNNBOS (Recurrent Neural Network Based Offloading scheme) is used to compute calculations in terms of energy source of mobile device along with active conditions of network, Load computations, delay possibility of request from device and quantitative amount of data being transferred for this purpose. We have simulated the above technique using python tool and observed RNN based offloading scheme is good in execution of application using MCC.}
}
@article{LIU202257,
title = {Hierarchical neighborhood entropy based multi-granularity attribute reduction with application to gene prioritization},
journal = {International Journal of Approximate Reasoning},
volume = {148},
pages = {57-67},
year = {2022},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000809},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Gene selection, Granular computing, Multi-granularity attribute reduction, Neighborhood rough set, Trilevel thinking},
abstract = {As a prominent model of granular computing, neighborhood rough set provides clear granularity organization and expression in terms of inherent parameter (neighborhood radius). Such characteristic is widely captured in a plenitude of attribute reduction procedures, while igniting a tricky issue of tuning parameters. In this study, we therefore propose a parameter-free multi-granularity attribute reduction scheme. Fundamentally, our scheme applies three-way decision as thinking in threes. First, data-aware multi-granularity structure is automatically induced from self-contained distance space instead of manually edited or appointed granularities. Second, a novel multi-granularity feature evaluation criterion named hierarchical neighborhood entropy is defined to measure the feature significance. Finally, a sequential forward searching algorithm is designed to find the optimal reduct. With application to gene prioritization, our method performed on microarray data is experimentally demonstrated to be more effective and efficient in differentially expressed genes discovery as compared with other well-established attribute reduction algorithms.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{BOSL2025101480,
title = {Dynamical measures of developing neuroelectric fields in emerging consciousness},
journal = {Current Opinion in Behavioral Sciences},
volume = {61},
pages = {101480},
year = {2025},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101480},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001311},
author = {William J Bosl and Jenny R {Capua Shenkar}},
abstract = {Human consciousness emerges over time. From the moment of conception, a process of neurodevelopment and complexification begins, generating and supporting a neuroelectric field that can be quantified by computational methods from dynamical systems theory. In the early embryo, genetically driven cellular processes are mediated by endogenous electromagnetic fields and intrinsic electrical fields produced by migrating neurons. In the ambient cellular environment, these interactions influence each other, impacting neural migration. The emergence of Theory of Mind, often considered a hallmark of conscious awareness, is accompanied by increasing neural connectivity, neuroelectric field complexity, and more integrated information processing. Neurodegeneration in old age and the often-associated decline in conscious awareness correlate closely with changes in the dynamical complexity of the neuroelectric field. Monitoring trajectories of the neuroelectric field and its complexity changes through the lifespan presents a developmental perspective and empirical correlation for studying the emergence and decline of human consciousness.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{QIAN2024120487,
title = {E3WD: A three-way decision model based on ensemble learning},
journal = {Information Sciences},
volume = {667},
pages = {120487},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120487},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524004006},
author = {Jin Qian and Di Wang and Ying Yu and XiBei Yang and Shang Gao},
keywords = {Three-way decision, Ensemble strategy, Cluster ensemble, Membership degree, Critical threshold},
abstract = {Three-way decision model is an effective way to deal with complex decision problems. However, since the three-way decision models now proposed are all based on a single decision criterion, the decision results typically reflect only one preference of decision-makers. Thus, these models may also not effectively deal with complex decision-making problems. To solve the above problems, this paper proposes a new three-way decision model based on ensemble learning. Specifically, we first obtain different three-way decision results by employing different decision criteria. Then, we can acquire the core and candidate sets of the positive and negative regions through set operations. Next, we use the K-means algorithm to divide the candidate sets into three disjoint subsets based on similarities. After that, we adopt a hierarchical filtering method to select suitable objects from the candidate sets and add them to the core sets. Finally, we employ four three-way decision models with different decision criteria as examples to conduct experiments on eight datasets. Experimental results show that our proposed model can obtain higher classification accuracy and lower deferment rate than other traditional three-way decision models under most experimental conditions.}
}
@article{ALDAYA2024116708,
title = {Tachyons in “momentum-space” representation},
journal = {Nuclear Physics B},
volume = {1008},
pages = {116708},
year = {2024},
issn = {0550-3213},
doi = {https://doi.org/10.1016/j.nuclphysb.2024.116708},
url = {https://www.sciencedirect.com/science/article/pii/S0550321324002748},
author = {V. Aldaya and J. Guerrero and F.F. López-Ruiz},
abstract = {Obtaining the momentum space associated with tachyonic “particles” from the Poincaré group manifold proves to be rather intricate, departing very much from the ordinary dual to Minkowski space directly parametrized by space-time translations of the Poincaré group. In fact, although described by the constants of motion (Noether invariants) associated with space-time translations, they depend non-trivially on the parameters of the rotation subgroup. However, once the momentum space is parametrized by the Noether invariants, it behaves as that of ordinary particles. On the other hand, the evolution parameter is no longer the one associated with time translation, whose Noether invariant, Po, is now a basic one. Evolution takes place in a spatial direction. These facts not only make difficult the computation of the corresponding representation, but also force us to a sound revision of several traditional ingredients related to Cauchy hypersurface, scalar product and, of course, causality. After that, the theory becomes consistent and could shed new light on some special physical situations like inflation or traveling inside a black hole.}
}
@article{MATSUDA2009970,
title = {Multiple cognitive deficits in patients during the mild cognitive impairment stage of Alzheimer's disease: how are cognitive domains other than episodic memory impaired?},
journal = {International Psychogeriatrics},
volume = {21},
number = {5},
pages = {970-976},
year = {2009},
issn = {1041-6102},
doi = {https://doi.org/10.1017/S1041610209990330},
url = {https://www.sciencedirect.com/science/article/pii/S1041610224025894},
author = {Osamu Matsuda and Masahiko Saito},
keywords = {Alzheimer's disease, COGNISTAT, mild cognitive impairment},
abstract = {ABSTRACT
Background: Little is known about how cognitive domains other than episodic memory are affected during the mild cognitive impairment (MCI) stage of Alzheimer's disease (AD). We attempted to clarify this issue in this study. Methods: Fifty-seven Japanese subjects were divided into two groups: one comprising people in the MCI stage of AD (MCI group, n = 28) and the other of normal controls (NC group, n = 29). Cognitive functions were assessed using the Japanese version of the neurobehavioral cognitive status examination (J-COGNISTAT). Results: The MCI group performed significantly worse than the NC group on subtests that assessed orientation, confrontational naming, constructive ability, episodic memory, and abstract thinking. Three-quarters of the MCI group had deficits in memory and other non-mnemonic domains, particularly constructive ability and abstract thinking. However, within-subject comparisons showed that the MCI group performed significantly worse on the memory subtest compared to any other subtest. Conclusions: Besides episodic memory, multiple non-mnemonic cognitive domains, such as constructive ability and abstract thinking, are also impaired during the MCI stage of AD; however, these non-mnemonic deficits are smaller than episodic memory impairment.}
}
@article{TSUTAKAWA2020102972,
title = {Envisioning how the prototypic molecular machine TFIIH functions in transcription initiation and DNA repair},
journal = {DNA Repair},
volume = {96},
pages = {102972},
year = {2020},
issn = {1568-7864},
doi = {https://doi.org/10.1016/j.dnarep.2020.102972},
url = {https://www.sciencedirect.com/science/article/pii/S1568786420302214},
author = {Susan E. Tsutakawa and Chi-Lin Tsai and Chunli Yan and Amer Bralić and Walter J. Chazin and Samir M. Hamdan and Orlando D. Schärer and Ivaylo Ivanov and John A. Tainer},
keywords = {TFIIH, Helicase, Transcription initiation, Transcription-coupled repair, Nucleotide excision repair, XPB, XPD, Translocase, DNA damage, DNA repair},
abstract = {Critical for transcription initiation and bulky lesion DNA repair, TFIIH provides an exemplary system to connect molecular mechanisms to biological outcomes due to its strong genetic links to different specific human diseases. Recent advances in structural and computational biology provide a unique opportunity to re-examine biologically relevant molecular structures and develop possible mechanistic insights for the large dynamic TFIIH complex. TFIIH presents many puzzles involving how its two SF2 helicase family enzymes, XPB and XPD, function in transcription initiation and repair: how do they initiate transcription, detect and verify DNA damage, select the damaged strand for incision, coordinate repair with transcription and cell cycle through Cdk-activating-kinase (CAK) signaling, and result in very different specific human diseases associated with cancer, aging, and development from single missense mutations? By joining analyses of breakthrough cryo-electron microscopy (cryo-EM) structures and advanced computation with data from biochemistry and human genetics, we develop unified concepts and molecular level understanding for TFIIH functions with a focus on structural mechanisms. We provocatively consider that TFIIH may have first evolved from evolutionary pressure for TCR to resolve arrested transcription blocks to DNA replication and later added its key roles in transcription initiation and global DNA repair. We anticipate that this level of mechanistic information will have significant impact on thinking about TFIIH, laying a robust foundation suitable to develop new paradigms for DNA transcription initiation and repair along with insights into disease prevention, susceptibility, diagnosis and interventions.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{YANG2018182,
title = {A Geodesign Method of Human-Energy-Water Interactive Systems for Urban Infrastructure Design: 10KM2 Near-Zero District Project in Shanghai},
journal = {Engineering},
volume = {4},
number = {2},
pages = {182-189},
year = {2018},
note = {Sustainable Infrastructure},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918301978},
author = {Perry Pei-Ju Yang and Cheryl Shu-Fang Chi and Yihan Wu and Steven Jige Quan},
keywords = {Geodesign, Urban design, Urban infrastructure, Energy performance, Iterative process, Multi-objective optimization},
abstract = {The grand challenges of climate change demand a new paradigm of urban design that takes the performance of urban systems into account, such as energy and water efficiency. Traditional urban design methods focus on the form-making process and lack performance dimensions. Geodesign is an emerging approach that emphasizes the links between systems thinking, digital technology, and geographic context. This paper presents the research results of the first phase of a larger research collaboration and proposes an extended geodesign method for a district-scale urban design to integrate systems of renewable energy production, energy consumption, and storm water management, as well as a measurement of human experiences in cities. The method incorporates geographic information system (GIS), parametric modeling techniques, and multidisciplinary design optimization (MDO) tools that enable collaborative design decision-making. The method is tested and refined in a test case with the objective of designing a near-zero-energy urban district. Our final method has three characteristics. ① Integrated geodesign and parametric design: It uses a parametric design approach to generate focal-scale district prototypes by means of a custom procedural algorithm, and applies geodesign to evaluate the performances of design proposals. ② A focus on design flow: It elaborates how to define problems, what information is selected, and what criteria are used in making design decisions. ③ Multi-objective optimization: The test case produces indicators from performance modeling and derives principles through a multi-objective computational experiment to inform how the design can be improved. This paper concludes with issues and next steps in modeling urban design and infrastructure systems based on MDO tools.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{DIETRICH200722,
title = {Who’s afraid of a cognitive neuroscience of creativity?},
journal = {Methods},
volume = {42},
number = {1},
pages = {22-27},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306003100},
author = {Arne Dietrich},
keywords = {Consciousness, Insight, Prefrontal cortex, Right brain, Divergent thinking, Neuroimaging, Attention},
abstract = {This article has two goals. First, the ideas outlined here can be seen as a sustained and disciplined demolition project aimed at sanitizing our bad habits of thinking about creativity. Apart from the enormous amount of fluff out there, the study of creativity is, quite unfortunately, still dominated by a number of rather dated ideas that are either so simplistic that nothing good can possibly come out of them or, given what we know about the brain, factually mistaken. As cognitive neuroscience is making more serious contact with the knowledge base of creativity, we must, from the outset, clear the ground of these pernicious fossil traces from a bygone era. The best neuroimaging techniques help little if we don’t know what to look for. Second, as an antidote to these theoretical duds, the article offers fresh ideas on possible mechanisms of creativity. Given that they are grounded in current understanding of cognitive and neural processes, it is hoped that these ideas represent steps broadly pointing in the right direction. In the end, the fundamental question we must ask ourselves is what, exactly, are the mental processes—or their critical elements—that yield creative thoughts.}
}
@article{SOUZA2025112323,
title = {Techniques for eliciting IoT requirements: Sensorina Map and Mind IoT},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112323},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112323},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003674},
author = {Sabrina Souza and Eriky Rodrigues and Maria Meireles and Tanara Lauschner and Leandro Carvalho and José Carlos Maldonado and Tayana Conte},
keywords = {Requirements engineering, Design Thinking, Requirements’ elicitation techniques, Internet of Things},
abstract = {Context:
The Internet of Things (IoT) involves heterogeneous devices that interact and process data via the Internet. In the development of IoT systems, requirement elicitation is crucial. However, challenges such as heterogeneity, interoperability, scalability, and requirements volatility necessitate new approaches or adapting traditional techniques.
Objective:
In this context, this work proposes the Sensorina Map and IoT Mind as techniques adapted from the Empathy Map and Mind Map, respectively, to support requirement elicitation in IoT systems.
Method:
Two empirical studies were conducted in an academic environment to assess the feasibility of the techniques, then, a case study in industry environment.
Results:
The first study analyzed the ease of use and evaluated if it assisted software engineers in remembering the system requirements. The participants’ perceptions were collected through a Focus Group, refining the techniques. Subsequently, an observational study evaluated the techniques’ usefulness and ease of use. The results of the study demonstrated that the participants considered the methods feasible. The case study results revealed that the Sensorina Map is more suitable for advanced stages. At the same time, the Mind IoT suits better the initial phases, emphasizing the need for practical examples and adaptations to suit diverse user profiles.
Conclusion:
This work is expected to advance research in IoT systems and benefit professionals and researchers in this area.}
}
@article{MATHIS20245814,
title = {Decoding the brain: From neural representations to mechanistic models},
journal = {Cell},
volume = {187},
number = {21},
pages = {5814-5832},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424009802},
author = {Mackenzie Weygandt Mathis and Adriana {Perez Rotondo} and Edward F. Chang and Andreas S. Tolias and Alexander Mathis},
keywords = {decoding, encoding, deep learning, data-driven, normative models, BCIs, language},
abstract = {Summary
A central principle in neuroscience is that neurons within the brain act in concert to produce perception, cognition, and adaptive behavior. Neurons are organized into specialized brain areas, dedicated to different functions to varying extents, and their function relies on distributed circuits to continuously encode relevant environmental and body-state features, enabling other areas to decode (interpret) these representations for computing meaningful decisions and executing precise movements. Thus, the distributed brain can be thought of as a series of computations that act to encode and decode information. In this perspective, we detail important concepts of neural encoding and decoding and highlight the mathematical tools used to measure them, including deep learning methods. We provide case studies where decoding concepts enable foundational and translational science in motor, visual, and language processing.}
}
@article{NOVOTOTSKYVLASOV1995S114,
title = {PS-12-13 Event-related brain activity analysis by mean wave halfperiod duration computation method},
journal = {Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control},
volume = {97},
number = {4},
pages = {S114},
year = {1995},
issn = {0924-980X},
doi = {https://doi.org/10.1016/0924-980X(95)92838-D},
url = {https://www.sciencedirect.com/science/article/pii/0924980X9592838D},
author = {V.Y. Novototsky-Vlasov}
}
@article{WANG2024111842,
title = {Three-way decision based island harmony search algorithm for robust flow-shop scheduling with uncertain processing times depicted by big data},
journal = {Applied Soft Computing},
volume = {162},
pages = {111842},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111842},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006161},
author = {Bing Wang and Pengfei Zhang and Xiaozhi Wang and Quanke Pan},
keywords = {Robust flow-shop scheduling, Island harmony search, Big data, Three-way decision, Surrogate worst-case scenario},
abstract = {This paper discusses an uncertain two-machine permutation flow-shop scheduling problem (2PFSP) with total weighted tardiness and common due date. Uncertain processing times are described by a large set of discrete scenarios, which is a type of big data. The objective is to minimize the schedule performance under the worst-case scenario. Identifying the worst-case scenario for each evaluated schedule is quite time-consuming in the situation that the scenario set size is large so that the objective evaluation might be computationally expensive. To handle this difficulty, three-way decision is used to preprocess the large-size scenario set to get a reduced scenario set so that the concept of surrogate worst-case scenario is adopted. A hybrid harmony search algorithm of combining three-island framework and the scenario-based local search is developed to solve the discussed problem. Based on the single-scenario knowledge of 2PFSP, a problem-specific scenario-dependent neighborhood structure is constructed under the surrogate worst-case scenario. An extensive experiment was carried out. The computational results show that the application of surrogate worst-case scenario based on three-way decision is effective in reducing the time consuming while keeping schedule performance evaluation. Being compared to the worst-case scenario objective evaluation, for an example in the case of the middle bad-scenario ratio, the surrogate worst-case scenario objective evaluation made the solution algorithm save 12.95 % in average CPU time for all instances while the relative performance difference is only 1.809 % in average. Being compared to possible alternative algorithms derived from the state-of-the-art algorithms, the developed algorithm is advantageous for the addressed problems.}
}
@article{KINNEAR2024101190,
title = {Lecturers' use of questions in undergraduate mathematics lectures},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101190},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101190},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000671},
author = {George Kinnear and Gemma Hood and Eloise Lardet and Colette Sheard and Colin Foster},
keywords = {Funneling, Lecturing styles, Questioning, Student participation, University mathematics},
abstract = {Mathematics lecturers frequently ask questions in their lectures, and these questions presumably play an important role in students’ thinking about and learning of the lecture content. We replicated and developed a coding scheme used in previous research in the US to categorise lecturers’ questions in a sample of 136 lectures given by 24 lecturers at a research-intensive UK university. We found that the coding scheme could be applied reliably, and that factual questions were predominant (as in previous research). We explore differences in the lecturers’ use of questions – both between our UK sample and the previous US work, and between individual lecturers in our sample. We note the presence of strings of related successive questions from the lecturer, which we term ‘question chains’. We explore the nature of these, examine their prevalence, and seek to account for them in terms of the lecturers’ possible intentions.}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{ANDERSON1998214,
title = {Stereovision: beyond disparity computations},
journal = {Trends in Cognitive Sciences},
volume = {2},
number = {6},
pages = {214-222},
year = {1998},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(98)01180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661398011802},
author = {Barton L Anderson},
keywords = {sterovision, disparity, 3-D sterograms, perceptual grouping, occlusion},
abstract = {One of the most powerful sources of information about three-dimensional (3-D) structure is provided by stereovision (or stereopsis). For over a century, theoretical and empirical investigations into this ability have focused on the role of binocular disparity in generating percepts of 3-D structure. Recent work in image segmentation demonstrates that stereovision can cause large changes in perceptual organization that cannot be understood on the basis of binocular disparity alone. It is argued that these phenomena reveal the need for theoretical tools beyond those that have dominated the study of visual perception over the past three decades.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@article{CHEVRETTE20212024,
title = {The confluence of big data and evolutionary genome mining for the discovery of natural products},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2024-2040},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00013f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008789},
author = {Marc G. Chevrette and Athina Gavrilidou and Shrikant Mantri and Nelly Selem-Mojica and Nadine Ziemert and Francisco Barona-Gómez},
abstract = {ABSTRACT
This review covers literature between 2003–2021 The development and application of genome mining tools has given rise to ever-growing genetic and chemical databases and propelled natural products research into the modern age of Big Data. Likewise, an explosion of evolutionary studies has unveiled genetic patterns of natural products biosynthesis and function that support Darwin's theory of natural selection and other theories of adaptation and diversification. In this review, we aim to highlight how Big Data and evolutionary thinking converge in the study of natural products, and how this has led to an emerging sub-discipline of evolutionary genome mining of natural products. First, we outline general principles to best utilize Big Data in natural products research, addressing key considerations needed to provide evolutionary context. We then highlight successful examples where Big Data and evolutionary analyses have been combined to provide bioinformatic resources and tools for the discovery of novel natural products and their biosynthetic enzymes. Rather than an exhaustive list of evolution-driven discoveries, we highlight examples where Big Data and evolutionary thinking have been embraced for the evolutionary genome mining of natural products. After reviewing the nascent history of this sub-discipline, we discuss the challenges and opportunities of genomic and metabolomic tools with evolutionary foundations and/or implications and provide a future outlook for this emerging and exciting field of natural product research.}
}
@incollection{TONDEUR2024184,
title = {Chapter 6 - Batch control spike},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {184-239},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000314},
author = {Yves Tondeur},
keywords = {Compliance with PBMS/methods innovation rule & ISO 17025, Critical step, Erroneous beliefs & mental models, Function of the labeled standards, Out-of-control analytical system, Performance improvement, Sample fortification integrity, Systematic errors, Technology-in-use definition, Thinking method, Traditional QC samples effectiveness, Working relative response factors},
abstract = {When we are unaware of the causes for the observed deviations, we are more likely to react. When we react or are unaware of problems, we fail. Increasing the effectiveness of quality control samples starts with addressing what we do not know about them. So, this chapter first clarifies what we want, then describes what we have, and lastly what can be done to fill the gaps. The development, validation, and application of the batch control spike is a great illustration highlighting the importance the quality of the performance feedback and learning capacity the control samples are supposed to provide. When done correctly—while contextually questioning the relevance and appropriateness of current operating criteria, imposed limits and standards—the introduction of the batch control spike allows a process of critical errors identification, compensation, and progressive elimination to take place so that at the end, no significant systematic errors remain. This fact is demonstrated using z-scores from multiple international round-robin studies. In the context of achieving accuracy, the batch control spike examines the relationships between the standards used, when they are spiked, how they are spiked, and their purpose, that is, it renders the technology-in-use (isotope dilution) transparent and keeps it honest. It is also an excellent teaching tool. It is a quality learning sample helping the transformation of our methods into “thinking tools.”}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{STONE2022419,
title = {On second thoughts: changes of mind in decision-making},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {5},
pages = {419-431},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000407},
author = {Caleb Stone and Jason B. Mattingley and Dragan Rangelov},
keywords = {decision-making, change of mind, sequential sampling, metacognition},
abstract = {The ability to change initial decisions in the face of new or potentially conflicting information is fundamental to adaptive behavior. From perceptual tasks to multiple-choice tests, research has shown that changes of mind often improve task performance by correcting initial errors. Decision makers must, however, strike a balance between improvements that might arise from changes of mind and potential energetic, temporal, and psychological costs. In this review, we provide an overview of the change-of-mind literature, focusing on key behavioral findings, computational mechanisms, and neural correlates. We propose a conceptual framework that comprises two core decision dimensions – time and evidence source – which link changes of mind across decision contexts, as a first step toward an integrated psychological account of changes of mind.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{ZHAN2024121679,
title = {Conceptualizing future groundwater models through a ternary framework of multisource data, human expertise, and machine intelligence},
journal = {Water Research},
volume = {257},
pages = {121679},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121679},
url = {https://www.sciencedirect.com/science/article/pii/S0043135424005803},
author = {Chuanjun Zhan and Zhenxue Dai and Shangxian Yin and Kenneth C. Carroll and Mohamad Reza Soltanian},
keywords = {Groundwater model, Deep learning, Machine intelligence, Multisource data, Human expertise},
abstract = {Groundwater models are essential for understanding aquifer systems behavior and effective water resources spatio-temporal distributions, yet they are often hindered by challenges related to model assumptions, parametrization, uncertainty, and computational efficiency. Machine intelligence, especially deep learning, promises a paradigm shift in overcoming these challenges. A critical examination of existing machine-driven methods reveals the inherent limitations, particularly in terms of the interpretability and the ability to generalize findings. To overcome these challenges, we develop a ternary framework that synergizes the valuable insights from multisource data, human expertise, and machine intelligence. This framework capitalizes on the distinct strengths of each element: the value and relevance of multisource data, the innovative capacity of human expertise, and the analytical efficiency of machine intelligence. Our goal is to conceptualize sustainable water management practices and enhance our understanding and predictive capabilities of groundwater systems. Unlike approaches that rely solely on abundant data, our framework emphasizes the quality and strategic use of available data, combined with human intellect and advanced computing, to overcome current limitations and pave the way for more realistic groundwater simulations.}
}