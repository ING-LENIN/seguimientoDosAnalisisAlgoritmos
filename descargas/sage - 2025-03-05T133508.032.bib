@article{doi:10.1177/00169862211061874,
author = {Selcuk Acar and Kelly Berthiaume and Katalin Grajzel and Denis Dumas and Charles “Tedd” Flemister and Peter Organisciak},
title = {Applying Automated Originality Scoring to the Verbal Form of Torrance Tests of Creative Thinking},
journal = {Gifted Child Quarterly},
volume = {67},
number = {1},
pages = {3–17},
year = {2023a},
doi = {10.1177/00169862211061874},
URL = {https://doi-org.crai.referencistas.com/10.1177/00169862211061874},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00169862211061874},
abstract = {In this study, we applied different text-mining methods to the originality scoring of the Unusual Uses Test (UUT) and Just Suppose Test (JST) from the Torrance Tests of Creative Thinking (TTCT)–Verbal. Responses from 102 and 123 participants who completed Form A and Form B, respectively, were scored using three different text-mining methods. The validity of these scoring methods was tested against TTCT’s manual-based scoring and a subjective snapshot scoring method. Results indicated that text-mining systems are applicable to both UUT and JST items across both forms and students’ performance on those items can predict total originality and creativity scores across all six tasks in the TTCT-Verbal. Comparatively, the text-mining methods worked better for UUT than JST. Of the three text-mining models we tested, the Global Vectors for Word Representation (GLoVe) model produced the most reliable and valid scores. These findings indicate that creativity assessment can be done quickly and at a lower cost using text-mining approaches.}
}

@article{doi:10.1177/21695067231192245,
author = {Stephen M. Fiore and Matthew Johnson and Paul Robertson and Pablo Diego-Rosell and Adam Fouse},
title = {Transdisciplinary Team Science: Transcending Disciplines to Understand Artificial Social Intelligence in Human-Agent Teaming},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {24},
number = {1},
pages = {419–424},
year = {2006b},
doi = {10.1177/21695067231192245},
URL = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
eprint = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
abstract = {We provide a transdisciplinary viewpoint on creating artificial social intelligence for human-agent teaming. We discuss theoretical, methodological, and technological insights, drawn from different disciplines, to more fully illuminate how cross-disciplinary research can inform research design and development. We unite ideas spanning human factors, cognitive and computer science, and organizational behavior. Grounding our ideas in real world challenges for human-AI teaming, and via a series of questions designed to facilitate synthesis across disciplines, we illustrate how transdisciplinary team science more effectively asks and answers complex questions on human-agent teaming. Our objective is to contribute to research and development in the field of human-AI and human-robot teaming by emphasizing a more human-centered perspective on AI.}
}

@article{doi:10.1177/21695067231192245,
author = {Stephen M. Fiore and Matthew Johnson and Paul Robertson and Pablo Diego-Rosell and Adam Fouse},
title = {Transdisciplinary Team Science: Transcending Disciplines to Understand Artificial Social Intelligence in Human-Agent Teaming},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {67},
number = {1},
pages = {419–424},
year = {2023c},
doi = {10.1177/21695067231192245},
URL = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
eprint = {https://doi-org.crai.referencistas.com/10.1177/21695067231192245},
abstract = {We provide a transdisciplinary viewpoint on creating artificial social intelligence for human-agent teaming. We discuss theoretical, methodological, and technological insights, drawn from different disciplines, to more fully illuminate how cross-disciplinary research can inform research design and development. We unite ideas spanning human factors, cognitive and computer science, and organizational behavior. Grounding our ideas in real world challenges for human-AI teaming, and via a series of questions designed to facilitate synthesis across disciplines, we illustrate how transdisciplinary team science more effectively asks and answers complex questions on human-agent teaming. Our objective is to contribute to research and development in the field of human-AI and human-robot teaming by emphasizing a more human-centered perspective on AI.}
}

@article{doi:10.1177/1094342003017002001,
author = {Bernd Hamann and E. Wes Bethel and Horst Simon and Juan Meza},
title = {NERSC “Visualization Greenbook” Future Visualization                Needs of the Doe Computational Science Community Hosted at NERSC},
journal = {The International Journal of High Performance Computing Applications},
volume = {17},
number = {2},
pages = {097–123},
year = {2003d},
doi = {10.1177/1094342003017002001},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
abstract = {In this paper we present the findings and recommendations that emerged from a one-day workshop held at Lawrence Berkeley National Laboratory (LBNL) on June 5, 2002, in conjunction with the National Energy Research Scientific Computing (NERSC) User Group (NUG) Meeting. The motivation for this workshop was to solicit direct input from the application science community on the subject of visualization. The workshop speakers and participants included computational scientists from a cross-section of disciplines that use the NERSC facility, as well as visualization researchers from across the country. We asked the workshop contributors how they currently visualize their results, and how they would like to do visualization in the future. We were especially interested in each individual’s view of how visualization tools and services could be improved in order to better meet the needs of future computational science projects. The outcome of this workshop is a set of findings and recommendations that are presented in more detail later in this paper, and are briefly summarized here. Scientific visualization is a crucial technological capability that plays an important role in understanding data created by computational science projects as well as experiments. In order to be effective, visualization technology should be easy to use for a non-expert. The term “easy to use” encompasses a number of different categories, including a short learning curve, tight integration with computational frameworks, availability on the desktop as well as the fixed visualization facility, tools that are tailored for each specific application domain, and low cost. Current visualization tools fall short in several key areas of capability. Few visualization tools are capable of processing large datasets, such as those commonly generated at NERSC. Better support for parallel visualization tools may prove useful in leveraging large parallel machines as visualization resources. Multivariate visualization - multiple grids, many species, and many dimensions - is needed in order to quickly gain insight into large datasets. Related “drill-down” capabilities, such as the ability to quickly move from macro to micro views (used in “data mining”), would be extremely helpful in understanding data but are missing from most visualization tools. Many application scientists perceive a conundrum when it comes to visualization support. Support for visualization within each individual program level is often inadequate or nonexistent due to funding constraints, yet support for visualization at the institutional level is also often inadequate or nonexistent. Better solutions are needed for remote visualization. Current approaches are further constrained by network bandwidth and access to resources. The proliferation of visualization tools and data formats poses challenges. Researchers must often master many different tools in order to achieve the desired results. Data format conversion is often required when moving between tools. Common data formats and frameworks for visualization tools are needed to reduce duplication of effort and better promote sharing of resources and results. Better communication is needed between the visualization and computational science communities. The computational scientists are often unaware of current trends and practices in the visualization community. By being more aware of the needs of the computational science community, the visualization research programs can be crafted so as to be more responsive to their needs. As a result of the workshop, we have developed a set of recommendations that can be summarized as follows: • Establish a coherent program that focuses on remote visualization. A remote visualization program should provide tools and infrastructure that can be used by multiple “virtual teams”. • Establish mechanisms whereby generally-applicable visualization technology is developed and deployed in a centralized fashion. • Develop a research program in interactive visualization with running codes that stresses the integrated design and development of coupled simulation-visualization methods. • Establish a research program in the areas of multi-field visualization and multi-dimensional data visualization. • Establish a research program in the area of automated data exploration for next generation petascale datasets.}
}

@article{doi:10.1177/1094342003017002001,
author = {Bernd Hamann and E. Wes Bethel and Horst Simon and Juan Meza},
title = {NERSC “Visualization Greenbook” Future Visualization                Needs of the Doe Computational Science Community Hosted at NERSC},
journal = {The International Journal of High Performance Computing Applications},
volume = {17},
number = {2},
pages = {097–123},
year = {2019e},
doi = {10.1177/1094342003017002001},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342003017002001},
abstract = {In this paper we present the findings and recommendations that emerged from a one-day workshop held at Lawrence Berkeley National Laboratory (LBNL) on June 5, 2002, in conjunction with the National Energy Research Scientific Computing (NERSC) User Group (NUG) Meeting. The motivation for this workshop was to solicit direct input from the application science community on the subject of visualization. The workshop speakers and participants included computational scientists from a cross-section of disciplines that use the NERSC facility, as well as visualization researchers from across the country. We asked the workshop contributors how they currently visualize their results, and how they would like to do visualization in the future. We were especially interested in each individual’s view of how visualization tools and services could be improved in order to better meet the needs of future computational science projects. The outcome of this workshop is a set of findings and recommendations that are presented in more detail later in this paper, and are briefly summarized here. Scientific visualization is a crucial technological capability that plays an important role in understanding data created by computational science projects as well as experiments. In order to be effective, visualization technology should be easy to use for a non-expert. The term “easy to use” encompasses a number of different categories, including a short learning curve, tight integration with computational frameworks, availability on the desktop as well as the fixed visualization facility, tools that are tailored for each specific application domain, and low cost. Current visualization tools fall short in several key areas of capability. Few visualization tools are capable of processing large datasets, such as those commonly generated at NERSC. Better support for parallel visualization tools may prove useful in leveraging large parallel machines as visualization resources. Multivariate visualization - multiple grids, many species, and many dimensions - is needed in order to quickly gain insight into large datasets. Related “drill-down” capabilities, such as the ability to quickly move from macro to micro views (used in “data mining”), would be extremely helpful in understanding data but are missing from most visualization tools. Many application scientists perceive a conundrum when it comes to visualization support. Support for visualization within each individual program level is often inadequate or nonexistent due to funding constraints, yet support for visualization at the institutional level is also often inadequate or nonexistent. Better solutions are needed for remote visualization. Current approaches are further constrained by network bandwidth and access to resources. The proliferation of visualization tools and data formats poses challenges. Researchers must often master many different tools in order to achieve the desired results. Data format conversion is often required when moving between tools. Common data formats and frameworks for visualization tools are needed to reduce duplication of effort and better promote sharing of resources and results. Better communication is needed between the visualization and computational science communities. The computational scientists are often unaware of current trends and practices in the visualization community. By being more aware of the needs of the computational science community, the visualization research programs can be crafted so as to be more responsive to their needs. As a result of the workshop, we have developed a set of recommendations that can be summarized as follows: • Establish a coherent program that focuses on remote visualization. A remote visualization program should provide tools and infrastructure that can be used by multiple “virtual teams”. • Establish mechanisms whereby generally-applicable visualization technology is developed and deployed in a centralized fashion. • Develop a research program in interactive visualization with running codes that stresses the integrated design and development of coupled simulation-visualization methods. • Establish a research program in the areas of multi-field visualization and multi-dimensional data visualization. • Establish a research program in the area of automated data exploration for next generation petascale datasets.}
}

@article{doi:10.1177/0361198105192900110,
author = {N. Kringos and A. Scarpas},
title = {Raveling of Asphaltic Mixes Due to Water Damage: Computational Identification of Controlling Parameters},
journal = {Transportation Research Record},
volume = {1929},
number = {1},
pages = {79–87},
year = {2005f},
doi = {10.1177/0361198105192900110},
URL = {https://doi-org.crai.referencistas.com/10.1177/0361198105192900110},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198105192900110},
abstract = {Open-graded asphalt mixes are often used for wearing surfaces of roads exposed to large amounts of rainfall throughout the year. The high permeability of the mix guarantees fast drainage of the water away from the surface and thus increases road safety. However, the large amounts of water that flow through the asphalt have a negative effect on the material characteristics of the mastic and cause debonding of the aggregates from the mastic, called raveling. To understand and quantify the physical processes and the mechanics leading to raveling, an extensive experimental and analytical investigation is being undertaken at Delft University of Technology in the Netherlands. One goal of the investigation is the development of the finite element tool RoAM (raveling of asphalt mixes), which is capable of simulating the gradual development of damage throughout asphalt mixes due to water infiltration. Desorption, diffusion, and dispersion are included as fundamental processes. This paper shows the results of a computational analysis to identify the impact of the different water damage phenomena and presents results of a sensitivity study of the relevant parameters. From the computational analyses it is concluded that simulation of water damage in asphaltic mixes is possible if the desorption characteristics as well as the diffusion and dispersion coefficients can be determined.}
}

@article{doi:10.1177/1077695812440942,
author = {Michael McDevitt and Shannon Sindorf},
title = {How to Kill a Journalism School: The Digital Sublime in the Discourse of Discontinuance},
journal = {Journalism & Mass Communication Educator},
volume = {67},
number = {2},
pages = {109–118},
year = {2012g},
doi = {10.1177/1077695812440942},
URL = {https://doi-org.crai.referencistas.com/10.1177/1077695812440942},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1077695812440942},
abstract = {The authors argue that journalism’s uncertain identity in academia has made it vulnerable to unreflective instrumentalism in the digital era. They show how instrumentalism intertwined with the digital sublime constitutes a rhetorically resonate rationale for closing a journalism school. Evidence comes from documents and testimony associated with discontinuance of the School of Journalism and Mass Communication at the University of Colorado. Vulnerability of the school became apparent in its own Advisory Board recommending closure. The authors warn against stakeholders in journalism education internalizing the fear and opportunism implicit in a discourse of the digital sublime, a discourse ultimately in service to discontinuance.}
}

@article{doi:10.1177/1059712313492176,
author = {Jonas Ruesch and Ricardo Ferreira and Alexandre Bernardino},
title = {A computational approach on the co-development of artificial visual sensorimotor},
journal = {Adaptive Behavior},
volume = {21},
number = {6},
pages = {452–464},
year = {2013h},
doi = {10.1177/1059712313492176},
URL = {https://doi-org.crai.referencistas.com/10.1177/1059712313492176},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712313492176},
abstract = {To follow a goal-directed behavior, an autonomous agent must be able to acquire knowledge about the causality between its motor actions and corresponding sensory feedback. Since the complexity of such sensorimotor relationships directly influences required cognitive resources, this work proposes that it is of importance to keep the agent’s sensorimotor relationships simple. This implies that the agent should be designed in a way such that sensory consequences can be described and predicted in a simplified manner. Living organisms implement this paradigm by adapting sensory and motor systems specifically to their behavior and environment. As a result, they are able to predict sensorimotor consequences with a strongly limited amount of (expensive) nervous tissue. In this context, the present work proposes that advantageous artificial sensory and motor layouts can be evolved by rewarding the ability to predict self-induced stimuli through simple sensorimotor relationships. Experiments consider a simulated agent recording realistic visual stimuli from natural images. The obtained results demonstrate the ability of the proposed method to (i) synthesize visual sensorimotor structures adapted to an agent’s environment and behavior, and (ii) serve as a computational model for testing hypotheses regarding the development of biological visual sensorimotor systems.}
}

@article{doi:10.1177/1059712313492176,
author = {Jonas Ruesch and Ricardo Ferreira and Alexandre Bernardino},
title = {A computational approach on the co-development of artificial visual sensorimotor},
journal = {Adaptive Behavior},
volume = {21},
number = {6},
pages = {452–464},
year = {2013i},
doi = {10.1177/1059712313492176},
URL = {https://doi-org.crai.referencistas.com/10.1177/1059712313492176},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1059712313492176},
abstract = {To follow a goal-directed behavior, an autonomous agent must be able to acquire knowledge about the causality between its motor actions and corresponding sensory feedback. Since the complexity of such sensorimotor relationships directly influences required cognitive resources, this work proposes that it is of importance to keep the agent’s sensorimotor relationships simple. This implies that the agent should be designed in a way such that sensory consequences can be described and predicted in a simplified manner. Living organisms implement this paradigm by adapting sensory and motor systems specifically to their behavior and environment. As a result, they are able to predict sensorimotor consequences with a strongly limited amount of (expensive) nervous tissue. In this context, the present work proposes that advantageous artificial sensory and motor layouts can be evolved by rewarding the ability to predict self-induced stimuli through simple sensorimotor relationships. Experiments consider a simulated agent recording realistic visual stimuli from natural images. The obtained results demonstrate the ability of the proposed method to (i) synthesize visual sensorimotor structures adapted to an agent’s environment and behavior, and (ii) serve as a computational model for testing hypotheses regarding the development of biological visual sensorimotor systems.}
}

@article{doi:10.1177/014362449401500308,
author = {R. Winwood and R. Benstead and R. Edwards and K.M. Letherman},
title = {Building fabric thermal storage: Use of computational fluid dynamics for modelling},
journal = {Building Services Engineering Research and Technology},
volume = {15},
number = {3},
pages = {3–17},
year = {1994j},
doi = {10.1177/014362449401500308},
URL = {https://doi-org.crai.referencistas.com/10.1177/014362449401500308},
eprint = {https://doi-org.crai.referencistas.com/10.1177/014362449401500308},
abstract = {In this study, we applied different text-mining methods to the originality scoring of the Unusual Uses Test (UUT) and Just Suppose Test (JST) from the Torrance Tests of Creative Thinking (TTCT)–Verbal. Responses from 102 and 123 participants who completed Form A and Form B, respectively, were scored using three different text-mining methods. The validity of these scoring methods was tested against TTCT’s manual-based scoring and a subjective snapshot scoring method. Results indicated that text-mining systems are applicable to both UUT and JST items across both forms and students’ performance on those items can predict total originality and creativity scores across all six tasks in the TTCT-Verbal. Comparatively, the text-mining methods worked better for UUT than JST. Of the three text-mining models we tested, the Global Vectors for Word Representation (GLoVe) model produced the most reliable and valid scores. These findings indicate that creativity assessment can be done quickly and at a lower cost using text-mining approaches.}
}

