@article{doi:10.1243/146808705X30503,
author = {A Babajimopoulos and D N Assanis and D L Flowers and S M Aceves and R P Hessel},
title = {A fully coupled computational fluid dynamics and multi-zone model with detailed chemical kinetics for the simulation of premixed charge compression ignition engines},
journal = {International Journal of Engine Research},
volume = {6},
number = {5},
pages = {497–512},
year = {2005a},
doi = {10.1243/146808705X30503},
URL = {https://doi-org.crai.referencistas.com/10.1243/146808705X30503},
eprint = {https://doi-org.crai.referencistas.com/10.1243/146808705X30503},
abstract = {Abstract Modelling the premixed charge compression ignition (PCCI) engine requires a balanced approach that captures both fluid motion as well as low- and high-temperature fuel oxidation. A fully integrated computational fluid dynamics (CFD) and chemistry scheme (i.e. detailed chemical kinetics solved in every cell of the CFD grid) would be the ideal PCCI modelling approach, but is computationally very expensive. As a result, modelling assumptions are required in order to develop tools that are computationally efficient, yet maintain an acceptable degree of accuracy. Multi-zone models have been previously shown accurately to capture geometry-dependent processes in homogeneous charge compression ignition (HCCI) engines. In the presented work, KIVA-3V is fully coupled with a multi-zone model with detailed chemical kinetics. Computational efficiency is achieved by utilizing a low-resolution discretization to solve detailed chemical kinetics in the multi-zone model compared with a relatively high-resolution CFD solution. The multi-zone model communicates with KIVA-3V at each computational timestep, as in the ideal fully integrated case. The composition of the cells, however, is mapped back and forth between KTVA-3V and the multi-zone model, introducing significant computational time savings. The methodology uses a novel re-mapping technique that can account for both temperature and composition non-uniformities in the cylinder. Validation cases were developed by solving the detailed chemistry in every cell of a KIVA-3V grid. The new methodology shows very good agreement with the detailed solutions in terms of ignition timing, burn duration, and emissions.}
}

@article{doi:10.1177/1532708617750178,
author = {Nicole Marie Brown},
title = {Methodological Cyborg as Black Feminist Technology: Constructing the Social Self Using Computational Digital Autoethnography and Social Media},
journal = {Cultural Studies ↔ Critical Methodologies},
volume = {19},
number = {1},
pages = {55–67},
year = {2019b},
doi = {10.1177/1532708617750178},
URL = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1532708617750178},
abstract = {This article reimagines the quantified self within the context of Black feminist technologies. Bringing computation and autoethnographic methods together using a methodology I call computational digital autoethnography, I harvest my social media data to create a corpus for analysis. I apply topic modeling to these data to uncover themes that are connected with broader societal issues affecting African American women. Applying a computational autoethnographic approach to a researcher’s own digitized data allows for yet another dimension of mixed-methods research. This radical intervention has the potential to transform the social sciences by bringing together two seemingly divergent methodological approaches in service to Black feminist ways of knowing.}
}

@article{doi:10.1177/1094342016637813,
author = {Alejandro Calderón and Alberto García and Félix García-Carballeira and Jesús Carretero and Javier Fernández},
title = {Improving performance using computational compression through memoization: A case study using a railway power consumption simulator},
journal = {The International Journal of High Performance Computing Applications},
volume = {30},
number = {4},
pages = {469–485},
year = {2016c},
doi = {10.1177/1094342016637813},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342016637813},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342016637813},
abstract = {The objective of data compression is to avoid redundancy in order to reduce the size of the data to be stored or transmitted. In some scenarios, data compression may help to increase global performance by reducing the amount of data at a competitive cost in terms of global time and energy consumption. We have introduced computational compression as a technique for reducing redundant computation, in other words, to avoid carrying out the same computation with the same input to obtain the same output. In some scenarios, such as simulations, graphic processing, and so on, part of the computation is repeated using the same input in order to obtain the same output, and this computation could have an important cost in terms of global time and energy consumption. We propose applying computational compression by using memoization in order to store the results for future reuse and, in this way, minimize the use of the same costly computation. Although memoization was proposed for sequential applications in the 1980s, and there are some projects that have applied it in very specific domains, we propose a novel, domain-independent way of using it in high-performance applications, as a means of avoiding redundant computation.}
}

@article{doi:10.1177/0954411920923253,
author = {Karol Calò and Giuseppe De Nisco and Diego Gallo and Claudio Chiastra and Ayla Hoogendoorn and David A Steinman and Stefania Scarsoglio and Jolanda J Wentzel and Umberto Morbiducci},
title = {Exploring wall shear stress spatiotemporal heterogeneity in coronary arteries combining correlation-based analysis and complex networks with computational hemodynamics},
journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
volume = {234},
number = {11},
pages = {1209–1222},
year = {2020d},
doi = {10.1177/0954411920923253},
note = {PMID:32460666},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954411920923253},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411920923253},
abstract = {Atherosclerosis at the early stage in coronary arteries has been associated with low cycle-average wall shear stress magnitude. However, parallel to the identification of an established active role for low wall shear stress in the onset/progression of the atherosclerotic disease, a weak association between lesions localization and low/oscillatory wall shear stress has been observed. In the attempt to fully identify the wall shear stress phenotype triggering early atherosclerosis in coronary arteries, this exploratory study aims at enriching the characterization of wall shear stress emerging features combining correlation-based analysis and complex networks theory with computational hemodynamics. The final goal is the characterization of the spatiotemporal and topological heterogeneity of wall shear stress waveforms along the cardiac cycle. In detail, here time-histories of wall shear stress magnitude and wall shear stress projection along the main flow direction and orthogonal to it (a measure of wall shear stress multidirectionality) are analyzed in a representative dataset of 10 left anterior descending pig coronary artery computational hemodynamics models. Among the main findings, we report that the proposed analysis quantitatively demonstrates that the model-specific inlet flow-rate shapes wall shear stress time-histories. Moreover, it emerges that a combined effect of low wall shear stress magnitude and of the shape of the wall shear stress–based descriptors time-histories could trigger atherosclerosis at its earliest stage. The findings of this work suggest for new experiments to provide a clearer determination of the wall shear stress phenotype which is at the basis of the so-called arterial hemodynamic risk hypothesis in coronary arteries.}
}

@article{doi:10.1177/20414196221085720,
author = {Adam A Dennis and Danny J Smyl and Chris G Stirling and Samuel E Rigby},
title = {A branching algorithm to reduce computational time of batch models: Application for blast analyses},
journal = {International Journal of Protective Structures},
volume = {14},
number = {2},
pages = {135–167},
year = {2023e},
doi = {10.1177/20414196221085720},
URL = {https://doi-org.crai.referencistas.com/10.1177/20414196221085720},
eprint = {https://doi-org.crai.referencistas.com/10.1177/20414196221085720},
abstract = {Numerical analysis is increasingly used for batch modelling runs, with each individual model possessing a unique combination of input parameters sampled from a range of potential values. Whilst such an approach can help to develop a comprehensive understanding of the inherent unpredictability and variability of explosive events, or populate training/validation data sets for machine learning approaches, the associated computational expense is relatively high. Furthermore, any given model may share a number of common solution steps with other models in the batch, and simulating all models from birth to termination may result in large amounts of repetition. This paper presents a new branching algorithm that ensures calculation steps are only computed once by identifying when the parameter fields of each model in the batch becomes unique. This enables informed data mapping to take place, leading to a reduction in the required computation time. The branching algorithm is explained using a conceptual walk-through for a batch of 9 models, featuring a blast load acting on a structural panel in 2D. By eliminating repeat steps, approximately 50% of the run time can be saved. This is followed by the development and use of the algorithm in 3D for a practical application involving 20 complex containment structure models. In this instance, a ∼20% reduction in computational costs is achieved.}
}

@article{doi:10.1243/0954408011530253,
author = {A G Abdul Ghani and M M Farid and X D Chen and P Richards},
title = {A computational fluid dynamics study on the effect of sterilization temperatures on bacteria deactivation and vitamin destruction},
journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
volume = {215},
number = {1},
pages = {9–17},
year = {2001f},
doi = {10.1243/0954408011530253},
URL = {https://doi-org.crai.referencistas.com/10.1243/0954408011530253},
eprint = {https://doi-org.crai.referencistas.com/10.1243/0954408011530253},
abstract = {Abstract The optimization of thermal processes such as sterilization relies on the accuracy of relevant kinetic data for bacterial inactivation and quality evolution. It is also dependent on the geometry and heating mechanism involved in the process. In these processes or systems, profiles of temperature distribution, bacteria concentration and concentrations of vitamins C (ascorbic acid), B1 (thiamin) and B2 (riboflavin) in a can filled with cherry juice during thermal sterilization have been obtained through numerical simulations. Different heating medium temperatures of 121, 130 and 140°C were tested. In order to generate these profiles, the continuity, momentum and energy equations are solved numerically, together with those of bacteria and vitamins concentrations, using the computational fluid dynamics code PHOENICS, combined with reaction kinetics models. Natural convection that occurs during thermal sterilization of viscous liquid (concentrated cherry juice, 74 °Brix) in a cylindrical can heated from all sides has been studied in this work. The simulations show clearly the dependences of the concentration of live bacteria and different vitamins on both the temperature distribution and the flow pattern as sterilization proceeds. The results also show that the best sterilization temperature may not always be 121 °C, depending on the quality requirements imposed on individual food material of concern.}
}

@article{doi:10.1177/1475090216642467,
author = {Max Haase and Gary Davidson and Jonathan Binns and Giles Thomas and Neil Bose},
title = {Full-scale resistance prediction in finite waters: A study using computational fluid dynamics simulations, model test experiments and sea trial measurements},
journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment},
volume = {231},
number = {1},
pages = {316–328},
year = {2017g},
doi = {10.1177/1475090216642467},
URL = {https://doi-org.crai.referencistas.com/10.1177/1475090216642467},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1475090216642467},
abstract = {The development of large medium-speed catamarans aims increasing economic viability and reducing the possible negative influence on the environment of fast sea transportation. These vessels are likely to operate at hump speed where wave-making can be the dominating component of the total resistance. Shallow water may considerably amplify the wave-making and hence the overall drag force. Computational fluid dynamics is used to predict the drag force of medium-speed catamarans at model and full scale in infinite and restricted water to study the impact on the resistance. Steady and unsteady shallow-water effects that occur in model testing or full-scale operation are taken into account using computational fluid dynamics as they are inherently included in the mathematical formulations. Unsteady effects in the ship-model response were recorded in model test experiments, computational fluid dynamics simulations and full-scale measurements and found to agree with each other. For a medium-speed catamaran in water that is restricted in width and depth, it was found that computational fluid dynamics is capable of accurately predicting the drag with a maximum deviation of no more than 6% when compared to experimental results in model scale. The influences of restricted depth and width were studied using computational fluid dynamics where steady finite width effects in shallow water and finite depth effects at finite width were quantified. Full-scale drag from computational fluid dynamics predictions in shallow water (h/L = 0.12 – 0.17) was found to be between full-scale measurements and extrapolated model test results. Finally, it is shown that current extrapolation procedures for shallow-water model tests over-estimate residuary resistance by up to 12% and underestimate frictional forces by up to 35% when compared to validated computational fluid dynamics results. This study concludes that computational fluid dynamics is a versatile tool to predict the full-scale ship resistance to a more accurate extent than extrapolation model test data and can also be utilised to estimate model sizes that keep finite-water effects to an agreed minimum.}
}

@article{doi:10.1177/1941738111403108,
author = {Michael Lavagnino and Steven P. Arnoczky and Julie Dodds and Niell Elvin},
title = {Infrapatellar Straps Decrease Patellar Tendon Strain at the Site of the Jumper’s Knee Lesion: A Computational Analysis Based on Radiographic Measurements},
journal = {Sports Health},
volume = {3},
number = {3},
pages = {296–302},
year = {2011h},
doi = {10.1177/1941738111403108},
note = {PMID:23016021},
URL = {https://doi-org.crai.referencistas.com/10.1177/1941738111403108},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1941738111403108},
abstract = {Background: The impetus for the use of patellar straps in the treatment of patellar tendinopathy has largely been based on empirical evidence and not on any mechanistic rationale. A computational model suggests that patellar tendinopathy may be a result of high localized tendon strains that occur at smaller patella–patellar tendon angles (PPTAs). Hypothesis: Infrapatellar straps will decrease the mean localized computational strain in the area of the patellar tendon commonly involved in jumper’s knee by increasing the PPTA. Study Design: Controlled laboratory study. Methods: Twenty adult males had lateral weightbearing and nonweightbearing radiographs of their knees taken with and without 1 of 2 infrapatellar straps at 60° of knee flexion. Morphologic measurements of PPTA and patellar tendon length with and without the straps were used as input data into a previously described computational model to calculate average and maximum strain at the common location of the jumper’s knee lesion during a simulated jump landing. Results: The infrapatellar bands decreased the predicted localized strain (average and maximum) in the majority of participants by increasing PPTA and/or decreasing patellar tendon length. When both PPTA and patellar tendon length were altered by the straps, there was a strong and significant correlation with the change in predicted average localized strain with both straps. Conclusion: Infrapatellar straps may limit excessive patella tendon strain at the site of the jumper’s knee lesion by increasing PPTA and decreasing patellar tendon length rather than by correcting some inherent anatomic or functional abnormality in the extensor apparatus. Clinical Relevance: The use of infrapatellar straps may help prevent excessive localized tendon strains at the site of the jumper’s knee lesion during a jump landing.}
}

@article{doi:10.1177/09544119221102704,
author = {Yongtao Lu and Yi Huo and Jia’ao Zou and Yanchen Li and Zhuoyue Yang and Hanxing Zhu and Chengwei Wu},
title = {Comparison of the design maps of TPMS based bone scaffolds using a computational modeling framework simultaneously considering various conditions},
journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
volume = {236},
number = {8},
pages = {1157–1168},
year = {2022i},
doi = {10.1177/09544119221102704},
note = {PMID:35647704},
URL = {https://doi-org.crai.referencistas.com/10.1177/09544119221102704},
eprint = {https://doi-org.crai.referencistas.com/10.1177/09544119221102704},
abstract = {In recent years, the triply periodic minimal surface (TPMS)-based scaffolds have been served as one of the crucial types of structures for biological replacements, the energy absorber, etc. Meanwhile, the development of additive manufacturing (AM) has facilitated the production of TPMS scaffolds with complex microstructures. However, the design maps of TPMS scaffolds, especially considering the AM constraints, remain unclear, which has hindered the design and application of TPMS scaffolds. The aims of the present study were to develop an efficient computational modeling framework for investigating the design maps of TPMS scaffolds simultaneously considering the AM constraints, the biological requirements, and the structural anisotropy. To demonstrate the computational framework, five widely-used topologies of the TPMS-based scaffolds (i.e. the Diamond, the Gyroid, the Fischer-Koch S, the F-RD, and the Schwarz P) were used, whose design maps for the surface-to-volume ratio and the effective elastic modulus were also investigated. The results showed that as the porosities increase, the design ranges of the surface-to-volume ratios decreases for all the structures. Compared with the effect of the constraint for the pore size, the minimal structural thickness for AM constraint has a greater effect on the surface-to-volume ratio. Regarding the elastic modulus, in the region of low porosity (approximately 0.5–0.7), the range for the effective elastic modulus of Schwarz P is the widest (approximately 2.24–32.6 GPa), but the Gyroid can achieve both high porosity and low effective elastic modulus (e.g. 0.61 GPa at the porosity of 0.90). These results and the method developed in the present study provided important basis and guidance for the design and application of the TPMS-based porous structures.}
}

@article{doi:10.1177/0361198196155000101,
author = {Hέlène Tattegrain-Veste and Thierry Bellet and Annie Pauziέ and Andrέ Chapon},
title = {Computational Driver Model in Transport Engineering: COSMODRIVE},
journal = {Transportation Research Record},
volume = {1550},
number = {1},
pages = {1–7},
year = {1996j},
doi = {10.1177/0361198196155000101},
URL = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0361198196155000101},
abstract = {With regard to road safety issues, a deep understanding of the driver as a logic system is crucial to predict the most probable behavior according to the contextual elements. Knowledge and data about human functional abilities exist. But the problem is to organize and structure them. The development of a computational approach in driver modelization is addressed. In the first part, a brief historical overview is presented of available driver models in ergonomics and psychological areas, and the distinction between predictive and explicative models in an implementation perspective is the focus. In the second part, the computational aspect of the work is described, along with the software concepts, the cognitive modeling needs, and the implementation choices. Object-oriented techniques were chosen because they provide a modular overview of the general system and offer a convenient representation of cognitive processes. Object-oriented formalism, in particular object modeling technique diagrams, acts as a bridge between the two domains of computer science and the human sciences. The objective is to determine whether it is possible to implement reliably a driver model using the techniques from artificial intelligence and based on the theoretical knowledge from cognitive sciences research. This attempt to establish links between different scientific domains, requiring a common tool, is a challenge. A first step of a work that will have to be developed in a long-term time scale, taking into account its quite ambitious objective, is described.}
}

