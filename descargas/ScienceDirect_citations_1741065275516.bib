@article{RONGHUA2024e27753,
title = {Improved ant colony optimization for safe path planning of AUV},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e27753},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27753},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024037848},
author = {Meng Ronghua and Cheng Xinhao and Wu Zhengjia and  {Du xuan}},
keywords = {Improved ant colony optimization, Safety factors, Dam inspections},
abstract = {In order to address the autonomous underwater vehicle navigation challenge for dam inspections, with the goal of enabling safe inspections and reliable obstacle avoidance, an improved smooth Ant Colony Optimization algorithm is proposed for path planning. This improved algorithm would optimize the smoothness of the path besides the robustness, avoidance of local optima, and fast computation speed. To achieve the goal of reducing turning time and improving the directional effect of path selection, a corner-turning heuristic function is introduced. Experimental simulation results show that the improved algorithm performs best than other algorithms in terms of path smoothness and iteration stability in path planning.}
}
@article{SALINGER1994139,
title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {139-156},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.}
}
@incollection{SRIPRASADH202545,
title = {Chapter 3 - Review of existing neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {45-66},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
author = {K. Sriprasadh},
keywords = {Deep learning, Neural network, Machine learning, Expert systems},
abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.}
}
@incollection{ASCHEID200711,
title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
editor = {Paolo Lenne and Rainer Leupers},
booktitle = {Customizable Embedded Processors},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {11-37},
year = {2007},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
author = {Gerd Ascheid and Heinrich Meyr},
abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{ADAMO2024109162,
title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109162},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109162},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.}
}
@article{JONES2013122,
title = {Understanding the integral: Students’ symbolic forms},
journal = {The Journal of Mathematical Behavior},
volume = {32},
number = {2},
pages = {122-141},
year = {2013},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
author = {Steven R. Jones},
keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.}
}
@article{ZHAO2024102465,
title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
journal = {Information Fusion},
volume = {110},
pages = {102465},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102465},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.}
}
@article{YANG2022101239,
title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
journal = {Journal of Informetrics},
volume = {16},
number = {1},
pages = {101239},
year = {2022},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.}
}
@article{XHAXHIU2024270,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
volume = {3},
number = {4},
pages = {270-277},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.}
}
@article{ADHIKARI20121374,
title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
journal = {Energy Procedia},
volume = {14},
pages = {1374-1379},
year = {2012},
note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
author = {R.S. Adhikari and N. Aste and M. Manfren},
keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.}
}
@article{UDDIN2021106,
title = {Application of Theory in Chronic Pain Rehabilitation Research and Clinical Practice},
journal = {The Open Sports Sciences Journal},
volume = {14},
pages = {106-113},
year = {2021},
issn = {1875-399X},
doi = {https://doi.org/10.2174/1875399X02114010106},
url = {https://www.sciencedirect.com/science/article/pii/S1875399X2100013X},
author = {Zakir Uddin and Joy C. MacDermid and Fatma A. Hegazy and Tara L. Packham},
keywords = {Chronic pain , Hypersensitivity , Theory , Rehabilitation , Disability , T-cell},
abstract = {Introduction
Chronic pain has multiple aetiological factors and complexity. Pain theory helps us to guide and organize our thinking to deal with this complexity. The objective of this paper is to critically review the most influential theory in pain science history (the gate control theory of pain) and focus on its implications in chronic pain rehabilitation to minimize disability.
Methods
In this narrative review, all the published studies that focused upon pain theory were retrieved from Ovoid Medline (from 1946 till present), EMBAS, AMED and PsycINFO data bases.
Results
Chronic pain is considered a disease or dysfunction of the nervous system. In chronic pain conditions, hypersensitivity is thought to develop from changes to the physiological top-down control (inhibitory) mechanism of pain modulation according to the pain theory. Pain hypersensitivity manifestation is considered as abnormal central inhibitory control at the gate controlling mechanism. On the other hand, pain hypersensitivity is a prognostic factor in pain rehabilitation. It is clinically important to detect and manage hypersensitivity responses and their mechanisms.
Conclusion
Since somatosensory perception and integration are recognized as a contributor to the pain perception under the theory, then we can use the model to direct interventions aimed at pain relief. The pain theory should be leveraged to develop and refine measurement tools with clinical utility for detecting and monitoring hypersensitivity linked to chronic pain mechanisms.}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@article{DING2025121721,
title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
journal = {Information Sciences},
volume = {694},
pages = {121721},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121721},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
author = {Haixin Ding},
keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.}
}
@article{LUO2023101957,
title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101957},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101957},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
author = {Yuhan Luo and Minna Ni and Feng Zhang},
keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.}
}
@article{FALZON2006629,
title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
journal = {European Journal of Operational Research},
volume = {170},
number = {2},
pages = {629-643},
year = {2006},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
author = {Lucia Falzon},
keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.}
}
@article{WANG2025111994,
title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {224},
pages = {111994},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.}
}
@article{COSMIDES198951,
title = {Evolutionary psychology and the generation of culture, part II: Case study: A computational theory of social exchange},
journal = {Ethology and Sociobiology},
volume = {10},
number = {1},
pages = {51-97},
year = {1989},
issn = {0162-3095},
doi = {https://doi.org/10.1016/0162-3095(89)90013-7},
url = {https://www.sciencedirect.com/science/article/pii/0162309589900137},
author = {Leda Cosmides and John Tooby},
keywords = {Reciprocal Altruism, Cooperation, Tit for tat, Cognition, Reasoning, Evolution, Learning, Culture},
abstract = {Models of the various adaptive specializations that have evolved in the human psyche could become the building blocks of a scientific theory of culture. The first step in creating such models is the derivation of a so-called “computational theory” of the adaptive problem each psychological specialization has evolved to solve. In Part II, as a case study, a sketch of a computational theory of social exchange (cooperation for mutual benefit) is developed. The dynamics of natural selection in Pleistocene ecological conditions define adaptive information processing problems that humans must be able to solve in order to participate in social exchange: individual recognition, memory for one's history of interaction, value communication, value modeling, and a shared grammar of social contracts that specifies representational structure and inferential procedures. The nature of these adaptive information processing problems places constraints on the class of cognitive programs capable of solving them; this allows one to make empirical predictions about how the cognitive processes involved in attention, communication, memory, learning, and reasoning are mobilized in situations of social exchange. Once the cognitive programs specialized for regulating social exchange are mapped, the variation and invariances in social exchange within and between cultures can be meaningfully discussed.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{HASSABIS2007299,
title = {Deconstructing episodic memory with construction},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {7},
pages = {299-306},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
author = {Demis Hassabis and Eleanor A. Maguire},
abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.}
}
@article{AGNOLI2020116385,
title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
journal = {NeuroImage},
volume = {207},
pages = {116385},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.}
}
@incollection{GARDNER2024103,
title = {Chapter 5 - Smart design for socially engaging environments},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {103-128},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.}
}
@article{KORDAKI2017122,
title = {Digital card games in education: A ten year systematic review},
journal = {Computers & Education},
volume = {109},
pages = {122-161},
year = {2017},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S036013151730043X},
author = {Maria Kordaki and Anthi Gousiou},
keywords = {Applications in subject areas, Interactive learning environments, Pedagogical issues, Review, Digital card games},
abstract = {This paper presents a 10-year review study that focuses on the investigation of the use of Digital Card Games (DCGs) as learning tools in education. Specific search terms keyed into 10 large scientific electronic databases identified 50 papers referring to the use of non-commercial DCGs in education during the last decade (2003–2013). The findings revealed that the DCGs reported in the reviewed papers: (a) were used for the learning of diverse subject disciplines across all educational levels and leaning towards the school curriculum, in two ways: game-construction and game-play, (b) were mainly proposed by their designers as meaningful, familiar and appealing learning contexts, in order to motivate and engage players/students and also to promote social, rich and constructivist educational experiences while at the same time integrating modern technologies and innovative gamed-based approaches, (c) were implemented using a plethora of digital tools, (d) mainly adopted social and constructivist views of learning during their design and use, although the views were explicitly reported in only a few of these, (e) were evaluated – in more than half of the studies – with positive results in terms of: student learning, attitudes towards DCGs and enrichment of social interaction and collaboration, (f) appeared to support students to acquire essential thinking skills through DCG-play. However, despite the rich DCG-game experiences reported in the reviewed papers, some essential but under-researched topics were also specified.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@article{EBBY200573,
title = {The powers and pitfalls of algorithmic knowledge: a case study},
journal = {The Journal of Mathematical Behavior},
volume = {24},
number = {1},
pages = {73-87},
year = {2005},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000768},
author = {Caroline Brayer Ebby},
keywords = {Algorithms, Computation, Learning, Student understanding, Sociocultural perspective, Reform curriculum},
abstract = {This study examines one child's use of computational procedures over a period of 3 years in an urban elementary school where teachers were using a standards-based curriculum. From a sociocultural perspective, the use of standard algorithms to solve mathematical problems is viewed as a cultural tool that both enables and constrains particular practices. As this student appropriated and mastered procedures for addition, subtraction, multiplication and division, she could solve problems that involved fairly straightforward computations or where she could easily model the action to determine an appropriate computation. At the same time, her use of these algorithms, along with other readily available tools, such as her fingers or multiplication tables, constrained her ability to reflect on the tens-structure of the number system, an effect that had serious consequences for her overall mathematical achievement. The results of this study suggest that even when not directly introduced, algorithms have such strong currency that they can mediate more reform-oriented instruction.}
}
@incollection{KURGANSKAYA2024760,
title = {Multi-scale modeling of crystal-fluid interactions: State-of-the-art, challenges and prospects},
editor = {Klaus Wandelt and Gianlorenzo Bussetti},
booktitle = {Encyclopedia of Solid-Liquid Interfaces (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {760-792},
year = {2024},
isbn = {978-0-323-85670-6},
doi = {https://doi.org/10.1016/B978-0-323-85669-0.00034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856690000349},
author = {I. Kurganskaya and R.D. Rohlfs and A. Luttge},
keywords = {Crystal-water interface, Electric double layer, Grand canonical Monte Carlo, Kinetic Monte Carlo, Kinetics, Mineral–water interface, Parameterization, Reaction pathways, Reaction probability, Reaction rates, Statistical mechanics of interfaces, Stepwave, Stochastic model, Upscaling, Voronoi},
abstract = {We describe theoretical and conceptual approaches to treat crystal-fluid interactions across the scales in the communities studying mineral-fluid interactions for a variety of purposes, from understanding fundamental principles to geological reservoir characterization and environmental mitigation. We delineate basics of theory, recent breakthroughs, and challenges in modeling approaches from the atomistic scale to the mesoscale. Quantum Mechanics, Molecular Dynamics, Kinetic Monte Carlo and Voronoi computational geometry are covered. We discuss possible theoretical and conceptual developments to overcome those challenges toward more reliable predictive models. A special attention is given to the development of interfaces between the techniques addressing different scales.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@incollection{BILLEN2023385,
title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
editor = {João C. Duarte},
booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
publisher = {Elsevier},
pages = {385-405},
year = {2023},
isbn = {978-0-323-85733-8},
doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
author = {Magali I. Billen},
keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.}
}
@article{SZYMANSKI2021,
title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
journal = {mSystems},
volume = {6},
number = {4},
year = {2021},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00769-21},
url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
author = {Erika Szymanski},
keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.}
}
@article{RANDALL1991219,
title = {Review of linear least squares computations: by R.W. Farebrother},
journal = {Linear Algebra and its Applications},
volume = {153},
pages = {219-223},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
author = {John H. Randall}
}
@article{GARDECKI2018138,
title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {6},
pages = {138-143},
year = {2018},
note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.}
}
@article{YONG2023e13529,
title = {Structure bionic topology design method based on biological unit cell},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13529},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.}
}
@incollection{CALVERT201369,
title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
editor = {Colin Harwood and Anil Wipat},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {40},
pages = {69-86},
year = {2013},
booktitle = {Microbial Synthetic Biology},
issn = {0580-9517},
doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
author = {Jane Calvert and Emma Frow},
keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.}
}
@article{FOSGERAU2021109911,
title = {Some remarks on CCP-based estimators of dynamic models},
journal = {Economics Letters},
volume = {204},
pages = {109911},
year = {2021},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2021.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.}
}
@article{VASILE201177,
title = {Entry points, interests and attitudes. An integrative approach of learning},
journal = {Procedia - Social and Behavioral Sciences},
volume = {11},
pages = {77-81},
year = {2011},
note = {Teachers for the Knowledge Society},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
author = {Cristian Vasile},
keywords = {multiple intelligence, entry points, personality, interests},
abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.}
}
@article{OLADEJO2024111880,
title = {The Hiking Optimization Algorithm: A novel human-based metaheuristic approach},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111880},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111880},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005148},
author = {Sunday O. Oladejo and Stephen O. Ekwe and Seyedali Mirjalili},
keywords = {Optimization, Metaheuristics, Hiking, Tobler’s Hiking function, Algorithm, Benchmark, Problem solving},
abstract = {In this paper, a novel metaheuristic called ‘The Hiking Optimization Algorithm’ (HOA) is proposed. HOA is inspired by hiking, a popular recreational activity, in recognition of the similarity between the search landscapes of optimization problems and the mountainous terrains traversed by hikers. HOA’s mathematical model is premised on Tobler’s Hiking Function (THF), which determines the walking velocity of hikers (i.e. agents) by considering the elevation of the terrain and the distance covered. THF is employed in determining hikers’ positions in the course of solving an optimization problem. HOA’s performance is demonstrated by benchmarking with 29 well-known test functions (including unimodal, multimodal, fixed-dimension multimodal, and composite functions), three engineering design problems (EDPs), (including I-beam, tension/compression spring, and gear train problems) and two N-P Hard problems (i.e. Traveling Salesman’s and Knapsack Problems). Moreover, HOA’s results are verified by comparison to 14 other metaheuristics, including Teaching Learning Based Optimization (TLBO), Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization, Grey Wolf Optimizer (GWO) as well as newly introduced algorithms such as Komodo Mlipir Algorithm (KMA), Quadratic Interpolation Optimization (QIO), and Coronavirus Optimization Algorithm (COVIDOA). In this study, we employ statistical tests such as the Wilcoxon rank sum, Friedman test, and Dunn’s post hoc test for the performance evaluation. HOA’s results are competitive and, in many instances, outperform the aforementioned well-known metaheuristics. The source codes of HOA and related metaheuristics can be accessed publicly via this link: https://github.com/DayoSun/The-Hiking-Optimization-Algorithm.}
}
@article{BEYNON2008476,
title = {Experimenting with computing},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {476-489},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
author = {Meurig Beynon and Steve Russ},
keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{PANULAONTTO2019292,
title = {The AXIOM approach for probabilistic and causal modeling with expert elicited inputs},
journal = {Technological Forecasting and Social Change},
volume = {138},
pages = {292-308},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518305870},
author = {Juha Panula-Ontto},
keywords = {Systems modeling, Modeling techniques, Decision support, Cross-impact analysis, Belief networks, Expert elicitation},
abstract = {Expert informants can be used as the principal information source in the modeling of socio-techno-economic systems or problems to support planning, foresight and decision-making. Such modeling is theory-driven, grounded in expert judgment and understanding, and can be contrasted with data-driven modeling approaches. Several families of approaches exist to enable expert elicited systems modeling with varying input information requirements and analytical ambitions. This paper proposes a novel modeling language and computational process, which combines aspects from various other approaches in an attempt to create a flexible and practical systems modeling approach based on expert elicitation. It is intended to have high fitness in modeling of systems that lack statistical data and exhibit low quantifiability of important system characteristics. AXIOM is positioned against Bayesian networks, cross-impact analysis, structural analysis, and morphological analysis. The modeling language and computational process are illustrated with a small example model. A software implementation is also presented.}
}
@article{KRUSKOPF2024104574,
title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104574},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.}
}
@article{BELLANTE2025104341,
title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
journal = {Computers & Security},
pages = {104341},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{MONNAHAN2024107024,
title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
journal = {Fisheries Research},
volume = {275},
pages = {107024},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
author = {Cole C. Monnahan},
keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{AUGIER2001307,
title = {Sublime Simon: The consistent vision of economic psychology's Nobel laureate},
journal = {Journal of Economic Psychology},
volume = {22},
number = {3},
pages = {307-334},
year = {2001},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00036-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000368},
author = {Mie Augier},
keywords = {Herbert Simon, Bounded rationality, Carnegie Mellon University, Economics and psychology},
abstract = {This essay contains a study of some of Herbert Simon's ideas, with particular emphasis on the role of bounded rationality in Simon's thinking and his contributions to economics and psychology. I describe Simon's visions for challenging rational choice theory, through limited rationality, and for bringing psychology into economics, putting this in perspective by describing the evolution of some of this thoughts, focusing on the continuity in his work.}
}
@article{LUCKRING2024100998,
title = {Prediction of concentrated vortex aerodynamics: Current CFD capability survey},
journal = {Progress in Aerospace Sciences},
volume = {147},
pages = {100998},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.100998},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000241},
author = {James M. Luckring and Arthur Rizzi},
abstract = {Concentrated vortex flows contribute to the aerodynamic performance of aircraft at elevated load conditions. For military interests, the vortex flows are exploited at maneuver conditions of combat aircraft and missiles. For transport interests, the vortex flows are exploited at takeoff and landing conditions as well as at select transonic conditions. Aircraft applications of these vortex flows are reviewed with a historical perspective followed by a discussion of the underlying physics of a concentrated vortex flow. A hierarchy of computational fluid dynamics simulation technology is then presented followed by findings from a capability survey for predicting concentrated vortex flows with computational fluid dynamics. Results are focused on military and civil fixed-wing aircraft; only limited results are included for missiles, and rotary-wing applications are not assessed. Opportunities for predictive capability advancement are then reported with comments related to digital transformation interests. A hierarchical approach that merges a physics-based perspective of the concentrated vortex flows with a systems engineering viewpoint of the air vehicle is also used to frame much of the discussion.}
}
@article{NORGAARD2023105308,
title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
journal = {Cognition},
volume = {230},
pages = {105308},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105308},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.}
}
@article{XIAO1995169,
title = {Three-dimensional melt flows in Czochralski oxide growth: high-resolution, massively parallel, finite element computations},
journal = {Journal of Crystal Growth},
volume = {152},
number = {3},
pages = {169-181},
year = {1995},
issn = {0022-0248},
doi = {https://doi.org/10.1016/0022-0248(95)00090-9},
url = {https://www.sciencedirect.com/science/article/pii/0022024895000909},
author = {Qiang Xiao and Jeffrey J. Derby},
abstract = {Three-dimensional, time-dependent features of melt flows which occur during the Czochralski growth of oxide crystals are analyzed using a theoretical bulk-flow model. The transition from a steady, axisymmetric flow to a time-dependent, three-dimensional state characterized by an annular wave structure is found to strongly affect the temperature distribution and heat transfer through the melt. The results are obtained using a novel, massively parallel implementation of the Galerkin finite element method which affords high spatial resolution of the computed flows.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@incollection{VANLOAN1992247,
title = {Chapter 6 A survey of matrix computations},
series = {Handbooks in Operations Research and Management Science},
publisher = {Elsevier},
volume = {3},
pages = {247-321},
year = {1992},
booktitle = {Computing},
issn = {0927-0507},
doi = {https://doi.org/10.1016/S0927-0507(05)80203-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927050705802038},
author = {Charles {Van Loan}},
abstract = {Publisher Summary
This chapter presents three-level introduction to the field of matrix computations. The chapter discusses analytic and computational tools that underpin numerical linear algebra. Low dimension examples are the rule with appropriate generalizations to follow. The central themes include (a) the language of matrix factorizations, (b) the art of introducing zeros into a matrix, (c) the exploitation of structure, and (d) the distinction between problem sensitivity and algorithmic stability. Matrix factorizations that play a central role in numerical linear algebra are also presented in the chapter. The chapter also discusses factorization. For each factorization, algorithms are surveyed, associated mathematical properties, and applications are discussed. One factorization (Chotesky) is used to illustrate various aspects of high performance matrix computations. Successful computing requires the design of codes that pay careful attention to the flow of data during execution.}
}
@article{SOLIMAN2025100131,
title = {A comparative analysis of encoder only and decoder only models for challenging LLM-generated STEM MCQs using a self-evaluation approach},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100131},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100131},
url = {https://www.sciencedirect.com/science/article/pii/S294971912500007X},
author = {Ghada Soliman and Hozaifa Zaki and Mohamed Kilany},
keywords = {NLP, LLM, SLM, Self-evaluation, MCQ},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including Multiple-Choice Question Answering (MCQA) evaluated on benchmark datasets with few-shot prompting. Given the absence of benchmark Science, Technology, Engineering, and Mathematics (STEM) datasets on Multiple-Choice Questions (MCQs) created by LLMs, we employed various LLMs (e.g., Vicuna-13B, Bard, and GPT-3.5) to generate MCQs on STEM topics curated from Wikipedia. We evaluated open-source LLM models such as Llama 2-7B and Mistral-7B Instruct, along with an encoder model such as DeBERTa v3 Large, on inference by adding context in addition to fine-tuning with and without context. The results showed that DeBERTa v3 Large and Mistral-7B Instruct outperform Llama 2-7B, highlighting the potential of LLMs with fewer parameters in answering hard MCQs when given the appropriate context through fine-tuning. We also benchmarked the results of these models against closed-source models such as Gemini and GPT-4 on inference with context, showcasing the potential of narrowing the gap between open-source and closed-source models when context is provided. Our work demonstrates the capabilities of LLMs in creating more challenging tasks that can be used as self-evaluation for other models. It also contributes to understanding LLMs’ capabilities in STEM MCQs tasks and emphasizes the importance of context for LLMs with fewer parameters in enhancing their performance.}
}
@article{LEALVILLASECA2025105833,
title = {Interpreting Deepkriging for spatial interpolation in geostatistics},
journal = {Computers & Geosciences},
volume = {196},
pages = {105833},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105833},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424003169},
author = {Fabian Leal-Villaseca and Edward Cripps and Mark Jessell and Mark Lindsay},
keywords = {Spatial statistics, Deep learning interpretability, Shapley values, Deepkriging, Batched Shapley},
abstract = {In the current era marked by an unprecedented abundance of data, the usage of conventional methods such as kriging persists in some applications of geostatistics, despite their limitations in adequately capturing the intricate relationships found in contemporary, multivariate datasets. Although deep neural networks (DNNs) have demonstrated remarkable efficacy in capturing complex nonlinear feature relationships across various domains, their success in geostatistical applications has been limited. This can be partly attributed to two significant challenges. Firstly, the opaque nature of these black box models raises concerns about the dependability of their outputs for critical decision-making, as the inner workings of the model remain less interpretable. Secondly, DNNs do not explicitly capture spatial dependencies within data. To address these shortcomings, we employ a methodology to interpret the recently proposed spatial DNNs known as Deepkriging, and we apply it to dry bulk rock density estimation, an often-overlooked aspect in mineral resource estimation. Through our adaptation of Shapley values—Batched Shapley—we overcome significant computational challenges to quantify feature importance for Deepkriging. This approach takes into account feature interactions, which is crucial for DNNs, as they rely on high-order interactions, especially in a complex application like mineral resource estimation. Additionally, we demonstrate in the 3D case that Deepkriging outperforms ordinary kriging and regression kriging in terms of mean squared errors, in both the purely spatial case and in the presence of auxiliary variables. Our study produces the first methodology to interpret Deepkriging, which is suitable for any model with a large number of features; it reaffirms the efficacy of Deepkriging through several comparisons in a 3D application, and most importantly; it underscores the adaptability and broader potential of DNNs to cater to various challenges in geostatistics.}
}
@incollection{BALANAY201949,
title = {3 - Tools for circular economy: Review and some potential applications for the Philippine textile industry},
editor = {Subramanian Senthilkannan Muthu},
booktitle = {Circular Economy in Textiles and Apparel},
publisher = {Woodhead Publishing},
pages = {49-75},
year = {2019},
series = {The Textile Institute Book Series},
isbn = {978-0-08-102630-4},
doi = {https://doi.org/10.1016/B978-0-08-102630-4.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026304000030},
author = {Raquel Balanay and Anthony Halog},
keywords = {Circular economy, Industrial sustainability, Life cycle thinking, Sustainable development, Systems modelling, Textile industry},
abstract = {Instituting circular economy (CE) for sustainability is the aim of taking stock of various analytical/assessment tools. A review of these tools reveals a continuing endeavor to incorporate in the procedures the systems and life cycle thinking and the triple bottom-line framework of sustainable development (economic, social, and environmental). Over time, the CE tools have been modified with the incorporation of some unique attributes in the cases being studied. Life cycle assessment (LCA) remains the popular and the only standardized procedure to analyze CE issues in industries, specifically in the environmental aspect. However, consistency, measurement, and aggregation issues are the major setbacks of having an integrated LCA for economic, social, and environmental impacts. The alternative tools used across the world to study the economic, social, and environmental aspects of CE have increased in both number and sophistication. Optimization and systems models have been increasingly used on a case-based format. Although the downside is the less standardized approach with less chances of comparability in terms of results, these models have been designed appropriately to tackle challenges associated with intricate, multifaceted, and encompassing sustainability and CE issues to improve policy development. In the textile industry, LCA as a popular tool is only used for environmental sustainability assessment but not much in social and economic aspects. The Philippine textile industry still has to catch up in the application of those tools for sustainability assessment. A framework has been suggested for the country's roadmap/guide to attain circularity in textile industry operations.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{MCLEAN20248,
title = {Autoantibodies against acetylcholine receptors are increased in archived serum samples from patients with schizophrenia},
journal = {Schizophrenia Research},
volume = {267},
pages = {8-13},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001129},
author = {Ryan Thomas McLean and Elizabeth Buist and David {St. Clair} and Jun Wei},
keywords = {Neurotransmitter receptor, CHRM4, GRM3, CHRNA4, CHRNA5 neuroimmunology},
abstract = {Previous studies have demonstrated that the levels of IgG against neurotransmitter receptors are increased in patients with schizophrenia. Genome-wide association (GWA) studies of schizophrenia confirmed that 108 loci harbouring over 300 genes were associated with schizophrenia. Although the functional implications of genetic variants are unclear, theoretical functional alterations of these genes could be replicated by the presence of autoantibodies. This study examined the levels of plasma IgG antibodies against four neurotransmitter receptors, CHRM4, GRM3, CHRNA4 and CHRNA5, using an in-house ELISA in 247 patients with schizophrenia and 344 non-psychiatric controls. Four peptides were designed based on in silico analysis with computational prediction of HLA-DRB1 restricted and B-cell epitopes. The relationship between plasma IgG levels and psychiatric symptoms, as defined by the Operational Criteria Checklist for Psychotic Illness and Affective Illness (OPCRIT), were examined. The results showed that the levels of plasma IgG against peptides derived from CHRM4 and CHRNA4 were significantly increased in patients with schizophrenia compared with control subjects, but there was no significant association of plasma IgG levels with any symptom domain or any specific symptoms. These preliminary results suggest that CHRM4 and CHRNA4 may be novel targets for autoantibody responses in schizophrenia, although the pathogenic relationship between increased serum autoantibody levels and schizophrenia symptoms remains unclear.}
}
@article{GU2023120105,
title = {Detection of Attention Deficit Hyperactivity Disorder in children using CEEMDAN-based cross frequency symbolic convergent cross mapping},
journal = {Expert Systems with Applications},
volume = {226},
pages = {120105},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006073},
author = {Danlei Gu and Aijing Lin and Guancen Lin},
keywords = {Cross-frequency coupling, Convergent cross mapping, Complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), Attention Deficit Hyperactivity Disorder (ADHD), Electroencephalogram (EEG), Dispersion},
abstract = {The cross-frequency coupling relationship of EEG signals is of great significance to identify abnormal EEG signals and diagnose diseases. This paper proposes a new algorithm, CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise)-based cross-frequency symbolic convergent cross mapping (CEEMDAN CF-SCCM). In the numerical simulation test, we have confirmed that CEEMDAN CF-SCCM is an effective method to quantify the cross-frequency information transmission of complex system signals from the three dimensions of robustness to noise, coupling sensitivity, and data length sensitivity. It can successfully distinguish the driving factors and response factors in the phase–amplitude interaction and has good robustness to noise. With this approach, we examined differences in cross-frequency phase–amplitude coupling between ADHD patients and normal individuals over classical brain frequency bands (Alpha, Beta, Delta, Gamma, Theta). According to the position of the electrodes, the brain was divided into four regions: front, back, left, and right, and the phase–amplitude coupling between different frequencies in each region was compared. Compared with the normal group, there was more information transmission in the anterior region of Delta waves and Theta waves. The front and left sides of the brain are responsible for thinking, mental and auditory functions. This information helps us gain insight into the brain dynamics of ADHD patients and contributes to the diagnosis of the disease.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@incollection{SHAYNAROSENBAUM201787,
title = {2.06 - Episodic and Semantic Memory},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {87-118},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21037-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210377},
author = {R. {Shayna Rosenbaum} and Alice S.N. Kim and Stevenson Baker},
keywords = {Aging, Amnesia, Autobiographical memory, Autonoetic consciousness, Child development, Default mode network, Familiarity, fMRI, Future imagining, Hippocampus, Medial temporal lobe, Mental time travel, Patient K.C., Personal semantic memory, Recollection, Spatial memory, Temporal neocortex},
abstract = {Much of the richness in human life derives from episodic memory, mental representations of detailed experiences from our personal pasts. To make sense of those experiences, knowledge about the world and oneself must also exist in a form that is free of context – known as semantic memory. This chapter revisits and builds on Tulving's distinction between episodic and semantic memory, with a focus on their differences, similarities, and interactions, informed by cognitive, neuropsychological, and neuroimaging studies. Extensions of this distinction into spatial memory, and beyond memory into future thinking, are considered in the context of process views of memory organization.}
}
@incollection{CATTANEO2015220,
title = {Mental Imagery: Visual Cognition},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {220-227},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086857024X},
author = {Zaira Cattaneo and Juha Silvanto},
keywords = {Brain stimulation, Creative thinking, Depictivist, Imagery, Imagery debate, Mathematics, Memory, Neuroimaging, Occipital cortex, Perception, Propositional, Reasoning, Vision, Visual cognition, Working memory},
abstract = {Mental imagery can be defined as a quasi-perceptual experience occurring in the absence of perceptual input. The present article provides a review of the key processes involved in mental imagery, the relationship of imagery to working memory, and of the debate on the underlying format of mental images. We also review the functional significance of imagery in a range of cognitive processes, such as memory, creative thinking, reasoning, and problem solving. Finally, the brain basis of mental imagery and its overlap with the cortical regions involved in visual perception are discussed.}
}
@article{FELSCHE2023101530,
title = {Evidence for abstract representations in children but not capuchin monkeys},
journal = {Cognitive Psychology},
volume = {140},
pages = {101530},
year = {2023},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101530},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000664},
author = {Elisa Felsche and Patience Stevens and Christoph J. Völter and Daphna Buchsbaum and Amanda M. Seed},
keywords = {Overhypotheses, Abstraction, Generalization, Animal cognition, Computational modeling, Cognitive development},
abstract = {The use of abstract higher-level knowledge (also called overhypotheses) allows humans to learn quickly from sparse data and make predictions in new situations. Previous research has suggested that humans may be the only species capable of abstract knowledge formation, but this remains controversial. There is also mixed evidence for when this ability emerges over human development. Kemp et al. (2007) proposed a computational model of how overhypotheses could be learned from sparse examples. We provide the first direct test of this model: an ecologically valid paradigm for testing two species, capuchin monkeys (Sapajus spp.) and 4- to 5-year-old human children. We presented participants with sampled evidence from different containers which suggested that all containers held items of uniform type (type condition) or of uniform size (size condition). Subsequently, we presented two new test containers and an example item from each: a small, high-valued item and a large but low-valued item. Participants could then choose from which test container they would like to receive the next sample – the optimal choice was the container that yielded a large item in the size condition or a high-valued item in the type condition. We compared performance to a priori predictions made by models with and without the capacity to learn overhypotheses. Children's choices were consistent with the model predictions and thus suggest an ability for abstract knowledge formation in the preschool years, whereas monkeys performed at chance level.}
}
@article{MEARA2000345,
title = {Vocabulary and neural networks in the computational assessment of texts written by second-language learners},
journal = {System},
volume = {28},
number = {3},
pages = {345-354},
year = {2000},
issn = {0346-251X},
doi = {https://doi.org/10.1016/S0346-251X(00)00016-6},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X00000166},
author = {Paul Meara and Catherine Rodgers and Gabriel Jacobs},
keywords = {Neural network, Computational assessment, Vocabulary, French},
abstract = {This paper explores the potential of a neural network in language assessment. Many examination systems rely on subjective judgments made by examiners as a way of grading the writing of non-native speakers. Some research (e.g. Engber, 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing 4(2), 139–155) has shown that these subjective judgements are influenced to a very large extent by the lexical choices made by candidates. We took Engber's basic model, but automated the evaluation of lexical content. A group of non-native speakers of French were asked to produce a short text in response to a picture stimulus. The texts were graded by French native speaker teachers. We identified a number of words which occurred in about half the texts, and coded each text for the occurrence and non-occurrence of each word. We then trained a neural network to grade the texts on the basis of these codings. The results suggest that it might be possible to teach a neural network to mimic the judgements made by human markers.}
}
@article{KATONA2023115228,
title = {Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor},
journal = {Journal of Computational and Applied Mathematics},
volume = {429},
pages = {115228},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2023.115228},
url = {https://www.sciencedirect.com/science/article/pii/S0377042723001723},
author = {Mihály Katona and Miklós Kuczmann and Tamás Orosz},
keywords = {Electrical machines, Optimisation, Finite element method, Robust design analysis, Design of Experiment methods},
abstract = {Mass-produced electrical machines are subjected to manufacturing uncertainties in terms of geometry. A robust design is inevitable to ensure the consistent performance of the electric motor. Some parts of the rotor geometry are often simplified, like the flux barrier at the end of the magnets. This paper presents a design optimisation regarding the torque ripple and the average torque. The aim is to assess the effects of the flux barrier on the main properties of a permanent magnet synchronous motor. Also, robust design analysis is presented on the flux barrier. The computational burden of the robust design analysis is immense, even if uniform uncertainties are assumed. In this case, different Design of Experiment (DoE) methods reduce the number of simulations. The efficiency of the DoE methods is compared in terms of simulation number and extreme value approximation. We found that the Central Composite method is the most accurate, while the Plackett–Burman is the most efficient in this particular case.}
}
@article{HUANG2025134690,
title = {Operation optimization of Combined Heat and Power microgrid in buildings consider renewable energy, electric vehicles and hydrogen fuel},
journal = {Energy},
volume = {319},
pages = {134690},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134690},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225003329},
author = {Yongyi Huang and Shoaib Ahmed and Soichiro Ueda and Xunyu Liang and Harun Or Rashid Howlader and Mohammed Elsayed Lotfy and Tomonobu Senjyu},
keywords = {Microgrid, Renewable energy, Electric vehicles, Monte Carlo simulation, K-means, Real-time pricing, Particle swarm optimization, Chance-constrained programming, Combined heat and power},
abstract = {This paper introduces a forward-thinking framework that integrates renewable energy, Electric Vehicles (EVs), and hydrogen within Combined Heat and Power (CHP) microgrids (MGs) for effective building energy management. By utilizing Particle Swarm Optimization (PSO) to find the optimal solution and incorporating Chance-Constrained Programming (CCP) to handle uncertainties in renewable energy generation and EV loads, this framework addresses the complexities of modern energy systems. The study employs Monte Carlo (MC) to simulate the EV load profile, applies K-means clustering to categorize load and renewable generation patterns, and uses a Sigmoid function-based model for Real-Time Pricing (RTP). The combination of PSO and CCP is used to optimize the system’s operating strategy. This evaluates the system’s economic benefits and impact on carbon emissions by analyzing different scenarios, such as weekdays versus weekends and various weather conditions (sunny, cloudy, rainy). The results show that due to the high price of hydrogen, it is currently costly to replace hydrogen completely. However, this integrated approach not only improves energy efficiency and reduces carbon footprint but also ensures system reliability under uncertain conditions, contributing to broader environmental sustainability.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{LI2024120,
title = {Conformal structure-preserving SVM methods for the nonlinear Schrödinger equation with weakly linear damping term},
journal = {Applied Numerical Mathematics},
volume = {205},
pages = {120-136},
year = {2024},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168927424001727},
author = {Xin Li and Luming Zhang},
keywords = {Damped nonlinear Schrödinger equation, Conformal properties, Supplementary variable method, High-order accuracy, Optimization model},
abstract = {In this paper, by applying the supplementary variable method (SVM), some high-order, conformal structure-preserving, linearized algorithms are developed for the damped nonlinear Schrödinger equation. We derive the well-determined SVM systems with the conformal properties and they are then equivalent to nonlinear equality constrained optimization problems for computation. The deduced optimization models are discretized by using the Gauss type Runge-Kutta method and the prediction-correction technique in time as well as the Fourier pseudo-spectral method in space. Numerical results and some comparisons between this method and other reported methods are given to favor the suggested method in the overall performance. It is worthwhile to emphasize that the numerical strategy in this work could be extended to other conservative or dissipative system for designing high-order structure-preserving algorithms.}
}
@incollection{KUMARI2025219,
title = {Chapter Nine - Harnessing artificial intelligence in identifying and isolation of marine peptides},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {56},
pages = {219-242},
year = {2025},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 2},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000461},
author = {Priyanshi Kumari and Bhavya Gaur and Vaibhav Mishra},
keywords = {Marine peptides, Artificial intelligence, Therapeutic values, Machine learning, Deep learning model, AMP's discovery},
abstract = {Marine ecosystem is a vast and relatively unexplored environment, where innumerable resources reside, including marine microbes, animals, algae, and other organisms' those have potential to produce different bioactive microbial peptides. Moreover, marine peptides are structurally unique and known for their exceptional bioactivity, with minimal to no harmful side effects. These bioactive peptides, isolated from marine sources, exhibit various properties, including antimicrobial, antiviral, anti-obesity, antioxidant, anti-inflammatory, and more hence, are deemed as future drugs. Furthermore, discovery of potential active peptides is exorbitant and laborious with traditional methods. Whereas, advanced computational techniques like Artificial Intelligence (AI) and their prime models make easier in the prediction and detection of important marine peptides. In this chapter we are highlighting modern AI based Machine Learning (ML) and Deep Learning (DL) models including k-Nearest Neighbour (kNN), Random Forest (RF), Artificial Neural Networks (ANNs) as ML, Fuzzy Logic or Adaptive Neuro-Fuzzy Inference System (ANFIS), Support Vector Machine (SVMs), and many more other DL models. Moreover, employing these advanced AI models to ease the isolation and identification of the bioactive microbial peptides from marine environments.}
}
@article{LOPEZ2023104398,
title = {Facets of social problem-solving as moderators of the real-time relation between social rejection and negative affect in an at-risk sample},
journal = {Behaviour Research and Therapy},
volume = {169},
pages = {104398},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001468},
author = {Roberto López and Christianne Esposito-Smythers and Annamarie B. Defayette and Katherine M. Harris and Lauren F. Seibel and Emma D. Whitmyre},
keywords = {Social problem-solving, Social rejection, Negative affect},
abstract = {Social rejection predicts negative affect, and theoretical work suggests that problem-solving deficits strengthen this relation in real-time. Nevertheless, few studies have explicitly tested this relation, particularly in samples at risk for suicide. This may be particularly important as social rejection and negative affect are significant predictors of suicide. The aim of the current study was to examine whether cognitive (i.e., perceiving problems as threats) and behavioral (i.e., avoidance) facets of problem-solving deficits moderated the real-time relation between social rejection and negative affect. The sample consisted of 49 young adults with past-month suicidal ideation. Demographic information, social problem-solving deficits, as well as depressive/anxiety symptoms and stress levels were assessed at baseline. Social rejection and negative affect were assessed using ecological momentary assessment over the following 28 days. Dynamic structural equation modeling was used to assess relations among study variables. After accounting for depressive/anxiety symptoms, stress levels, sex, and age, only avoidance of problems bolstered the real-time positive relation between social rejection severity and negative affect (b = 0.04, 95% credibility interval [0.003, 0.072]). Individuals with suicidal ideation who possess an avoidant problem-solving style may be particularly likely to experience heightened negative affect following social rejection and may benefit from instruction in problem-solving skills.}
}
@article{WU2025104172,
title = {TrustCNAV: Certificateless aggregate authentication of civil navigation messages in GNSS},
journal = {Computers & Security},
volume = {148},
pages = {104172},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104172},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004772},
author = {Zhijun Wu and Yun Bai and Yuan Zhang and Liang Liu and Meng Yue},
keywords = {Satellite navigation, Spoofing attacks, Elliptic curve, Aggregate authentication, Authentication protocol},
abstract = {The Global Navigation Satellite System (GNSS) is capable of accurate positioning because it can provide high-precision data. These data are transmitted to the receiver in the form of navigation messages, called civil navigation messages (CNAV). As it is transmitted in an open, transparent environment without data integrity protection mechanisms and secure data transmission measures, the CNAV is suspected to spoofing attacks. In 2023, the OPSGROUP has received approximately 50 reports of GPS spoofing activity. A spoofed plane's navigation system will show it as being in a different place - a security risk if a jet is guided to fly into a hostile country's airspace. To prevent the forging of GNSS positioning data by spoofing attacks targeting CNAV, we propose a certificateless aggregation authentication for CNAV by using the elliptic curve discrete logarithm problem and the combination of the GNAV structural characteristics, called TrustCNAV. Security proof and performance analysis indicate that this authentication scheme can resist spoofing attacks and ensure data security of CNAV, also it avoids pairing operations with high computational complexity, thus meeting security requirements without causing too much time and communication consumption.}
}
@article{SELESNICK2012115,
title = {Quantum-like logics and schizophrenia},
journal = {Journal of Applied Logic},
volume = {10},
number = {1},
pages = {115-126},
year = {2012},
note = {Special issue on Automated Specification and Verification of Web Systems},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570868311000656},
author = {S.A. Selesnick and G.S. Owen},
keywords = {Logic, Quantum logic, Linear logic, Schizophrenia},
abstract = {Many researchers in different disciplines have independently concluded that brains are, possibly among other things, vector processing devices. In this paper we offer support for this hypothesis coming from a new perspective. Namely, we test it against some known anomalies in the processing by schizophrenic patients of certain logical tasks: they perform better at them than normal controls, despite the observation that they do not generally employ “normal” or “commonsense” logic. On the assumption that they are compelled to use the intrinsic logic of the brain instead of commonsense logic, and that this logic is linear or quantum-like, we are able to resolve these and other anomalies. Our conclusions support the idea that human brains (at least) perform intrinsic logical operations according to the dictates of a linear (or Grassmannian, or quantum-like) logic rather than “classical” or Aristotelian logic (which seems not to be intrinsic to brains, these having evolved under the pressure of different constraints). If this is the case, then commonsense logic must be acquired through experience and the construction of contexts, an ability schizophrenic patients seem to lack, and who are consequently compelled to rely on the intrinsic logic, which is quantum-like and more efficient at certain tasks. Moreover, the proclivity toward errors of von Domarus type (namely the inference that shared attributes imply identity), which seems to be endemic to human thinking and has been discussed in connection with schizophrenia, is also explained on this basis.}
}
@incollection{TAYLOR1995227,
title = {Chapter 13 - Computational needs for process tomography},
editor = {R.A. Williams and M.S. Beck},
booktitle = {Process Tomography},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {227-249},
year = {1995},
isbn = {978-0-08-093801-1},
doi = {https://doi.org/10.1016/B978-0-08-093801-1.50017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080938011500174},
author = {R.W. Taylor}
}
@article{AMIROUCHE1991293,
title = {Gain in computational efficiency by vectorization in the dynamic simulation of multi-body systems},
journal = {Computers & Structures},
volume = {41},
number = {2},
pages = {293-302},
year = {1991},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(91)90432-L},
url = {https://www.sciencedirect.com/science/article/pii/004579499190432L},
author = {F.M.L. Amirouche and N.H. Shareef},
abstract = {This paper presents a new technique developed for increasing the computational efficiency of the dynamic simulation of multi-body systems, providing the computer code with the speed of execution, which is an order of magnitude ahead of the procedure outlined in S. K. Ider and F. M. L. Amirouche [J. appl. Mech.56, (2) (1989)]. This technique is useful with the finite element based algorithm for the solution of dynamical equations of motion for the constrained and unconstrained systems with flexible/rigid interconnected bodies. The implementation of the technique has totally eliminated the costly multiplications of large Boolean matrices, where intensive cpu utilization was required. The overall expensive computer time has been drastically reduced, particularly for the three-dimensional systems involving large degrees of freedom, as a result of their intricate geometry. The algorithmic procedure has been presented in a matrix form and is based on the recursive formulation using Kane's equation, strain energy, mode synthesis, finite element approach, a stable and efficient method for reducing the number of equations subsequent to the constraints resulting from closed loops and/or prescribed motions. Further enhancement in the speed of execution has been achieved by subjecting the developed code to vectorization on the vector-processing machine. A study of simple robot with flexible links has been presented comparing the execution times on the scalar machine (IBM-3081) and the vector-processor (IBM-3090) with and without vector options. Performance figures has been plotted demonstrating the large gains achieved by the technique developed.}
}
@article{CUI2019305,
title = {Developing reflection analytics for health professions education: A multi-dimensional framework to align critical concepts with data features},
journal = {Computers in Human Behavior},
volume = {100},
pages = {305-324},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300718},
author = {Yi Cui and Alyssa Friend Wise and Kenneth L. Allen},
keywords = {Reflection, Learning analytics, Natural language processing, Professional education, Dental education, Health professions education},
abstract = {Reflection is a key activity in self-regulated learning (SRL) and a critical part of health professions education that supports the development of effective lifelong-learning health professionals. Despite widespread use and plentiful theoretical models, empirical understanding of and support for reflection in health professions education remains limited due to simple manual assessment and rare feedback to students. Recent moves to digital reflection practices offer opportunities to computationally study and support reflection as a part of SRL. The critical task in such an endeavor, and the goal of this paper, is to align high-level reflection qualities that are valued conceptually with low-level features in the data that are possible to extract computationally. This paper approaches this goal by (a) developing a unified framework for conceptualizing reflection analytics in health professions education and (b) empirically examining potential data features through which these elements can be assessed. Synthesizing the prior literature yields a conceptual framework for health professions reflection comprised of six elements: Description, Analysis, Feelings, Perspective, Evaluation, and Outcome. These elements then serve as the conceptual grounding for the computational analysis in which 27 dental students’ reflections (in six reflective statement types) over the course of 4 years were examined using selected LIWC (Linguistic Inquiry and Word Count) indices. Variation in elements of reflection across students, years, and reflection-types supports use of the multi-dimensional analysis framework to (a) increase precision of research claims; (b) evaluate whether reflection activities are engaged in as intended; and (c) diagnose aspects of reflection in which specific students need support. Implications for the development of health professions reflection analytics that can contribute to SRL and promising areas for future research are discussed.}
}
@article{SELVERSTON1988109,
title = {A consideration of invertebrate central pattern generators as computational data bases},
journal = {Neural Networks},
volume = {1},
number = {2},
pages = {109-117},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90013-5},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900135},
author = {Allen I Selverston},
abstract = {The essential features of real neural networks are discussed with respect to their usefulness for connectionist modeling. These features are broken down into cellular and synaptic properties and related to a form of neural circuit known as central pattern generators. The gastric and pyloric rhythm of the lobster stomatogastric system are presented as possible computational data bases for modeling studies.}
}
@article{ROTHMCDUFFIE2018173,
title = {Middle school mathematics teachers’ orientations and noticing of features of mathematics curriculum materials},
journal = {International Journal of Educational Research},
volume = {92},
pages = {173-187},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0883035518305512},
author = {Amy {Roth McDuffie} and Jeffrey Choppin and Corey Drake and Jon Davis},
keywords = {Curriculum, Curriculum analysis, Teacher orientation, Middle school mathematics, Teacher noticing},
abstract = {We report findings on teachers’ noticing of features in the teacher resources of mathematics curriculum programs. Based on prior analysis, we selected teachers using one of two curriculum types: delivery mechanism or thinking device. The participating teachers and the curriculum programs aimed to align with the Common Core Standards for Mathematics, and thus, they ostensibly held a common aim for instruction. We analyzed 147 lesson planning interviews with 20 middle school mathematics teachers. We found that teachers attended to similar features of teacher resources; however, patterns for interpreting and planning decisions varied based on teachers’ orientations and curriculum type.}
}
@article{RASS202385,
title = {Adaptive dynamical systems modelling of transformational organizational change with focus on organizational culture and organizational learning},
journal = {Cognitive Systems Research},
volume = {79},
pages = {85-108},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000049},
author = {Lars Rass and Jan Treur and Wioleta Kucharska and Anna Wiewiora},
keywords = {Transformational Change, Organizational Culture, Organizational Learning, Safety Culture},
abstract = {Transformative Organizational Change becomes more and more significant both practically and academically, especially in the context of organizational culture and learning. However computational modeling and a formalization of organizational change and learning processes are still largely unexplored. This paper aims to provide an adaptive network model of transformative organizational change and translate a selection of organizational learning and change processes into computationally modelled processes. Additionally, it sets out to connect the dynamic systems view of organizations to self-modelling network models. The creation of the model and the implemented mechanisms of organizational processes are based on extrapolations of an extensive literature study and grounded in related work in this field, and then applied to a specified hospital-related case scenario in the context of safety culture. The model was evaluated by running several simulations and variations thereof. The results of these were investigated by qualitative analysis and comparison to expected emergent behaviour based on related available academic literature. The simulations performed confirmed the occurrence of an organizational transformational change towards a constant learning culture by offering repeated and effective learning and changes to organizational processes. Observations about various interplays and effects of the mechanism have been made, and they exposed that acceptance of mistakes as a part of learning culture facilitates transformational change and may foster sustainable change in the long run. Further, the model confirmed that the self-modelling network model approach applies to a dynamic systems view of organizations and a systems perspective of organizational change. The created model offers the basis for the further creation of self-modelling network models within the field of transformative organizational change and the translated mechanisms of this model can further be extracted and reused in a forthcoming academic exploration of this field.}
}
@article{KUZNETSOV20206378,
title = {Harmonic balance analysis of pull-in range and oscillatory behavior of third-order type 2 analog PLLs},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6378-6383},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1773},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323818},
author = {N.V. Kuznetsov and M.Y. Lobachev and M.V. Yuldashev and R.V. Yuldashev and G. Kolumbán},
keywords = {Phase-locked loop, third-order PLL, type 2 PLL, nonlinear analysis, harmonic balance method, describing function, global stability, birth of oscillations, hold-in range, pull-in range, lock-in range, Egan conjecture},
abstract = {The most important design parameters of each phase-locked loop (PLL) are the local and global stability properties, and the pull-in range. To extend the pull-in range, engineers often use type 2 PLLs. However, the engineering design relies on approximations which prevent a full exploitation of the benefits of type 2 PLLs. Using an exact mathematical model and relying on a rigorous mathematical thinking this problem is revisited here and the stability and pull-in properties of the third-order type 2 analog PLLs are determined. Both the local and global stability conditions are derived. As a new idea, the harmonic balance method is used to derive the global stability conditions. That approach offers an extra advantage, the birth of unwanted oscillations can be also predicted. As a verification it is shown that the sufficient conditions of global stability derived by the harmonic balance method proposed here and the well-known direct Lyapunov approach coincide with each other, moreover, the harmonic balance predicts the birth of oscillations in the gap between the local and global stability conditions. Finally, an example when the conditions for local and global stability coincide, is considered.}
}
@incollection{ERNST2021265,
title = {Chapter 22 - Pharmaceutical toxicology},
editor = {Martin Wehling},
booktitle = {Principles of Translational Science in Medicine (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {265-279},
year = {2021},
isbn = {978-0-12-820493-1},
doi = {https://doi.org/10.1016/B978-0-12-820493-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204931000088},
author = {Steffen W. Ernst and Richard Knight and Jenny Royle and Laura Stephenson},
keywords = {Regulatory toxicology, discovery toxicology, dose-resonse relationship, pharmaceutial safety, drug develoment, risk assessment},
abstract = {This chapter aims to highlight the core principles of pharmaceutical toxicology. It is an interrelated discipline that needs to be applied at all stages of the drug development process to appropriately characterise the safety profile of a drug compound and acknowledge the uncertainties associated with models available. With a strategic mindset, the preclinical safety activities aim to build a comprehensive profile of the drug so that potential hazards can be identified and the risks for healthy trial subjects or patients quantified, and, if necessary, suitable means for eliminating or reducing unacceptable risks can be put in place. We focus on the 2 distinct phases of pharmaceutical toxicology:  Discovery toxicology and regulatory toxicology, to explain how the thinking is built upon at each stage and how the mindset shifts from enabling the selection of an optimally derisked clinical candidate through to thorough risk characterisation and management of those risks for clinical development. Considering attrition due to safety reasons, whether clinical or preclinical, is one of the main reasons for drug project failure, safety assessments should be viewed with equal importance as drug efficacy assessments.}
}
@incollection{GOI2024353,
title = {13 - Perspective on photonic neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {353-375},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000098},
author = {Elena Goi and Min Gu},
keywords = {Neuromorphic photonics, photonic memories, all-optical AI microscopy, hybrid platforms},
abstract = {Bioinspired neuromorphic algorithms can process information more rapidly and more accurately than conventional algorithms, in the attempt to achieve brain-like capacity and efficiency in tasks that are challenging for traditional computers but easy for humans. With the development of applications more performing than ever, the computational requirements for running neuromorphic models are increasing exponentially, motivating efforts to develop new, specialized hardware for fast and efficient execution. Neuromorphic photonics, the implementation of neuromorphic information processing with optoelectronic hardware, is a new computational paradigm based on photons aiming to achieve brain-like information processing in the optical domain, and an interdisciplinary field that is expanding in a multitude of directions. In this chapter, we first revise what we believe are currently the main theoretical and technical challenges in the field and then give a broad perspective on the new directions and opportunities that, in our opinion, represent the current frontiers of neuromorphic photonics.}
}
@article{ROSSITER20237555,
title = {A suite of MATLAB livescript files to support learning of elementary control and feedback concepts},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7555-7560},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.657},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323010315},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, independent learning, visualisation, livescripts},
abstract = {This paper builds on a body of work in the community which is focussed on sharing learning and teaching resources, especially those which might support a first course in control. Here attention is given to some of the mathematical, analytical and numerical computations which are required to support simple system and feedback analysis and design. The aim is to provide resources which allow students to focus on core concepts and understanding so that the numerical computations are not an obstacle to their investigations. More specifically, this paper focuses on a number of MATLAB livescript files which have been produced to help students visualise the impact of parameter and design choices on system behaviour, while simultaneously empowering them to understand the source code and thus upskill them for the future. The paper gives an overview of the livescripts available so users can decide whether these could be useful in their own context; all are freely available on the author's website (Rossiter, 2021).}
}
@article{GAO2025111002,
title = {Learning and knowledge-guided evolutionary algorithm for the large-scale buffer allocation problem in production lines},
journal = {Computers & Industrial Engineering},
pages = {111002},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111002},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225001482},
author = {Sixiao Gao and Fan Zhang and Shuo Shi},
keywords = {Buffer allocation, Large-scale, Evolutionary algorithm, Learning-guided, Knowledge-guided},
abstract = {The large-scale buffer allocation problem (LBAP) in production lines represents a significant optimization challenge, centered on the efficient allocation of limited temporary storage areas. Prior research has predominantly addressed the LBAP through dynamic programming, search algorithms, and metaheuristics. However, these methodologies are often problem-specific and inefficient when applied to large-scale scenarios. Consequently, there is a pressing need to investigate innovative algorithms beyond existing approaches. This paper presents a novel learning and knowledge-guided evolutionary algorithm designed for the LBAP in production lines. The proposed algorithm develops an adaptive genetic algorithm and a variable neighborhood search algorithm, incorporating a simulated annealing-based strategy. An online Q-learning algorithm is employed to dynamically select the more effective of the two preceding algorithms for solution updates, while the simulated annealing-based strategy regulates the acceptance of these updated solutions. Furthermore, The proposed algorithm dynamically adjusts crossover, mutation, and shaking rates to adapt to the neighborhood structure. It also leverages conflict knowledge obtained from prior update experiences to inform the search process, thereby enhancing solution quality and computational efficiency. Numerical results indicate that the proposed algorithm surpasses state-of-the-art methods in addressing the LBAP. Additionally, empirical ablation studies demonstrate that the knowledge-guided approach efficiently explores promising solution regions by eliminating low-value solutions, while the learning-guided approach effectively generates improved solutions by selecting optimal strategies. This proposed algorithm significantly advances dynamic production resource allocation in large-scale systems.}
}
@article{DOSHI2020103202,
title = {Recursively modeling other agents for decision making: A research perspective},
journal = {Artificial Intelligence},
volume = {279},
pages = {103202},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103202},
url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
author = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
keywords = {Decision theory, Game theory, Hierarchical beliefs, Multiagent systems, Recursive modeling, Theory of mind},
abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.}
}
@article{ZHANG20211358,
title = {Deep learning-based evaluation of factor of safety with confidence interval for tunnel deformation in spatially variable soil},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1358-1367},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001268},
author = {Jinzhang Zhang and Kok Kwang Phoon and Dongming Zhang and Hongwei Huang and Chong Tang},
keywords = {Deep learning, Convolutional neural network (CNN), Tunnel safety, Confidence interval, Random field},
abstract = {The random finite difference method (RFDM) is a popular approach to quantitatively evaluate the influence of inherent spatial variability of soil on the deformation of embedded tunnels. However, the high computational cost is an ongoing challenge for its application in complex scenarios. To address this limitation, a deep learning-based method for efficient prediction of tunnel deformation in spatially variable soil is proposed. The proposed method uses one-dimensional convolutional neural network (CNN) to identify the pattern between random field input and factor of safety of tunnel deformation output. The mean squared error and correlation coefficient of the CNN model applied to the newly untrained dataset was less than 0.02 and larger than 0.96, respectively. It means that the trained CNN model can replace RFDM analysis for Monte Carlo simulations with a small but sufficient number of random field samples (about 40 samples for each case in this study). It is well known that the machine learning or deep learning model has a common limitation that the confidence of predicted result is unknown and only a deterministic outcome is given. This calls for an approach to gauge the model's confidence interval. It is achieved by applying dropout to all layers of the original model to retrain the model and using the dropout technique when performing inference. The excellent agreement between the CNN model prediction and the RFDM calculated results demonstrated that the proposed deep learning-based method has potential for tunnel performance analysis in spatially variable soils.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{HU2024112598,
title = {Unraveling the dynamics of stacking fault nucleation in ceramics: A case study of aluminum nitride},
journal = {Computational Materials Science},
volume = {231},
pages = {112598},
year = {2024},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112598},
url = {https://www.sciencedirect.com/science/article/pii/S092702562300592X},
author = {Yixuan Hu and Yumeng Zhang and Simanta Lahkar and Xiaodong Wang and Qi An and Kolan {Madhav Reddy}},
keywords = {Ceramics, Aluminum nitride, Deformation, Stacking faults, Generalized stacking fault energy},
abstract = {Stacking fault (SF), originating from the emission of partial dislocations, wields significant influence over the structural and physicochemical traits of ceramic materials. Yet, the intricate atomic dynamics driving SF nucleation remain obscured. Here, we introduce an improved methodology for computing the generalized stacking fault energy (GSFE) in ceramics, integrating uneven Degrees of Freedom (DOFs) for distinct lattice sites. This refinement has yielded substantial energy advantages over the traditional rigid shift method inherited from metallic systems. Our findings underscore that the relaxation of nonmetallic N atoms within the SF region is pivotal for achieving a more realistic SF simulation. This, in turn, unveils the involvement of N atom migration within the SF region between different aluminum tetrahedral sites during SF nucleation. By alleviating the energy barrier, this relaxation contrasts with previous simulations where nonmetallic elements remained more rigid. This work demonstrates the atomic dynamics of SF nucleation in ceramics and breaks the conventional wisdom of uniformly applying constraints for GSFE computations.}
}
@article{HENNE2019157,
title = {A counterfactual explanation for the action effect in causal judgment},
journal = {Cognition},
volume = {190},
pages = {157-164},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719301301},
author = {Paul Henne and Laura Niemi and Ángel Pinillos and Felipe {De Brigard} and Joshua Knobe},
keywords = {Action effect, Omissions, Omission effect, Causal reasoning, Counterfactual thinking, Causation by omission},
abstract = {People’s causal judgments are susceptible to the action effect, whereby they judge actions to be more causal than inactions. We offer a new explanation for this effect, the counterfactual explanation: people judge actions to be more causal than inactions because they are more inclined to consider the counterfactual alternatives to actions than to consider counterfactual alternatives to inactions. Experiment 1a conceptually replicates the original action effect for causal judgments. Experiment 1b confirms a novel prediction of the new explanation, the reverse action effect, in which people judge inactions to be more causal than actions in overdetermination cases. Experiment 2 directly compares the two effects in joint-causation and overdetermination scenarios and conceptually replicates them with new scenarios. Taken together, these studies provide support for the new counterfactual explanation for the action effect in causal judgment.}
}
@article{KHISTY200577,
title = {Possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {13},
number = {2},
pages = {77-92},
year = {2005},
note = {Handling Uncertainty in the Analysis of Traffic and Transportation Systems (Bari, Italy, June 10–13 2002)},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X05000161},
author = {C. Jotin Khisty and Turan Arslan},
keywords = {Paradigm shift, Planning, Rationality, Systemicity, Transportation, Uncertainty},
abstract = {This paper describes and discusses the possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty: (a) through the introduction of the concept of ‘systemicity’; (b) by expanding the spectrum of the existing planning paradigm currently in use; (c) by reducing complexity through the application of tests of adequacy, dependency, suitability, and adaptability; (d) through the introduction of soft systems thinking; and (e) by using ‘abductive’ in addition to deductive and inductive inferencing. It is concluded that the application of these strategies, adjustments, and tests to the existing planning procedure will hopefully enrich and strengthen our planning effort and make it more robust.}
}
@article{JAHEL2023122624,
title = {The future of social-ecological systems at the crossroads of quantitative and qualitative methods},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122624},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122624},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523003098},
author = {Camille Jahel and Robin Bourgeois and Jérémy Bourgoin and William's Daré and Marie {De Lattre-Gasquet} and Etienne Delay and Patrice Dumas and Christophe {Le Page} and Marc Piraux and Rémi Prudhomme},
keywords = {Quantitative, Qualitative, Anticipation, Foresight, Power relationship, Discontinuities},
abstract = {Urgent calls to transform societies toward more sustainability make the practice of anticipation more and more necessary. The progressive development of computational technologies has opened room for a growing use of quantitative methods to explore the future of social-ecological systems, in addition to qualitative methods. This warrants investigating issues of power relationships and discontinuities and unknowns that arise when mingling quantitative and qualitative anticipatory methods. We first reflected on the semantics attached to these methods. We then conducted a comparative analysis on the way the articulation of quantitative and qualitative methods was conducted, based on an in-depth analysis of a set of eleven anticipatory projects completed by several external case studies. We propose insights to classify projects according to the timing (successive, iterative or convergent) and the purpose of the articulation (imagination, refinement, assessment and awareness raising). We use these insights to explore methodological implications and power relationships and then discuss the ways to inform or frame anticipatory projects that seek to combine these methods.}
}
@article{TERAN2017384,
title = {Dynamic Profiles Using Sentiment Analysis for VAA's Recommendation Design},
journal = {Procedia Computer Science},
volume = {108},
pages = {384-393},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.265},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730902X},
author = {Luis Terán and Jose Mancera},
keywords = {Voting Advice Applications, Dynamic Profiles, Recommender Systems, Decision-Making, Elections},
abstract = {In the context of elections, the Internet opens new and promising possibilities for parties and candidates looking for a better political strategy and visibility. In this way they can also organize their election campaign to gather funds, to mobilize support, and to enter into a direct dialogue with the electorate. This paper presents an ongoing research of recommender systems applied on e-government, particularly it is an extension of so-called voting advice applications (VAA’s). VAA’s are Web applications that support voters, providing relevant information on candidates and political parties by comparing their political interests with parties or candidates on different political issues. Traditional VAA’s provide recommendations of political parties and candidates focusing on static profiles of users. The goal of this work is to develop a candidate profile based on different parameters, such as the perspective of voters, social network activities, and expert opinions, to construct a more accurate dynamic profile of candidates. Understanding the elements that compose a candidate profile will help citizens in the decision-making process when facing a lack of information related to the behavior and thinking of future public authorities. At the end of this work, a fuzzy-based visualization approach for a VAA design is given using as a case study the National Elections of Ecuador in 2013.}
}
@article{ZHOU2024124298,
title = {Hyperspectral imaging combined with blood oxygen saturation for in vivo analysis of small intestinal necrosis tissue},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {315},
pages = {124298},
year = {2024},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2024.124298},
url = {https://www.sciencedirect.com/science/article/pii/S1386142524004645},
author = {Yao Zhou and LeChao Zhang and DanFei Huang and Yong Zhang and LiBin Zhu and Xiaoqing Chen and Guihua Cui and Qifan Chen and XiaoJing Chen and Shujat Ali},
keywords = {Hyperspectral imaging, Tissue oxygenation, Small intestine tissue, Isosbestic points},
abstract = {Acute mesenteric ischemia (AMI) is a clinically significant vascular and gastrointestinal condition, which is closely related to the blood supply of the small intestine. Unfortunately, it is still challenging to properly discriminate small intestinal tissues with different degrees of ischemia. In this study, hyperspectral imaging (HSI) was used to construct pseudo-color images of oxygen saturation about small intestinal tissues and to discriminate different degrees of ischemia. First, several small intestine tissue models of New Zealand white rabbits were prepared and collected their hyperspectral data. Then, a set of isosbestic points were used to linearly transform the measurement data twice to match the reference spectra of oxyhemoglobin and deoxyhemoglobin, respectively. The oxygen saturation was measured at the characteristic peak band of oxyhemoglobin (560 nm). Ultimately, using the oxygenated hemoglobin reflectance spectrum as the benchmark, we obtained the relative amount of median oxygen saturation in normal tissues was 70.0 %, the IQR was 10.1 %, the relative amount of median oxygen saturation in ischemic tissues was 49.6 %, and the IQR was 14.6 %. The results demonstrate that HSI combined with the oxygen saturation computation method can efficiently differentiate between normal and ischemic regions of the small intestinal tissues. This technique provides a powerful support for internist to discriminate small bowel tissues with different degrees of ischemia, and also provides a new way of thinking for the diagnosis of AMI.}
}
@article{OUDMAN2018214,
title = {Effects of different cue types on the accuracy of primary school teachers' judgments of students' mathematical understanding},
journal = {Teaching and Teacher Education},
volume = {76},
pages = {214-226},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17302305},
author = {Sophie Oudman and Janneke {van de Pol} and Arthur Bakker and Mirjam Moerbeek and Tamara {van Gog}},
keywords = {Teacher judgment, Judgment accuracy, Cue utilization, Primary education, Mathematics education, Decimals},
abstract = {To gain insight into how teachers' judgment accuracy can be improved, we investigated effects of cue-type availability. While thinking aloud, 21 teachers judged their fourth grade students' (n = 176) decimal magnitude understanding. Sensitivity (correctly judging what students did understand) did not improve from availability of both answer cues (students' answers to prior practice problems) and student cues (knowledge of students triggered by knowing their names), and was lower when only answer cues were available, compared to only student cues. Specificity (correctly judging what students did not understand) was higher when only answer cues were available, compared to only student cues or both student and answer cues.}
}
@article{MA2024100647,
title = {Design of online teaching interaction mode for vocational education based on gamified-learning},
journal = {Entertainment Computing},
volume = {50},
pages = {100647},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000156},
author = {Zhongbao Ma and Wei Li},
keywords = {Gamified-learning, Traveler-type problems, Genetic-based algorithms, Game design},
abstract = {Along with the process of building China's modern vocational education system, China's higher vocational education has made great progress. With the development of computer and Internet technology, gamified learning, as a new way of learning, combines the advantages of computer games and online learning, which not only meets the needs of people to learn anytime and anywhere, but also increases the fun of learning activities. In this paper, we developed a gamified learning software with traveler-type problems as the research content, through the interaction with the game, so that students can think in the game and learn knowledge through the game. Through the questionnaire for research and analysis, this game is good game fun and can stimulate learning interest well. In addition, this paper carries out an in-depth study of the game's help system, optimizes the algorithm for the help system, and proposes an improved genetic algorithm. The reverse learning method is adopted to improve the accuracy and convergence speed of the optimal solution; then the Metropolis criterion is used to improve the crossover and mutation operators to enhance the local search ability of the algorithm; finally, the concept of realistic elite learning is introduced to further enhance the local search ability of the algorithm. The simulation results show that the algorithm is effectively improved in convergence performance and solution accuracy, which can significantly improve the response speed of the help system, effectively improve the game's fun, and improve the game's playability.}
}
@article{KANWISHER2025102969,
title = {Animal models of the human brain: Successes, limitations, and alternatives},
journal = {Current Opinion in Neurobiology},
volume = {90},
pages = {102969},
year = {2025},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2024.102969},
url = {https://www.sciencedirect.com/science/article/pii/S0959438824001314},
author = {Nancy Kanwisher},
abstract = {The last three decades of research in human cognitive neuroscience have given us an initial “parts list” for the human mind in the form of a set of cortical regions with distinct and often very specific functions. But current neuroscientific methods in humans have limited ability to reveal exactly what these regions represent and compute, the causal role of each in behavior, and the interactions among regions that produce real-world cognition. Animal models can help to answer these questions when homologues exist in other species, like the face system in macaques. When homologues do not exist in animals, for example for speech and music perception, and understanding of language or other people's thoughts, intracranial recordings in humans play a central role, along with a new alternative to animal models: artificial neural networks.}
}
@incollection{MILLER2017141,
title = {8 - Doctoral and professional programs},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {141-169},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000088},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Critical thinking, Basic/applied/clinical, Problem identification, Research design, Teams, PhD/PharmD, Postdoc/postdoctoral, Writing/publishing.},
abstract = {In this chapter on graduate and professional education, we explore Doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD, and to postdoc or not? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits and skills underpin this discussion. We outline possible career choices, touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what's best for you is something you will have to decipher, but hopefully only after you consult with family, friends, and advisors or mentors. Regardless, “the big leap” is coming, so get ready.}
}
@article{CARPENTER1992457,
title = {Chapter 4 Cognitively guided instruction: Building on the knowledge of students and teachers},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {457-470},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80005-9},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800059},
author = {Thomas P. Carpenter and Elizabeth Fennema},
abstract = {This chapter summarizes the results of a series of correlational, experimental, and case studies on Cognitively Guided Instruction (CGI), a program designed to help teachers understand children's thinking and use this knowledge to make instructional decisions. Results of the studies show that teachers' knowledge and beliefs about students' thinking are related to students' achievement. There were significant differences between CGI classes and control classes on the emphasis on problem solving and low level skills, the freedom given to students to construct their own strategies for solving problems, the teachers' knowledge of their students thinking, and the students' achievement in both problem solving and skills.}
}
@article{EVANS201123659,
title = {Advancing Science through Mining Libraries, Ontologies, and Communities*},
journal = {Journal of Biological Chemistry},
volume = {286},
number = {27},
pages = {23659-23666},
year = {2011},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.R110.176370},
url = {https://www.sciencedirect.com/science/article/pii/S0021925819487164},
author = {James A. Evans and Andrey Rzhetsky},
keywords = {Biophysics, Computation, Computer Modeling, Drug Design, Epigenetics, Computational Biology, Information Cascade, Sociology of Science, Text Mining},
abstract = {Life scientists today cannot hope to read everything relevant to their research. Emerging text-mining tools can help by identifying topics and distilling statements from books and articles with increased accuracy. Researchers often organize these statements into ontologies, consistent systems of reality claims. Like scientific thinking and interchange, however, text-mined information (even when accurately captured) is complex, redundant, sometimes incoherent, and often contradictory: it is rooted in a mixture of only partially consistent ontologies. We review work that models scientific reason and suggest how computational reasoning across ontologies and the broader distribution of textual statements can assess the certainty of statements and the process by which statements become certain. With the emergence of digitized data regarding networks of scientific authorship, institutions, and resources, we explore the possibility of accounting for social dependences and cultural biases in reasoning models. Computational reasoning is starting to fill out ontologies and flag internal inconsistencies in several areas of bioscience. In the not too distant future, scientists may be able to use statements and rich models of the processes that produced them to identify underexplored areas, resurrect forgotten findings and ideas, deconvolute the spaghetti of underlying ontologies, and synthesize novel knowledge and hypotheses.}
}
@incollection{SCHNEEGANS2008241,
title = {13 - Dynamic Field Theory as a Framework for Understanding Embodied Cognition},
editor = {Paco Calvo and Antoni Gomila},
booktitle = {Handbook of Cognitive Science},
publisher = {Elsevier},
address = {San Diego},
pages = {241-271},
year = {2008},
series = {Perspectives on Cognitive Science},
issn = {15564495},
doi = {https://doi.org/10.1016/B978-0-08-046616-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008046616300013X},
author = {Sebastian Schneegans and Gregor Schöner},
abstract = {Publisher Summary
Embodied cognition is an approach to cognition that has roots in motor behavior. This approach emphasizes that cognition typically involves acting with a physical body on an environment in which that body is immersed. The approach of embodied cognition postulates that understanding cognitive processes entails understanding their close link to the motor surfaces that may generate action and to the sensory surfaces that provide sensory signals about the environment. To a certain extent, the embodiment stance implies a mistrust of the abstraction inherent in much information processing thinking, in which the interface between cognitive processes and their sensorimotor support is drawn at a level that is quite removed from both the sensory and the motor systems. New theoretical tools are needed to address cognition within the embodiment perspective. This chapter reviews one set of theoretical concepts which is believed to be particularly suited to address the constraints of embodiment and situatedness. It refers to this set of concepts as Dynamical Systems Thinking.}
}
@incollection{STEIN202139,
title = {Chapter 2 - Brain–minds: What’s the best metaphor?},
editor = {Dan J. Stein},
booktitle = {Problems of Living},
publisher = {Academic Press},
pages = {39-59},
year = {2021},
isbn = {978-0-323-90239-7},
doi = {https://doi.org/10.1016/B978-0-323-90239-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902397000055},
author = {Dan J. Stein},
keywords = {Psychiatry, Philosophy, Realism, Psychiatric classification, Pluralism, Erklären, Verstehen, Epistemic humility, Practical wisdom},
abstract = {This chapter addresses the question of how best to think about the brain–mind from both philosophical and psychiatric perspectives. The section on philosophy of mind notes the positions of physicalism, dualism, and functionalism, and proposes that emergent materialism has particular advantages. The section on psychiatry notes the positions of behaviourism and existentialism. Two key metaphors of the brain–mind are then critiqued: the hydraulic model of psychoanalysis, and the computational model of cognitive science. A third metaphor, that of ‘wetware’, which emphasizes that the brain–mind cannot simply be divided into hardware and software, but rather that it must be approached as a complex psychobiological phenomenon, is proposed. Several advantages of this metaphor are discussed, including that it is consistent with emergent materialism and a view of the brain–mind as embodied and embedded in social activity, as well as with current cognitive-affective and psychiatric science.}
}
@article{VANOPHEUSDEN2019127,
title = {Tasks for aligning human and machine planning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {127-133},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300622},
author = {Bas {van Opheusden} and Wei Ji Ma},
abstract = {Research on artificial intelligence and research on human intelligence rely on similar conceptual foundations and have long inspired each other [1,2•]. However, achieving concrete synergy has been difficult, with one obstacle being a lack of alignment of the tasks used in both fields. Artificial intelligence research has traditionally focused on tasks that are challenging to solve, often using human performance as a benchmark to surpass [3, 4, 5, 6, 7]. By contrast, cognitive science and psychology have moved towards tasks that are simple enough to allow for detailed computational modeling of people’s choices. These divergent objectives have led to a divide in the complexity of tasks studied, both in perception and cognition. The purpose of this paper is to explore the middle ground: are there tasks that are reasonably attractive to both fields and could provide fertile ground for synergy?}
}
@incollection{BLISS19921,
title = {REASONING SUPPORTED BY COMPUTATIONAL TOOLS},
editor = {MICHAEL R. KIBBY and J. ROGER HARTLEY},
booktitle = {Computer Assisted Learning: Selected Contributions from the CAL '91 Symposium},
publisher = {Pergamon},
address = {Amsterdam},
pages = {1-9},
year = {1992},
isbn = {978-0-08-041395-2},
doi = {https://doi.org/10.1016/B978-0-08-041395-2.50007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080413952500078},
author = {JOAN BLISS and JON OGBORN and RICHARD BOOHAN and JONATHAN BRIGGS and TIM BROSNAN and DEREK BROUGH and HARVEY MELLAR and ROB MILLER and CAROLINE NASH and CATHY RODGERS and BABIS SAKONIDIS},
abstract = {Abstract
This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3-4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{SHIN2023104897,
title = {Pedagogical discourse markers in online algebra learning: Unraveling instructor's communication using natural language processing},
journal = {Computers & Education},
volume = {205},
pages = {104897},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104897},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523001744},
author = {Jinnie Shin and Renu Balyan and Michelle P. Banawan and Tracy Arner and Walter L. Leite and Danielle S. McNamara},
keywords = {Pedagogical communication, Online learning, Video lectures, Natural language processing},
abstract = {Despite the proliferation of video-based instruction and its benefits—such as promoting student autonomy and self-paced learning—the complexities of online teaching remain a challenge. To be effective, educators require extensive training in digital teaching methodologies. As such, there's a pressing need to examine and comprehend the intricacies of instructors' communication patterns within this context. This research addresses the pressing need to understand pedagogical discourse in online video lectures in Algebra classes by employing computational linguistic tools and natural language processing (NLP). Using transcripts from 125 Algebra 1 video lectures—comprising 4962 instances of pedagogical discourse—from five instructors at Math Nation, a virtual math learning environment, we analyzed the conveyance of linguistic, attitudinal, and emotional nuances. With the aid of 26 Coh-Metrix and SÉANCE features, we classified educators' language choices, achieving an accuracy of 86.7%. Furthermore, variations in language choices, as signified by discourse markers, were examined through a K-means clustering approach. The resulting 17 clusters were grouped into interpersonal, structural, and cognitive pedagogic functions. Through this exploration, we demonstrate the promising potential of NLP in efficiently deciphering pedagogical communication patterns in video lectures. These insights open a new avenue for research, aimed at assessing the efficacy of digital instruction by scrutinizing pedagogical discourse characteristics in computer-based learning environments.}
}