@article{BRUNDAGE201532,
title = {Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014)},
journal = {Futures},
volume = {72},
pages = {32-35},
year = {2015},
note = {Confronting Future Catastrophic Threats To Humanity},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2015.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715000932},
author = {Miles Brundage},
keywords = {Existential risk, Artificial intelligence, Superintelligence, Responsible innovation},
abstract = {A new book by Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, is reviewed. Superintelligence explores the future of artificial intelligence and related technologies and the risks they may pose to human civilization. The book ably demonstrates the potential for serious thinking aimed at the long-term future. Bostrom succeeds in arguing that the development of superintelligent machines will, if not properly managed, create catastrophic risks to humanity. The book falls short in some respects, and some sections are more compelling and novel than others. Overall, however, Bostrom’s book succeeds in demolishing the “null hypothesis” according to which the possibility and risks of superintelligence can continue to be ignored, and is a must-read for those interested in the long-term future of humanity.}
}
@article{LOMBARDI2022100601,
title = {Understanding emerging patterns and dynamics through the lenses of the cyber-physical universe},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100601},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100601},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002264},
author = {Mauro Lombardi and Simone Vannuccini},
keywords = {cyber-physical universe, ubiquitous computing, information technology, artificial intelligence, decision making},
abstract = {Summary
The complex interaction among contemporary techno- and socio-economic processes has set the stage for the emergence of a cyber-physical universe, the novel landscape in which agents behave and interact, and which is centered on the fundamental role played by information and computation at all levels. In this paper, we weave into a single analysis the different threads that lead to (and characterize) the cyber-physical universe and outline a map of its building blocks and the complex dynamics at work in the new environment. The resulting description is used to assess how decision-making processes should evolve in order to be able to address the opportunities and challenges of the current era of deep and extended changes. The analysis offers an encompassing interpretative grid to understand and unpack patterns in the contemporary socio-technical systems that experience a fundamental informational turn; this can inform new research trajectories and help open up new areas for scientific inquiry.}
}
@article{DEY2016177,
title = {A probabilistic approach to diagnose faults of air handling units in buildings},
journal = {Energy and Buildings},
volume = {130},
pages = {177-187},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306958},
author = {Debashis Dey and Bing Dong},
keywords = {Air Handling Unit, Bayesian belief network, APAR rules, Fault detection and diagnosis},
abstract = {Air handling unit (AHU) is one of the most extensively used equipment in large commercial buildings. This device is typically customized and lacks quality system integration which can result in hardwire failures and control errors. Air handling unit Performance Assessment Rules (APAR) is a fault detection tool that uses a set of expert rules derived from mass and energy balances to detect faults in air handling units. APAR is computationally simple enough that it can be embedded in commercial building automation and control systems and relies only upon sensor data and control signals that are commonly available in these systems. Although APAR has advantages over other methods, for example no training data required and easy to implement commercially, most of the time it is unable to provide the root diagnosis of the faults. For instance, a fault on temperature sensor could be bias, drifting bias, inappropriate location, or complete failure. In addition a fault in mixing box can be return and/or outdoor damper leak or stuck. In addition, when multiple rules are satisfied, the list of faults increases. There is no proper way to have the correct diagnosis for rule based fault detection system. To overcome this limitation, we proposed Bayesian Belief Network (BBN) as a diagnostic tool. BBN can be used to simulate diagnostic thinking of FDD experts through a probabilistic way. In this study we developed a new way to detect and diagnose faults in AHU through combining APAR rules and Bayesian Belief network. Bayesian Belief Network is used as a decision support tool for rule based expert system. BBN is highly capable to prioritize faults when multiple rules are satisfied simultaneously. Also it can get information from previous AHU operating conditions and maintenance records to provide proper diagnosis. The proposed model is validated with real time measured data of a campus building. The results show that BBN correctly prioritize faults that are verified by manual investigation.}
}
@article{KNYAZEV201817,
title = {Resting state connectivity mediates the relationship between collectivism and social cognition},
journal = {International Journal of Psychophysiology},
volume = {123},
pages = {17-24},
year = {2018},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167876017305470},
author = {Gennady G. Knyazev and Alexander N. Savostyanov and Andrey V. Bocharov and Ekaterina A. Merkulova},
keywords = {Collectivism, Social cognition, Medial prefrontal cortex, Connectivity, Mediation analysis},
abstract = {Humans are intrinsically social beings and it is natural that self-processing is associated with social cognition. The degree to which the self is perceived as a part of social environment is modulated by cultural stereotypes, such as collectivism and individualism. Here, we tested the hypothesis that individuals who endorse collectivist values would spontaneously think more about their relationships with other people and this association would be mediated by connectivity between the medial prefrontal cortex (MPFC) and the rest of the brain. Connectivity was evaluated based on resting state EEG data using the recently developed methods, which combine beamformer spatial filtering with seed based connectivity estimation. The formal mediation analysis revealed that collectivism is associated with an enhanced connectivity of MPFC with a set of cortical regions that are frequently co-activated in moral reasoning, empathy, and theory of mind tasks and with diminished connectivity with the precuneus\posterior cingulate cortex, which is involved in self-centered cognition. The relationship between collectivism and social cognition was mediated by MPFC connectivity with the left middle temporal gyrus implying that in participants with collectivistic attitude, thinking about relationships with other people may be associated with semantic memory retrieval and reasoning on moral issues and others' intentions.}
}
@article{VARGASCARPINTERO2025120104,
title = {Development of an integrated multi-criteria framework to assess the implementation potential of biobased value chains and webs with a territorial approach},
journal = {Industrial Crops and Products},
volume = {223},
pages = {120104},
year = {2025},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2024.120104},
url = {https://www.sciencedirect.com/science/article/pii/S0926669024020818},
author = {Ricardo Vargas-Carpintero},
keywords = {Biobased value chain, Biobased value web, Biorefinery, Territorial bioeconomy system, Multi-criteria assessment, Land-based bioeconomy},
abstract = {Biobased value chains and webs (BVCW) encompass value adding activities and actors from biomass production, its processing into biobased products for manifold sectors, until their commercialization and use. BVCW are part of territorial bioeconomy systems and are shaped by contextual settings. The design and development of BVCW involve strategic decisions towards their sustainable implementation. Throughout the design and development of BVCW, the adoption of an integral approach that links technical aspects of biomass-to-product pathways with non-technical aspects and context factors is necessary to increase the BVCW implementation potential. Accordingly, an active incorporation of the territorial context of BVCW in the design process is required. In view of these requirements, in this study an integrated, multi-criteria framework is developed to assess the implementation potential in BVCW design. For this purpose, key elements from existing biorefinery and biomass supply chain design methodologies are identified and integrated in a multi-criteria framework that allows the consideration of both an internal and external perspective of the BVCW in relation to the context. The conceptualized framework serves as an evaluation approach to check the implementability of biomass-to-product pathways BVCW configurations in form of by means of a multi-criteria catalogue. The set of criteria integrates relevant aspects for the design and development of BVCW from land-based biomass (e.g. crops and crop residues). It entails key criteria related to the functionality of the biomass-to-product pathway in technical-economic terms and the surrounding biophysical, social and economic context. The further operationalization of the multi-criteria catalogue by means of an indicator-based assessment could enable the prioritization and selection of BVCW configurations with best implementation potential. In this way, the framework provides a practical approach for decision-makers, local actors and researchers involved in the design and development of BVCW tailored to the territorial context.}
}
@article{EVANS2003454,
title = {In two minds: dual-process accounts of reasoning},
journal = {Trends in Cognitive Sciences},
volume = {7},
number = {10},
pages = {454-459},
year = {2003},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2003.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661303002250},
author = {Jonathan St.B.T. Evans},
abstract = {Researchers in thinking and reasoning have proposed recently that there are two distinct cognitive systems underlying reasoning. System 1 is old in evolutionary terms and shared with other animals: it comprises a set of autonomous subsystems that include both innate input modules and domain-specific knowledge acquired by a domain-general learning mechanism. System 2 is evolutionarily recent and distinctively human: it permits abstract reasoning and hypothetical thinking, but is constrained by working memory capacity and correlated with measures of general intelligence. These theories essentially posit two minds in one brain with a range of experimental psychological evidence showing that the two systems compete for control of our inferences and actions.}
}
@article{KASHYAPKASHYAP2021395,
title = {The universal language: mathematics or music?},
journal = {Journal for Multicultural Education},
volume = {15},
number = {4},
pages = {395-415},
year = {2021},
issn = {2053-535X},
doi = {https://doi.org/10.1108/JME-05-2021-0064},
url = {https://www.sciencedirect.com/science/article/pii/S2053535X21000197},
author = {RaviRavi KashyapKashyap},
keywords = {Mathematics, Multicultural, Music, Education policy, Artistic encoding of knowledge, Universal language},
abstract = {Purpose
Music could be a challenger for mathematics and a potential candidate for the title “The Universal Language.” This paper aims to discuss the primary objectives of engaging with music, including the therapeutic benefits. Similarities, between mathematics and music and how studying one might enhance one’s abilities of the other are pointed out.
Design/methodology/approach
A formal definition for a universal language is given. A qualitative approach, supplemented with rigorous reasoning, is adopted. The narrative relies on the author’s experiences, teaching mathematical concepts and musical interactions, with students from several countries. A vast amount of literature is reviewed and the corresponding findings are connected toward the arguments made.
Findings
The paper demonstrates that one day, once we understand both mathematics and music better, we might see both of them as the same language. Until then, it is essential to supplement mathematics with music. The educational implications, for all fields, are to ensure that the future creators of knowledge are equally adept at both music and mathematics. The wider policy connotations are to create a blueprint for a society with a vibrant musical and artistic environment.
Originality/value
This study illuminates new ways of thinking about music and mathematics. The possibility that many seemingly complex entities (including our universe, virtual computer worlds, mathematical operations, etc.), are made up of combinations of much simpler building blocks is hinted at. Familiarity with any intricate element of life, without getting flustered, is bound to produce remarkable results in other such endeavors.}
}
@article{EVANS2022281,
title = {The explainability paradox: Challenges for xAI in digital pathology},
journal = {Future Generation Computer Systems},
volume = {133},
pages = {281-296},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000838},
author = {Theodore Evans and Carl Orge Retzlaff and Christian Geißler and Michaela Kargl and Markus Plass and Heimo Müller and Tim-Rasmus Kiehl and Norman Zerbe and Andreas Holzinger},
keywords = {Explainable AI, Digital pathology, Usability, Trust, Artificial intelligence},
abstract = {The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine and proposes empirically-backed principles for their safer and more effective design.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{20213341,
title = {Tim Behrens},
journal = {Neuron},
volume = {109},
number = {21},
pages = {3341-3343},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007200},
abstract = {Summary
Tim Behrens discusses with Neuron creative ways to facilitate virtual meetings, the multiple ways that the pandemic has affected different people, and his advice for the younger generation of neuroscientists in general and computational scientists in particular.}
}
@article{DANAHY2001127,
title = {Technology for dynamic viewing and peripheral vision in landscape visualization},
journal = {Landscape and Urban Planning},
volume = {54},
number = {1},
pages = {127-138},
year = {2001},
note = {Our Visual Landscape: analysis, modeling, visualization and protection},
issn = {0169-2046},
doi = {https://doi.org/10.1016/S0169-2046(01)00131-1},
url = {https://www.sciencedirect.com/science/article/pii/S0169204601001311},
author = {John W. Danahy},
keywords = {Visualization, Real-time immersive virtual reality, Panorama, Peripheral vision, Foveal vision, Dynamic viewing},
abstract = {The dynamic qualities of looking around and moving about, directly sensing spatial queues, using one’s peripheral vision, and focusing with foveal vision on objects of attention are fundamental to a person’s visual experience in landscape. Unfortunately, the visual media commonly used to structure scientific analysis, professional design, decision-making and artistic interpretation of visual landscapes are quite weak at portraying the dynamic and peripheral dimensions of human vision. Also, visual media whether it be manual drawing, photomontage or state-of-the-art computer animation tend to be time consuming and difficult to apply to these dimensions of seeing. The absence of a convenient, cost-effective means for showing all the fundamental visual aspects of landscape in a balanced way is a serious limitation. This deficiency begs the following questions. Is the current state of knowledge in visual landscape management biased by the relative ease with which established media, such as illustration, photography, and photo-realistic rendering can be used? Do the characteristics of these media bias our perception and thinking about landscape toward static foveal aspects of visual experience? Are our ideas about dynamic viewing and computer animation limited by the didactic frame-by-frame approach characteristic of cinematography and video? Can the introduction of equally robust tools and methods for dynamic and peripheral viewing balance any bias caused by current visualization technology? If McLuhan’s insights about media are correct, then we need to do more research on this question. This paper suggests that the field of landscape visualization needs to develop instruments for research that more fully capture the fundamental components of human vision before we can properly study the question or advance practice. It outlines some ways the next generation of visualization technology can be used to balance the disproportionate emphasis on foveal ways of visual thinking commonly used in the past for the study of visual landscapes. The paper explains this deficiency and proposes some area for research and development of visualization instruments more capable of redressing this imbalance. The paper outlines this issue and proposes that as electronic media and computational media become more developed and are applied to the realm of visual concerns, it will become more practical to include peripheral vision and dynamic viewing in deliberations about visual landscapes. This paper reflects on the potential of visualization automation techniques to overcome these shortcomings through illustrations of project work using innovative software tools developed to explore this question at the Centre for Landscape Research (CLR) at the University of Toronto.}
}
@article{ZHOU2025121668,
title = {Sparse loss-aware ternarization for neural networks},
journal = {Information Sciences},
volume = {693},
pages = {121668},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121668},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015822},
author = {Ruizhi Zhou and Lingfeng Niu and Dachuan Xu},
keywords = {Machine learning, Ternary neural networks, Loss-aware quantization, Sparse regularization, ADMM},
abstract = {Deep neural networks (DNNs) have shown great success in machine learning tasks and widely used in many fields. However, the substantial computational and storage requirements inherent to DNNs are usually high, which poses challenges for deploying deep learning models on resource-limited devices and hindering further applications. To address this issue, the lightweight nature of neural networks has garnered significant attention, and quantization has become one of the most popular approaches to compress DNNs. In this paper, we introduce a sparse loss-aware ternarization (SLT) model for training ternary neural networks, which encodes the floating-point parameters into {−1,0,1}. Specifically, we abstract the ternarization process as an optimization problem with discrete constraints, and then modify it by applying sparse regularization to identify insignificant weights. To deal with the challenges brought by the discreteness of the model, we decouple discrete constraints from the objective function and design a new algorithm based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments are conducted on public datasets with popular network architectures. Comparisons with several state-of-the-art baselines demonstrate that SLT always attains comparable accuracy while having better compression performance.}
}
@article{ARMSTRONG20161,
title = {A NIT-picking analysis: Abstractness dependence of subtests correlated to their Flynn effect magnitudes},
journal = {Intelligence},
volume = {57},
pages = {1-6},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300812},
author = {Elijah L. Armstrong and Jan {te Nijenhuis} and Michael A. {Woodley of Menie} and Heitor B.F. Fernandes and Olev Must and Aasa Must},
keywords = {Abstract thinking, Flynn effect, Intelligence, National Intelligence Test, Estonia, g loading},
abstract = {We examine the association between the strength of the Flynn effect in Estonia and highly convergent panel-ratings of the ‘abstractness’ of nine subtests on the National Intelligence Test, in order to test the theory that the Flynn effect results in part from an increase in the use of abstract reference frames in solving cognitive problems. The vectors of abstractness ratings and Flynn effect gains, controlled for guessing) exhibit a near-zero correlation (r=−.02); however, abstractness correlates positively with (and is therefore confounded by) g-loadings (r=.61). A General Linear Model is used to determine the degree to which the abstractness vector predicts the Flynn effect vector, independently of subtest g-loadings and the portion of the secular IQ gain due to guessing (the Brand effect). Consistent with the abstract reasoning model of the Flynn effect, abstractness positively predicts Flynn effect magnitudes, once controlled for confounds (sr=.44), which indicates an increasing tendency to utilize factors external to the items in order to abstract their solutions.}
}
@article{LI20234116,
title = {A call for caution in the era of AI-accelerated materials science},
journal = {Matter},
volume = {6},
number = {12},
pages = {4116-4117},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523005283},
author = {Kangming Li and Edward Kim and Yao Fehlis and Daniel Persaud and Brian DeCost and Michael Greenwood and Jason Hattrick-Simpers},
abstract = {It is safe to state that the field of matter has successfully entered the fourth paradigm, where machine learning and artificial intelligence (AI) are universally seen as useful, if not truly intelligent. AI’s utilization is near-ubiquitous from the prediction of novel materials to reducing computational overhead for material simulations; its value has been demonstrated time and again by both theorists and experimentalists. There is, however, a worrying trend toward large datasets and overparameterized models being all we need to accelerate science through accurate and robust machine learning systems.}
}
@article{TALANOV201641,
title = {Emotional simulations and depression diagnostics},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {41-50},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300676},
author = {Max Talanov and Jordi Vallverdú and Bin Hu and Philip Moore and Alexander Toschev and Diana Shatunova and Anzhela Maganova and Denis Sedlenko and Alexey Leukhin},
keywords = {Dopamine, Serotonin, Fear, Artificial intelligence, Simulation, Rat brain, Affective computing, Emotion modelling, Neuromodulation},
abstract = {In this work we propose the following hypothesis: the neuromodulatory mechanisms that control the emotional states of mammals can be translated and re-implemented in a computer by controlling the computational performance of a hosted computational system. In our specific implementation, we represent the simulation of the ‘fear-like’ state based on the three dimensional neuromodulatory model of affects, in this paper ‘affects’ refer to the basic emotional inborn states, inherited from works of Hugo Lövheim. Whilst dopamine controls attention, serotonin is the key for inhibition, and fear is a elicitator for inhibitory and protective processes. This inhibition can promote [in a cognitive system] to blocking behaviour which can be labelled as ’depression’. Therefore, our interest is how to reimplement biomimetically both action-regulators without the computational system to resulting in a ‘failed’ scenario. We have simulated 1000ms of the dopamine system using NEST Neural Simulation Tool with the rat brain as the model. The results of the simulation experiments are reported with an evaluation to demonstrate the correctness of our hypothesis.}
}
@incollection{VOINOV201733,
title = {Participatory Modeling for Sustainability},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies},
publisher = {Elsevier},
address = {Oxford},
pages = {33-39},
year = {2017},
isbn = {978-0-12-804792-7},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10532-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105329},
author = {Alexey Voinov},
keywords = {Biases, Modeling process, Social media, Stakeholders, Wicked problem},
abstract = {Sustainability is a wicked problem, which is hard to define in a unique way. It cannot be solved and should be treated in a participatory approach involving as many stakeholders in the process as possible. Participatory modeling is an efficient method for dealing with wicked problems. It involves stakeholders in an open-ended process of shared learning and can be essential for developing sustainable technologies. While there may be various levels of participation, the process evolves around a model of the system at stake. The model is built in interaction with the stakeholders; it provides formalism to synchronize stakeholder thinking and knowledge about the system and to move toward consensus about the possible decision making.}
}
@incollection{AKAL202471,
title = {Chapter Four - AI methods in microbial metabolite determination},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {71-85},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000023},
author = {H. Ceren Akal and Rumeysa Nur Kara-Aktaş and Sebnem Ozturkoglu-Budak},
keywords = {Metabolite, Microorganism-derived, Computational, Artificial intelligence},
abstract = {The multitude of microorganism species and the amount of data requiring examination is increasing day by day, which has made it very difficult to make informative determinations and analysis to be conducted by human labour. Artificial intelligence (AI) applications are crucial in mitigating these difficulties. AI is a multidisciplinary field that tries to imitate human-like abilities through learning, analysing, problem-solving and interpretation via digital systems. It can take part in many fields where human labour is required. It is widely used in various scientific disciplines and industries, including biotechnology, microbiology, medicine, etc. Machine learning, a subbranch of AI, is one of the most frequently used auxiliary methods. Critical topics are examined rapidly and meaningfully via machine-learning such as drug production, microbial detection, antimicrobial resistance, vaccine predictions, and disease diagnoses. The aim of this chapter is to highlight the relevance of computational methods for the determination of microbial metabolites which are mainly described in literatures. These computational methods are related with the advanced AI tools of data/genome mining, multivariate data analysis, molecular networking, mathematical modelling, and optimization. These novel methods create new perspectives to the isolation and/or determination of microbial metabolites which are unwanted or essential to human health.}
}
@article{SHIVERSMCNAIR201836,
title = {User-Centered Design In and Beyond the Classroom: Toward an Accountable Practice},
journal = {Computers and Composition},
volume = {49},
pages = {36-47},
year = {2018},
note = {User-Centered Design and Usability in the Composition Classroom},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S8755461518300379},
author = {Ann Shivers-McNair and Joy Phillips and Alyse Campbell and Hanh H. Mai and Alice Yan and John Forrest Macy and James Wenlock and Savannah Fry and Yishan Guan},
keywords = {user-centered design, user experience, usability testing, design thinking},
abstract = {The authors, an instructor and students, describe our practice of user-centered design on three levels: in the design and structure of an advanced undergraduate course in which we all participated, in student projects designed during the course, and in our reflections on the course presented here. We argue that principles of user-centered design can and should be more than course concepts and assignments; they can be core practices of the course that hold both students and teachers accountable for the impacts of their rhetorical choices. We offer a model for other teacher-scholars looking to involve students in the design of their courses and in writing together about their work.}
}
@article{MOEBEHRENS2013e201304003,
title = {THE BIOLOGICAL MICROPROCESSOR, OR HOW TO BUILD A COMPUTER WITH BIOLOGICAL PARTS},
journal = {Computational and Structural Biotechnology Journal},
volume = {7},
number = {8},
pages = {e201304003},
year = {2013},
issn = {2001-0370},
doi = {https://doi.org/10.5936/csbj.201304003},
url = {https://www.sciencedirect.com/science/article/pii/S200103701460026X},
author = {Gerd HG Moe-Behrens},
abstract = {Systemics, a revolutionary paradigm shift in scientific thinking, with applications in systems biology, and synthetic biology, have led to the idea of using silicon computers and their engineering principles as a blueprint for the engineering of a similar machine made from biological parts. Here we describe these building blocks and how they can be assembled to a general purpose computer system, a biological microprocessor. Such a system consists of biological parts building an input / output device, an arithmetic logic unit, a control unit, memory, and wires (busses) to interconnect these components. A biocomputer can be used to monitor and control a biological system.}
}
@article{MEINTZ1994273,
title = {Future directions in computational nursing sciences},
journal = {Mathematical and Computer Modelling},
volume = {19},
number = {6},
pages = {273-288},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90199-6},
url = {https://www.sciencedirect.com/science/article/pii/0895717794901996},
author = {S.L. Meintz and E.A. Yfantis and W.P. Graebel},
keywords = {Computational nursing, Health care data sets, Interdisciplinary, Nurmetrics, Nursing informatics, Nursing science, Supercomputers},
abstract = {The advent of the supercomputer and its capabilities for dealing with terabyte-sized data bases has provided a unique opportunity for nursing sciences to enhance and add to its theories. An interdisciplinary team has formed at the University of Nevada, Las Vegas (UNLV), to provide new tools and methodologies for analyzing large-scale data bases. Their first project is a study of infant mortality. The strategy and goals for this project are presented, along with an assessment of the present state of health care data base analysis.}
}
@article{PADGETT1994185,
title = {Computational intelligence standards: motivation, current activities and progress},
journal = {Computer Standards & Interfaces},
volume = {16},
number = {3},
pages = {185-203},
year = {1994},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(94)90011-6},
url = {https://www.sciencedirect.com/science/article/pii/0920548994900116},
author = {Mary Lou Padgett and Walter J Karplus and Steve Deiss and Robert Shelton},
keywords = {Terminology, Artificial neural networks, Specification, Virtual reality},
abstract = {Computational Intelligence is an emerging technology of keen interest to the developers of computer standards and interfaces. Coherent communications among the diverse set of users of computational AI is necessary for the protection of all parties and can help further the serious development of artificial neural networks, fuzzy systems, evolutionary programming and virtual reality. Current activities of the IEEE Neural Networks Council Standards Committee encompass all these areas, emphasizing the development of glossaries and symbologies, performance measures and interface standards for these interrelated fields. Progress toward these goals is described in this paper.}
}
@article{CORFIELD2011571,
title = {Understanding the infinite II: Coalgebra},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {4},
pages = {571-579},
year = {2011},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S003936811100077X},
author = {David Corfield},
keywords = {Philosophy, Mathematics, Category theory, Coalgebra, Infinite},
abstract = {In this paper we give an account of the rise and development of coalgebraic thinking in mathematics and computer science as an illustration of the way mathematical frameworks may be transformed. Originating in a foundational dispute as to the correct way to characterise sets, logicians and computer scientists came to see maximizing and minimizing extremal axiomatisations as a dual pair, each necessary to represent entities of interest. In particular, many important infinitely large entities can be characterised in terms of such axiomatisations. We consider reasons for the delay in arriving at the coalgebraic framework, despite many unrecognised manifestations occurring years earlier, and discuss an apparent asymmetry in the relationship between algebra and coalgebra.}
}
@article{MILDNER2019763,
title = {Spontaneous Thought as an Unconstrained Memory Process},
journal = {Trends in Neurosciences},
volume = {42},
number = {11},
pages = {763-777},
year = {2019},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166223619301626},
author = {Judith N. Mildner and Diana I. Tamir},
keywords = {spontaneous thought, memory, computational model, mind wandering, default network},
abstract = {The stream of thought can flow freely, without much guidance from attention or cognitive control. What determines what we think about from one moment to the next? Spontaneous thought shares many commonalities with memory processes. We use insights from computational models of memory to explain how the stream of thought flows through the landscape of memory. In this framework of spontaneous thought, semantic memory scaffolds episodic memory to form the content of thought, and drifting context modulated by one's current state – both internal and external – constrains the area of memory to explore. This conceptualization of spontaneous thought can help to answer outstanding questions such as: what is the function of spontaneous thought, and how does the mind select what to think about?}
}
@article{BAICANG2025129857,
title = {Multi-modal Information Fusion for Multi-task End-to-end Behavior Prediction in Autonomous Driving},
journal = {Neurocomputing},
pages = {129857},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129857},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005296},
author = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
keywords = {Autonomous driving, Multi-modal fusion, Vehicle behavior prediction, End-to-End, Attention mechanism},
abstract = {ABSTRACT
Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012rad, RMSE 0.109rad, and MAE 0.074rad, and speed prediction errors of MSE 0.321km/h, RMSE 0.567km/h, and MAE 0.373km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.}
}
@article{KUNZE2024249,
title = {Bioinspired approaches for resource-efficient material flow in production – an innovative actuator concept for peristaltic-based transport},
journal = {Procedia CIRP},
volume = {125},
pages = {249-254},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003974},
author = {Henriette Kunze and Marcel Lorenz},
keywords = {tensegrity, biotensegrity, assembly technologies},
abstract = {In automated material flow, in a wide variety of areas, the primary goal is usually to handle a wide spectrum of components as time- and cost-efficiently as possible. In view of the current and future challenges in industrial production, it is becoming apparent that ecological requirements are becoming increasingly important in automation solutions. For example, in form of resource efficiency, transformability and material efficiency. In this context, especially materials handling technology is subject of various optimization approaches, as no value is added to the part handled. The question: "How does material flow occur in nature?" thus offers biologically inspired approaches to thinking about transport in the industrial sector. This paper first presents a selection of concepts or existing mechanisms that are adaptable in materials- handling technology and have been developed based on a biological model. In the second part of this paper, a new concept is presented that is modeled on peristalsis as a transport mechanism. The approach presented here uses tensegrity-structures for assembly, which are characterized by their high material efficiency and flexibility. The transport movement is achieved by peristaltic typical contraction or relaxation of the respective structure parts.}
}
@article{YUZGEC2025113169,
title = {Accelerated opposition learning based chaotic single candidate optimization algorithm: A new alternative to population-based heuristics},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113169},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113169},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002163},
author = {Ugur Yuzgec},
keywords = {Opposition learning, Chaotic, Single candidate, Engineering design, Benchmark},
abstract = {This study considers the Single Candidate Optimizer (SCO) as an alternative to population-based heuristics, that is faster than them. Although the SCO algorithm is a fast single-candidate-based heuristic, it has certain limitations. To overcome these limitations and enhance the search performance of SCO, several solutions were proposed in this study. First, owing to the single-candidate nature of the SCO, the initial solution position can play a critical role. To compensate for this, an accelerated opposition-learning mechanism was integrated into the SCO. In addition, instead of the equation that is active when the number of unsuccessful improvement attempts is reached in the SCO structure, a mutation operator including chaotic functions (Levy, Gauss, and Cauchy) has been incorporated into the algorithm. Again, equations based on new approaches were added to the SCO algorithm to update the position of the candidate solution during the exploration and exploitation phases. Finally, the standard boundary value control mechanism is replaced with a more effective one. The algorithm developed in this study is named Accelerated Opposition Learning based Chaotic Single Candidate Optimizer (AccOppCSCO), inspired by the accelerated opposition learning mechanism and the mutation operator involving chaotic behaviors. The search capability of the proposed AccOppCSCO algorithm was first analyzed using four different methods: convergence, search history, trajectory, and computational complexity. The effectiveness of the mechanisms used in the AccOppCSCO algorithm for four different two-dimensional benchmark problems from the IEEE Congress on Evolutionary Computation 2014 (CEC2014) package was demonstrated. Subsequently, the performance of the proposed AccOppCSCO algorithm was evaluated on the CEC2014 and IEEE Congress on Evolutionary Computation 2020 (CEC2020) benchmark problems with different dimensions. The results show that the AccOppCSCO algorithm works effectively in the CEC2014 and CEC2020 test sets and offers better optimization results than SCO. The AccOppCSCO algorithm ranked first in the overall evaluation of the 30-dimensional CEC2014 comparison results with State of the Art (SOTA) heuristics from the literature. Finally, for ten different engineering design problems, the AccOppCSCO algorithm was analyzed and compared with the original SCO and other SOTA heuristics. The results show that AccOppCSCO is effective for engineering design problems. This emphasizes that the algorithm can work effectively on a wide range of problems and can be used in various applications. The source code of the AccOppCSCO algorithm for the CEC2014 benchmark suite is publicly available at https://github.com/uguryuzgec/AccOppCSCO.}
}
@article{BESHKOV2024109370,
title = {Topological structure of population activity in mouse visual cortex encodes densely sampled stimulus rotations},
journal = {iScience},
volume = {27},
number = {4},
pages = {109370},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109370},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005911},
author = {Kosio Beshkov and Marianne Fyhn and Torkel Hafting and Gaute T. Einevoll},
keywords = {Neuroscience, Sensory neuroscience, Cognitive neuroscience},
abstract = {Summary
The primary visual cortex is one of the most well understood regions supporting the processing involved in sensory computation. Following the popularization of high-density neural recordings, it has been observed that the activity of large neural populations is often constrained to low dimensional manifolds. In this work, we quantify the structure of such neural manifolds in the visual cortex. We do this by analyzing publicly available two-photon optical recordings of mouse primary visual cortex in response to visual stimuli with a densely sampled rotation angle. Using a geodesic metric along with persistent homology, we discover that population activity in response to such stimuli generates a circular manifold, encoding the angle of rotation. Furthermore, we observe that this circular manifold is expressed differently in subpopulations of neurons with differing orientation and direction selectivity. Finally, we discuss some of the obstacles to reliably retrieving the truthful topology generated by a neural population.}
}
@article{SPENCE2022100433,
title = {Gastrophysics: Getting creative with pairing flavours},
journal = {International Journal of Gastronomy and Food Science},
volume = {27},
pages = {100433},
year = {2022},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2021.100433},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X21001323},
author = {Charles Spence},
keywords = {Food pairing, Flavour pairing hypothesis, Sonic seasoning, Computational gastronomy, Data engineering, Gastrophysics},
abstract = {Traditionally, in the West, the decision about which flavours to pair in a tasting experience has been as much the personal choice of the chef or, more likely, the sommelier, as anything else. However, the last couple of decades have seen a rapid growth of research interest in the pairing of flavours. Nowadays, one can find examples of people pairing everything from beer with food, tea with cheese and chocolate, etc. As interest in the marketing potential of flavour pairing has risen, along with the growing public fascination in the topic, scientists have become increasingly interested in trying to understand the principles (both cognitive/intellectual and perceptual) underlying the successful pairing of flavours. In this narrative review, the relative strengths and weaknesses of the chemical, computational (gastronomy), and perceptual approaches to pairing flavours are highlighted. Thereafter, I show how the various principles of pairing (both perceptual and cognitive/intellectual) can be extended beyond the domain of pairing flavour with flavour to consider the rapidly growing are of sonic seasoning. The latter term refers to those situations in which specific pieces of music or soundscapes are matched, or paired, with particular tastes/flavours based on the crossmodal correspondences. The review ends by considering the future development of pairings flavours, and assessing novel means of establishing connections between flavours and other sensations.}
}
@article{BOSCH2017,
title = {Graduate Biomedical Science Education Needs a New Philosophy},
journal = {mBio},
volume = {8},
number = {6},
year = {2017},
issn = {2150-7511},
doi = {https://doi.org/10.1128/mbio.01539-17},
url = {https://www.sciencedirect.com/science/article/pii/S2161212917003111},
author = {Gundula Bosch and Arturo Casadevall},
keywords = {Ph.D., education, graduate},
abstract = {ABSTRACT
There is a growing realization that graduate education in the biomedical sciences is successful at teaching students how to conduct research but falls short in preparing them for a diverse job market, communicating with the public, and remaining versatile scientists throughout their careers. Major problems with graduate level education today include overspecialization in a narrow area of science without a proper grounding in essential critical thinking skills. Shortcomings in education may also contribute to some of the problems of the biomedical sciences, such as poor reproducibility, shoddy literature, and the rise in retracted publications. The challenge is to modify graduate programs such that they continue to generate individuals capable of conducting deep research while at the same time producing more broadly trained scientists without lengthening the time to a degree. Here we describe our first experiences at Johns Hopkins and propose a manifesto for reforming graduate science education.}
}
@article{XU2025101288,
title = {Multi-criteria feature selection on maritime emission abatement alternatives},
journal = {Research in Transportation Business & Management},
volume = {59},
pages = {101288},
year = {2025},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2025.101288},
url = {https://www.sciencedirect.com/science/article/pii/S2210539525000033},
author = {Kaiqi Xu and Mario P. Brito and Patrick Beullens},
keywords = {Analytic hierarchy process, Multi-criteria decision making, Emission reduction, Technology selection, Sustainability port},
abstract = {To comply with MARPOL Annex VI, stakeholders face multi-criteria decision-making in technology selection. This study provides an Analytic Hierarchy Process (AHP)-based method to support stakeholders in selecting emission abatement technology aligned with their business demands, taking into account a range of sustainability criteria. The analysis reveals that there is no one-size-fits-all solution to technology selection. Low-sulfur fuel oil and LNG are preferable alternative fuels for large-size commercial (long-sea shipping) vessels due to their better capacity storage savings, while a dual-fuel engine offers flexibility in fuel changeover. Electrification offers zero-emission performance, lower noise levels, and peak energy solutions benefiting cruise ships and short-distance or harbor boats, but tugboats need greener diesel to meet performance criteria. From a policy perspective, our model provides insights into the effects of green transition processes in Norway and Singapore on stakeholders' decisions with respect to port infrastructure and land transport at the portside.}
}
@article{MIDGLEY2019181,
title = {Anticipatory practice and the making of surplus food},
journal = {Geoforum},
volume = {99},
pages = {181-189},
year = {2019},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518302720},
author = {Jane L. Midgley},
keywords = {Surplus food, Anticipation, Market devices, Redistribution, United Kingdom},
abstract = {This paper explores the practices that have evolved between a global food retailer and a leading charitable surplus food redistributor to enable the utilization of surplus food in community and charitable meal settings in the UK. I argue that to understand surplus food and its potential futures (consumed or wasted), closer engagement with anticipatory thinking is needed. Drawing on interview data with key stakeholders and observations of the food industry redistribution process the paper explores the anticipatory actions taken by different actors as they attempt to manage the possible futures of foods that become categorized as surplus. The paper shows how different market devices are used to manage market concerns about surplus food and work to assure its future consumption. The devices focus on managing the risks of the food becoming unsafe and the associated legal liabilities. The market concerns, as expressions of anticipatory thinking, inform a series of anticipatory practices throughout the redistribution process to enable all actors, and especially the Retailer, to trust in the process. The paper concludes by noting how reliant the redistribution process is on anticipatory practices, especially pre-emption and improvisation to make the process workable, but also how these work to contain the various concerns within market arrangements. The paper highlights the importance of anticipation as a theoretical basis for exploring surplus food and the concept of surplus more widely.}
}
@article{DELLANNA2022105064,
title = {Evolving Fuzzy logic Systems for creative personalized Socially Assistive Robots},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105064},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105064},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622002251},
author = {Davide Dell’Anna and Anahita Jamshidnejad},
keywords = {Evolving Fuzzy logic Systems, Personalized Socially Assistive Robots, Robot creativity},
abstract = {Socially Assistive Robots (SARs) are increasingly used in dementia and elderly care. In order to provide effective assistance, SARs need to be personalized to individual patients and account for stimulating their divergent thinking in creative ways. Rule-based fuzzy logic systems provide effective methods for automated decision-making of SARs. However, expanding and modifying the rules of fuzzy logic systems to account for the evolving needs, preferences, and medical conditions of patients can be tedious and costly. In this paper, we introduce EFS4SAR, a novel Evolving Fuzzy logic System for Socially Assistive Robots that supports autonomous evolution of the fuzzy rules that steer the behavior of the SAR. EFS4SAR combines traditional rule-based fuzzy logic systems with evolutionary algorithms, which model the process of evolution in nature and have shown to result in creative behaviors. We evaluate EFS4SAR via computer simulations on both synthetic and real-world data. The results show that the fuzzy rules evolved over time are not only personalized with respect to the personal preferences and therapeutic needs of the patients, but they also meet the following criteria for creativity of SARs: originality and effectiveness of the therapeutic tasks proposed to the patients. Compared to existing evolving fuzzy systems, EFS4SAR achieves similar effectiveness with higher degree of originality.}
}
@article{HOSSEINI2019186,
title = {A morphological approach for kinetic façade design process to improve visual and thermal comfort: Review},
journal = {Building and Environment},
volume = {153},
pages = {186-204},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.02.040},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319301416},
author = {Seyed Morteza Hosseini and Masi Mohammadi and Alexander Rosemann and Torsten Schröder and Jos Lichtenberg},
keywords = {Kinetic façade, Biomimicry, Morphological approach, Comfort condition, Parametric design thinking},
abstract = {Visual and thermal comfort for occupants significantly depend on exterior environmental climatic conditions, which are continuously changing. In particular, optimizing visual and thermal comfort simultaneously is a difficult topic due to mutual conflicts between them. This literature review article studies the façade, as a complex interface between inside of buildings and the outside that has a capability to function as a protective or regulatory element against severe fluctuations of external climate. Six interrelated subjects are studied including kinetic façade, biomimicry, building form as a microclimate modifier, energy efficiency, comfort condition, parametric design thinking. The literature review process answers following research questions: (1) what are the interdisciplinary subjects corresponding to kinetic façade design process for creating an innovative architectural process? (2) What is the most important factor in kinetic façade design with the aim to improve occupants’ visual and thermal comfort simultaneously based on multidisciplinary investigation? Many research has been carried out about kinetic façade concepts strategies, principles, and criteria. However, interdisciplinary studies for proposing kinetic façade form is relatively rare. Also, adaptive daylight façade with daily solar geometry variation has been highly required. Therefore, generative-parametric and quick form finding method for responding to different climates would be a solution for providing more adaptability to dynamic daylight. This study aims to propose a kinetic façade design process which have capability to improve occupant visual & thermal comfort simultaneously by controlling on-site renewable energy resources consist of solar radiation and wind. Façade as an only interface between inside and outside of building, far from the literal and historical perceptions, is recognized by intrinsic functional attributes including complexity, heterogeneity and multidisciplinary. Moreover, the interrelated subjects impact façade form individually and aggregately regard to functional scenario that is changed the perception of kinetic façade from elegant and fashionable state to a functional and practical element.}
}
@article{MONRO199293,
title = {Parallel computation of ECG fields},
journal = {Journal of Electrocardiology},
volume = {25},
pages = {93-100},
year = {1992},
note = {Research and Applications in Computerized Electrocardiology},
issn = {0022-0736},
doi = {https://doi.org/10.1016/0022-0736(92)90068-B},
url = {https://www.sciencedirect.com/science/article/pii/002207369290068B},
author = {D.M. Monro and D.M. Budgett},
keywords = {electrocardiogram, modeling, forward solution, magnetic resonance imaging, inverse solution, parallel computers},
abstract = {A parallel implementation of a finite difference model for computing the electric field of cardiac sources is presented. On a relatively inexpensive SIMD parallel computer, a full-forward solution is obtained in minutes, using accurate thoracic detail including anisotropy if required. Because the computation is based on a volume grid with constant size voxels, it readily accepts anatomical data from classified magnetic resonance imaging scans. By using a variation of the colored successive over-relaxation iteration, our finite difference model takes full advantage of the performance of massively parallel computers. Evaluations of the accuracy and performance of the model show the practicality of using specific anatomical models to recover the electrocardiographic field distributions for individual subjects. A relatively modest parallel machine is capable of assembling and computing a specific direct inverse solution from body surface potentials within an hour of measurement, assuming the magnetic resonance imaging classification has been previously completed.}
}
@article{DOOLITTLE2019889,
title = {Making Evolutionary Sense of Gaia},
journal = {Trends in Ecology & Evolution},
volume = {34},
number = {10},
pages = {889-894},
year = {2019},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534719301417},
author = {W. Ford Doolittle},
keywords = {Gaia hypothesis, evolution, differential persistence, clade selection},
abstract = {The Gaia hypothesis in a strong and frequently criticized form assumes that global homeostatic mechanisms have evolved by natural selection favoring the maintenance of conditions suitable for life. Traditional neoDarwinists hold this to be impossible in theory. But the hypothesis does make sense if one treats the clade that comprises the biological component of Gaia as an individual and allows differential persistence – as well as differential reproduction – to be an outcome of evolution by natural selection. Recent developments in theoretical and experimental evolutionary biology may justify both maneuvers.}
}
@article{EARL2019303,
title = {Elusive optima: A process tracing analysis of procedural rationality in mobile phone connection plan choices},
journal = {Journal of Economic Behavior & Organization},
volume = {161},
pages = {303-322},
year = {2019},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167268119300988},
author = {Peter E. Earl and Lana Friesen and Christopher Shadforth},
keywords = {Consumer capabilities, Choice overload, Procedural rationality, Process tracing},
abstract = {This paper reports an experiment in which subjects were rewarded on the basis of how close they came to finding the cheapest mobile phone plan to serve a particular usage remit by searching freely in the Internet. During the task, subjects were required to ‘think aloud’ and recordings were made of what they said and what they did on their computer screens. Analysis of the screen-capture movie recordings revealed major shortfalls in procedural rationality, including poor strategic thinking about how to deal with choice overload, poor conceptual understanding of mobile phone plans and pricing systems, as well as cognitive and calculation errors. Our novel method leads to a very different policy focus from that implied by viewing the problem in terms of excess information per se and irrationality as driven by innate heuristics and biases.}
}
@article{LI2022126546,
title = {Dynamic forecasting performance and liquidity evaluation of financial market by Econophysics and Bayesian methods},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {588},
pages = {126546},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126546},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121008190},
author = {Jiang-Cheng Li and Chen Tao and Hai-Feng Li},
keywords = {Econophysics, Agent-based model, Liquidity risk assessment, Machine learning thinking, Microcosmic evolution models},
abstract = {In a complex financial system, what is the forecasting performance of macro and micro evolution models of Econophysics on asset prices? For this problem, from the perspective of machine learning, we study the dynamic forecasting and liquidity assessment of financial markets, based on econophysics and Bayesian methods. We establish eight dynamic prediction methods, based on our proposed likelihood estimation and Bayesian estimation methods of macro and micro evolution models of econophysics. Combined machine learning thinking and real data, we empirically study and simulate the out-of-sample dynamic forecasting analysis of eight proposed methods and compare with the benchmark GARCH model. A variety of loss functions, superior predictive ability test (SPA), Akaike and Bayesian information criterion (AIC and BIC) methods are introduced to further evaluate the forecasting performance of our proposed methods. The research of out of sample prediction shows that (1) the method of the simplified stochastic model with Bayesian method for only sample return has the best forecasting performance; (2) the method of the stochastic model with Bayesian method for only return samples has the worst forecasting performance. For the liquidity assessment problem, there is a strong correlation between the trading probability evaluated by the proposed eight methods and the real turnover rate, and an increase of liquidity is correspond to the increase of asset risk. In other words, it suggests that all proposed methods can well evaluate market liquidity.}
}
@article{WHITE200337,
title = {Promoting productive mathematical classroom discourse with diverse students},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {1},
pages = {37-53},
year = {2003},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(03)00003-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312303000038},
author = {Dorothy Y. White},
keywords = {Classroom discourse, Questioning techniques, Equity/diversity, Elementary mathematics teaching},
abstract = {Productive mathematical classroom discourse allows students to concentrate on sense making and reasoning; it allows teachers to reflect on students’ understanding and to stimulate mathematical thinking. The focus of the paper is to describe, through classroom vignettes of two teachers, the importance of including all students in classroom discourse and its influence on students’ mathematical thinking. Each classroom vignette illustrates one of four themes that emerged from the classroom discourse: (a) valuing students’ ideas, (b) exploring students’ answers, (c) incorporating students’ background knowledge, and (d) encouraging student-to-student communication. Recommendations for further research on classroom discourse in diverse settings are offered.}
}
@article{HUANG201724,
title = {Energy and carbon performance evaluation for buildings and urban precincts: review and a new modelling concept},
journal = {Journal of Cleaner Production},
volume = {163},
pages = {24-35},
year = {2017},
note = {Achieving Low/no Fossil-carbon Economies based upon the Essential Transformations to Support them},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615018235},
author = {Bin Huang and Ke Xing and Stephen Pullen},
keywords = {Buildings, Integrated modelling, Life cycle energy, Systems thinking, Urban precincts},
abstract = {With the accelerating pace of urbanisation around the world, the planning, development and operation of buildings and precincts have become increasingly important with respect to energy use and the associated carbon footprint of the modern built environment. Over recent decades, much effort, both in research and in practice, has been devoted to building construction and urban planning for the improvement of energy efficiency and greenhouse gas emissions. However, the accuracy of modelling and evaluation of energy and carbon performance for buildings and urban precincts remains limited, affected by inadequate energy intensity data and highly integrated building systems, as well as the complex interactions between buildings and the urban eco-system. This paper presents a critical review of current measures and models for representing and assessing life cycle energy as well as associated emissions profiles at both the building and the precinct levels. It also identifies influential factors and explores interactions among buildings, surrounding environment and user behaviours at the urban precinct level by taking a systems perspective. Based on such a review, this study maps out some key challenges for integrating energy and carbon metrics, and finally proposes a precinct-level system boundary definition and an integrated model following systems thinking. The proposed model can facilitate a critical thinking approach about the evaluations of global energy and emissions, and support the quantification of energy consumption and associated emissions for building precinct systems.}
}
@article{MOSTERT202448,
title = {The Shortfalls of Mental Health Compartment Models: A Call to Improve Mental Health Investment Cases in Developing Countries},
journal = {Value in Health Regional Issues},
volume = {41},
pages = {48-53},
year = {2024},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2212109923001449},
author = {Cyprian M. Mostert and Andrew Aballa and Linda Khakali and Willie Njoroge and Jasmit Shah and Samim Hasham and Zul Merali and Lukoye Atwoli},
keywords = {developing countries, investment cases, mental health compartment model},
abstract = {Objectives
There are irregularities in investment cases generated by the Mental Health Compartment Model. We discuss these irregularities and highlight the costing techniques that may be introduced to improve mental health investment cases.
Methods
This analysis uses data from the World Bank, the World Health Organization Mental Health Compartment Model, the United Nations Development Program, the Kenya Ministry of Health, and Statistics from the Kenyan National Commission of Human Rights.
Results
We demonstrate that the Mental Health Compartment Model produces irrelevant outcomes that are not helpful for clinical settings. The model inflated the productivity gains generated from mental health investment. In some cases, the model underestimated the economic costs of mental health. Such limitation renders the investment cases poor in providing valuable intervention points from the perspectives of both the users and the providers.
Conclusions
There is a need for further calibration and validation of the investment case outcomes. The current estimated results cannot be used to guide service provision, research, and mental health programming comprehensively.}
}
@incollection{HLAVAEEK20041,
title = {Chapter I - Reality, Mathematics, and Computation},
editor = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška},
series = {North-Holland Series in Applied Mathematics and Mechanics},
publisher = {North-Holland},
volume = {46},
pages = {1-49},
year = {2004},
booktitle = {Uncertain Input Data Problems and the Worst Scenario Method},
issn = {0167-5931},
doi = {https://doi.org/10.1016/S0167-5931(04)80005-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167593104800056},
author = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@article{CHAUHAN2023107757,
title = {Personalized optimal room temperature and illuminance for maximizing occupant's mental task performance using physiological data},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107757},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107757},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301937X},
author = {Hardik Chauhan and Youjin Jang and Surakshya Pradhan and Hyosoo Moon},
keywords = {Indoor environment quality, Physiological response, Occupant performance, Machine learning, Particle swarm optimization},
abstract = {Indoor room temperature and illuminance level are critical factors of indoor environment quality (IEQ), affecting human mental task performance. These effects are reflected in their physiological responses such as heart rate, electrodermal activity, and skin temperature. Occupants' individual preferences, sensitivity, and physiological responses to different combinations of room temperature and illuminance level can differ among individuals. Despite previous studies investigating the individual and combined effects of different IEQ parameters, the limited research on the cross-modal relationship between room temperature and illuminance level and its impact on mental task performance highlights its significance. Moreover, to achieve personalized insights, it is essential to incorporate individual physiological responses, and this necessitates the development of an optimization model to comprehensively examine their impact. To address these issues, this study proposes a personalized model that optimizes room temperature and illuminance levels to enhance mental task performance using occupants' physiological data. Having the random forest algorithm, this study first predicted mental task performance, which includes four mental abilities such as attention, perception, working memory, and thinking ability using the occupant's physiological data. Then, the particle swarm optimization algorithm was employed to optimize room temperature and illuminance level to maximize the predicted mental task performance. The results of the proposed model align with observed values of room temperature and illuminance level during experiments, validating the adoption of a personalized approach. The findings contribute to future insights and guidelines for the design and management of indoor environments to maximize occupants' performance.}
}
@incollection{LEACH202221,
title = {Chapter 2 - AI and the limits of human creativity in urban planning and design},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {21-37},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000135},
author = {Neil Leach},
keywords = {AlphaGo, AI, Strategy, Urban planning, Creativity, Perception},
abstract = {What can architects learn from AlphaGo? This chapter explores the lessons to be learnt from the famous match where AlphaGo, a machine-learning system developed by DeepMind, beat leading Korean Go player, Lee Sedol. It explores the ramifications of this victory on a series of different levels, from the global impact of the match on research into AI to the impact on Xkool Technologies and Spacemaker AI, two architectural start-ups developing AI systems for architecture and urban planning. It makes a particular comparison between the operations of AlphaGo and the strategic thinking of urban planning, arguing that AI now puts the future of urban planners—and possibly also architects—at risk. It then goes on to appraise the famous Move 37 made by AlphaGo in Game 2 of this match. It argues that, despite appearances, this move was not actually creative. Finally, it explores how we might view human creativity in the light of comments made about AlphaGo. The chapter concludes by speculating whether the ultimate lesson of AlphaGo is that creativity is simply a question of “perceived creativity.”}
}
@incollection{STAUFFER2006i,
title = {Biology, Sociology, Geology by Computational Physicists},
editor = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins},
series = {Monograph Series on Nonlinear Science and Complexity},
publisher = {Elsevier},
volume = {1},
pages = {i-276},
year = {2006},
booktitle = {Biology, Sociology, Geology by Computational Physicists},
issn = {1574-6917},
doi = {https://doi.org/10.1016/S1574-6917(05)01001-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574691705010019},
author = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins}
}
@article{PARR2025105984,
title = {Inferring when to move},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {169},
pages = {105984},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105984},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004536},
author = {Thomas Parr and Ashwini Oswal and Sanjay G. Manohar},
keywords = {Computational neuroscience, Generative, Bayesian, Active inference, Movement, Dynamical systems, Parkinson’s disease},
abstract = {Most of our movement consists of sequences of discrete actions at regular intervals—including speech, walking, playing music, or even chewing. Despite this, few models of the motor system address how the brain determines the interval at which to trigger actions. This paper offers a theoretical analysis of the problem of timing movements. We consider a scenario in which we must align an alternating movement with a regular external (auditory) stimulus. We assume that our brains employ generative world models that include internal clocks of various speeds. These allow us to associate a temporally regular sensory input with an internal clock, and actions with parts of that clock cycle. We treat this as process of inferring which clock best explains sensory input. This offers a way in which temporally discrete choices might emerge from a continuous process. This is not straightforward, particularly if each of those choices unfolds during a time that has a (possibly unknown) duration. We develop a route for translation to neurology, in the context of Parkinson’s disease—a disorder that characteristically slows down movements. The effects are often elicited in clinic by alternating movements. We find that it is possible to reproduce behavioural and electrophysiological features associated with parkinsonism by disrupting specific parameters—that determine the priors for inferences made by the brain. We observe three core features of Parkinson’s disease: amplitude decrement, festination, and breakdown of repetitive movements. Our simulations provide a mechanistic interpretation of how pathology and therapeutics might influence behaviour and neural activity.}
}
@article{NARIMANI202441,
title = {Intelligent Control for Aerospace Engineers: A Novel Educational Framework},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {16},
pages = {41-46},
year = {2024},
note = {2nd IFAC Workshop on Aerospace Control Education - WACE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.459},
url = {https://www.sciencedirect.com/science/article/pii/S240589632401228X},
author = {Mohammad Narimani and Seyyed Ali Emami and Paolo Castaldi},
keywords = {Aerospace control education, Intelligent control systems, Neural networks, Reinforcement learning, Model-based control, Adaptive control, Expert systems},
abstract = {The integration of intelligent control techniques into aerospace engineering education remains a challenge. This paper presents a novel approach for teaching intelligent control specifically designed for aerospace engineers, bridging the gap between theoretical foundations and practical applications. The proposed framework encompasses a comprehensive curriculum covering model-based and model-free approaches, leveraging neural networks, reinforcement learning, and other computational intelligence techniques. It emphasizes hands-on experiences through simulation-based exercises, hardware-in-the-loop experiments, and design projects tailored to different aerospace vehicle categories, including multi-rotor UAVs, helicopters, fixed-wing aircraft, and Hypersonic Flight Vehicles. The framework also addresses assessment methods, industry collaborations, and case studies to enhance student learning outcomes.}
}
@incollection{NIE2019205,
title = {Two-Stage Land Use Optimization for A Food-Energy-Water Nexus System: A Case Study In Texas Edwards Region},
editor = {Salvador Garcia Muñoz and Carl D. Laird and Matthew J. Realff},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {47},
pages = {205-210},
year = {2019},
booktitle = {Proceedings of the 9th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818597-1.50033-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185971500333},
author = {Yaling Nie and Styliani Avraamidou and Xin Xiao and Efstratios N. Pistikopoulos and Jie Li},
keywords = {Land use optimization, Food-Energy-Water Nexus, multi-period planning},
abstract = {Efficient land use planning and scheduling in Food-Energy-Water Nexus (FEW-N) related systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on FEW-N is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, multi-scale and multi-period problems, and multiple stakeholders. To address these challenges, we developed a generic optimization-based land allocation approach, which provides i) a composite FEW-N metric to help solve the multi-objective optimization problem and carry out assessments, and ii) a two-stage decomposition strategy to solve the multi-scale and multi-period planning and scheduling problem. The developed strategy was applied in a case study within the Texas Edwards Region. Computational results indicate that the approach can provide a comprehensive FEW-N metric to select strategies for optimal land allocation and limit stresses in the FEW-N, and achieve trade-off solutions for the multi-scale and multi-period FEW land use systems.}
}
@incollection{FAVERO2023509,
title = {Chapter 25 - Raster objects},
editor = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
booktitle = {Data Science, Analytics and Machine Learning with R},
publisher = {Academic Press},
pages = {509-519},
year = {2023},
isbn = {978-0-12-824271-1},
doi = {https://doi.org/10.1016/B978-0-12-824271-1.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128242711000111},
author = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
keywords = {Spatial analysis, Maps, Raster objects, R},
abstract = {At the end of this chapter, you will be able to:•Understand what a raster object is;•Load and use raster objects;•Combine raster objects with shapefiles;•Manipulate and cut out raster objects;•Use the raster objects in such a way as to demand less computational time for the execution of tasks;•View raster objects in R language.}
}
@article{KHAN2021104263,
title = {A novel hybrid gravitational search particle swarm optimization algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104263},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104263},
url = {https://www.sciencedirect.com/science/article/pii/S095219762100110X},
author = {Talha Ali Khan and Sai Ho Ling},
keywords = {PSO, GSA, Hybrid, DNA computation},
abstract = {Particle Swarm Optimization (PSO) algorithm is a member of the swarm computational family and widely used for solving nonlinear optimization problems. But, it tends to suffer from premature stagnation, trapped in the local minimum and loses exploration capability as the iteration progresses. On the contrary, Gravitational Search Algorithm (GSA) is proficient for searching global optimum, however, its drawback is its slow searching speed in the final phase. To overcome these problems in this paper a novel Hybrid Gravitational Search Particle Swarm Optimization Algorithm (HGSPSO) is presented. The key concept behind the proposed method is to merge the local search ability of GSA with the capability for social thinking (gbest) of PSO. To examine the effectiveness of these methods in solving the abovementioned issues of slow convergence rate and trapping in local minima five standard and some modern CEC benchmark functions are used to ensure the efficacy of the presented method. Additionally, a DNA sequence problem is also solved to confirm the proficiency of the proposed method. Different parameters such as Hairpin, Continuity, H-measure, and Similarity are employed as objective functions. A hierarchal approach was used to solve this multi-objective problem where a single objective function is first obtained through a weighted sum method and the results were then empirically validated. The proposed algorithm has demonstrated an extraordinary performance per solution stability and convergence.}
}
@article{DAS2023100065,
title = {Informatics on a social view and need of ethical interventions for wellbeing via interference of artificial intelligence},
journal = {Telematics and Informatics Reports},
volume = {11},
pages = {100065},
year = {2023},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2772503023000257},
author = {Kabita Das and Manaswini Pattanaik and Smitimayee Basantia and Radhashyam Mishra and Debashreemayee Das and Kanhucharan Sahoo and Biswaranjan Paital},
keywords = {Artificial intelligence, Ethical enquiry, Ethics in technology, Human conduct, Moral value, Social cognition, Human intelligence},
abstract = {The main focus of this paper was to discuss and appraise the attribution of intelligence and value judgement on Artificial Intelligence (AI) and its regulated use in society. Humans are tool-making creatures and AI is used for civilization via tools. During the time of pre-civilization, tools were simple in the form of crude construction, using hand skills but at present, the achievements are the substitution of machinery to relieve/replace human intellect. AI is the scientific technique of bringing learning, adaptation, and self-organization of machines. It encompasses various concepts and methods, deployed by researchers in many diverse fields of computation and cognition. This is the computational mode of a brain, based on artificial neural networks. The usefulness of AI ethically, initiates a big question i.e. if the human mind is not self-sufficient for any work without harming the moral sentiment of others then, how can people believe in a computational model of the mind, is a machine, morally responsible for any good or bad action. We highlight issues on the use of AI in the replacement of the human mind asking what is the value of humans in this age of AI? Can AI reciprocate and respect human values better than human beings? Can AI replace human intelligence? In the case of ethical enquiry, it is rather a herculean task to consider a machine's action to be moral or immoral, after all, it is just a machinery action devoid of any moral quality.}
}
@article{URECH2022101731,
title = {A simulation-based design framework to iteratively analyze and shape urban landscapes using point cloud modeling},
journal = {Computers, Environment and Urban Systems},
volume = {91},
pages = {101731},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101731},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001381},
author = {Philipp R.W. Urech and Muhammad Omer Mughal and Carlos Bartesaghi-Koc},
keywords = {Point-cloud modeling, Computational fluid dynamics, Laser-scanned data, Urban landscape design, Design performance},
abstract = {The topic of this paper evolves on the discourse of digital modeling in landscape design. Current design methods stagger to address physical forms and dynamics present in the environment. This status quo limits possibilities to integrate scientific evidence when developing spatial and aesthetic configurations in urban landscapes. Remote sensing technology such as laser scanning measures physical forms to reproduce them as geo-specific digital 3D models, while dynamic simulation is widely used to predict how scenarios will perform under given conditions. However, there is still a need for a holistic design process that is capable of integrating both the measured physical forms and physical dynamics. This paper presents a novel framework using point cloud modeling to shape design scenarios that are iteratively evaluated for their performance. The proposed framework is demonstrated through a case study in Singapore. New spatial configurations are tested for the site through an iterative and comparative analysis of the design performance. The case study exposes (1) a site-specific design approach by iteratively modeling a laser-scanned point cloud model, (2) a workflow to convert the geometric data from the point cloud models into voxels and meshes, (3) an integration of computational fluid dynamics (CFD) simulation during design development as per-point attributes, and (4) a comparison of the configurations to identify best performing scenarios. This design framework can support city managers, planners, urban and landscape designers to better inform their decision-making process by relying on accurate scientific feedback. By guiding the design process with the consideration of the built environment as a complex adaptive system, it will be possible to improve how open spaces and ecosystem services perform in cities, and to design landscapes that can mitigate dynamic events such as urban heat islands.}
}
@article{KROGER2013189,
title = {An ERP study of passive creative conceptual expansion using a modified alternate uses task},
journal = {Brain Research},
volume = {1527},
pages = {189-198},
year = {2013},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0006899313009566},
author = {Sören Kröger and Barbara Rutter and Holger Hill and Sabine Windmann and Christiane Hermann and Anna Abraham},
keywords = {Creativity, ERP, N400, Conceptual expansion, Alternate uses task, Divergent thinking, Semantic cognition},
abstract = {A novel ERP paradigm was employed to investigate conceptual expansion, a central component of creative thinking. Participants were presented with word pairs, consisting of everyday objects and uses for these objects, which had to be judged based on the two defining criteria of creative products: unusualness and appropriateness. Three subject-determined trial types resulted from this judgement: high unusual and low appropriate (nonsensical uses), low unusual and high appropriate (common uses), and high unusual and high appropriate (creative uses). Word pairs of the creative uses type are held to passively induce conceptual expansion. The N400 component was not specifically modulated by conceptual expansion but was, instead, generally responsive as a function of unusualness or novelty of the stimuli (nonsense=creative>common). Explorative analyses in a later time window (500–900ms) revealed that ERP activity in this phase indexes appropriateness (nonsense>creative=common). In the discussion of these findings with reference to the literature on semantic cognition, both components are proposed as indexing processes relevant to conceptual expansion as they are selectively involved in the encoding and integration of a newly established semantic connection between two previously unrelated concepts.}
}
@article{WHITACRE2020100816,
title = {The roles of tools and models in a prospective elementary teachers’ developing understanding of multidigit multiplication},
journal = {The Journal of Mathematical Behavior},
volume = {60},
pages = {100816},
year = {2020},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100816},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320300808},
author = {Ian Whitacre and Chepina Rumsey},
keywords = {Prospective teachers, Mental computation, Multiplication, Tools, Models},
abstract = {It is important for prospective elementary teachers to understand multidigit multiplication deeply; however, the development of such understanding presents challenges. We document the development of a prospective elementary teacher’s reasoning about multidigit multiplication during a Number and Operations course. We present evidence of profound progress in Valerie’s understanding of multidigit multiplication, and we highlight the roles of particular tools and models in her developing reasoning. In this way, we contribute an illuminating case study that can inform the work of mathematics teacher educators. We discuss specific instructional implications that derive from this case.}
}
@article{HASELI2023184,
title = {HECON: Weight assessment of the product loyalty criteria considering the customer decision's halo effect using the convolutional neural networks},
journal = {Information Sciences},
volume = {623},
pages = {184-205},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015201},
author = {Gholamreza Haseli and Ramin Ranjbarzadeh and Mostafa Hajiaghaei-Keshteli and Saeid {Jafarzadeh Ghoushchi} and Aliakbar Hasani and Muhammet Deveci and Weiping Ding},
keywords = {Customer loyalty, Deep learning, Convolutional neural networks, Multi-criteria decision-making, Halo effect},
abstract = {The economic pressures and increasing competition in markets have led to the CEOs of companies being forced to make the right strategic decisions in the development of products for selling the right products to the right customers. To achieve this goal, companies need to know which criteria of their products lead to customer loyalty to that product. In the past, various methods have been introduced to obtain the importance (weight) of criteria that use the opinions of experts or customers. There is a halo effect in human decisions that leads to biases in evaluating the criteria by influencing human emotions. This study introduces a new method for weight assessment of the product loyalty criteria by considering the customer's decisions halo effect using the convolutional neural network (CNN) called the halo effect using the convolutional neural networks (HECON) method. In the HECON method to consider the halo effect of the customer decisions, a CNN model is proposed as the deep learning pipeline to obtain more accurate weights of the criteria. The HECON method to obtain the weight of the criteria and identify criteria that lead to product loyalty needs to collect the feedback of a large number of customers based on the net promoter score (NPS) scale. The innovation of the HECON method is to obtain the effect level of each product criterion on selection and loyalty to the product through the feedback of a large number of customers by considering the halo effect on the customers' thinking. To date, the analyzing methods have often not been able to identify the halo effect in evaluating the reasons for customer loyalty to the product. The halo effect indicates sometimes some of the product criteria secretly affect the customers' opinions that require deep neural networks to analyze them. By using the deep CNN model of the HECON method to evaluate product criteria for understanding customer behavior, companies will be able to identify customers' behavior and develop their products exactly following the customer's desires. To evaluate the performance and demonstrate the applicability of the HECON method, presented two case studies. It is presented that there are challenging differences between the results of the HECON method with the other methods because the HECON method considers the halo effect on the customer decisions and demonstrates better performance.}
}
@article{HUANG2012250,
title = {The effectiveness of using procedural scaffoldings in a paper-plus-smartphone collaborative learning context},
journal = {Computers & Education},
volume = {59},
number = {2},
pages = {250-259},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000310},
author = {Hui-Wen Huang and Chih-Wei Wu and Nian-Shing Chen},
keywords = {Cooperative/collaborative learning, Improving classroom teaching, Teaching/learning strategies},
abstract = {The purpose of this study was to evaluate the effectiveness of using procedural scaffoldings in fostering students’ group discourse levels and learning outcomes in a paper-plus-smartphone collaborative learning context. All participants used built-in camera smartphones to learn new knowledge by scanning Quick Response (QR) codes, a type of two-dimensional barcode, embedded in paper-based learning materials in this study. Sixty undergraduate and graduate students enrolled at a four-year university in southern Taiwan participated in this study. Participants were randomly assigned into two different groups, using procedural scaffoldings learning and non-procedural scaffoldings learning. The learning unit about the Long Tail, an important concept used in products sales, was the learning task that participants were expected to complete. During the experiment, pretest–posttest and the completed group worksheets were used to collect data. The researchers applied content analyses, chi-square test, t-test, and ANCOVA to answer research questions. The findings indicated that participants in the experimental group using procedural scaffoldings achieved better learning outcomes than their counterparts in the control group in terms of group discourse levels, group learning, and individual learning.}
}
@article{KAMPPINEN1998481,
title = {Evolution and culture: the Darwinian view on infosphere},
journal = {Futures},
volume = {30},
number = {5},
pages = {481-484},
year = {1998},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(98)00050-0},
url = {https://www.sciencedirect.com/science/article/pii/S0016328798000500},
author = {Matti Kamppinen},
abstract = {This essay looks at the idea that human culture is an evolving system, a complex entity that undergoes evolutionary processes. This idea can also be expressed as follows: the cultural infosphere has the same mode of operation as the organic biosphere. There are three parts to the essay: it begins with some highlights from the history of evolutionary thinking; second, it explains the mechanisms of cultural selection; and third, it discusses the vision of the future provided by evolutionary thinking. The kind of evolutionary thinking focused upon is one that takes Charles Darwin seriously. The depth, reach and relevance of Darwinian thinking has been aptly exposed by Daniel C. Dennett,[1]and this essay assesses its worth in futures research.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{ENDICOTT2003403,
title = {Moral reasoning, intercultural development, and multicultural experiences: relations and cognitive underpinnings},
journal = {International Journal of Intercultural Relations},
volume = {27},
number = {4},
pages = {403-419},
year = {2003},
note = {Special Training Issue},
issn = {0147-1767},
doi = {https://doi.org/10.1016/S0147-1767(03)00030-0},
url = {https://www.sciencedirect.com/science/article/pii/S0147176703000300},
author = {Leilani Endicott and Tonia Bock and Darcia Narvaez},
keywords = {Moral development, Intercultural development, Flexible thinking, Cognitive complexity, Multicultural experience, Schema theory},
abstract = {The relation between moral reasoning and intercultural sensitivity is discussed. We hypothesize that multicultural experiences are related to both types of development, describe the cognitive processes through which multicultural experiences theoretically facilitate development, and present empirical data supporting the association. Though the underlying developmental constructs were initially conceptualized as stage theories, we borrow from cognitive science and contemporary theories of human learning (Derry, 1996) to think of moral and intercultural development in terms of increasing sociocognitive flexibility. Intercultural and moral development share the common element of a critical shift from rigid to flexible thinking. In moral reasoning, this is characterized by the shift from conventional to post-conventional thinking. In intercultural development, a similar movement occurs between the ethnocentric and ethnorelative orientations of intercultural sensitivity. In order to test our hypothesis, college students (n=70) took measures of intercultural development (Intercultural Development Inventory), moral judgment (Defining Issues Test), and multicultural experience (Multicultural Experience Questionnaire). The results indicate that moral judgment and intercultural development are significantly related to one another. Both are related to multicultural experiences, particularly depth of the experiences, as opposed to breadth.}
}
@article{WANG1996579,
title = {The IDS model of intelligent design system},
journal = {Computers & Structures},
volume = {61},
number = {3},
pages = {579-586},
year = {1996},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(96)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0045794996000545},
author = {Xiaotong Wang},
abstract = {Existing models of intelligent design system nowadays are generally logic-based, which solve only simple and small-scale design problems. In the author's opinion, these models which concentrate only on far-fetched use of logical inference and abstract knowledge deviate from designer's thinking and decision process; the crux of the deviation is the lack of imitating thinking with mental imagery ability. Considering the nature of design problems and imitating rational thinking with alternate use of pattern association and symbolic operation, a new intelligent design system (IDS) model and its implementation techniques are presented. Imitation of thinking with mental imagery which is also called pattern association in the IDS model is considered by applying artificial neural network (ANN) techniques. The pattern association in the IDS model imitates the rule of human thinking, “comprehending by analogy”, to some extent. Because of the robustness of the pattern-type knowledge used in pattern association, IDS provides a practical way in producing a design scheme using incomplete and/or undeterminate input data, which is very difficult to achieve in general expert design systems. According to the IDS model, an intelligent structural layout design system of wing (ISDW) is developed. ISDW realizes mapping from key parameters of design requirements and the environment of the wing to the layout design of wing structure in not only graphic form, but also in readable data form. After getting a layout of wing structure, the user will modify it interactively by Auto-CAD, and then return to the ISDW environment to produce FEM meshes by an intelligent meshing interface in order to do the preliminary static and dynamic structural analysis. The design schemes created by the system proved to be proper and usable, and this concludes that IDS model is practicable and practical.}
}
@incollection{MARKOVA2015443,
title = {Representations, Social Psychology of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {443-449},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.24084-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868240841},
author = {Ivana Marková},
keywords = {Anchoring, Cognitive polyphasia, Common sense, Communication, Dialogicality, Ego–Alter–Object, Ethics, Figurative scheme, Imagination, Interactional epistemology, Intervention strategies, Language, Objectification, Social representations, Themata},
abstract = {The theory of social representations studies the formation and transformation of meanings and activities of complex social phenomena like health and illness, political problems or environmental issues in and through language and communication, history and culture. There are two mutually interdependent meanings of social representations. The first meaning concerns the theory of social representations as an interactional theory of knowledge. It refers to networks of concepts and figurative schemes that are generated in and through tradition, common sense, daily knowledge, and communication; these are shared by particular groups and communities. The main features of this theory are the Ego–Alter–Object, the field, the interdependence of asymmetries and symmetries, ethics, figurative scheme, and cognitive polyphasia. Second, social representations refer to concrete social phenomena and to forms of apprehending and creating social realities in and through communication, experience, social practices, and interventions. Human thinking is characterized by the capacity to make distinctions and understand phenomena as dyadic antinomies or themata. Thematization of dyadic antinomies is linked with anchoring and objectification, through which social representations are formed and transformed.}
}
@article{SHU2025111052,
title = {Optimal power flow in hybrid AC-DC systems considering N-k security constraints in the preventive-corrective control stage},
journal = {Electric Power Systems Research},
volume = {238},
pages = {111052},
year = {2025},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624009349},
author = {Hongchun Shu and Hongfang Zhao and Mengli Liao},
keywords = {Flexible DC transmission, AC-DC hybrid system, -, Safety constraints, Optimal power flow},
abstract = {The optimal power flow methods for AC-DC systems containing VSC-HVDC generally only consider the economy during normal operation, overlooking the distribution of line transmission power in fault conditions. As a result, lines that continue to operate after a fault may experience overloading or operate at full capacity. Thus, a method for optimal power flow calculation is proposed that incorporates N-k security constraints in the preventive-corrective control stage for secure and economic operation of hybrid AC-DC systems. This method ensures that the line transmission power in the system meets the limits in the normal, short-term fault, and long-term fault states. In addition to the optimal power flow in the normal state, the method incorporates the system's imbalance as an indicator to evaluate system resilience. It combines this indicator with the economic, network loss, and performance metrics of the system, forming a two-stage bi-level multi-objective optimization model. Furthermore, to address the curse of dimensionality in anticipating system fault sets, a method for generating the anticipated fault set using non-sequential Monte Carlo simulation is proposed, along with a fault scenario search approach based on robust thinking to identify the most severe faults. Finally, the traditional IEEE 30-bus system was improved, and simulation verification was conducted using examples of an AC/DC system with a three-terminal DC network and a wind-solar-storage hybrid AC/DC system with a three-terminal DC network. The simulation results indicate that the proposed optimal power flow method considering the preventive-corrective control stage with N-k security constraints can effectively enhance system resilience. Furthermore, it improves the economic efficiency while ensuring the secure operation of the system.}
}
@article{FUKAI2021145,
title = {Neural mechanisms for learning hierarchical structures of information},
journal = {Current Opinion in Neurobiology},
volume = {70},
pages = {145-153},
year = {2021},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2021.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001252},
author = {Tomoki Fukai and Toshitake Asabuki and Tatsuya Haga},
abstract = {Spatial and temporal information from the environment is often hierarchically organized, so is our knowledge formed about the environment. Identifying the meaningful segments embedded in hierarchically structured information is crucial for cognitive functions, including visual, auditory, motor, memory, and language processing. Segmentation enables the grasping of the links between isolated entities, offering the basis for reasoning and thinking. Importantly, the brain learns such segmentation without external instructions. Here, we review the underlying computational mechanisms implemented at the single-cell and network levels. The network-level mechanism has an interesting similarity to machine-learning methods for graph segmentation. The brain possibly implements methods for the analysis of the hierarchical structures of the environment at multiple levels of its processing hierarchy.}
}
@article{WILKINSON2013394,
title = {The past and the future of business marketing theory},
journal = {Industrial Marketing Management},
volume = {42},
number = {3},
pages = {394-404},
year = {2013},
note = {Theoretical Perspectives in Industrial Marketing Management},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850113000266},
author = {Ian F. Wilkinson and Louise C. Young},
keywords = {Complex adaptive systems, Business relations and networks, Dynamics and evolution, Agent based models, Mechanisms},
abstract = {A complex systems approach to understanding and modelling business marketing systems is described. The focus is on the dynamics and evolution of such systems and the processes and mechanisms driving this, rather than the more usual comparative static, variables based statistical models. Order emerges in a self-organising, bottom up way from the local or micro actions and interactions of those involved. We describe the development of our thinking regarding this approach and its main features, including the development of agent based simulation models and the identification and modelling of underlying mechanisms and processes. We conclude by discussing the implications of this approach for business marketing theory and research.}
}
@article{THANKACHAN2024101283,
title = {A mathematical formulation of learner cognition for personalised learning experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101283},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000779},
author = {Jeena A. Thankachan and Bama Srinivasan},
keywords = {Virtual Learning Environment (VLE), Cognitive Evaluation Metrics (CEM), Multimode evaluation, Cognitive abilities, Learning tasks, Reinforcement learning},
abstract = {The paper focuses on the assessment of cognitive skills within Virtual Learning Environments (VLEs). In response to the global shift to remote learning amid the COVID-19 pandemic, VLEs, which include learning management systems (LMS) and online collaboration platforms, gained prominence. The proposed work leverages an established Cattell–Horn–Carroll (CHC) theory to propose eight metrics, which collectively form a part of Cognitive Evaluation Metrics (CEM). The proposed metrics introduce a novel computational approach for multimode evaluation of learners’ cognitive abilities for each learning task within a learning environment. The paper details the formalism for the evaluation of the metrics and makes a contribution towards the potential of the proposed methodology to evaluate cognitive abilities. Additionally, the work implements CEM integration into the learner module of a Game-Based Learning (GBL) environment. Analysis of simulations in the GBL environment, along with statistical analysis, provides insights into the normal distribution of cognitive metrics. This reveals diverse ranges in various abilities such as long or short term memory, working memory, reasoning, attention, and processing speed. The paper also explores the impact of virtual assistants, which highlights their limited relevance to enhance cognitive abilities but serve as valuable on-demand support resources.}
}
@article{SUN2024103771,
title = {Supply chain planning with free trade zone and uncertain demand},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {192},
pages = {103771},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524003624},
author = {Haoying Sun and Manoj Vanajakumari and Chelliah Sriskandarajah and Subodha Kumar},
keywords = {Supply chain management, Robust optimization, Dynamic lot sizing},
abstract = {Our research is inspired by the subcontracting problem at a major oil field services company in North America. The company’s supply chain consists of suppliers bringing raw materials to a Free Trade Zone (FTZ). The FTZ receives raw materials in full containers from various suppliers, and then the company ships them to various plants (e.g. oil excavation sites) frequently via subcontractors. This allows the company to focus on managing only the inbound transportation and inventory at the FTZ. The demand for each raw material is stochastic. We derive an algorithm running at polynomial time for the stochastic programming formulation and perform μ− regret Robust Optimization to handle the demand uncertainty. We also use a Sample Average Approximation method to alleviate the high computational requirement of the robust optimization model. The modeling approach demonstrated by this paper not only meets the needs of this specific company and industry but also can be applied to other industries with similar supply chain structures.}
}
@article{YIM2014144,
title = {A development of a quantitative situation awareness measurement tool: Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE)},
journal = {Annals of Nuclear Energy},
volume = {65},
pages = {144-157},
year = {2014},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2013.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0306454913005598},
author = {Ho Bin Yim and Seung Min Lee and Poong Hyun Seong},
keywords = {Quantitative measure, Situation awareness, Graphical expression, NPP MCR operators},
abstract = {Operator performance measures are used for multiple purposes, such as control room design, human system interface (HSI) evaluation, training, and so on. Performance measures are often focused on results; however, especially for a training purpose – at least in a nuclear industry, more detailed descriptions about processes are required. Situation awareness (SA) measurements have directly/indirectly played as a complimentary measure and provided descriptive insights on how to improve performance of operators for the next training. Unfortunately, most of the well-developed SA measurement techniques, such as Situation Awareness Global Assessment Technique (SAGAT) need an expert opinion which sometimes troubles easy spread of measurement’s application or usage. A quantitative SA measurement tool named Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE) is introduced to resolve some of these concerns. CoRSAGE is based on production rules to represent a human operator’s cognitive process of problem solving, and Bayesian inference to quantify it. Petri Net concept is also used for graphical expressions of SA flow. Three components – inference transition, volatile/non-volatile memory tokens – were newly developed to achieve required functions. Training data of a Loss of Coolant Accident (LOCA) scenario for an emergency condition and an earthquake scenario for an abnormal condition by real plant operators were used to validate the tool. The validation result showed that CoRSAGE performed a reasonable match to other performance results.}
}
@article{BRENT19961,
title = {Advances in the computational study of language acquisition},
journal = {Cognition},
volume = {61},
number = {1},
pages = {1-38},
year = {1996},
note = {Compositional Language Acquisition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(96)00779-2},
url = {https://www.sciencedirect.com/science/article/pii/S0010027796007792},
author = {Michael R. Brent},
abstract = {This paper provides a tutorial introduction to computational studies of how children learn their native languages. Its aim is to make recent advances accessible to the broader research community, and to place them in the context of current theoretical issues. The first section locates computational studies and behavioral studies within a common theoretical framework. The next two sections review two papers that appear in this volume: one on learning the meanings of words and one on learning the sounds of words. The following section highlights an idea which emerges independently in these two papers and which I have dubbed autonomous bootstrapping. Classical bootstrapping hypotheses propose that children begin to get a toe-hold in a particular linguistic domain, such as syntax, by exploiting information from another domain, such as semantics. Autonomous bootstrapping complements the cross-domain acquisition strategies of classical bootstrapping with strategies that apply within a single domain. Autonomous bootstrapping strategies work by representing partial and/or uncertain linguistic knowledge and using it to analyze the input. The next two sections review two more more contributions to this special issue: one on learning word meanings via selectional preferences and one on algorithms for setting grammatical parameters. The final section suggests directions for future research.}
}
@article{BEATY201922,
title = {Network neuroscience of creative cognition: mapping cognitive mechanisms and individual differences in the creative brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {27},
pages = {22-30},
year = {2019},
note = {Creativity},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301219},
author = {Roger E Beaty and Paul Seli and Daniel L Schacter},
abstract = {Network neuroscience research is providing increasing specificity on the contribution of large-scale brain networks to creative cognition. Here, we summarize recent experimental work examining cognitive mechanisms of network interactions and correlational studies assessing network dynamics associated with individual creative abilities. Our review identifies three cognitive processes related to network interactions during creative performance: goal-directed memory retrieval, prepotent-response inhibition, and internally-focused attention. Correlational work using prediction modeling indicates that functional connectivity between networks — particularly the executive control and default networks — can reliably predict an individual’s creative thinking ability. We discuss potential directions for future network neuroscience, including assessing creative performance in specific domains and using brain stimulation to test causal hypotheses regarding network interactions and cognitive mechanisms of creative thought.}
}
@article{FOWLER20245,
title = {Will variants of uncertain significance still exist in 2030?},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {1},
pages = {5-10},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2023.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929723004007},
author = {Douglas M. Fowler and Heidi L. Rehm},
abstract = {Summary
In 2020, the National Human Genome Research Institute (NHGRI) made ten “bold predictions,” including that “the clinical relevance of all encountered genomic variants will be readily predictable, rendering the diagnostic designation ‘variant of uncertain significance (VUS)’ obsolete.” We discuss the prospects for this prediction, arguing that many, if not most, VUS in coding regions will be resolved by 2030. We outline a confluence of recent changes making this possible, especially advances in the standards for variant classification that better leverage diverse types of evidence, improvements in computational variant effect predictor performance, scalable multiplexed assays of variant effect capable of saturating the genome, and data-sharing efforts that will maximize the information gained from each new individual sequenced and variant interpreted. We suggest that clinicians and researchers can realize a future where VUSs have largely been eliminated, in line with the NHGRI’s bold prediction. The length of time taken to reach this future, and thus whether we are able to achieve the goal of largely eliminating VUSs by 2030, is largely a consequence of the choices made now and in the next few years. We believe that investing in eliminating VUSs is worthwhile, since their predominance remains one of the biggest challenges to precision genomic medicine.}
}
@article{SFARD20121,
title = {Introduction: Developing mathematical discourse—Some insights from communicational research},
journal = {International Journal of Educational Research},
volume = {51-52},
pages = {1-9},
year = {2012},
note = {Developing mathematical discourse–Some insights from communicational research},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511001327},
author = {Anna Sfard},
keywords = {Mathematics, Discourse, Learning, Development, Cognition, Emotions, Interactions},
abstract = {Quite diverse in their foci and specific themes, the seven articles collected in this special issue are unified by their common conceptual framework. Grounded in the premise that thinking can be usefully defined as self-communicating and that mathematics can thus be viewed as a discourse, the communicational framework provides a unified set of conceptual tools with which to investigate cognitive, affective and social aspects of mathematics learning. The communicational tools are employed by the authors as they investigate diverse aspects of mathematical discourse and explore its development in the classroom and beyond. The seven studies combine together to produce a set of insights, some of which go against widespread beliefs about teaching and learning mathematics.}
}
@article{BOUDIN2016448,
title = {Opinion dynamics: Kinetic modelling with mass media, application to the Scottish independence referendum},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {448-457},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008602},
author = {Laurent Boudin and Francesco Salvarani},
keywords = {Opinion formation, Mass media, Kinetic equations},
abstract = {We consider a kinetic model describing some mechanisms of opinion formation in the framework of referendums, where the individuals, who can interact between themselves and modify their opinion by means of spontaneous self-thinking, are moreover under the influence of mass media. We study, at the numerical level, both the transient and the asymptotic regimes. In particular, we point out that a plurality of media, with different orientations, is a key ingredient to allow pluralism and prevent consensus. The forecasts of the model are compared to some surveys related to the Scottish independence referendum of 2014.}
}
@article{PIOLOPEZ2023103585,
title = {Morphoceuticals: Perspectives for discovery of drugs targeting anatomical control mechanisms in regenerative medicine, cancer and aging},
journal = {Drug Discovery Today},
volume = {28},
number = {6},
pages = {103585},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103585},
url = {https://www.sciencedirect.com/science/article/pii/S1359644623001010},
author = {Léo Pio-Lopez and Michael Levin},
keywords = {Biomedicine, Drug discovery, Morphogenesis},
abstract = {Morphoceuticals are a new class of interventions that target the setpoints of anatomical homeostasis for efficient, modular control of growth and form. Here, we focus on a subclass: electroceuticals, which specifically target the cellular bioelectrical interface. Cellular collectives in all tissues form bioelectrical networks via ion channels and gap junctions that process morphogenetic information, controlling gene expression and allowing cell networks to adaptively and dynamically control growth and pattern formation. Recent progress in understanding this physiological control system, including predictive computational models, suggests that targeting bioelectrical interfaces can control embryogenesis and maintain shape against injury, senescence and tumorigenesis. We propose a roadmap for drug discovery focused on manipulating endogenous bioelectric signaling for regenerative medicine, cancer suppression and antiaging therapeutics.}
}
@article{MOSKOWITZ200387,
title = {The intertwining of psychophysics and sensory analysis: historical perspectives and future opportunities—a personal view},
journal = {Food Quality and Preference},
volume = {14},
number = {2},
pages = {87-98},
year = {2003},
issn = {0950-3293},
doi = {https://doi.org/10.1016/S0950-3293(02)00072-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950329302000721},
author = {Howard R. Moskowitz},
keywords = {History, Psychology, Psychophysics},
abstract = {From today’s point of view, psychophysics and sensory analysis appear conjoined, at least from the vantage point of sensory analysis. This paper shows how psychophysical thinking has not only entered sensory analysis, but also shaped some of the ways that modern day sensory analysts conceptualize their problems and go about solving them. The paper also shows how this was not always the case. The rapprochement of the two fields has only gradually developed as sensory analysis has come to accept psychophysical thinking. The paper concludes by listing a series of trends that may bring the two fields even closer in the future.}
}
@article{MININA2022104684,
title = {Neuron quantum computers and a way to unification of science: A compendium of Efim Liberman's scientific work},
journal = {Biosystems},
volume = {217},
pages = {104684},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104684},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000727},
author = {Svetlana V. Minina and Nikita E. Shklovskiy-Kordi},
keywords = {Efim liberman, cAMP, Biological computation, Biophysics, Chaimatics, Quantum biology, Unity of science, Quantum computation, Molecular cell computer, Quantum regulator},
abstract = {In 1972, Efim Liberman, a Soviet biophysicist, pioneered a brand-new approach to studying the operation of the brain, the live cell and the human mind by publishing a paper titled “Cell as a molecular computer” (1972). In this paper, Liberman posited that a consecutive/parallel stochastic molecular computer (MCC) controls a living cell. An MCC operates with molecule-words (DNA, RNA, proteins) according to the program recorded in DNA and RNA. Computational operations are implemented by molecular operators acting as enzymes. An MCC is present in each live cell. A neuron cell MCC can be involved in solving tasks for the entire organism. Neuron MCC investigation was started with studying an impact of an intracellular injection of cyclic AMP on electric activity of a neuron. Cyclic nucleotides were considered as input words for an MCC, which are generated inside a neuron as a result of synaptic activity. This led Efim Liberman to the idea that, in order to solve complex physical problems, which are encountered by a neuron and require rapid solutions, the molecular computer adjusts the operation of the quantum molecular regulator, which uses the “computational environment” of the cytoskeleton and quantum properties of the elementary hypersound quasiparticles for completing mathematical operations for the minimum price of action. Efim Liberman suggested that the human self-consciousness is a quantum computer of even a higher level and designated it as an extreme quantum regulator. In order to describe such systems, he suggested to join biology, physics and mathematics into a unified science, and formulated its four fundamental principles. Results of Efim Liberman’s theoretical and experimental studies on the topic of biological computation are summarized in this review.}
}
@article{CHATRABHUJ2024101045,
title = {Design of an iterative method for environmental-sustainable development: Integrating bioinspired computing techniques},
journal = {Environmental Development},
volume = {51},
pages = {101045},
year = {2024},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2024.101045},
url = {https://www.sciencedirect.com/science/article/pii/S2211464524000836},
author = { Chatrabhuj and Kundan Meshram},
keywords = {Sustainable development, Bioinspired computing, Hybrid algorithms, Agent-based modelling, High-performance computing},
abstract = {The need for sustainable development has grown in response to global environmental, social, and economic challenges. Conventional computational methods frequently struggle to address the complex nature of the Sustainable Development Goals (SDGs), lacking the ability to balance global search with local optimization and failing to prioritize goals related to sustainability. To address these restrictions, this work introduces the Integrated Bioinspired Computing Model for Sustainable Development (IBCMSD). By combining Genetic Algorithms (GAs), Artificial Neural Networks (ANNs), and Ant Colony Optimization (ACO), a cohesive hybrid model is developed that improves exploration and exploitation, balance for increased efficiency, and solution quality. It is implemented on High-Performance Computing (HPC) clusters to ensure scalability and resilience when dealing with complicated optimization challenges. Furthermore, using a multidisciplinary co-design method completes the model with multiple views, increasing its relevance and applicability in real-world circumstances. IBCMSD makes a significant contribution to computational sustainability by leveraging bioinspired computing, potentially enabling informed decision-making and SDG accomplishment across multiple domains.}
}
@article{LI2024112467,
title = {Study on correlation between perioperative cognitive function and nutritional status in elderly patients with gastric cancer},
journal = {Experimental Gerontology},
volume = {193},
pages = {112467},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2024.112467},
url = {https://www.sciencedirect.com/science/article/pii/S0531556524001098},
author = {Rong Li and Yuping Liu and Yingtao Meng and Xianlin Qu and Meimei Shang and Lihui Yang and Jie Chai},
keywords = {Elderly, Gastric cancer, Perioperative period, Cognitive function, Nutritional status, Correlation, Analysis},
abstract = {Objective: To investigate the cognitive function and nutritional status of elderly patients with gastric cancer during perioperative period, and to analyze their correlation. Methods: Aged patients undergoing gastric cancer surgery in The Affiliated Cancer Hospital of Shandong First Medical University from March to October 2021 were selected as the subjects of this study. The monitoring data of cognitive function and nutritional status were retrospectively analyzed from 1 to 3 days before surgery, 1 and 3 days after surgery, 7 days after surgery (before discharge) and 30 days after surgery to analyze the correlation between cognitive function and nutritional status in elderly patients with gastric cancer. Results: the incidence of mild cognitive impairment in elderly patients with gastric cancer was 52.43 %, the visual space of the two groups' (mild cognitive impairment) ability of execution, name, attention, language, abstract thinking, delayed memory and cognitive function scores were lower than 1 set of directional force (cognitive function in normal group), statistically significant difference (P < 0.05). The nutritional status of elderly patients with gastric cancer was lower than that of healthy elderly group at the same period (P < 0.05). The scores of visual spatial executive function, name, attention, delayed memory, orientation and total score of cognitive function in elderly gastric cancer patients were positively correlated with nutritional status (P < 0.05). Conclusions: The cognitive function and nutritional status of elderly patients with gastric cancer are both in a low state during treatment and a higher level of cognitive function can help patients maintain a more correct nutritional cognition, and the nutritional status of patients will be relatively better. There is a positive correlation between cognitive function and nutritional status in elderly patients with gastric cancer, which should be paid attention to in the treatment.}
}
@article{KIM201828,
title = {Degree of satisfaction-difference (DOSD) method for measuring consumer acceptance: A signal detection measurement with higher reliability than hedonic scaling},
journal = {Food Quality and Preference},
volume = {63},
pages = {28-37},
year = {2018},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950329317301738},
author = {Min-A Kim and Danielle {van Hout} and Jean-Marc Dessirier and Hye-Seong Lee},
keywords = {Acceptance test, Affective product discrimination, Range effects, Indirect scaling, Satisfaction, Reference framing},
abstract = {Predictions of consumer acceptance are often based on hedonic scores, but these are determined not only by the consumer level of product liking, but also by consumer scale usage, which in turn is affected by thinking style and experimental contexts. To improve the validity and reliability of consumer acceptance measurement, a new indirect scaling method, the ‘Degree of Satisfaction-Difference (DOSD)’, was developed using a reminder design and signal detection theory (SDT). In DOSD, a product-specified ‘cognitive warm-up’ was used to evoke the consumer personal context and the internal evaluative criteria prior to product evaluation. In DOSD, each test product was presented together with a fixed-reference (identified as such) and consumers were asked to evaluate their satisfaction with the reference first with a sureness rating, and then to evaluate the test product for both absolute satisfaction and comparative satisfaction to the reference. The reliability of DOSD was tested against traditional hedonic scaling using an independent samples design of two consumer groups with equivalent cognitive reflection test profiles, each including High Reflection Thinkers (HRTs) and Low Reflection Thinkers (LRTs) in equal proportion. Each group tested two sets of skin lotions differing in product range, either using DOSD or hedonic scaling. When examining the affective discriminations of the two common products in terms of d′ values between product sets, the LRT subjects generated inconsistent responses with hedonic scaling, but reproducible responses with DOSD. The HRT subjects performed consistently using both scaling methods. These results validate DOSD’s superior reliability in affective tests and demonstrate its potential as an alternative consumer acceptance measurement to hedonic scaling.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@incollection{BROWN201589,
title = {Space, Linguistic Expression of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {89-93},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570172},
author = {Penelope Brown},
keywords = {Adpositions, Language and cognition, Language universals, Locative constructions, Motion verbs, Space, Spatial frames of reference, Topological language},
abstract = {Spatial cognition is central to human thinking, and spatial language is thus an important area of study, as it may reveal fundamental properties of human thought. Recent research has shown that spatial language is much more divergent across languages than had previously been thought, suggesting significant cultural patterning of spatial conceptualization. This article reviews spatial language cross-linguistically, sets out a typological framework for the language of space, and considers the relationship of spatial language to spatial cognition, in the context of extensive linguistic diversity in the spatial domain.}
}
@article{ADRIAENSEN2023106294,
title = {Systems-theoretic interdependence analysis in robot-assisted warehouse management},
journal = {Safety Science},
volume = {168},
pages = {106294},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523002369},
author = {Arie Adriaensen and Liliane Pintelon and Francesco Costantino and Giulio {Di Gravio} and Riccardo Patriarca},
keywords = {FRAM, Human-machine interaction, Industry 4.0, Industry 5.0, Cobots},
abstract = {The safe and efficient application of collaborative robots requires an understanding of actual work practices transformation, emerging from the adoption of new technological instruments. Functional systems-thinking is largely absent in literature about collaborative robot applications. In this context, this study proposes a framework that combines two safety analysis methods, being the Functional Resonance Analysis Method and Interdependence Analysis. Both safety and efficiency are examined by selected case study highlights to gain an in-depth understanding of human operators’ role as the central driver of human–machine (eco)systems in a warehouse distribution system, in which warehouse robot assistance is provided. Whereas the Functional Resonance Analysis Method first maps the work system interactions as a whole, Interdependence Analysis is subsequently applied to investigate individual inter-agent exchanges by the principles of Observability, Predictability, and Directability as a core principle for goal coordination between multiple agents, including warehouse robot agents. The case study examples reveal the combined effects of the working system environment and the robot application but also demonstrate possible operational solutions to deal with socio-technical complexity.}
}
@article{NAGOEV2020615,
title = {Model of the reasoning process in a multiagent cognitive system},
journal = {Procedia Computer Science},
volume = {169},
pages = {615-619},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303252},
author = {Zalimhan Nagoev and Inna Pshenokova and Murat Anchekov},
keywords = {Multi-Agent Systems, Neurocognitive Architecture, Simulation Model, Artificial Intelligence Systems, Reasoning Models},
abstract = {A model of the reasoning process in a multiagent cognitive system for the synthesis of intelligent solutions of the problem is presented. The approach based on the computational abstraction of multi-agent neurocognitive systems that illustrates architectural conformity to self-organizing neurocognitive networks of the brain. The model represents the process of reasoning in the form of cognitive blocks that synthesize intelligent solutions and allow the user to effectively solve the tasks.}
}
@article{SON2015120,
title = {The history of Western futures studies: An exploration of the intellectual traditions and three-phase periodization},
journal = {Futures},
volume = {66},
pages = {120-137},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002079},
author = {Hyeonju Son},
keywords = {History of Western futures studies, Intellectual tradition, Periodization, Rationalization of futures, Industrialization of futures, Fragmentation of futures.},
abstract = {The main purpose of this paper is to present a three-phase periodization of modern Western futures studies to construct historical classification. In order to reach this goal, the following intellectual traditions are introduced to review the philosophical and historical contexts that affect the very foundations of futures studies: (a) religions, (b) utopias, (c) historicism, (d) science fiction, and (e) systems thinking. The first phase (beginning in 1945 to the 1960s) was the era of scientific inquiry and rationalization of the futures characterized by the prevalence of technological forecasting, the rise of alternative futures in systematic ways, and the growth of professionalization of futures studies. In the first phase, futures had become objects of rationalization removed from the traditional approaches such as utopia, grandiose evolutionary ideas, naive prophecies, science fiction, religious attitudes, and mystical orientation. The second phase (the 1970s and the 1980s) saw the creation the global institution and industrialization of the futures. This era was marked by the rise of worldwide discourse on global futures, the development of normative futures, and the deep involvement of the business community in futures thinking. In the second phase, futures studies-industry ties were growing and the future-oriented thoughts extensively permeated the business decision-making process. The third phase (the 1990s – the present) reflects the current era of the neoliberal view and fragmentation of the futures. This phase is taking place in the time of neoliberal globalization and risk society discourses and is characterized by the dominance of foresight, the advance of critical futures studies, and the intensification of fragmentation. In the third phase, futures practice tends to be confined to the support of strategic planning, and hence is experiencing an identity crisis and loss of its earlier status of humanity-oriented futures.}
}
@article{LEE2021101596,
title = {Measuring Mohr social capital},
journal = {Poetics},
volume = {88},
pages = {101596},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101596},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000863},
author = {Monica Lee and Amaç Herdağdelen and Minsu Park and John Levi Martin},
abstract = {We here bring together two different traditions of thinking about social capital. One, the Tocquevillian, looks to associations and group memberships as the core of social capital. The other, the Colemanian, looks to interpersonal networks as the core of social capital. We argue that the most common way of articulating how humans use these types of relationships in different ways—the distinction between “bridging” and “bonding” social capital—is epistemically unstable. What might be possible, however, is to use the insights developed by Ronald Burt regarding tie non-redundancy to study associational social capital. We do this by drawing on the insights of the approach consistently adopted and developed by John Mohr, which emphasizes duality and diversity, to develop measures of group affiliation-based social capital. We accordingly, for both Tocquevillian and Colemanian social capital, distinguish measures that focus on the mass of social capital from those that focus on its diversity. To illustrate, we use de-identified data from 77 Million U.S. Facebook Groups users to measure their degree of all resulting types of social capital. We show that our understanding of who has the most social capital varies greatly by whether we are considering Tocquevillian or Colemanian capital, and whether we are focusing on mass or diversity.}
}
@article{BATTISTELLI2022,
title = {Online Strategies To Improve Quantitative Skills in Microbiology Laboratory Classes},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {1},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00333-21},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000995},
author = {Joseph M. Battistelli and Rima B. Franklin},
keywords = {quantitative literacy, quantitative biology, problem solving, word problems, math skills, formula question, Canvas, spreadsheets, algebra, formula questions},
abstract = {Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills.
ABSTRACT
Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills. This can create a substantial grading burden for faculty teaching online and/or large enrollment courses, but the “formula question” feature present in many learning management systems (LMS) offers a solution. Using this feature, faculty set up a basic scaffold for an algebraic word problem, and the LMS can then automatically generate and grade many different versions of the question. In this paper, we describe the use of “formula questions” in an undergraduate microbiology course and specifically focus on how the strategic use of algebraic word problems at multiple points throughout the semester can help build quantitative literacy. Key to the success of this approach is that faculty provide a review of foundational mathematical skills early in the semester, even in upper-level classes. This should include reacquainting students with formatting conventions (e.g., rounding and scientific notation), familiarizing them with any idiosyncrasies of the technology platforms, and demonstrating how to solve math problems using spreadsheets. This initial effort increases student success when more complex problems are introduced later in the semester. Though the tips summarized in this paper focus on undergraduate microbiology teaching laboratories using Canvas, the approach can easily be modified to help students develop their critical thinking and quantitative reasoning skills at other levels and in other disciplines.}
}
@article{COUVELAS2020326,
title = {Bioclimatic building design theory and application},
journal = {Procedia Manufacturing},
volume = {44},
pages = {326-333},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.238},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308258},
author = {Agnes Couvelas},
keywords = {Modern architecture, Cultural heritage, Sustainable architecture design, Bioclimatic Performance Optimization, Inter-locality},
abstract = {Ecological thinking is the recognition of the dialectic unity between natural and man-made environment, the respect to what exists around us, and the concomitant “openness” toward others. Here, I present examples from my own work to describe a number of passive bioclimatic approaches focused on the above principles. First, the use of the wind as an expressive element in building design, including the enhancement of air flow in the interior space, the moderation of wind and sand accumulation, the moderation of the sound carried by prevailing winds, and the conversion of the wind into a means of protection against its own force. Second, the use of adaptive building envelopes and shading systems to achieve control of natural light, ventilation and temperature of the inner space through their own transformability, surface openings and materials, including planting as a building material; in a sense, treating buildings as living organisms. Three of these examples have been included in the H2020-MSCA-RISE OptArch project, in which I am scientifically responsible for the work package WP5 entitled “Improvement of bioclimatic design through optimization of performance”.}
}
@article{LOCKWOOD2019100688,
title = {Computing as a mathematical disciplinary practice},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100688},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318300282},
author = {Elise Lockwood and Anna F. DeJarnette and Matthew Thomas},
keywords = {Computation, Mathematical disciplinary practices, Mathematicians},
abstract = {In this paper, we make a case for computing as a mathematical disciplinary practice. We present results from interviews with research mathematicians in which they reflected on the use of computing in their professional work. We draw on their responses to present evidence that computing is an inherent part of doing mathematics and is a practice they want their students to develop. We also discuss the mathematicians’ perspectives on how they learned and teach computing, and we suggest that much needs to be explored about how to teach computing effectively. Our overarching goal is to draw attention to the importance of the teaching and learning of computing, and we argue that it is an imperative topic of study in mathematics education research.}
}
@article{SCHWARZ201359,
title = {Business wargaming for teaching strategy making},
journal = {Futures},
volume = {51},
pages = {59-66},
year = {2013},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328713000864},
author = {Jan Oliver Schwarz},
keywords = {Business wargaming, Teaching, Simulation, Management education, Strategy making, Strategic thinking},
abstract = {An increasingly complex and dynamic business environment requires new approaches to teaching strategy to management students. Business wargaming, a dynamic strategic simulation, is discussed as a management simulation which can respond to the contemporary challenges in management education. Reflecting on the practical use of business wargaming in the classroom, it is described how such simulations prepare management students for making strategic decisions in complex and dynamic environments characterised by high uncertainty concerning the future.}
}
@article{LIN20162176,
title = {New statistical analysis in marketing research with fuzzy data},
journal = {Journal of Business Research},
volume = {69},
number = {6},
pages = {2176-2181},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315006517},
author = {Hsin-Cheng Lin and Chen-Song Wang and Juei Chao Chen and Berlin Wu},
keywords = {Decision making, Fuzzy statistics, Fuzzy data, Marketing research},
abstract = {This research proposes new statistical methods for marketing research and decision making. The study employs a soft computing technique and a new statistical tool to evaluate people's thinking. Because the classical measurement system has difficulties in dealing with the non-real valued information, the study aims to find an appropriate measurement system to overcome this problem. The main idea is to decompose the data into a two-dimensional type, centroid and its length (area). The two-dimensional questionnaires this study proposes help reaching market information.}
}
@article{SATTARI2021104981,
title = {Application of Bayesian network and artificial intelligence to reduce accident/incident rates in oil & gas companies},
journal = {Safety Science},
volume = {133},
pages = {104981},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104981},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520303787},
author = {Fereshteh Sattari and Renato Macciotta and Daniel Kurian and Lianne Lefsrud},
keywords = {Artificial intelligence, Bayesian network, Machine learning, Keyword analysis, Incident data, Process safety management, Latent causes},
abstract = {Process safety management (PSM) is a framework that demonstrates a company’s commitment to process safety, a better understanding of hazards and risks, a comprehensive assessment and management of risks, and enhanced learning from experience to improve overall safety and operational performance. Companies often use an incident data reporting system to execute PSM. While companies keep incident data in thousands of reports, rarely do they glean full value in learning from these to prevent and reduce future incidents. To overcome this challenge, this research applied machine learning and keyword analysis to label and classify 8199 incident reports from an oil and gas company into nine groups identified in the latest version of PSM guidelines published by the Center for Chemical Process Safety (CCPS). To converge on an optimal solution, two different Bayesian network techniques (Tabu and hill climbing) were applied. Both methods resulted in the same map, showing that the Total Number of Incidents has the maximum dependency (50%) on Asset Integrity & Reliability; this means focusing resources on this aspect could reduce the total number of incidents by half. Cross correlation analysis (CCA) was also applied, which validated and confirmed this result. This analysis identifies which measures enhance the company’s safety management strategy to reduce these latent causes, but also supports critical thinking, enhanced communication, and learning culture to improve organizational safety.}
}
@article{HAJELA20021,
title = {Soft computing in multidisciplinary aerospace design—new directions for research},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {1},
pages = {1-21},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(01)00015-X},
url = {https://www.sciencedirect.com/science/article/pii/S037604210100015X},
author = {Prabhat Hajela},
abstract = {There has been increased activity in the study of methods for multidisciplinary analysis and design. This field of research has been a busy one over the past decade, driven by advances in computational methods and significant new developments in computer hardware. There is a concern, however, that while new computers will derive their computational speed through parallel processing, current algorithmic procedures that have roots in serial thinking are poor candidates for use on such machines—a paradigm shift is required! Among new advances in computational methods, soft computing techniques have enjoyed a remarkable period of development and growth. Of these, methods of neural computing, evolutionary search, and fuzzy logic have been the most extensively explored in problems of multidisciplinary analysis and design. The paper will summarize important accomplishments to-date, of neurocomputing, fuzzy logic, and evolutionary search, including immune network modeling, in the field of multidisciplinary aerospace design.}
}
@article{1995462,
title = {95/06537 Historical rates of atmospheric Pb deposition using 210Pb dated peat cores: Corroboration, computation, and interpretation},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {6},
pages = {462},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)98112-5},
url = {https://www.sciencedirect.com/science/article/pii/0140670195981125}
}
@article{GORMONG20231988,
title = {Neighboring Group Effects on the Rates of Cleavage of Si–O–Si-Containing Compounds},
journal = {The Journal of Organic Chemistry},
volume = {88},
number = {4},
pages = {1988-1995},
year = {2023},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c02126},
url = {https://www.sciencedirect.com/science/article/pii/S002232632300107X},
author = {Ethan A. Gormong and Dorian S. Sneddon and Theresa M. Reineke and Thomas R. Hoye},
abstract = {ABSTRACT
The presence of a nearby tethered functional group (G, G = tertiary amide or amine) can significantly impact the rate of cleavage of an Si–O bond. We report here an in situ1H NMR spectroscopic investigation of the relative rates of cleavage of model substrates containing two different Si–O substructures, namely alkoxydisiloxanes [GRO–Si­(Me2)–O–SiMe3] and carbodisiloxanes [GR–Si­(Me2)–O–SiMe3]. The trends in the relative rates (which slowed with increasing chain length, with a notable exception) of alkoxydisiloxane hydrolyses were probed via computation. The results correlated well with the experimental data. In contrast to the hydrolysis of the alkoxydisiloxanes, the carbodisiloxanes were not fully hydrolyzed, but rather formed an equilibrium mixture of starting asymmetric disiloxane, two silanols, and a new symmetrical disiloxane. We also uncovered a facile siloxy-metathesis reaction of an incoming silanol with the carbodisiloxane substrate [e.g., Me2NR–Si­(Me2)–O–SiMe3 + HOSiEt3 ⇋ Me2NR–Si­(Me2)–O–SiEt3 + HOSiMe3] facilitated by the pendant dimethylamino group, a process that was also probed by computation.}
}
@article{CHEN2025121691,
title = {A novel attribute reduction algorithm based on granular sequential three-way decision},
journal = {Information Sciences},
volume = {694},
pages = {121691},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016050},
author = {Yuliang Chen and Yunlong Cheng and Binbin Luo and Yabin Shao and Mingfu Zhao and Qinghua Zhang},
keywords = {Granular computing, Sequential three-way decision, Granular rough sets, Attribute reduction},
abstract = {Attribute reduction plays a crucial role in knowledge discovery, and sequential three-way decision (S3WD) provides a new method for attribute reduction. However, the three regions of the S3WD model are usually represented as three sets, which leads to two disadvantages. On one hand, it is difficult to obtain the condition of a decision rule when multiple equivalence classes are merged into a set because different equivalence classes have different descriptions. On the other hand, if the boundary region of the upper level of S3WD is a set, one has to partition the upper level with all the acquired attributes rather than the newly added attribute. That is, there is double counting. Therefore, this paper focuses on how to retain the topology of equivalence classes in S3WD, and how to use this topology to enhance semantic interpretation and improve computational efficiency. To this end, a granular version of S3WD, called granular sequential three-way decision (GS3WD), is first developed to retain the information structure of equivalence classes. And then, three acceleration strategies and an efficient granular sequential three-way reduction (GS3WR) are proposed. Finally, a concept tree can be generated simultaneously in the process of GS3WR, and the decision rules with multi-granularity can be extracted from this concept tree directly. Experimental results show that GS3WR can obtain the same core attributes and reducts as the representative attribute reduction algorithms in rough sets and the computational efficiency is improved by hundreds of times.}
}
@incollection{SUGHRUE2024205,
title = {Chapter 12 - Connectomic approaches to neurosurgical planning},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {205-214},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000112},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain tumor surgery, Cerebral cortex, Cognitive deficits, fMRI, Graph theory, Neuro-Oncology, Onco-functional balance},
abstract = {In this chapter, we introduce how connectomics can provide an improved understanding of the structural and functional organization of the human brain which can be applied for intracerebral brain surgery. In particular, such connectomic thinking expands our ability to improve patient functional outcomes after surgery beyond mere motor and language functions by also considering the anatomy responsible for complex cognitive functions. We introduce the concept of “disconnection surgery,” where the surgical decisions when removing a tumor can be thought of a series of specific cuts that we plan to perform on the periphery of the tumor such that we can disconnect the tumor from the surrounding networks. Connectomics allows us to define the risks associated with specific tumors and surgical decisions, which can subsequently guide the operation but also tailor preoperative patient discussion. Novel mathematical concepts from the field of network neuroscience on graph theory are also introduced so as to better define truly eloquent brain regions on an individualized basis.}
}
@article{SCOLOZZI2017957,
title = {The anthroposphere as an anticipatory system: Open questions on steering the climate},
journal = {Science of The Total Environment},
volume = {579},
pages = {957-965},
year = {2017},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2016.10.086},
url = {https://www.sciencedirect.com/science/article/pii/S0048969716322604},
author = {Rocco Scolozzi and Davide Geneletti},
keywords = {System thinking, Future studies, Climate change, System dynamics, Anticipatory system, Transdisciplinary},
abstract = {Climate change research and action counteracting it affect everyone and would involve cross-societal transformations reshaping the anthroposphere in its entirety. Scrutinizing climate-related science and policies, we recognize attempts to steer the evolution of climate according to expected (or modelled) futures. Such attempts would turn the anthroposphere into a large “anticipatory system”, in which human society seeks to anticipate and, possibly, to govern climate dynamics. The chief aim of this discussion paper is to open a critical debate on the climate change paradigm (CCP) drawing on a strategic and systemic framework grounded in the concept of anticipatory system sensu Rosen (1991). The proposed scheme is ambitiously intended to turn an intricate issue into a complex but structured problem that is to say, to make such complexity clear and manageable. This framework emerges from concepts borrowed from different scientific fields (including future studies and system dynamics) and its background lies in a simple quantitative literature overview, relying upon a broad level of analysis. The proposed framework will assist researchers and policy makers in thinking of CCP in terms of an anticipatory system, and in disentangling its interrelated (and sometimes intricate) aspects. In point of fact, several strategic questions related to CCP were not subjected to an adequate transdisciplinary discussion: what are the interplays between physical processes and social-political interventions, who is the observer (what he/she is looking for), and which paradigm is being used (or who defines the desirable future). The proposed scheme allows to structure such various topics in an arrangement which is easier to communicate, highlighting the linkages in between, and making them intelligible and open to verification and discussion. Furthermore, ideally developments will help scientists and policy makers address the strategic gaps between the evidence-based climatological assessments and the plurality of possible answers as applied to the geopolitical contingencies.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@article{GLASSMEYER2021100873,
title = {Identifying and supporting teachers’ robust understanding of proportional reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100873},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100873},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000341},
author = {David Glassmeyer and Aaron Brakoniecki and Julie M. Amador},
keywords = {Content knowledge, Knowledge resource, Proportional reasoning, Proportions, Ratio, Teachers},
abstract = {This case study uses the Framework for Teachers’ Robust Understanding of Proportional Reasoning for Teaching (Weiland et al., 2020) to characterize how 51 mathematics teachers solved a comparison proportional problem. We found 50 of the 51 teachers productively drew upon four knowledge resources: (1) proportional situation, (2) ratios as part: part or part: whole, (3) unit rates, and (4) ratio as measure. This study details these and teachers’ less commonly used knowledge resources, as well as counterproductive statements related to the knowledge resources. We analyze the structure of the comparison proportion problem and suggest why teachers drew on particular knowledge resources. Lastly, we highlight how counterproductive statements highlight areas of focus for mathematics teacher educators and extends the operationalizing of the robust proportional reasoning framework for mathematics education researchers.}
}
@article{KIM201716,
title = {A study on metadata structure and recommenders of biological systems to support bio-inspired design},
journal = {Engineering Applications of Artificial Intelligence},
volume = {57},
pages = {16-41},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616301786},
author = {Sun-Joong Kim and Ji-Hyun Lee},
keywords = {Bio-inspired design, Biological system metadata modeling, Knowledge-based system, Recommendation system, Ontology},
abstract = {Bio-inspired design was introduced as an alternative method to encourage breakthrough innovations during design projects by stimulating analogical reasoning and thinking of designers. However, the method did not perform as well as researchers expected because most designers, who are novices in the fields of biology and ecology, cannot infer the proper analogue (i.e. biological system) from nature. To resolve this fundamental problem, a causal model based representation framework for ‘analogical reasoning’ – searching and selecting the biological systems to apply – have been developed. In addition, ontology based repository structures and retrieval systems have been proposed to support ‘analogical thinking’ of designers. Nevertheless, these systematic approaches still restrict the candidates and inevitably lose potential biological systems relevant to the design project, due to the ‘physical relation’ biased problem and the ambiguity of the indexing mechanism of both current representation frameworks and retrieval systems. For example, the causality based support system known as a robust representation framework for a single biological system, stores information of a biological system only by its internal ‘physical relations’ and retrieves biological systetabms only by the physical relevance. However, from the perspective of ecological thinking, the further relatedness of ‘physical, biological, and ecological relations’ composes the holistic concept used to identify an organism in the flow of evolution because the ‘biological and ecological relations’ are also involved in the traits that designers may be interested in. Therefore, the supplementary information for ‘biological and ecological relations’ must be added to index the biological and environmental interactions, and to use the connectivity among entire organisms in the retrieval process. In this research, a causality based holistic representation framework for biological systems and an ‘all-connected’ ontology based repository and retrieval system are developed as a knowledge-based recommendation system to support bio-inspired design. The knowledge-based system we developed allows engineering designers to search and select a particular biological system and extract design strategy without much biological knowledge. This effort provides more opportunities in a bio-inspired design process by adding potential biological systems that might previously not have been considered.}
}