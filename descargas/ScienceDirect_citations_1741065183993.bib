@article{SIGAYRET2022104505,
title = {Unplugged or plugged-in programming learning: A comparative experimental study},
journal = {Computers & Education},
volume = {184},
pages = {104505},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104505},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000768},
author = {Kevin Sigayret and André Tricot and Nathalie Blanc},
keywords = {Elementary education, Improving classroom teaching, Programming and programming languages, Teaching/learning strategies},
abstract = {In recent years, computer programming has reappeared in school curricula with the aim of transmitting knowledge and skills beyond the simple ability to code. However, there are different ways of teaching this subject and very few experimental studies compare plugged-in and unplugged programming learning. The purpose of this study is to highlight the impact of plugged-in or unplugged learning on students' performance and subjective experience. To this end, we designed an experimental study with 217 primary school students divided into two groups and we measured their knowledge of computational concepts, ability to solve algorithmic problem, motivation toward the instruction, self-belief and attitude toward science. The programming sessions were designed to be similar between the two conditions, only the tools were different. Computers and Scratch software were used in the plugged-in group while the unplugged group used paper instructions, pictures, figurines and body movements instead. The results show better learning performance in the plugged-in group. Furthermore, although motivation dropped slightly in both groups, this drop was only significant in the unplugged condition. Gender also seems to be an important factor, as girls exhibit a lower post-test motivation and a lower willingness to pursue their practice in programming outside the school context. However, this effect on motivation was only observable in the plugged-in group which suggests that educational programming software may have a positive but gendered motivational impact.}
}
@article{GAO20222707,
title = {Similarity reductions for a generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation in fluid dynamics},
journal = {Chinese Journal of Physics},
volume = {77},
pages = {2707-2712},
year = {2022},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0577907322001228},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Fluid dynamics, Generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation, Similarity reductions, Symbolic computation},
abstract = {Rather intriguing, the paper Chin. J. Phys. 73 (2021) 600-612 has studied a (3+1)-dimensional B-type Kadomtsev–Petviashvili equation in fluid dynamics, while fluid dynamics has a wide range of applications, including those for geophysics, mechanical engineering, civil engineering, chemical engineering, astrophysics and biology. In this paper, taking into consideration certain nonlinear waves in fluid dynamics, we investigate a generalized variable-coefficient version of the aforementioned equation. Making use of symbolic computation, with respect to the amplitude or elevation of the relevant wave, we construct out two sets of the similarity reductions, which rely on the variable coefficients in the generalized equation.}
}
@article{EDELMANN20181,
title = {Formal studies of culture: Issues, challenges, and current trends},
journal = {Poetics},
volume = {68},
pages = {1-9},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X18301323},
author = {Achim Edelmann and John W. Mohr},
keywords = {Formal study of culture, Cultural matrix approach, Measuring duality, Formalist theorization of culture, Computational hermeneutics},
abstract = {Over the last two decades, the formal study of culture has grown into one of the most exciting, systematic, and dynamic sub-fields in sociology. In this essay, we take stock of recent developments in this field. We highlight four emerging themes: (1) the maturation of the field that has occurred over the last two decades, (2) the rise and formalization of the “cultural matrix” approach to studying culture, (3) the development of various efforts to advance a more formal theory of culture, and (4) the proliferation of Big Data and the development of new kinds of quantitative and computational approaches to the study of culture, including the emergence of a new area focused on “computational hermeneutics.” We conclude by discussing future opportunities, challenges, and questions in formalizing culture.}
}
@article{ZHOU2022100001,
title = {Science in One Health: A new journal with a new approach},
journal = {Science in One Health},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949704322000014},
author = {Xiao-Nong Zhou and Marcel Tanner},
keywords = {One Health, Human health, Animal health, Ecosystem health, Research and implementation science},
abstract = {One Health recognizes the close links and interdependence among human health, animal health and environmental health. With the pandemic of COVID-19 and the risk of many emerging or reemerging infectious diseases of zoonotic nature as well as the spreading antimicrobial resistance, One Health has become one of top concerns globally, as it entails the essential global public health challenges from antimicrobial resistance over zoonoses, to climate change, food security and societal well-being. Research priorities in One Health include the study on interactions of human-animal-plants-nature ecology interface, systems thinking, integrated surveillance and response systems, and the overall One Health governance as part of the global health and sustainability governance. The now launched journal, Science in One Health, aims to be a resource platform that disseminates scientific evidence, knowledge, and tools on the One Health approaches and respective possible socio-ecological interventions. Thus, aims at providing fruitful exchanges of information and experience among researchers, and decision makers as well as public health actors.}
}
@article{FISCHER199721,
title = {Computational environments supporting creativity in the context of lifelong learning and design},
journal = {Knowledge-Based Systems},
volume = {10},
number = {1},
pages = {21-28},
year = {1997},
note = {Information Technology Support for Creativity},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(97)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S0950705197000105},
author = {Gerhard Fischer and Kumiyo Nakakoji},
keywords = {Creativity support, Domain-oriented design environments (DODEs), Lifelong-learning},
abstract = {Much of our intelligence and creativity results from the collective memory of communities of practice and of the artifacts and technology surrounding them. Rather than studying individual creativity in isolation, we have developed a conceptual framework of creativity in the context of everyday practice — where design activities prevail and learning is constantly required. The conceptual framework explores new role distributions between people and computers based on theories that view design as reflection-in-action and breakdowns as opportunities for learning and creativity. We use an example from the domain of multimedia information design to illustrate how creativity is supported by domain-oriented design environments. The paper describes the mechanisms, architectures and processes underlying these environments.}
}
@article{VARGASCARPINTERO2025120104,
title = {Development of an integrated multi-criteria framework to assess the implementation potential of biobased value chains and webs with a territorial approach},
journal = {Industrial Crops and Products},
volume = {223},
pages = {120104},
year = {2025},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2024.120104},
url = {https://www.sciencedirect.com/science/article/pii/S0926669024020818},
author = {Ricardo Vargas-Carpintero},
keywords = {Biobased value chain, Biobased value web, Biorefinery, Territorial bioeconomy system, Multi-criteria assessment, Land-based bioeconomy},
abstract = {Biobased value chains and webs (BVCW) encompass value adding activities and actors from biomass production, its processing into biobased products for manifold sectors, until their commercialization and use. BVCW are part of territorial bioeconomy systems and are shaped by contextual settings. The design and development of BVCW involve strategic decisions towards their sustainable implementation. Throughout the design and development of BVCW, the adoption of an integral approach that links technical aspects of biomass-to-product pathways with non-technical aspects and context factors is necessary to increase the BVCW implementation potential. Accordingly, an active incorporation of the territorial context of BVCW in the design process is required. In view of these requirements, in this study an integrated, multi-criteria framework is developed to assess the implementation potential in BVCW design. For this purpose, key elements from existing biorefinery and biomass supply chain design methodologies are identified and integrated in a multi-criteria framework that allows the consideration of both an internal and external perspective of the BVCW in relation to the context. The conceptualized framework serves as an evaluation approach to check the implementability of biomass-to-product pathways BVCW configurations in form of by means of a multi-criteria catalogue. The set of criteria integrates relevant aspects for the design and development of BVCW from land-based biomass (e.g. crops and crop residues). It entails key criteria related to the functionality of the biomass-to-product pathway in technical-economic terms and the surrounding biophysical, social and economic context. The further operationalization of the multi-criteria catalogue by means of an indicator-based assessment could enable the prioritization and selection of BVCW configurations with best implementation potential. In this way, the framework provides a practical approach for decision-makers, local actors and researchers involved in the design and development of BVCW tailored to the territorial context.}
}
@article{EDELSON2021100986,
title = {How fuzzy-trace theory predicts development of risky decision making, with novel extensions to culture and reward sensitivity},
journal = {Developmental Review},
volume = {62},
pages = {100986},
year = {2021},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2021.100986},
url = {https://www.sciencedirect.com/science/article/pii/S0273229721000411},
author = {Sarah M. Edelson and Valerie F. Reyna},
keywords = {Risk-taking, Risky decision making, Reward sensitivity, COVID-19, Fuzzy-trace theory, Adolescence},
abstract = {Comprehensive meta-analyses of risky decision making in children, adolescents, and adults have revealed that age trends in disambiguated laboratory tasks confirmed fuzzy-trace theory’s prediction that preference for risk decreases monotonically from childhood to adulthood. These findings are contrary to predictions of dual systems or neurobiological imbalance models. Assumptions about increasing developmental reliance on mental representations of the gist of risky options are essential to account for this developmental trend. However, dual systems theory appropriately emphasizes how cultural context changes behavioral manifestation of risk preferences across age and neurobiological imbalance models appropriately emphasize developmental changes in reward sensitivity. All of the major theories include the assumption of increasing behavioral inhibition. Here, we integrate these theoretical constructs—representation, cultural context, reward sensitivity, and behavioral inhibition—to provide a novel framework for understanding and improving risky decision making in youth. We also discuss the roles of critical tests, scientific falsification, disambiguating assessments of psychological and neurological processes, and the misuse of such concepts as ecological validity and reverse inference. We illustrate these concepts by extending fuzzy-trace theory to explain why youth are a major conduit of viral infections, including the virus that causes COVID-19. We conclude by encouraging behavioral scientists to embrace new ways of thinking about risky decision making that go beyond traditional stereotypes about adolescents and that go beyond conceptualizing ideal decision making as trading off degrees of risk and reward.}
}
@article{GABRIEL2008330,
title = {A friend is a present you give to your “Self”: Avoidance of intimacy moderates the effects of friends on self-liking},
journal = {Journal of Experimental Social Psychology},
volume = {44},
number = {2},
pages = {330-343},
year = {2008},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022103107001126},
author = {Shira Gabriel and Mauricio Carvallo and Lisa M. Jaremka and Brooke Tippin},
keywords = {The self, Social comparison, Friendship, Avoidance of intimacy, Attachment style},
abstract = {The current research proposes that thinking about friends improves feelings about the self and does so differentially depending on avoidance of intimacy. Based on previous findings that individuals who avoid intimacy in relationships (avoidant individuals) contrast their self-concepts with primed friends whereas those who pursue intimacy in relationships (non-avoidant individuals) assimilate their self-concepts to primed friends [Gabriel, S., Carvallo, M., Dean, K., Tippin, B. D., & Renaud, J. (2005). How I see “Me” depends on how I see “We”: The role of avoidance of intimacy in social comparison. Personality and Social Psychology Bulletin, 31, 156–157], we predicted that friends who embody negative aspects of self would lead avoidant individuals to like themselves more, whereas friends who embody positive aspects of self would lead non-avoidant individuals to like themselves more. A pretest determined that good friends were seen as more similar to positive and ideal aspects of the self, whereas friends about whom participants had more mixed feelings (ambivalent friends) were seen as more similar to disliked and feared aspects of the self. Four experiments supported the main hypotheses. In Experiment 1, non-avoidant individuals like themselves more when good friends were primed. In Experiment 2, avoidant individuals like themselves more when ambivalent friends were primed. In Experiment 3, non-avoidant individuals liked themselves better after thinking about a friend’s positive traits, whereas avoidant individuals liked themselves better after thinking about a friend’s negative traits. In Experiment 4, all individuals under self-esteem threat strategically brought friends to mind who would help them like themselves more.}
}
@article{LAVALLEY2024108825,
title = {Transdiagnostic failure to adapt interoceptive precision estimates across affective, substance use, and eating disorders: A replication and extension of previous results},
journal = {Biological Psychology},
volume = {191},
pages = {108825},
year = {2024},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2024.108825},
url = {https://www.sciencedirect.com/science/article/pii/S030105112400084X},
author = {Claire A. Lavalley and Navid Hakimi and Samuel Taylor and Rayus Kuplicki and Katherine L. Forthman and Jennifer L. Stewart and Martin P. Paulus and Sahib S. Khalsa and Ryan Smith},
keywords = {Interoception, Depression, Anxiety, Substance use, Eating disorders, Precision, Priors, Bayesian perception, Computational modeling},
abstract = {Recent Bayesian theories of interoception suggest that perception of bodily states rests upon a precision-weighted integration of afferent signals and prior beliefs. In a previous study, we fit a computational model of perception to behavior on a heartbeat tapping task to test whether aberrant precision-weighting could explain misestimation of cardiac states in psychopathology. We found that, during an interoceptive perturbation designed to amplify afferent signal precision (inspiratory breath-holding), healthy individuals increased the precision-weighting assigned to ascending cardiac signals (relative to resting conditions), while individuals with anxiety, depression, substance use disorders, and/or eating disorders did not. In this pre-registered study, we aimed to replicate and extend our prior findings in a new transdiagnostic patient sample (N = 285) similar to the one in the original study. As expected, patients in this new sample were also unable to adjust beliefs about the precision of cardiac signals – preventing the ability to accurately perceive changes in their cardiac state. Follow-up analyses combining samples from the previous and current study (N = 719) also afforded power to identify group differences between narrower diagnostic categories, and to examine predictive accuracy when logistic regression models were trained on one sample and tested on the other. With this confirmatory evidence in place, future studies should examine the utility of interoceptive precision measures in predicting treatment outcomes and test whether these computational mechanisms might represent novel therapeutic targets.}
}
@article{CHASTAIN200083,
title = {Cultivating design competence: online support for beginning design studio},
journal = {Automation in Construction},
volume = {9},
number = {1},
pages = {83-91},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00053-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000539},
author = {Thomas Chastain and Ame Elliott},
abstract = {A primary lesson of a beginning design studio is the development of a fundamental design competence. This entails acquiring skills of integration, projection, exploration, as well as critical thinking—forming the basis of thinking “like a designer”. Plaguing the beginning architectural design student as she develops this competence are three typical problems: a lagging visual intelligence, a linking of originality with creativity, and the belief that design is an act of an individual author instead of a collaborative activity. We believe that computation support for design learning has particular attributes for helping students overcome these problems. These attributes include its inherent qualities for visualization, for explicitness, and for sharing. This paper describes five interactive multi-media exercises exploiting these attributes which were developed to support a beginning design studio. The paper also reports how they have been integrated into the course curriculum. Le développement des compétences en design: support on-line pour le studio de design élémentaire Une des premières leçons lors du studio de design est le développement d’une compétence fondamentale en conception. Ceci implique l’acquisition des habiletés d’intégration, de projection, d’exploration ainsi que la pensée critique—antérieurement les bases de la façon de penser nommée “comme un concepteur”. Il y a trois problèmes fondamentaux qui pèsent sur l’étudiant débutant en architecture lors du développement cette compétence: une intelligence visuelle insuffisante, le fait de lier l’originalité à la créativité, et la croyance que le processus de conception est une activité individuelle, plutôt que collaborative. Nous sommes de l’avis que le soutien en informatique lors de l’apprentissage de la conception architecturale posséde des attributs bien particuliers pour aider les étudiants à surmonter ces difficultés. Ces attributs comprennent des qualités inhérentes pour la visualisation, pour être explicite, et pour le partage. Ce papier décrit cinq exercices de médias interactifs qui exploitent ces attributs, et qui ont été développés pour supporter un studio de design élémentaire. Il présente aussi un reportage sur la façon dont ces exercices ont été intégrés dans le curriculum du cours.}
}
@incollection{CARPENDALE2013125,
title = {Chapter Six - A Relational Developmental Systems Approach to Moral Development},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {125-153},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000063},
author = {Jeremy I.M. Carpendale and Stuart I. Hammond and Sherrie Atwood},
keywords = {Developmental systems theory, Moral development, Moral norms, Nativism, Social interaction},
abstract = {Morality and cooperation are central to human life. Psychological explanations for moral development and cooperative behavior will have biological and evolutionary dimensions, but they can differ radically in their approach to biology. In particular, many recent proposals have pursued the view that aspects of morality are innate. We briefly review and critique two of these claims. In contrast to these nativist assumptions about the role of biology in morality, we present an alternative approach based on a relational developmental systems view of moral development. The role for biology in this approach is in setting up the conditions—the developmental system—in which forms of interaction and later forms of thinking emerge.}
}
@article{NG2024100090,
title = {Using cospaces in augmented reality digital story creation: A thematic analysis},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100090},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000400},
author = {Davy Tsz Kit Ng and Wan Yee Winsy Lai and Morris Siu-yung Jong and Chi Wui Ng},
keywords = {Digital storytelling, CoSpaces, Online community, Augmented reality, Language learning},
abstract = {With the digital affordances of augmented reality (AR) technologies, research has shown their value for contextualized, interactive and collaborative language learning through supporting real-world immersion. In recent years, CoSpaces has been a popular AR learning tool with an extensive library of 3D models and constructive gadgets, as well as a visual programming platform. With this tool, students can create projects of digital stories by building personalized AR artifacts, scenes, and storylines, and then share their projects in a dynamic and global community of children. This study examined the characteristics of 39 selected CoSpaces’ open projects via thematic analysis and categorization into five learning contexts: (1) art, history, culture and design, (2) STEM, (3) classroom English and everyday communication, (4) fairy tale/literature, and (5) campus tour. Furthermore, this study identified six language learning competencies derived from digital story creation: (1) discovering knowledge, (2) connecting to prior experience and knowledge, (3) conducting research, (4) problem-solving, (5) expressing and creating digitally, as well as (6) presenting, appreciating and evaluating. Digital literacy refers to the ability to use technology to find, evaluate, create, and communicate information. In addition, three major types of digital literacy skills necessary for AR digital storytelling processes have been identified, encompassing digital creativity, technoligcal proficiency, and research skills. Our results contribute to discovering educational values in developing digital language competency through AR digital story creation. Recommendations are offered for future research and for educators to design appropriate AR learning experiences.}
}
@article{CHANG2023101823,
title = {Stakeholder requirement evaluation of smart industrial service ecosystem under Pythagorean fuzzy environment for complex industrial contexts: A case study of renewable energy park},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101823},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101823},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002816},
author = {Yuan Chang and Xinguo Ming and Zhihua Chen and Tongtong Zhou and Xiaoqiang Liao and Wenyan Song},
keywords = {Smart industrial product-service system (IPS), Requirement evaluation, Service ecosystem, Pythagorean fuzzy sets, Multi-criteria decision making, Viable systems model},
abstract = {This study focuses on ways to systematically evaluate stakeholder requirements when developing a smart industrial service ecosystem (SISE) in a complex industrial context. The SISE development requires considering the service requirement from both the complex industrial context and service ecosystem manners. This study proposes a systematic framework for stakeholder requirement evaluation in SISE. The first part of the framework is the industrial context-viable system model with ecological thinking (IC-VESM) to elicit the service requirements for the SISE, which facilitates a systematic analysis of the service value proposition and service requirement elicitation in the operational lifecycle of an entire industrial context. This second part of the framework proposes a method for evaluating service requirements that is both feasible and systematic. This is achieved by combining the Fuzzy Kano and AHP methods in a Pythagorean fuzzy (PF) environment. The PF Kano computes the categories and determines the weights of service requirements from a consumer perspective, while the PF AHP hierarchically analyzes the service requirements and provides pairwise comparison paths for design experts. Finally, an illustrative case study in a renewable energy context was used to demonstrate the feasibility and effectiveness of the methodology. The proposed theoretical model provides more reliable and systematic outcomes than traditional methods when eliciting service requirements and evaluating complex smart industrial service solutions. The study has practical implications by providing useful insights for companies to recognize key smart service requirements in complex industrial contexts and to improve sustainable development.}
}
@article{RASANAN2024857,
title = {Beyond discrete-choice options},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {9},
pages = {857-870},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400175X},
author = {Amir Hosein Hadian Rasanan and Nathan J. Evans and Laura Fontanesi and Catherine Manning and Cynthia Huang-Pollock and Dora Matzke and Andrew Heathcote and Jörg Rieskamp and Maarten Speekenbrink and Michael J. Frank and Stefano Palminteri and Christopher G. Lucas and Jerome R. Busemeyer and Roger Ratcliff and Jamal Amani Rad},
abstract = {While decision theories have evolved over the past five decades, their focus has largely been on choices among a limited number of discrete options, even though many real-world situations have a continuous-option space. Recently, theories have attempted to address decisions with continuous-option spaces, and several computational models have been proposed within the sequential sampling framework to explain how we make a decision in continuous-option space. This article aims to review the main attempts to understand decisions on continuous-option spaces, give an overview of applications of these types of decisions, and present puzzles to be addressed by future developments.}
}
@article{CALUDE2023844,
title = {What perceptron neural networks are (not) good for?},
journal = {Information Sciences},
volume = {621},
pages = {844-857},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013743},
author = {Cristian S. Calude and Shahrokh Heidari and Joseph Sifakis},
keywords = {Perceptrons, Sensitive and robust functions, Quantum computing},
abstract = {Perceptron Neural Networks (PNNs) are essential components of intelligent systems because they produce efficient solutions to problems of overwhelming complexity for conventional computing methods. Many papers show that PNNs can approximate a wide variety of functions, but comparatively, very few discuss their limitations and the scope of this paper. To this aim, we define two classes of Boolean functions – sensitive and robust –, and prove that an exponentially large set of sensitive functions are exponentially difficult to compute by multi-layer PNNs (hence incomputable by single-layer PNNs). A comparatively large set of functions in the second one, but not all, are computable by single-layer PNNs. Finally, we used polynomial threshold PNNs to compute all Boolean functions with quantum annealing and present in detail a QUBO computation on the D-Wave Advantage. These results confirm that the successes of PNNs, or lack of them, are in part determined by properties of the learned data sets and suggest that sensitive functions may not be (efficiently) computed by PNNs.}
}
@article{GREENE201766,
title = {The rat-a-gorical imperative: Moral intuition and the limits of affective learning},
journal = {Cognition},
volume = {167},
pages = {66-77},
year = {2017},
note = {Moral Learning},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717300690},
author = {Joshua D. Greene},
keywords = {Deontology, Utilitarianism, Consequentialism, Reinforcement learning, Model-free learning, Machine learning, Ethics, Normative ethics, Moral judgment},
abstract = {Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I’ve argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions.}
}
@article{WARD202154,
title = {On value-laden science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {54-62},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301783},
author = {Zina B. Ward},
keywords = {Values, Values in science, Argument from inductive risk, Motivating reasons, Justifying reasons},
abstract = {Philosophical work on values in science is held back by widespread ambiguity about how values bear on scientific choices. Here, I disambiguate several ways in which a choice can be value-laden and show that this disambiguation has the potential to solve and dissolve philosophical problems about values in science. First, I characterize four ways in which values relate to choices: values can motivate, justify, cause, or be impacted by the choices we make. Next, I put my proposed taxonomy to work, using it to clarify one version of the argument from inductive risk. The claim that non-epistemic values must play a role in scientific choices that run inductive risk makes most sense as a claim about values being needed to justify such choices. The argument from inductive risk is not unique: many philosophical arguments about values in science can be more clearly understood and assessed by paying close attention to how values and choices are related.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{SPARAPANI2023102186,
title = {Factors associated with classroom participation in preschool through third grade learners on the autism spectrum},
journal = {Research in Autism Spectrum Disorders},
volume = {105},
pages = {102186},
year = {2023},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2023.102186},
url = {https://www.sciencedirect.com/science/article/pii/S1750946723000867},
author = {Nicole Sparapani and Nancy Tseng and Laurel Towers and Sandy Birkeneder and Sana Karimi and Cameron J. Alexander and Johanna Vega Garcia and Taffeta Wood and Amanda Dimachkie Nunnally},
keywords = {Autism, Instructional opportunities, Mathematical tasks, Teacher language, Active engagement, Spontaneous communication},
abstract = {Background
Access to mathematics instruction that involves opportunities for critical thinking and procedural fluency promotes mathematics learning. Studies have outlined effective strategies for teaching mathematics to children on the autism spectrum, however, the focus of these interventions often represent a narrow set of mathematical skills and concepts centered on procedural learning without linking ideas to underlying concepts.
Methods
This study utilized classroom video observations to evaluate the variability in and nature of mathematical learning opportunities presented to 76 autistic students within 49 preschool–3rd grade general and special education learning contexts. We examined teacher instructional practices and student participation across 109 mathematical tasks within larger mathematics lessons.
Results
Students were most often presented with mathematical tasks that required low-level cognitive demand, such as tasks focusing on rote memorization and practicing predetermined steps to solve basic algorithms. Furthermore, the nature of the mathematical task was linked with the language that teachers used, and this in turn, was associated with students’ participation within the learning opportunity.
Conclusions
Our findings indicate that features of talk within specific types of mathematical tasks, including math-related talk and responsive language, were associated with increased student active engagement and spontaneous communication. The knowledge gained from this study contributes to the development of optimized instructional practices for school-aged children on the autism spectrum—information that could be used to prepare both preservice and in-service teachers.}
}
@article{YANG2024109519,
title = {Global optimization strategy of prosumer data center system operation based on multi-agent deep reinforcement learning},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109519},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109519},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao and Xue Tian},
keywords = {Data center system, Global cooperative optimization, D3QN, VDN},
abstract = {The escalating issues of high energy consumption and carbon emissions in data centers (DCs) necessitate the optimization of system operations. However, early optimization strategies were overly simplistic and lacked automated updating and iterative capabilities. With the evolution of artificial intelligence (AI), researchers have applied deep reinforcement learning (DRL) algorithms to system operations. However, the optimization focus has been limited to the internal systems, lacking global optimization. In this paper, a global optimization control strategy based on the Dueling double-deep Q network (D3QN) and value decomposition network (VDN) algorithms is proposed to make the DCs system operate more closely with the upstream, midstream, and downstream. By adjusting battery charging/discharging capacity, computational workload, and waste heat utilization heating temperature global synergistic optimization is achieved. Compared with without optimization, renewable energy waste, operation cost, total electricity consumption, and grid electricity consumption are reduced by 18.37%, 9.78%, 4.01%, and 29.74%, respectively. Additionally, a detailed comparison between non-algorithmic optimization and algorithmic optimization is provided, offering valuable insights for substantial energy savings and emissions reduction in DCs. The results demonstrate the importance of fully exploring the interactive potential between upstream energy supply, midstream computational workload, and downstream waste heat recovery to achieve synergistic global optimization of “computing power", “thermal energy" and “electrical energy" for the sustainable and green development of DCs or other prosumer buildings.}
}
@article{MORAWSKI200231,
title = {Are measurement-oriented courses getting too difficult for Polish students?},
journal = {Measurement},
volume = {32},
number = {1},
pages = {31-38},
year = {2002},
issn = {0263-2241},
doi = {https://doi.org/10.1016/S0263-2241(01)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0263224101000537},
author = {Roman Z Morawski},
keywords = {Measurement, Abstract thinking, Experimentation skills, Teaching methodology},
abstract = {The measurement is assumed to be the most reliable means of acquiring information on physical reality; consequently, measurement science and technology is of fundamental importance for all the branches of engineering which have to deal with real-world objects and phenomena. Unfortunately, the ability of today’s students of engineering to grasp basic ideas of measurement science and to master basic skills related to measurement technology seems to be seriously endangered, inter alia, by the omnipresence of computer-related topics in engineering curricula. Paradoxically, it is also endangered by some cultural changes that undermine the historically established role of abstract thinking in the development of Latin civilisation. Educators cannot avoid the question: what kind of remedial measures should be undertaken? This paper aims to contribute to better understanding of various difficulties the teachers of measurement-related courses are facing today.}
}
@article{PIETARINEN2025105410,
title = {Synechism 2.0: Contours of a new theory of continuity in bioengineering},
journal = {BioSystems},
volume = {250},
pages = {105410},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105410},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725000206},
author = {Ahti-Veikko Pietarinen and Vera Shumilina},
keywords = {Charles S. Peirce, Synechism, Collective agency, Synthetic intelligence, Michael E. Levin, Bioengineering, Bioelectricity},
abstract = {The methodological principle of synechism, the all-pervading continuity first proposed by Charles Peirce in 1892, is reinvigorated in the present paper to prompt a comprehensive reevaluation of the integrated concepts of life, machines, agency, and intelligence. The evidence comes from the intersections of synthetic bioengineering, developmental biology, and cognitive and computational sciences. As a regulative principle, synechism, “that continuity governs the whole domain of experience in every element of it”, has been shown to infiltrate fundamental issues of contemporary biology, including cognition in different substrates, embodied agency, collectives (swarm and nested), intelligence on multiple scales, and developmental bioelectricity in morphogenesis. In the present paper, we make explicit modern biology's turn to this fundamental feature of science in its rejection of conceptual binaries, preference for collectives over individuals, quantitative over qualitative, and multiscale applicability of the emerging hypotheses about the integration of the first principles of the diversity of life. Specifically, synechism presents itself as the bedrock for research encompassing biological machines, chimaeras, organoids, and Xenobots. We then review a synechistic framework that embeds functionalist, information-theoretic, pragmaticist and inferentialist approaches to springboard to continuum-driven biosystemic behaviour.}
}
@article{WANG20071997,
title = {DIANA: A computer-supported heterogeneous grouping system for teachers to conduct successful small learning groups},
journal = {Computers in Human Behavior},
volume = {23},
number = {4},
pages = {1997-2010},
year = {2007},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2006.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563206000094},
author = {Dai-Yi Wang and Sunny S.J. Lin and Chuen-Tsai Sun},
keywords = {Cooperative learning, Small-group learning, Computer assisted grouping system, Group composition, Thinking styles, University students},
abstract = {Teachers interested in small-group learning can benefit from using psychological factors to create heterogeneous groups. In this paper we describe a computer-supported grouping system named DIANA that uses genetic algorithms to achieve fairness, equity, flexibility, and easy implementation. Grouping was performed so as to avoid the creation of exceptionally weak groups. We tested DIANA with 66 undergraduate computer science students assigned to groups of three either randomly (10 groups) or using an algorithm reflecting [Sternberg, R. J. (1994). Thinking styles: theory and assessment at the interface between intelligence and personality. In R. J. Sterberg, & P. Ruzgis (Eds.), Personality and Intelligence (pp. 169–187). New York: Cambridge University Press.] three thinking styles (12 groups). The results indicate that: (a) the algorithm-determined groups were more capable of completing whatever they were “required to do” at a statistically significant level, (b) both groups were equally capable of solving approximately 80% of what they “chose to do,” and (c) the algorithm-determined groups had smaller inter-group variation in performance. Levels of satisfaction with fellow group member attitudes, the cooperative process, and group outcomes were also higher among members of the algorithm-determined groups. Suggestions for applying computer-supported group composition systems are offered.}
}
@article{ROBINSON20231189,
title = {Opportunities and challenges for microbiomics in ecosystem restoration},
journal = {Trends in Ecology & Evolution},
volume = {38},
number = {12},
pages = {1189-1202},
year = {2023},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534723002112},
author = {Jake M. Robinson and Riley Hodgson and Siegfried L. Krauss and Craig Liddicoat and Ashish A. Malik and Belinda C. Martin and Jakki J. Mohr and David Moreno-Mateos and Miriam Muñoz-Rojas and Shawn D. Peddle and Martin F. Breed},
keywords = {ecosystem restoration, microbiome, microbiomics, metagenomics, restoration ecology, innovation},
abstract = {Microbiomics is the science of characterizing microbial community structure, function, and dynamics. It has great potential to advance our understanding of plant–soil–microbe processes and interaction networks which can be applied to improve ecosystem restoration. However, microbiomics may be perceived as complex and the technology is not accessible to all. The opportunities of microbiomics in restoration ecology are considerable, but so are the practical challenges. Applying microbiomics in restoration must move beyond compositional assessments to incorporate tools to study the complexity of ecosystem recovery. Advances in metaomic tools provide unprecedented possibilities to aid restoration interventions. Moreover, complementary non-omic applications, such as microbial inoculants and biopriming, have the potential to improve restoration objectives by enhancing the establishment and health of vegetation communities.}
}
@article{BAKER2021101933,
title = {Who is marginalized in energy justice? Amplifying community leader perspectives of energy transitions in Ghana},
journal = {Energy Research & Social Science},
volume = {73},
pages = {101933},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621000268},
author = {Erin Baker and Destenie Nock and Todd Levin and Samuel A. Atarah and Anthony Afful-Dadzie and David Dodoo-Arhin and Léonce Ndikumana and Ekundayo Shittu and Edwin Muchapondwa and Charles Van-Hein Sackey},
abstract = {There is a divide in energy access studies, between technologically-focused modeling papers in engineering and economics, and energy justice frameworks and principles grounded in social sciences. Quantitative computational models are necessary when analyzing energy, and more specifically electricity, systems, as they are technologically-complex systems that can diverge from intuitive patterns. To assure energy justice, these models must be reflective of, and informative to, a wide range of stakeholders, including households and communities alongside utilities, governments, and others. Yet, moving from a qualitative understanding of preferences to quantitative modeling is challenging. In this perspective piece, we pilot the use of the value-focused thinking framework to inform stakeholder engagement. The result is a strategic objective hierarchy that highlights the tradeoffs and the social, economic and technological factors that need to be measured in models. We apply the process in Ghana, using a survey, stakeholder workshops, and follow-up interviews to uncover key tradeoffs and stakeholder-derived objectives. We discuss three key areas that have been rarely, if ever, well-represented in energy models: (1) the relationship between the dynamics of electricity end-use and the technology and economic structure of the system; (2) explicit tradeoffs between electricity access, cost, and reliability as defined by stakeholders; and (3) the definition of new objectives, such as minimizing hazards related to theft. We conclude that this model of engagement provides an opportunity to tie together rigorous qualitative analysis and stakeholder engagement with crucial quantitative models of the electricity system.}
}
@incollection{KERN202469,
title = {Chapter 5 - The turbinates—an overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {69-96},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000056},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Acetylcholine, secondary atrophic rhinitis, autonomic nervous system, turbinate anatomy, middle turbinate anatomy, inferior turbinate anatomy, capacitance vessels (sinusoids), “diffusor function,” functional residual capacity of the nose (FRCn), “,” hypertrophy (increase in cell ), hyperplasia (increase in cell ), nasal cycle, nasal obstruction, on-urgical urbinate eduction djunctive rocedure (n-sTRAP), out-fracture (lateralization), squamous metaplasia, submucous resection, ozaena, “resistor function,” total inferior turbinectomy, turbinates, turbinectomy, turbinoplasty, acoustic rhinometry, and rhinomanometry},
abstract = {This chapter presents an overview of the turbinates. To the best of our knowledge, it was a New Yorker, William M. Jarvis, MD, who in 1882 described three cases of utilizing a snare to affect a partial turbinectomy. At the dawn of the 20th century, most surgeons were promoting total inferior turbinectomy not only for nasal airway obstruction but astoundingly also for hearing impairment and tinnitus. The turbinate enlargement or “hypertrophy” is neither the cause nor a complication of hearing loss. Fortunately, and for the most part, dazed blunders and egregious errors in thinking by esteemed experts, for the most part, have remedied itself through scientific studies, since the late 19th century. This chapter traces the more than a century long history of turbinate thinking and surgery offering both sides of the turbinate debate in their “own words.” All the various procedures used to reduce the inferior turbinate are presented. To be as fair minded as possible, numerous authors are quoted, spanning more than one hundred years; some observed and reported the serious adverse effects of aggressive turbinate surgery, pleading for a conservative approach to inferior turbinate surgical intervention, while others claimed that turbinectomy was without any serious sequelae which is challenged by the facts.}
}
@incollection{ZHUGE2016149,
title = {15 - Limitations and challenges},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {149-151},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000159},
author = {Hai Zhuge},
keywords = {Summarization, limitations, challenges, representations, computing},
abstract = {The limitation of summarisation lies in the natural differences between human and machine, between languages, and between the ways of observation and thinking of authors and those of readers. The significant evolution of documents in form and function in cyber-physical society challenges the paradigm of summarization research.}
}
@article{ROMAN1992161,
title = {Pavane: a system for declarative visualization of concurrent computations},
journal = {Journal of Visual Languages & Computing},
volume = {3},
number = {2},
pages = {161-193},
year = {1992},
issn = {1045-926X},
doi = {https://doi.org/10.1016/1045-926X(92)90014-D},
url = {https://www.sciencedirect.com/science/article/pii/1045926X9290014D},
author = {Gruia-Catalin Roman and Kenneth C Cox and C.Donald Wilcox and Jerome Y Plun},
abstract = {This paper describes the conceptual model, specification method and visualization methodology for Pavane—a visualization environment concerned with exploring, monitoring and presenting concurrent computations. The underlying visualization model is declarative in the sense that visualization is treated as a mapping from program states to a three-dimensional world of geometric objects. The latter is rendered in full color and may be examined freely by a viewer who is allowed to navigate through the geometric world. The state-to-geometry mapping is defined as a composition of several simpler mappings. The choice is determined by methodological and architectural considerations. This paper shows how this decomposition was molded by two methodological objectives: (1) the desire visually to capture abstract formal properties of programs (e.g. safety and progress) rather than operational details; and (2) the need to support complex animations of atomic computational events. All mappings are specified using a rule-based notation; rules may be added, deleted and modified at any time during the visualization. An algorithm for termination detection in diffusing computations is used to illustrate the specification method and to demonstrate its conceptual elegance and flexibility. A concurrent version of a popular artificial intelligence program provides a vehicle for demonstrating how we derive graphical representations and animation scenarios from key formal properties of the program, i.e. from those safety and progress assertions about the program which turn out to be important in verifying its correctness.}
}
@incollection{KRINGELBACH2019139,
title = {24 - Whole-brain modeling of neuroimaging data: Moving beyond correlation to causation},
editor = {Amir Raz and Robert T. Thibault},
booktitle = {Casting Light on the Dark Side of Brain Imaging},
publisher = {Academic Press},
pages = {139-143},
year = {2019},
isbn = {978-0-12-816179-1},
doi = {https://doi.org/10.1016/B978-0-12-816179-1.00024-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161791000244},
author = {Morten L. Kringelbach and Gustavo Deco},
keywords = {Whole-brain modeling, neuroimaging, causative mechanisms},
abstract = {Neuroimaging has offered an unprecedented window on human brain activity. While this advance has led to great expectations, many neuroscientists have grown increasingly frustrated with the lack of causal insights that this technique has provided into human brain function, in turn, leading to heated discussions on the potential rise of neophrenology. Elsewhere in this book, you can read about the apparent failure of brain imaging to tell us much new or meaningful about thinking and cognition in general. Such views are true to a certain extent; brain imaging often takes indirect measures of neural activity such as blood flow and, just because such brain measures correlate with behavioral output, does not mean that they cause the output. But, these new tools do measure important information about brain activity that could potentially tell us a great deal about brain and mind.}
}
@article{BAMBINI2022106196,
title = {It is time to address language disorders in schizophrenia: A RCT on the efficacy of a novel training targeting the pragmatics of communication (PragmaCom)},
journal = {Journal of Communication Disorders},
volume = {97},
pages = {106196},
year = {2022},
issn = {0021-9924},
doi = {https://doi.org/10.1016/j.jcomdis.2022.106196},
url = {https://www.sciencedirect.com/science/article/pii/S0021992422000156},
author = {Valentina Bambini and Giulia Agostoni and Mariachiara Buonocore and Elisabetta Tonini and Margherita Bechi and Ilaria Ferri and Jacopo Sapienza and Francesca Martini and Federica Cuoco and Federica Cocchi and Luca Bischetti and Roberto Cavallaro and Marta Bosia},
keywords = {Pragmatics, Schizophrenia, Rehabilitation, Concretism, Metaphor, Functioning},
abstract = {Introduction: Language and communication disruptions in schizophrenia are at the center of a large body of investigation. Yet, the remediation of such disruptions is still in its infancy. Here we targeted what is known to be one of the most damaged language domains in schizophrenia, namely pragmatics, by conducting a pragmatics-centered intervention with a randomized controlled trial design and assessing also durability and generalization. To the best of our knowledge, this is the first study with these characteristics. Methods: Inspired by the Gricean account of natural language use, we tailored a novel treatment addressing the pragmatics of communication (PragmaCom) and we tested its efficacy in a sample of individuals with schizophrenia randomized to the experimental group or to an active control group. The primary outcome with respect to the efficacy of the PragmaCom was measured by changes in pragmatic abilities (as evaluated with the global score of the Assessment of Pragmatic Abilities and Cognitive Substrates test) from baseline to 12 weeks and at 3-month follow-up. The secondary outcome was measured by changes in metaphor comprehension, abstract thinking, and global functioning from baseline to 12 weeks and at 3-month follow-up. Results: Relative to the control group, at post-test the PragmaCom group showed greater and enduring improvement in global pragmatic skills and in metaphor comprehension. At follow-up, these improvements persisted and the PragmaCom exerted beneficial effects also on functioning. Conclusions: Despite the limited sample size, we believe that these findings offer initial yet encouraging evidence of the possibility to improve pragmatic skills with a theoretically grounded approach and to obtain durable and clinically relevant benefits. We argue that it is time that therapeutic efforts embrace communicative dysfunctions in order to improve illness outcome.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{YAO2022107747,
title = {Regional attention reinforcement learning for rapid object detection},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107747},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107747},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200057X},
author = {Hongge Yao and Peng Dong and Siyi Cheng and Jun Yu},
keywords = {Regional attention, Reinforcement learning, Object detection, Information fusion, Location and recognition},
abstract = {When people observe a picture, they first pay attention to local areas of the picture, rather than the whole areas, then combine them with previous experience in the brain, and finally make judgments through thinking. This is human visual logic. In this paper, we propose a regional attention reinforcement learning model for object detection. The proposed model uses human visual logical to solve the detection problem of small and complex targets in the picture. The model uses a recurrent network structure as the main framework to extract historical information, and fuse the historical information with the current concerned information. At each recurrent time step, it can pay attention to the fused information, especially pay more attention to the information that may have objects. Experimental results show that the proposed method has more than 5% improved in recognition accuracy to the conventional methods. In terms of FLOPs, the conventional methods normally require 170 M, while the proposed method only needs 25.4M This means that the proposed method has higher detection efficiency.}
}
@incollection{FINI2019161,
title = {Chapter 7 - Sustainable Procurement and Transport of Construction Materials},
editor = {Vivian W.Y. Tam and Khoa N. Le},
booktitle = {Sustainable Construction Technologies},
publisher = {Butterworth-Heinemann},
pages = {161-209},
year = {2019},
isbn = {978-0-12-811749-1},
doi = {https://doi.org/10.1016/B978-0-12-811749-1.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117491000055},
author = {Alireza Ahmadian Fard Fini and Ali Akbarnezhad},
keywords = {Sustainable construction materials, life cycle thinking, procurement and transport, prefabrication technologies},
abstract = {Construction industry is the largest global consumer of materials. This huge share comes with the huge responsibility to account for economic, environmental, and social impacts associated with the materials through adoption of sustainable procurement strategies. Sustainable material procurement requires reconciliation among economic, environmental and social impacts of procurement decisions throughout the life cycle of materials. However, this is challenging mainly due to the broad range of economic, environmental and social impacts associated with different stages of material’s life cycle as well as the overlapping impacts that various supply decisions may have on multiple performance areas. Current practices of material procurement are, on the other hand, predominantly influenced by economy of construction stage and little attention is paid to environmental and social considerations over a long-term horizon. Moreover, material supply decisions made currently in practice are commonly traditional and tend to largely overlook the opportunities made available by advances in material science, computing, and decision-making areas. This chapter starts by presenting an overview of sustainability challenges associated with current material procurement practices to highlight the need for adoption of new sustainable approaches and technologies. It then continues by highlighting the challenges associated with adoption of new approaches and the important sustainability criteria to be considered in selection of new sustainable materials, technologies, and procurement strategies. A comprehensive decision-making framework for identifying the most sustainable procurement options in a construction project among various procurement options available is then presented. The framework is founded on the concepts of life cycle thinking and supply chain structure which are incorporated in to a computational module to compare the life cycle impacts of various supply decision based on the selection criteria determined collaboratively by different project stakeholders. The results of such comparative analysis leads to a ranking of various procurement decision alternatives comprised of different combinations of supply decision including material type, material supply structure, location of supplier, and mode of transport.}
}
@article{NADOLSKI2019210,
title = {Complex systems analysis of hybrid warfare},
journal = {Procedia Computer Science},
volume = {153},
pages = {210-217},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307318},
author = {Molly Nadolski and James Fairbanks},
keywords = {Multi-level modelling, Sociotechnical systems, Complex Systems, Toolsets, Unstructured Spaces, Conceptual Modeling, Quantitative Modeling},
abstract = {Being empowered with the appropriate toolset will enable decision-makers to analyze how best to intervene in ever-changing complex systems. This research project explored deconstructing qualitative methods to identify and document requirements to connect the models to computational social science approaches. Previous efforts from our research provided a customizable toolset that assesses the current and future impact that decisions, policies, or strategies can deliver in a system to tackle particularly complex problems. This paper presents a portion of the research effort that developed a threat analysis framework by establishing formally documented research methods to effectively combine conceptual and computational tools. This enables more accurate, efficient, and foresightful knowledge capture and depictions of a particular problem space. The case that the tools and approach are tested against is Russia’s application of grey zone warfare tools in Moldova.}
}
@article{JACKSON2012370,
title = {Information technology use and creativity: Findings from the Children and Technology Project},
journal = {Computers in Human Behavior},
volume = {28},
number = {2},
pages = {370-376},
year = {2012},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563211002147},
author = {Linda A. Jackson and Edward A. Witt and Alexander Ivan Games and Hiram E. Fitzgerald and Alexander {von Eye} and Yong Zhao},
keywords = {Videogames, Creativity, Children, Technology use},
abstract = {This research examined relationships between children’s information technology (IT) use and their creativity. Four types of information technology were considered: computer use, Internet use, videogame playing and cell phone use. A multidimensional measure of creativity was developed based on Sternberg and Lubart, 1999, Subrahmanyam et al., 2006 test of creative thinking. Participants were 491 12-year olds; 53% were female, 34% were African American and 66% were Caucasian American. Results indicated that videogame playing predicted of all measures of creativity. Regardless of gender or race, greater videogame playing was associated with greater creativity. Type of videogame (e.g., violent, interpersonal) was unrelated to videogame effects on creativity. Gender but not race differences were obtained in the amount and type of videogame playing, but not in creativity. Implications of the findings for future research to test the causal relationship between videogame playing and creativity and to identify mediator and moderator variables are discussed.}
}
@article{NIKIFORIDOU2010795,
title = {Statistical literacy at university level: the current trends},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {795-799},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.236},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810023414},
author = {Zoi Nikiforidou and Aspasia Lekka and Jenny Pange},
keywords = {statistical literacy, Statistics Education},
abstract = {Active and critical citizens, in contemporary information-driven societies, are considered to possess capacities and skills of statistical literacy. There are numerous definitions and descriptions concerning statistical literacy, statistical reasoning and statistical thinking. Thus, all these terms converge to the principle that statistical citizenship develops from school settings and relates mainly to the processes of evaluating, interpreting and communicating data. If these are not acquired on time, then students build up errors and misunderstandings. In the current paper general issues concerning Statistics Education at the University level are addressed and aspects for future research are stressed in terms of technology use, content and pedagogic approaches.}
}
@article{GUO2017677,
title = {Research on Element Importance of Shafting Installation Based on QFD and FMEA},
journal = {Procedia Engineering},
volume = {174},
pages = {677-685},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302059},
author = {Qi Guo and Kuangjie Sheng and Zheng Wang and Xilin Zhang and hengyi Yang and Rui Miao},
keywords = {Quality Function Deployment, Failure Mode and Effects Analysis, Marine Shafting, Comprehensive Importance},
abstract = {Development in today's shipbuilding economy is transforming from the quantitative growth to the quality growth. Quality function deployment (QFD) and failure mode and effects analysis (FMEA) adopt different ways of thinking, they remedy their respective limitations for each other, that can effectively guide the quality control. This paper is combined of HuDong ZhongHua Shipbuilding (group) co. LTD.’s shafting installation process, starting from the QFD customer requirements for finding the importance of production process elements and correction by FMEA, ultimately acquire the comprehensive importance of shafting installation process elements.}
}
@article{SZYJEWSKI20203476,
title = {Future management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3476-3485},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031944X},
author = {Zdzisław Szyjewski},
keywords = {Future management, forecasting the future, new technologies},
abstract = {Accurate forecasting, good identification of trends is the basis of business success. Strategic management methods and techniques that use experience and historical economic data are not adequate to the rapidly changing business environment. In particular, technological changes, and in particular the widespread use of ICT, forces a new approach to management style and changes in the way data is acquired on the basis of which future decisions are made. Innovation thinking, a flexible and dynamic approach to making future-oriented decisions using new technologies are the foundations of future management. Therefore, the aim of the paper is to show the role and position of technology in creating the future.}
}
@incollection{RIVELA202293,
title = {Chapter 6 - Life Cycle Sustainability Assessment-based tools},
editor = {Carmen Teodosiu and Silvia Fiore and Almudena Hospido},
booktitle = {Assessing Progress Towards Sustainability},
publisher = {Elsevier},
pages = {93-118},
year = {2022},
isbn = {978-0-323-85851-9},
doi = {https://doi.org/10.1016/B978-0-323-85851-9.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858519000183},
author = {Beatriz Rivela and Brandon Kuczenski and Dolores Sucozhañay},
keywords = {Life Cycle Thinking, Life Cycle Assessment, Life Cycle Costing, Social Life Cycle Assessment, Life Cycle Sustainability Assessment},
abstract = {This chapter establishes a baseline of ideas of what Life Cycle Thinking means: going beyond the traditional focus, understanding and including the whole environmental, social, and economic implications of decision-making processes to identify potential conflicts, synergies, and trade-offs. The life cycle methodologies for sustainability assessment are described, providing an overview of the tools and criteria currently applied, available software and databases, and ongoing challenges. While Environmental Life Cycle Assessment (LCA) is a consolidated tool, based on the ISO standards, Life Cycle Costing (LCC), the tool aimed at the assessment of the economic domain using a life cycle perspective, has not been widely integrated into sustainability assessment until the last decade. Concerning the social dimension, Social Life Cycle Assessment (S-LCA) is still at an early stage of development, but it is a promising methodology to face the challenge of integrating the social aspects towards a holistic approach to sustainable development.}
}
@article{GRAF2021100836,
title = {A cycle for validating a learning progression illustrated with an example from the concept of function},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100836},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100836},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320301000},
author = {Edith Aurora Graf and Peter W. {van Rijn} and Cheryl L. Eames},
keywords = {Learning progressions, Learning trajectories, Validation, Empirical recovery, Mathematics assessment},
abstract = {A learning progression, or learning trajectory, describes the evolution of student thinking from early conceptions to the target understanding within a particular domain. As a complex theory of development, it requires conceptual and empirical support. In earlier work, we proposed a cycle for the validation of a learning progression with four steps: 1) Theory Development, 2) Examination of Empirical Recovery, 3) Comparison to Competing Models, and 4) Evaluation of Instructional Efficacy. A group of experts met to discuss the application of learning sciences to the design, use, and validation of classroom assessment. Learning progressions, learning trajectories, and how they can support classroom assessment were the main focuses. Revisions to the cycle were suggested. We describe the adapted cycle and illustrate how the first third of it has been applied towards the validation of a learning progression for the concept of function.}
}
@article{DOOLITTLE2019889,
title = {Making Evolutionary Sense of Gaia},
journal = {Trends in Ecology & Evolution},
volume = {34},
number = {10},
pages = {889-894},
year = {2019},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534719301417},
author = {W. Ford Doolittle},
keywords = {Gaia hypothesis, evolution, differential persistence, clade selection},
abstract = {The Gaia hypothesis in a strong and frequently criticized form assumes that global homeostatic mechanisms have evolved by natural selection favoring the maintenance of conditions suitable for life. Traditional neoDarwinists hold this to be impossible in theory. But the hypothesis does make sense if one treats the clade that comprises the biological component of Gaia as an individual and allows differential persistence – as well as differential reproduction – to be an outcome of evolution by natural selection. Recent developments in theoretical and experimental evolutionary biology may justify both maneuvers.}
}
@article{TRAENKLE1994197,
title = {Solving microstructure electrostatistics with MIMD parallel supercomputers and Split-C},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {53},
pages = {197-213},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)85049-6},
url = {https://www.sciencedirect.com/science/article/pii/0377025794850496},
author = {Frank Traenkle and Matthew I. Frank and Mary K. Vernon and Sangtae Kirn},
keywords = {Microstructure electrostatics, Multiple Instruction Multiple Data (MIMD) model, Parallel computational algorithms, Split-C},
abstract = {We consider parallel computational algorithms for the boundary integral solutions of the Laplace equation for use in the simulation of electrorheological fluids and as a model study of a class of elliptic partial differential equations that appear in basic microscopic descriptions of heterogeneous structured continua. The viewpoint is that of constructing large scale simulations that bridge micro- and macro-length and time scales on state-of-the-art parallel supercomputers. Because of long range interactions, fast communications are the key to scalable N-Body algorithms. The communication scheduling strategies of Fuentes and Kim are examined in two contexts on the Thinking Machines CM-5 parallel computer. First, an N-Body simulation implementation in C using the standard send-receive message passing primitives in the CMMD 2.0 library shows that communication scheduling leads to dramatic improvements in computational performance. Second, an implementation in Split-C, which uses highly efficient activemessages to implement shared memory communication, reduces communication overhead by an order of magnitude. Taken together, these two developments portend great promise for the development of efficient large scale simulations using portable, high level parallel programming languages.}
}
@article{AGARWAL1992251,
title = {Computational fluid dynamics on parallel processors},
journal = {Computing Systems in Engineering},
volume = {3},
number = {1},
pages = {251-259},
year = {1992},
note = {High-Performance Computing for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(92)90110-5},
url = {https://www.sciencedirect.com/science/article/pii/0956052192901105},
author = {R.K. Agarwal and J.C. Lewis},
abstract = {Greater computational power is needed for solving computational fluid dynamics problems of interest in engineering design. Parallel computers offer the promise of providing orders of magnitude increases in computational power compared with current uniprocessor vector supercomputers. This paper is mainly concerned with the implementation of a three-dimensional Navier-Stokes code MDNS3D on concurrent computers with grain sizes ranging from fine to coarse. An overview of commercially available parallel machines and the current state of the art in parallel algorithms is presented. The implementation of MDNS3D on machines such as the CRAY Y-MP/8, IBM 3090S, BBN Butterfly II, Intel iPSC/2, Symult 2010, MASPAR, and the Connection Machine CM-2, is described. Particular attention is paid to differences in implementation on SIMD and MIMD architectures. Factors affecting the performance of the code on different architectures are addressed. In addition, user interface and software portability issues are considered for various machines. Finally, future trends in parallel hardware and software development are assessed, and the factors important in determining the most suitable architecture for performing very large scale calculations are discussed.}
}
@article{SMYE2022105015,
title = {Interdisciplinary approaches to metastasis},
journal = {iScience},
volume = {25},
number = {9},
pages = {105015},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105015},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222012871},
author = {Stephen W. Smye and Robert A. Gatenby},
abstract = {Summary
Interdisciplinary research is making a significant contribution to understanding metastasis - one of the grand challenges in cancer research. Examples drawn from apparently unconnected areas of physics, and described at a recent workshop on metastasis, illustrate the value of interdiscplinary thinking.}
}
@article{SCHUELLER1997197,
title = {A state-of-the-art report on computational stochastic mechanics},
journal = {Probabilistic Engineering Mechanics},
volume = {12},
number = {4},
pages = {197-321},
year = {1997},
note = {A State-of-the-Art Report on Computational Stochastic Mechanics},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(97)00003-9},
url = {https://www.sciencedirect.com/science/article/pii/S0266892097000039},
author = {G.I. Schuëller}
}
@article{GROUT2014680,
title = {Taking Computer Science and Programming into Schools: The Glyndŵr/BCS Turing Project},
journal = {Procedia - Social and Behavioral Sciences},
volume = {141},
pages = {680-685},
year = {2014},
note = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814035435},
author = {Vic Grout and Nigel Houlden},
keywords = {Computer science, programming, computing curriculum, teacher training, British Computer Society (BCS) Academy, Computing At School (CAS), Council of Professors and Heads of Computing (CPHC), Lego NXT Mindstorm, Raspberry Pi, Robot C, Scratch, Picoboards ;},
abstract = {2012 and 2013 have been challenging years for Computer Science (CS) education in the UK. After decades of national neglect, there has been a sudden impetus to reintroduce CS into the 11-16 age school curriculums. Immediate obstacles include a generation of children with no CS background and an estimated need for 20,000 new CS teachers - existing UK IT teachers being insufficiently qualified and experienced. The Computing at School (CAS) movement has been instrumental in this quantum transition from an IT to Computing syllabus, as have the British Computer Society (BCS), leading UK universities and a number of major international technology companies, including Microsoft, Google, IBM, British Telecom and Facebook.This paper discusses the background to this position and the progress being made to address these challenges. It describes, in particular, the work of the BCS-funded Glyndwr University ‘Turing Project’ in introducing Welsh high-school students and staff to high-level programming and ‘computational thinking’. The Turing Project uses an innovative combination of Lego NXT Mindstorm robots, Raspberry Pi computers and PicoBoard hardware together with the Robot C and Scratch programming platforms. The paper discusses initial objectives and the general approach, describes focused delivery across different age groups and ability ranges and presents results and analysis demonstrating the effectiveness of the programme. Lessons learnt and future directions are considered in conclusion.}
}
@article{YANG201451,
title = {Reactivity of concurrent verbal reporting in second language writing},
journal = {Journal of Second Language Writing},
volume = {24},
pages = {51-70},
year = {2014},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2014.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1060374314000113},
author = {Chengsong Yang and Guangwei Hu and Lawrence Jun Zhang},
keywords = {Reactivity, Think-aloud, Second language acquisition (SLA), L2 writing, Argumentative writing, Chinese EFL writers},
abstract = {This paper reports an empirical study designed to explore whether concurrent verbal reporting has a reactive effect on the process of second language writing. Ninety-five Chinese EFL learners were randomly assigned to an argumentative writing task under three conditions: metacognitive thinking aloud (MTA), nonmetacognitive thinking aloud (NMTA), and no thinking aloud (NTA), after they completed a similar baseline writing task. Their essays were analyzed in terms of linguistic fluency, complexity, accuracy, and overall quality to examine if there were any significant between-group differences that could be taken as evidence of reactivity. After controlling for baseline differences, analyses revealed no traces of reactivity left on a majority of measures except that: (a) the two think-aloud conditions significantly increased dysfluencies in participants’ essays; (b) they also tended to reduce syntactic variety of the essays; and (c) MTA significantly prolonged time on task and retarded the speed of written production. These negative effects are interpreted in light of Kellogg's (1996) cognitive model of writing as suggesting no serious interference with L2 writing processes and are taken as cautions for, rather than counterevidence against, the use of the think-aloud method to obtain L2 writing process data.}
}
@article{CAO2024101244,
title = {Explanatory models in neuroscience, Part 1: Taking mechanistic abstraction seriously},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101244},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101244},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400038X},
author = {Rosa Cao and Daniel Yamins},
keywords = {Mechanism, Models, Explanation, Constraints, Similarity, Mapping, Abstraction, Functional abstraction, Neural networks, Computation, Philosophy, Vision, Constraint, Prediction, Transform, Levels of explanation, Mechanistic explanation, Neuroscience, Understanding},
abstract = {Despite the recent success of neural network models in mimicking animal performance on various tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system requires us to characterize its parts, organization, and activities, and how those give rise to behaviors of interest. However, it remains controversial what it takes for a model to be mechanistic, and whether computational models such as neural networks qualify as explanatory on this approach. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when an appropriate notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify an abstract level of description that is still detailed enough to be “runnable”, and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are just those already in use in experimental neuroscience and deployed in the construction of more familiar computational models — just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.}
}
@article{GUPTA20062290,
title = {Towards a new paradigm for innovative training methods for capacity building in remote sensing},
journal = {Advances in Space Research},
volume = {38},
number = {10},
pages = {2290-2298},
year = {2006},
note = {Remote Sensing of Oceanographic Processes and Land Surfaces; Space Science Education and Outreach},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2006.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0273117706004285},
author = {R.K. Gupta and P.M. Bala Manikavelu and D. Vijayan and T.S. Prasad},
keywords = {Thinking curricula, Innovative training methods, Capacity building, Remote sensing},
abstract = {Everybody uses a bulb to illustrate an idea but nobody shows where the current comes from. Majority of remote sensing user community comes from natural and social sciences domain while remote sensing technology evolves from physical and engineering sciences. To ensure inculcation and internalization of remote sensing technology by application/resource scientists, trainer needs to transfer physical and engineering concepts in geometric manner. Here, the steering for the transfer of knowledge (facts, procedures, concepts and principles) and skills (thinking, acting, reacting and interacting) needs to take the trainees from Known to Unknown, Concrete to Abstract, Observation to Theory and Simple to Complex. In the initial stage of training/education, experiential learning by instructor led exploring of thematic details in false colour composite (FCC) as well as in individual black and white spectral band(s) imagery by trainees not only creates interest, confidence build-up and orientation towards purposeful learning but also helps them to overcome their inhibitions towards the physical and engineering basal. The methodology to be adopted has to inculcate productive learning, emphasizing more on thinking and trial and error aspects as opposed to reproductive learning based dominantly on being told and imitation. The delivery by trainer needs to ensure dynamic, stimulating and effective discussions through deluging questions pertaining to analysis, synthesis and evaluation nature. This would ensure proactive participation from trainees. Hands-on module leads to creative concretization of concepts. To keep the trainees inspired to learn in an auto mode during post-training period, they need to consciously swim in the current and emerging knowledge pool during training programme. This is achieved through assignment of seminar delivery task to the trainees. During the delivery of seminar, peers and co-trainees drive the trainee to communicate the seminar content not only in what but also in how and why mode. The interest culminated in this manner keeps the entropy of the trainee minimized even during post-training professional life. So, such germinated trainee would always generate positive induction among colleagues; thus, helping in realizing multiplier effect. Based upon above thought process(es), the paper discusses the concept of “thinking curricula” and associated cares needed in training deliveries.}
}
@article{CORTI19942717,
title = {A computational study of metastability in vapor—liquid equilibrium},
journal = {Chemical Engineering Science},
volume = {49},
number = {17},
pages = {2717-2734},
year = {1994},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(94)E0093-6},
url = {https://www.sciencedirect.com/science/article/pii/0009250994E00936},
author = {David S. Corti and Pablo G. Debenedetti},
abstract = {Computer simulations are ideally suited to study systems under arbitrary constraints; hence they are useful for the investigation of metastability. Different types of constraints were applied to the three-dimensional Lennard—Jones fluid in the vapor—liquid coexistence region. Constraining the magnitude of allowed density fluctuations (restricted ensemble) has little effect on the equation of state and on phase equilibrium predictions for reduced temperatures lower than 0.95. Thermodynamic integrations along constrained and unstable paths are in good agreement with chemical potential calculations, indicating that imposing the density constraint does not violate microscopic reversibility. Restricted ensemble calculations were also used to calculate the width of the transition region where the mechanism of phase separation in the superheated liquid changes from nucleation to spinodal decomposition. The width of this region decreases as the temperature is reduced away from criticality. Free energy barriers to isotropic compression were used to determine the width of the transition region from nucleation to spinodal decomposition in the supercooled vapor. This transition region also becomes narrower as the distance from the critical point increases. The pressure of the deeply superheated liquid was found to be sensitive to the maximum size of voids that are allowed to form.}
}
@article{PUDANE2017517,
title = {Human Emotional Behavior Simulation in Intelligent Agents: Processes and Architecture},
journal = {Procedia Computer Science},
volume = {104},
pages = {517-524},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301680},
author = {Mara Pudane and Egons Lavendelis and Michael A. Radin},
keywords = {Affective agents, Emotive agents, Human behavior simulation, Agent internal architecture},
abstract = {The paper describes and discusses processes needed for human emotional behaviour simulation, in particular, emotion incorporation into rational thinking, as well as presents corresponding agent architecture. Such system would enable various application fields, perhaps one of the most important being enhancing smart devices with emotions. Decreasing frequency of social contact has become an urgent issue, particularly among young people. Emotional and social intelligence are however highly desired set of skills which is impossible to develop without interacting with others. Although this problem has been acknowledged, and there are some efforts to facilitate social contact, e.g., by augmented virtual reality games, that is still not enough. There is a need to develop environment that would allow learning exactly social and emotional skills. This on-going research aims at developing intelligent agents that are able to express and incorporate affects into rational processes.}
}
@article{MIYAMOTO2022231473,
title = {Data-driven optimization of 3D battery design},
journal = {Journal of Power Sources},
volume = {536},
pages = {231473},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231473},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322004803},
author = {Kaito Miyamoto and Scott R. Broderick and Krishna Rajan},
keywords = {Lithium-ion batteries, 3D miniature batteries, Optimization of 3D battery architecture, Machine learning, Multiobjective optimization},
abstract = {To power microelectronics for the internet-of-things applications, high-performance miniature batteries, called microbatteries, are critically important. Given their limited size, the three-dimensional design of microbatteries is key to maximizing their performance. Therefore, a computational strategy to identify the target battery architecture has major implications for performance improvement. In this paper, we propose a data-driven 3D battery optimization system at the full cell level that combines an automatic geometry generator based on Monte Carlo Tree Search and highly accurate machine-learning-based performance simulators. The performance of the proposed method is demonstrated by designing high-performance 3D batteries with more than 5.5 times efficiency compared with the approach based on a randomized algorithm. One of the designed geometries displayed greater power and energy densities due to more than 10% reduced internal resistance than the reported state-of-the-art geometry at the current density of higher than 15.8 mA/cm2. The results demonstrate the effectiveness of the method.}
}
@article{USKOKOVIC2023e15015,
title = {Natural sciences and chess: A romantic relationship missing from higher education curricula},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15015},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15015},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022223},
author = {Vuk Uskoković},
keywords = {Chemistry, Chess, Creativity, Culture, Education, Instruction, Science},
abstract = {Chess is a game that delicately weaves analytical thinking around artistic experience, yet recent conversions of STEM (Science-Technology-Engineering-Mathematics) to STEAM (Science-Technology-Engineering-Art-Mathematics) have omitted adding chess as an elementary coursework to K-12 and higher education curricula. Chess, as per arguments presented in this essay, can be considered as a language and a tool for furthering the development of artistic skills among scientists and analytical, pattern-recognition skills among artists. It can also serve as a missing link between science and art in STEAM curricula thanks to its finding itself halfway between the two. A handful of analogies are drawn here from chess, illustrated sporadically with positions from real-life chess games and converted to lessons in creativity for students in natural sciences. The discussion centered around these analogies is reinforced by a literature review of studies conducted over the past 80 years to assess the effect of exposing students to lessons in chess on their learning in distant domains. Overall, great benefits can emerge from complementing science education with chess and it is hoped that chess will become an integral part of basic education in primary schools and universities worldwide in the near future.}
}
@article{TRAYVICK2024116109,
title = {Speech and language patterns in autism: Towards natural language processing as a research and clinical tool},
journal = {Psychiatry Research},
volume = {340},
pages = {116109},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116109},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124003949},
author = {Jadyn Trayvick and Sarah B. Barkley and Alessia McGowan and Agrima Srivastava and Arabella W. Peters and Guillermo A. Cecchi and Jennifer H. Foss-Feig and Cheryl M. Corcoran},
keywords = {Autism, Speech, Language, Natural language processing, Automated speech analysis, Acoustics, Computational phenotyping},
abstract = {Speech and language differences have long been described as important characteristics of autism spectrum disorder (ASD). Linguistic abnormalities range from prosodic differences in pitch, intensity, and rate of speech, to language idiosyncrasies and difficulties with pragmatics and reciprocal conversation. Heterogeneity of findings and a reliance on qualitative, subjective ratings, however, limit a full understanding of linguistic phenotypes in autism. This review summarizes evidence of both speech and language differences in ASD. We also describe recent advances in linguistic research, aided by automated methods and software like natural language processing (NLP) and speech analytic software. Such approaches allow for objective, quantitative measurement of speech and language patterns that may be more tractable and unbiased. Future research integrating both speech and language features and capturing “natural language” samples may yield a more comprehensive understanding of language differences in autism, offering potential implications for diagnosis, intervention, and research.}
}
@article{SOUSA2015113,
title = {Symmetry-based generative design and fabrication: A teaching experiment},
journal = {Automation in Construction},
volume = {51},
pages = {113-123},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514002283},
author = {José Pedro Sousa and João Pedro Xavier},
keywords = {Architecture, Geometry, Symmetry, Computational design, Digital fabrication, Design education},
abstract = {Throughout history, symmetry has been widely explored as a geometric strategy to conceive architectural forms and spaces. Nonetheless, its concept has changed and expanded overtime. Nowadays, it is understood as an ordering principle resulting from the application of isometric transformations that keep the original object invariant. Departing from this notion, scientists, philosophers and designers have extended it to embrace other geometric scenarios. Following this idea, exploring symmetry does not mean the generation of simple and predictable design solutions. On the contrary, it is a creative window to achieve geometric complexity based on very simple rules. In this context, this paper aims at discussing the relevance of exploring symmetry in architectural design today by means of digital technologies. It argues that the coupled use of computational design and digital fabrication processes allows designers to explore and materialize a higher level of design complexity in a structured and controlled way, especially when non-isometric transformations are involved. As the background for testing and illustrating its arguments, this paper describes a teaching experiment conducted in the Constructive Geometry course at the FAUP, following design-to-fabrication methodologies.}
}
@article{MCGOWEN2010169,
title = {Metaphor or Met-Before? The effects of previouos experience on practice and theory of learning mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {3},
pages = {169-179},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000404},
author = {Mercedes A. McGowen and David O. Tall},
keywords = {Metaphor, Met-before, Epistemological obstacle, Embodiment, Local straightness},
abstract = {While the general notion of ‘metaphor’ may offer a thoughtful analysis of the nature of mathematical thinking, this paper suggests that it is even more important to take into account the particular mental structures available to the individual that have been built from experience that the individual has ‘met-before.’ The notion of ‘met-before’ offers not only a principle to analyse the changing meanings in mathematics and the difficulties faced by the learner—which we illustrate by the problematic case of the minus sign—it can also be used to analyse the met-befores of mathematicians, mathematics educators and those who develop theories of learning to reveal implicit assumptions that support our thinking in some ways and act as impediments in others.}
}
@article{SUN20112118,
title = {How digital scaffolds in games direct problem-solving behaviors},
journal = {Computers & Education},
volume = {57},
number = {3},
pages = {2118-2125},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S036013151100128X},
author = {Chuen-Tsai Sun and Dai-Yi Wang and Hui-Ling Chan},
keywords = {Human–computer interface, Interactive learning environments, Secondary education, Teaching/learning strategies},
abstract = {Digital systems offer computational power and instant feedback. Game designers are using these features to create scaffolding tools to reduce player frustration. However, researchers are finding some unexpected effects of scaffolding on strategy development and problem-solving behaviors. We used a digital Sudoku game named Professor Sudoku to classify built-in critical features, frustration control and demonstration scaffolds, and to investigate their effects on player/learner behaviors. Our data indicate that scaffolding support increased the level at which puzzles could be solved, and decreased frustration resulting from excessive numbers of retries. However, it also reduced the number of unassisted placements (i.e., independently filled cells), and increased reliance on scaffolding tools, both of which are considered disadvantageous for learning. Among the three scaffold types, frustration control reduced the potential for players to feel stuck at certain levels, but also reduced the frequency of use of critical feature-making tools, which are thought to have greater heuristic value. We conclude that the simultaneous provision of critical feature and frustration control scaffolds may increase player reliance on available support, thereby reducing learning opportunities. Providing players with critical features and demonstration scaffolds at the same time increases reliance on available support for some players, but for most it encourages the development of solving strategies.}
}
@incollection{RUNCO200771,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity},
publisher = {Academic Press},
address = {Burlington},
pages = {71-113},
year = {2007},
isbn = {978-0-12-602400-5},
doi = {https://doi.org/10.1016/B978-012602400-5/50003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780126024005500034},
author = {Mark A. Runco},
abstract = {Publisher Summary
This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared to left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with EEG, PET, cerebral blood flow, and MRI techniques. Numerous EEG studies suggest that there are particular brain-wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{GAO2023103794,
title = {Developing virtual acoustic terrain for Urban Air Mobility trajectory planning},
journal = {Transportation Research Part D: Transport and Environment},
volume = {120},
pages = {103794},
year = {2023},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S1361920923001918},
author = {Zhenyu Gao and Alex Porcayo and John-Paul Clarke},
keywords = {Urban Air Mobility, Sustainable aviation, Noise modeling, Trajectory planning, Optimization},
abstract = {Urban Air Mobility (UAM) is a transformative concept that must operate harmoniously within the constraints imposed by societal impacts. Noise-aware flight trajectory planning can address UAM’s community noise concerns. However, the traditional trajectory optimization paradigm requires repetitive computations of a flight’s noise footprints in complex urban environments and is computationally expensive. In this work, we propose virtual acoustic terrain, a novel concept to enable an efficient trajectory optimization paradigm. By applying acoustic ray tracing and the principle of reciprocity in a complex urban environment, we convert different noise constraints into 3D exclusion zones which UAM operations should avoid to maintain limited noise impact. It combines with the physical urban terrain to define an acceptable fly zone for non-repetitive noise-aware trajectory optimization. This framework provides a new angle to future urban area airspace management and can also accommodate other forms of societal constraints.}
}
@incollection{MACHINMASTROMATTEO2025376,
title = {Literacy of the Future},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {376-387},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@article{BAILEY20158,
title = {Metacognitive beliefs moderate the relationship between catastrophic misinterpretation and health anxiety},
journal = {Journal of Anxiety Disorders},
volume = {34},
pages = {8-14},
year = {2015},
issn = {0887-6185},
doi = {https://doi.org/10.1016/j.janxdis.2015.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0887618515000791},
author = {Robin Bailey and Adrian Wells},
keywords = {Health anxiety, Metacognition, Catastrophic misinterpretation, Moderation, S-REF model},
abstract = {Catastrophic misinterpretations of bodily symptoms have a central role in cognitive-behavioural models of health anxiety. However, the metacognitive (S-REF) model postulates that psychological disturbance is linked more to beliefs about thinking i.e., metacognition. Equally the relationship between catastrophic misinterpretation and health anxiety should be moderated by metacognition, in particular negative beliefs about the uncontrollability and danger of thinking (MCQNeg). Participants (N=351) completed measures to examine the relationship between these variables. Results indicated positive relationships between metacognition, catastrophic misinterpretation, and health anxiety. Moderation analysis showed that the effect of catastrophic misinterpretations on health anxiety was explained by the proposed interaction with metacognition. Follow-up regression analysis demonstrated the interaction term explained variance in health anxiety when controlling for other variables, and was a stronger unique predictor of health anxiety than catastrophic misinterpretation. Metacognition appears to be an important factor in the relationship between catastrophic misinterpretation and health anxiety, and would have important implications for existing models and treatment.}
}
@incollection{PAGEL201749,
title = {Chapter Four - Testing for Machine Consciousness},
editor = {J.F. Pagel and Philip Kirshtein},
booktitle = {Machine Dreaming and Consciousness},
publisher = {Academic Press},
address = {San Diego},
pages = {49-65},
year = {2017},
isbn = {978-0-12-803720-1},
doi = {https://doi.org/10.1016/B978-0-12-803720-1.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037201000049},
author = {J.F. Pagel and Philip Kirshtein},
keywords = {Thinking, intelligence, attention, intentionality, volition, self-awareness, artificial intelligence, AI, autonomous entity, Turing Test, Chinese Room Test},
abstract = {Thinking, intelligence, data integration, and attention are aspects of consciousness for which tests have been designed. A short history of the Computer Science field, a description, and an assessment of results obtained to this point for the Turing Test and Chinese Room Test are part of this chapter. Alternative definitions of artificial intelligence are presented. Applied tests for consciousness including those for intelligence, attention, intentionality, volition, and self-awareness are discussed as applied to the assessment of machine systems. Strong AI and the concept of autonomous entities are defined and addressed. The presence of dream-equivalent states is discussed as a potential marker for human-equivalent consciousness.}
}
@article{CASTANEDA2023102391,
title = {A simulation-based approach for assessing the innovation barriers in the manufacturing firms},
journal = {Technology in Society},
volume = {75},
pages = {102391},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001963},
author = {Monica Castaneda and Milton M. Herrera and Alberto Méndez-Morales},
keywords = {Product innovation, Process innovation, Manufacturing sector, System dynamics, Barriers to innovation, Innovation policy},
abstract = {One of the most important challenges organisations’ faces to innovate is dealing with different types of barriers. Particularly, the case of manufacturing firms confronts several barriers, such as demand uncertainty, product imitation, lack of employees, scarcity of government funding, absence of internal and external financing. This paper aims to provide new insights regarding to the innovation barriers faced by the manufacturing firms. To do this, we implemented a computational model for analysing the barriers to innovation in the Colombian case. In this model, product and processes innovation are studied. It was concluded that for the innovation of process, the highly important barrier is the shortcoming of internal financing, while for the innovation of product is the lack of employees. Results show that the government expenditure is scarce compared to private and external investment.}
}
@article{TURNER2024,
title = {Old Strategies, New Environments: Reinforcement Learning on Social Media},
journal = {Biological Psychiatry},
year = {2024},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322324018201},
author = {Georgia Turner and Amanda M. Ferguson and Tanay Katiyar and Stefano Palminteri and Amy Orben},
keywords = {Development, Mental health, Reinforcement learning, Reward learning, Social media, Social reward},
abstract = {The rise of social media has profoundly altered the social world, introducing new behaviors that can satisfy our social needs. However, it is not yet known whether human social strategies, which are well adapted to the offline world we developed in, operate as effectively within this new social environment. Here, we describe how the computational framework of reinforcement learning (RL) can help us to precisely frame this problem and diagnose where behavior-environment mismatches emerge. The RL framework describes a process by which an agent can learn to maximize their long-term reward. RL, which has proven to be successful in characterizing human social behavior, consists of 3 stages: updating expected reward, valuating expected reward by integrating subjective costs such as effort, and selecting an action. Specific social media affordances, such as the quantifiability of social feedback, may interact with the RL process at each of these stages. In some cases, affordances can exploit RL biases that are beneficial offline by violating the environmental conditions under which such biases are optimal, such as when algorithmic personalization of content interacts with confirmation bias. Characterizing the impact of specific aspects of social media through this lens can improve our understanding of how digital environments shape human behavior. Ultimately, this formal framework could help address pressing open questions about social media use, including its changing role across human development and its impact on outcomes such as mental health.}
}
@article{CAO2020118,
title = {Computational parameter identification of strongest influence on the shear resistance of reinforced concrete beams by fiber reinforcement polymer},
journal = {Structures},
volume = {27},
pages = {118-127},
year = {2020},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2020.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352012420302435},
author = {Yan Cao and Qingming Fan and Sadaf {Mahmoudi Azar} and Rayed Alyousef and Salim T. Yousif and Karzan Wakil and Kittisak Jermsittiparsert and Lanh {Si Ho} and Hisham Alabduljabbar and Abdulaziz Alaskar},
keywords = {FRP: reinforced concrete, Shear resistance, Selection procedure, ANFIS},
abstract = {Bars made of fiber reinforcement polymer (FRP) are in common usage for concrete reinforcing instead of steel reinforcing since steel could be affected by corrosion. The concrete beams reinforced by FRP bars have been studied mostly in longitudinal direction without shear reinforcement. The primary objective of this investigation was to design and advance an algorithm for selection procedure of the parameters influence on prediction of shear resistance of reinforced concrete beams by FRP. Six input parameters were used which represent geometric and mechanical properties of the bars as well as shear features. These parameters are: web width, tensile reinforcement depth, ratio of shear and depth, concrete compressive strength, ratio of FRP reinforcement, FRP modulus of elasticity and beam shear resistance. The searching algorithm is based on combination of artificial neural network and fuzzy logic principle or adaptive neuro fuzzy inference system (ANFIS). Based on the obtained results ratio of shear and depth has the strongest influence on the prediction of shear resistance of reinforced concrete beams by FRP. Moreover, combination of tensile reinforcement depth and ratio of shear and depth is the most influential combination of two parameters on the prediction of shear resistance of reinforced concrete beams by FRP. Finally, combination of tensile reinforcement depth, ratio of shear and depth and FRP modulus of elasticity is the most influential combination of three parameters on the prediction of shear resistance of reinforced concrete beams by FRP.}
}
@article{SNEDDON20252898,
title = {Rapid (≤25 °C) cycloisomerization of anhydride-tethered triynes to benzynes – origin of a remarkable anhydride linker-induced rate enhancement††Electronic supplementary information (ESI) available. CCDC 2353613. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc07232d},
journal = {Chemical Science},
volume = {16},
number = {6},
pages = {2898-2906},
year = {2025},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc07232d},
url = {https://www.sciencedirect.com/science/article/pii/S2041652025000392},
author = {Dorian S. Sneddon and Paul V. Kevorkian and Thomas R. Hoye},
abstract = {The hexadehydro-Diels–Alder (HDDA) reaction is a cycloisomerization between a conjugated diyne and a tethered diynophile that generates ortho-benzyne derivatives. Considerable fundamental understanding of aryne reactivity has resulted from this body of research. The multi-yne cycloisomerization substrate is typically pre-formed and the (rate-limiting) closure of this diyne/diynophile pair to produce the isomeric benzyne generally requires thermal input, often requiring reaction temperatures of >100 °C and times of 16–48 h to achieve near-full conversion. We report here that diynoic acids can be dimerized and that the resulting substrate, having a 3-atom anhydride linker (i.e., OCOCO), then undergoes HDDA cyclization within minutes at or below room temperature. This allows for the novel in situ assembly and cyclization of HDDA benzyne precursors in an operationally simple protocol. Experimental kinetic data along with DFT computations are used to identify the source of this surprisingly huge rate acceleration afforded by the anhydride linker: >107 faster than the analogous multi-yne having, instead, a CH2OCH2 ether linker.}
}
@article{LEE1993255,
title = {Interval computation as deduction in chip},
journal = {The Journal of Logic Programming},
volume = {16},
number = {3},
pages = {255-276},
year = {1993},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(93)90045-I},
url = {https://www.sciencedirect.com/science/article/pii/074310669390045I},
author = {J.H.M. Lee and M.H. {Van Emden}},
abstract = {Logic programming realizes the ideal of “computation is deduction,” but not when floating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that floating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the benefits of logic programming to computation involving floating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic that was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artificial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the looking-ahead inference rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when floating-point computations have been used. We believe ours is the first system with this property.}
}
@incollection{VALLERO202151,
title = {Chapter 3 - Transitional and translational sciences},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {51-87},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821953900012X},
author = {Daniel A. Vallero},
keywords = {Translational science, Geodesign, Landfill fires, Tire fires, Coal mine fires, Nuclear accidents, “As low as reasonably practicable” (ALARP), Rational methods, Modularity, Interoperability},
abstract = {This chapter introduces two aspects critical to environmental systems science. The first is attention to ways to move from reductionism to systems thinking. The second is the need to translate the methods, results, and meaning of scientific discoveries from one discipline to all those needed to address an environmental or public health problem. To aid in this discussion, several examples of environmental episodes are discussed with an eye toward root causes. Knowledgebase needs to support the transition to systems thinking are discussed, including modularity and interoperability of models and methods}
}
@article{ADENIJI2023,
title = {Draft genome sequence of active gold mine isolate Pseudomonas iranensis strain ABS_30},
journal = {Microbiology Resource Announcements},
volume = {12},
number = {12},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/MRA.00849-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23009234},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola and Julie C. {Dunning Hotopp}},
keywords = {bioremediation, biosynthetic clusters, genome sequence, gold mine, , secondary metabolites},
abstract = {ABSTRACT
Pseudomonas iranensis ABS_30, isolated from gold mining soil, exhibits metal-resistant properties valuable for heavy metal removal. We report the draft genome sequencing of the P. iranensis ABS_30 strain, which is 5.9 Mb in size.}
}
@article{OXMAN1994141,
title = {Precedents in design: a computational model for the organization of precedent knowledge},
journal = {Design Studies},
volume = {15},
number = {2},
pages = {141-157},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90021-3},
url = {https://www.sciencedirect.com/science/article/pii/0142694X94900213},
author = {Rivka E Oxman},
keywords = {case-based reasoning, design precedents, memory organization},
abstract = {A computational model for the organization of design precedent knowledge is developed. The model is composed of distinct chunks of knowledge called design stories. A formalism for the design story is proposed which represents the linkage between design issue, concept and form in designs. Stories are structured in memory according to a semantic network. The lexicon of the semantic network acts as a memory index. The memory structure and indexing system are demonstrated to enhance search and to support cross-contextual browsing and exploration in the precedent library. The approach is demonstrated in a pilot design aid system in the task domain of early conceptual design in architecture.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of Priestia megaterium AB-S79 strain isolated from active gold mine},
journal = {Microbiology Resource Announcements},
volume = {13},
number = {2},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01055-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23010629},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola},
keywords = {bioremediation, biosynthetic traits, genome analysis, , secondary metabolites, genomics},
abstract = {ABSTRACT
We screened and isolated Priestia megaterium strain AB-S79 from active gold mine soil, then sequenced its genome to unravel its biosynthetic traits. The isolate with a 5.7-Mb genome can be utilized as a reference in genome-guided strain selection for metabolic engineering and other biotechnological operations.}
}
@article{CHUNG201356,
title = {Table-top role playing game and creativity},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {56-71},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000478},
author = {Tsui-shan Chung},
keywords = {Creativity, Role playing game, Divergent thinking test, Priming},
abstract = {The current study aims to observe whether individuals who engaged in table-top role playing game (TRPG) were more creative. Participants total 170 (52 TRPG players, 54 electronic role playing game (ERPG) players and 64 Non-players) aged from 19 to 63. In the current study, an online questionnaire is used, adopting the verbal subtests of Wallach–Kogan Creativity Tests and the McCrae and Costa Big Five Personality Inventory. It is found that TRPG players score higher in divergent thinking tests. Priming and instruction giving methods lower the performance of all participants, in particular, when the instruction is memory provoking. ERPG players score lowest among the three groups. TRPG could be regarded as a form of improvisation. It could also be a preferable activity for the promotion of creativity. It is low cost and no formal setting is required to play. Many ERPGs are originated from TRPGs, therefore, with the popularity of ERPG, there should be advantages in promoting TRPG.}
}
@article{CHEN2010573,
title = {Generating ontologies with basic level concepts from folksonomies},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {573-581},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000621},
author = {Wen-hao Chen and Yi Cai and Ho-fung Leung and Qing Li},
keywords = {Folksonomy, Ontology, Basic level categories, Category utility},
abstract = {This paper deals with the problem of ontology generation. Ontology plays an important role in knowledge representation, and it is an artifact describing a certain reality with specific vocabulary. Recently many researchers have realized that folksonomy is a potential knowledge source for generating ontologies. Although some results have already been reported on generating ontologies from folksonomies, most of them do not consider what a more acceptable and applicable ontology for users should be, nor do they take human thinking into consideration. Cognitive psychologists find that most human knowledge is represented by basic level concepts which is a family of concepts frequently used by people in daily life. Taking cognitive psychology into consideration, we propose a method to generate ontologies with basic level concepts from folksonomies. Using Open Directory Project (ODP) as the benchmark, we demonstrate that the ontology generated by our method is reasonable and consistent with human thinking.}
}
@incollection{GALLISTEL199235,
title = {Classical Conditioning as an Adaptive Specialization: A Computational Model},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {28},
pages = {35-67},
year = {1992},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60487-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604879},
author = {C.R. Gallistel},
abstract = {Publisher Summary
This chapter analyzes the results of some modern classical conditioning experiments from the perspective of a computational model based on the assumption that the underlying learning process is specifically adapted to the domain of multivariate, nonstationary time series. It focuses on the quantitative results from experiments on the effects of partial reinforcement on the rate of acquisition and extinction because the other predictions of the model have been discussed and associative models are conspicuously unsuccessful at making quantitative predictions in this area. The model gives a mathematical characterization of the learning process from which one can derive the results of conditioning experiments. It is unlike these models in the sense that it is not in the associative tradition. The model replaces the associative explanatory framework with a framework that treats the conditioning process as a computational mechanism adapted through evolution to the peculiarities of one domain-a mechanism that solves one and only one of the several fundamentally distinct learning problems that confront mobile, multicellular organisms.}
}
@article{CAMARGOJUNIOR2016190,
title = {Optimal economic result and risk of parallel development of concept options in dynamic markets},
journal = {RAI Revista de Administração e Inovação},
volume = {13},
number = {3},
pages = {190-198},
year = {2016},
issn = {1809-2039},
doi = {https://doi.org/10.1016/j.rai.2016.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1809203916300377},
author = {Alceu Salles {Camargo Júnior} and Abraham Sin Oih Yu},
keywords = {New product development, Economic result and risk of projects, Option thinking},
abstract = {New product development is an essential competence to organizations. Launching success products requires elaborate and precise knowledge about the technological platforms, like the most important market needs and characteristics, and the project team have to employ information systems to support the project decisions, which must be rapid and accurate. However, when the market characteristics are much dynamic and change rapidly or the development project aims at a really new product, the levels of uncertainties are greater, and the project team must employ more robust strategies of risk management. Option thinking is useful to develop several concept alternatives of some crucial subsystems of the new product in order to achieve new technical and market knowledge by repeating cycles of design, built and tested by several and different prototypes in parallel. These different prototypes develop, test and can accumulate knowledge about each one, different technologies, architectures and quality attributes or the usability for potential customers. This study achieves the optimal number of concept options to develop in parallel in order to maximize the economic performance of the development project of a new product constituted of two important subsystems. Mathematical models simulating the sequential decision process are developed to determine the economic result and risk of a two-subsystem product innovation project. Our results point the parallel development of concept options as a robust strategy to manage new product development mostly in adverse conditions, that is, with greater levels of uncertainties.}
}
@article{HENRY2016119,
title = {Hofmeister series: The quantum mechanical viewpoint},
journal = {Current Opinion in Colloid & Interface Science},
volume = {23},
pages = {119-125},
year = {2016},
issn = {1359-0294},
doi = {https://doi.org/10.1016/j.cocis.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1359029416301108},
author = {Marc Henry},
keywords = {Quantum mechanics, Phase coherence, Living cells, Condensed matter, Hofmeister series, Water, Aqueous solutions, Harmonic ratios},
abstract = {It is suggested that electromagnetic quantum vacuum fluctuations are at the very deep root of the so-called “specific ions effects” in concentrated solutions or in living cells. A many-body quantum-mechanical frame of thinking is proposed based on the concept of quantum coherence taking into account explicitly density and excitation frequencies of molecules and/or ionic species. It is also proposed that Hofmeister phenomena could have a natural explanation in the harmonic relationships between sets of characteristic frequencies ruled by quantum mechanical laws. It then follows that physical chemistry of concentrated media and biology should be ruled more by a quantum “symphony” between indistinguishable constituents rather than localized two-body electrical interactions between molecular or ionic species.}
}
@incollection{KUMAR20031,
title = {1 - An introduction to computational development},
editor = {Sanjeev Kumar and Peter J. Bentley},
booktitle = {On Growth, Form and Computers},
publisher = {Academic Press},
address = {London},
pages = {1-43},
year = {2003},
isbn = {978-0-12-428765-5},
doi = {https://doi.org/10.1016/B978-012428765-5/50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124287655500347},
author = {Sanjeev Kumar and Peter J. Bentley}
}
@incollection{ROWLAND2003341,
title = {Chapter 16 - Interpreting Analytical Spectra with Evolutionary Computation},
editor = {Gary B. Fogel and David W. Corne},
booktitle = {Evolutionary Computation in Bioinformatics},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {341-365},
year = {2003},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-797-2},
doi = {https://doi.org/10.1016/B978-155860797-2/50018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607972500184},
author = {Jem J. Rowland},
abstract = {Publisher Summary
This chapter deals with analytical techniques that are used to probe the activity and chemical makeup of cells. Metabolomics, the study of the entire biochemical constituents of a cell at any one time, is found to provide a rich means of monitoring organism activity. It can reveal explanations for different characteristics of seemingly similar organisms and can be used to relate function with gene. Spectroscopies are well suited to the study and interpretation of the metabolome in functional genomics. Another important technique in functional genomics is the measurement of gene expression via transcriptome arrays. This chapter outlines the various ways in which evolutionary computation (EC) can provide the basis for powerful tools for spectral interpretation and thus for functional genomics. It mentions various methods of forming predictive models from multivariate, often quasi-continuous data. It also discusses ways in which the effectiveness of such conventional techniques may be enhanced by combining them with evolutionary techniques.}
}
@article{XU201766,
title = {Emerging Trends for Microbiome Analysis: From Single-Cell Functional Imaging to Microbiome Big Data},
journal = {Engineering},
volume = {3},
number = {1},
pages = {66-70},
year = {2017},
issn = {2095-8099},
doi = {https://doi.org/10.1016/J.ENG.2017.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S2095809917301595},
author = {Jian Xu and Bo Ma and Xiaoquan Su and Shi Huang and Xin Xu and Xuedong Zhou and Wei E. Huang and Rob Knight},
keywords = {Microbiome, Method development, Single-cell analysis, Big data, China Microbiome Initiative},
abstract = {Method development has always been and will continue to be a core driving force of microbiome science. In this perspective, we argue that in the next decade, method development in microbiome analysis will be driven by three key changes in both ways of thinking and technological platforms: ① a shift from dissecting microbiota structure by sequencing to tracking microbiota state, function, and intercellular interaction via imaging; ② a shift from interrogating a consortium or population of cells to probing individual cells; and ③ a shift from microbiome data analysis to microbiome data science. Some of the recent method-development efforts by Chinese microbiome scientists and their international collaborators that underlie these technological trends are highlighted here. It is our belief that the China Microbiome Initiative has the opportunity to deliver outstanding “Made-in-China” tools to the international research community, by building an ambitious, competitive, and collaborative program at the forefront of method development for microbiome science.}
}
@article{GALBUSERA2022103109,
title = {Game-based training in critical infrastructure protection and resilience},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103109},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103109},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003284},
author = {Luca Galbusera and Monica Cardarilli and Marina {Gómez Lara} and Georgios Giannopoulos},
keywords = {Critical infrastructure, Resilience, Preparedness, Training, Exercises, Serious games, Gamification},
abstract = {Several institutions worldwide are reflecting on the relevance of training and exercises to critical infrastructure protection and resilience. This is witnessed, for instance, by Council Directive 2008/114/EC in the EU and the Homeland Security Exercise and Evaluation Program in the US. Contributing to the research actions in the field, the present article discusses methodological approaches, tools, techniques, and technologies relevant to this domain. In particular, we report on a recent training initiative elaborated by the authors and involving a game-based, modelling-and-simulation-backed, computer-assisted exercise for critical infrastructure expert audiences. This was developed taking advantage of JRC's Geospatial Risk and Resilience Assessment Platform (GRRASP) and critical infrastructure analysis methodologies integrated therein. The overarching objective was to enhance system thinking and raise awareness of resilience aspects while familiarizing participants with specific analysis tools and scientific models.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{GARAS2024100885,
title = {A data analytics case study analyzing IRS SOI migration data using no code, low code technologies},
journal = {Journal of Accounting Education},
volume = {66},
pages = {100885},
year = {2024},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100885},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000010},
author = {Samy Garas and Susan L. Wright},
keywords = {Robotic process automation, UiPath, Alteryx, Tableau, Data automation, Data analytics, Data visualizations, Regional migration, Government planning, Business planning},
abstract = {Organizations generate and accumulate vast amounts of structured and unstructured data that have value for formulating and supporting strategic decisions. The advancement of no-code and low-code software has enabled the use of this data to provide significant data insights and business intelligence by employing multiple forms of data analytics. The imperative to cultivate a robust and proficient group of individuals with expertise in data analytics has led to a substantial increase in the number of educational programs focused on data science and analytics. Accounting educators can capitalize on these trends by integrating data analytics and software skills into the accounting curriculum. This case offers essential materials to aid in the development of the curriculum to support accounting and analytics educators. This case serves many objectives by providing a professional setting in which you take on the role of junior data analyst, offering necessary context and motivation for completing the tasks. The case allows you to analyze extensive data sets obtained from the IRS Statistics of Income (SOI) website in order to investigate migration patterns based on state, year, age, and income categories. UiPath-robotic process automation (RPA), Alteryx-based data analysis, and Tableau-based data visualization tools are employed to extract, generate, and present descriptive statistics and to conduct a simple times series analysis. These insights are highly valuable to decision makers in business and government organizations. You are encouraged to engage in critical thinking and to consider the potential impacts of migratory patterns on choices made by firm executives and public policy makers. Migration patterns have a significant impact on firm management decisions, influencing either to expand or reduce current operations and indicating the availability and expansion of new talent pools. Migration patterns have a significant impact on the decision made by public policy makers, particularly in relation to public utilities, infrastructure, and other services and benefits. You analyze temporal data to deduce the influence of changes in the tax code and shifts in the economy. You gain expertise in managing large data sets, exploring features of analytics software, and creating compelling visualizations to effectively communicate important discoveries. Instructors and students are given comprehensive instructions and videos to facilitate the efficient application of these technologies.}
}
@incollection{SUGHRUE2024151,
title = {Chapter 6 - Reimagining neurocognitive functions as emergent phenomena: What resting state is really showing us},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {151-157},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000082},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain hub, Brain landscape, Network control theory, Neurocognitive function, Resting-state fMRI, Structural connectome},
abstract = {In this chapter, we introduce a new way of thinking about neurocognitive functioning and related dysfunction. We discuss how structural wiring patterns, global rhythms in deep structures, and electrochemical gain from neurotransmitters play a key role in the internal dynamics of what the brain is doing. Importantly, together, these elements dictate how the brain can or cannot obtain different brain states. Simultaneously, disruption in intrinsic structures and internal dynamics alters the energetic landscape causing some brain states to become more favorable or less favorable. Importantly, we go on to describe how landscapes arise from structural connectomes, and how these connections can dictate spontaneous behavioral patterns and tendencies in normal as well as pathologic states, such as a depressed patient being stuck in a self-ruminating and negative state. Resting-state fMRI also provides a keyhole into these processes as the entire set of the structural connectome creates the patterns of functional connectivity seen in resting-state brain activity.}
}
@incollection{KUMAR2025185,
title = {Chapter 9 - Future prospective of neuromorphic computing in artificial intelligence: A review, methods, and challenges},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {185-197},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000080},
author = {Vivek Kumar and Kapil Joshi and Rajiv Kumar and Minakshi Memoria and Ashulekha Gupta and F. Ajesh},
keywords = {Neuromorphic computing, Artificial intelligence, Deep learning, Machine learning, Human brain modeling},
abstract = {Neuromorphic computing in the area of artificial intelligence (AI) offers the appeal of human brain modeling. In the Fourth Industrial Revolution era, AI is among the most advanced scientific knowledge that can integrate human behavior and intelligence into machines. Even though neuromorphic computing has been around since the 1980s, it is still a relatively new field. In the last 10 years, in particular, there has been a significant amount of study and the advancement of AI. The next stage of AI is thought to be Neuromorphic Computing. The development of neuromorphic computing technology will be crucial. The most potent computational device in existence, the human brain has long served as an inspiration for AI. This study discusses neuromorphic computing, a new form of sophisticated computing that draws inspiration from brain intelligence. The objective of this paper is to give a summary of the present status of AI and neuromorphic computing to express a viewpoint on the potential and challenges that lie ahead for the main applications of neuromorphic computing. We discuss the prospects for further development of these systems and highlight features of neuromorphic computing that are promising for the field's future.}
}
@article{DO2020110730,
title = {Capturing creative requirements via requirements reuse: A machine learning-based approach},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110730},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301631},
author = {Quoc Anh Do and Tanmay Bhowmik and Gary L. Bradshaw},
keywords = {Requirements reuse, Requirements engineering, Creativity in RE, Boilerplate, Natural language processing, Machine learning},
abstract = {The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework’s ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels.}
}
@article{ROSSITER202017604,
title = {Using interactive tools to facilitate student self-testing of dynamics and PI compensation},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17604-17609},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2677},
url = {https://www.sciencedirect.com/science/article/pii/S240589632033439X},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, staff efficiency, student engagement, independent learning},
abstract = {Virtual laboratories have become a common tool in recent years for supporting student learning and engagement. This paper presents a new tool for helping students self-assess their competence in basic dynamics for 1st and 2nd order systems alongside simple PI compensation techniques. The tools provide a supported environment for helping students work towards the correct answer by providing succinct feedback on incorrect responses and opportunities to try again, while displaying relevant information. A partner interactive tool is also provided which focuses solely on assessment with no feedback, so that students can assess their ability to get correct answers in a scenario that only the first attempt counts. This paper gives the thinking behind the tools, their coding and also accessibility for students.}
}
@incollection{VERSCHAFFEL2010401,
title = {Mathematics Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {401-406},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00517-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947005170},
author = {L. Verschaffel and B. Greer and E. {De Corte}},
keywords = {Adaptive expertise, Assessment, Collaboration, Competence, Constructivism, Design experiment, High-stakes testing, Mathematical, Mathematics education, Mathematics learning, Mathematics teaching, Prior knowledge, Routine expertise, Situated cognition, Standards},
abstract = {This article presents a review of important recent themes and developments in research on the learning and teaching of mathematical knowledge and thinking. As a framework, we use a model for designing a powerful environment for learning and teaching mathematics; this model is structured according to four interrelated components, namely competence, learning, intervention, and assessment (CLIA-model) (De Corte et al., 2004). We argue and illustrate that our empirically based knowledge of each of these four interconnected components has substantially advanced over the past decades, enabling a progressively better understanding of not only the components that constitute a mathematical disposition, but also the nature of the learning and developmental processes that should be induced in students to facilitate the acquisition of competence, the characteristics of learning environments that are powerful in initiating and evoking those processes, and finally, the kind of assessment instruments that are appropriate to help monitor and support learning and teaching.}
}
@article{REISS1967193,
title = {Individual thinking and family interaction—II. A study of pattern recognition and hypothesis testing in families of normals, character disorders and schizophrenics},
journal = {Journal of Psychiatric Research},
volume = {5},
number = {3},
pages = {193-211},
year = {1967},
issn = {0022-3956},
doi = {https://doi.org/10.1016/0022-3956(67)90002-7},
url = {https://www.sciencedirect.com/science/article/pii/0022395667900027},
author = {David Reiss},
abstract = {The present study investigated the relationship between family interaction and individual pattern recognition in five families of normals, five families of character disorders and five families of schizophrenics. Following a period of family interaction, members of normal families showed improvement in pattern recognition; members of families of schizophrenics showed deterioration or no change and members of character disorder families were in between. During the period of family interaction, members of normal families were independent and adventuresome in testing their pattern concepts whereas members of families of schizophrenics were cautios, copied each other's performance but showed little pooling of ideas. These findings support the hypothesis that family interaction can influence perceptual process in its individual members in a short time and points to some particular relationships between family interaction and individual perception.}
}
@article{SHUBBAR2024382,
title = {Bridging Qatar's food demand and self-sufficiency: A system dynamics simulation of the energy–water–food nexus},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {382-399},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000423},
author = {Haya Talib Shubbar and Furqan Tahir and Tareq Al-Ansari},
keywords = {Carbon emissions, Energy-water-food nexus, Food self-sufficiency, Food security, Qatar, System dynamics},
abstract = {The food sector in Qatar is confronted with formidable challenges due to its harsh environmental conditions. Striving for total food self-sufficiency in such an environment would inevitably exert pressure on the energy and water sectors. This heightened demand for energy and water translates into increased costs and escalates environmental impacts. Consequently, this study embarks on an in-depth analysis of food production within the context of Qatar's energy-water-food nexus, aiming to demonstrate how varying degrees of food self-sufficiency may impact the demand on Qatar's water and energy sectors, as well as on greenhouse gas (GHG) emissions. Moreover, this study demonstrates to what extent specific subsystems within the nexus can be modified to enhance sustainability. An energy-water-food nexus is meticulously crafted within the proposed framework to elucidate the intricate interdependencies among these sectors, incorporating pertinent external variables. These interconnections are then transmuted into a system dynamics model (SDM), facilitating a nuanced exploration of potential transformations and their ripple effects. Furthermore, a life-cycle thinking approach explicitly tailored to Qatar was implemented to estimate GHG emissions accurately. Four distinct scenarios are rigorously examined using the SDM, spanning from a status quo perspective to ambitious transitions toward full food self-sufficiency. The findings of the scenarios indicate that scenario 4, which partially provides the country with its food demands locally using desalinated water, treated wastewater, and groundwater and satisfies 20 % of its energy demand from solar energy, is the most ideal with an annual 5.36 × 1010 kWh/year energy consumption, 1.73 × 1012 l/year water demand, and 3.26 × 1010 kg CO2 eq./year emissions. The outcomes underscore the imperative for prioritizing less energy-intensive resources to mitigate overall energy consumption. Additionally, achieving an optimal national scenario necessitates a judicious equilibrium between food imports and domestic production.}
}
@incollection{MARTIGNON2001382,
title = {Algorithms},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {382-385},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00549-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005490},
author = {L. Martignon},
abstract = {The concept of algorithm is central to the modern view of a thinking machine, be it the human mind or the modern computer. An algorithm is a well-defined mathematical recipe for the solution of a well-defined task. It is presented as a finite set of steps or instructions that can be applied to unlimited sets of possibilities. There is a clear-cut rule for the operation to be performed at each step, as well as a clear-cut specification of the conditions under which to terminate the process. An algorithm may contain loops, that is, there may be steps that return to previous steps. Algorithms can be sequential or parallel. An algorithm that produces a ‘yes’ or ‘no’ answer is, decision algorithm. An algorithm that constructs or determines a specific solution to a given problem is a computation algorithm.}
}
@article{KOPPAKA2024,
title = {Mechanism and Selectivity of Bi(V)-Aryl Oxyfunctionalization in Trifluoroacetic Acid Solvents},
journal = {Organometallics},
year = {2024},
issn = {0276-7333},
doi = {https://doi.org/10.1021/acs.organomet.4c00319},
url = {https://www.sciencedirect.com/science/article/pii/S0276733324003509},
author = {Anjaneyulu Koppaka and Dongdong Yang and Sanaz Mohammadzadeh Koumleh and Burjor Captain and Roy A. Periana and Daniel H. Ess},
abstract = {The oxidative functionalization of aromatic sp2 C–H bonds to C–O bonds is a difficult transformation. For main-group metals, the oxyfunctionalization step of a metal-aryl bond is generally slow and potentially problematic if carried out in a relatively strong acid solvent where protonation could prevent oxyfunctionalization. In this work, we experimentally and computationally analyzed the oxyfunctionalization reaction of (Ph)3BiV(TFA)2 (TFA = trifluoroacetate) in a trifluoroacetic acid (TFAH) solvent. Experiments showed a single oxyfunctionalization product phenyl TFA (PhTFA) and two equivalents of benzene. Explicit/continuum solvent density functional theory calculations revealed that a direct intramolecular reductive functionalization pathway is lower in energy than radical or ionic pathways, and surprisingly from (Ph)3BiV(TFA)2, the reductive functionalization pathway is potentially competitive with protonation. In contrast, for (Ph)2BiV(TFA)3 oxyfunctionalization is significantly lower in energy than protonation. For BiIII-phenyl intermediates, redox neutral protonation is significantly lower in energy than a second functionalization. We also examined the oxyfunctionalization versus protonation of BiV-phenyl complexes with a coordinated biphenyl ligand and a coordinated biphenyl sulfone ligand, which both resulted in oxyfunctionalization. For the biphenyl ligand complex, a protonation-first mechanism is proposed, while for the biphenyl sulfone ligand, an oxyfunctionalization first mechanism is consistent with both calculations and experiments.
}
}
@article{NYSTROM201077,
title = {Ontological musings on how nature computes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {77-86},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000116},
author = {J.F. Nystrom},
keywords = {Universe as computation, Quantum vacuum, Computational cosmography},
abstract = {Modern physical theory and modern computational techniques are used to provide conjecture on how nature computes. I utilize time-domain simulation of physical phenomena and build analogies between elements of computation and the “things” of Universe computation, resulting, for example, in the identification of the quantum vacuum as the power source for Universe computation. While reviewing how Universe can be viewed as a computation, we find the need for Negative Universe (which is a part of the quantum vacuum mechanism). This idea is compared with Penrose’s current model which utilizes a separate Platonic world outside of physical Universe. Lastly, in the Discussion, I present an updated version of computational cosmography as a model for Universe as computation.}
}
@incollection{MILLER2023203,
title = {Chapter 7 - The calculated uncertainty of scientific discovery: From Maths to Deep Maths},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {203-226},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016971612300024X},
author = {D. Douglas Miller},
keywords = {Mathematics, Philosophy, Statistics, Null hypothesis, Artificial intelligence, Data dimensionality, Machine learning, Algorithms, Deep learning, Stochastic gradient descent, Model optimization, Bias, Neural networks, Backpropagation, Large language models, Model generalizability},
abstract = {Throughout history, diverse Maths have underpinned numerous important natural and physical science discoveries. In their initial development and application, these Maths were often incompletely or imperfectly understood, with constants and “fudge factors” needed to account for statistical uncertainties to advance a scientific discipline. Some polymaths have acted as philosophers in support of new ways of thinking, based on their novel discoveries about the natural and physical world. Deep Maths integral to artificial intelligence (AI), machine learning and deep learning (DL), are also subject to human imperfections (i.e., computational errors, operator assumptions) and stochastic uncertainties (i.e., modeling biases, convergence optimizers). Mathematicians and domain experts can collaborate to increase AI model accuracy by improving training data quality (i.e., curating, reducing dimensionality), mitigating human and machine biases, and understanding data contexts prior to query. Since the advent of DL and through the design of multilayered feedforward neural networks then large language models, scientists have applied advanced AI computing capabilities to push the limits of this technology trend. Recently, AI's capacity to uncover newly modeled insights has been hyped beyond the proven limits of DL model accuracy. History has witnessed the acceptance of new knowledge (primarily by peers) based on the accuracy and/or reproducibility of empirical observations and on varied interpretations of mathematical proofs. Societal enthusiasm for science or technology insertion is often limited by the general public's understanding of the underlying Maths and Deep Maths, and related human fears and concerns of displacement (i.e., lost jobs, ecological impact, less privacy, etc.). Today's proponents of societal progress based on new discoveries and technologies are motivated by a range of influences (i.e., humanity, control, security, profit, etc.), creating additional uncertainties that can deflect initial scientific enthusiasm and/or delay widespread adoption.}
}
@article{RAI201651,
title = {Fragmentary shape recognition: A BCI study},
journal = {Computer-Aided Design},
volume = {71},
pages = {51-64},
year = {2016},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010448515001542},
author = {Rahul Rai and Akshay V. Deshpande},
keywords = {Brain–Computer Interfaces (BCI), Fragmentary shape recognition, User studies, Cognitive load in shape processing, Natural interactions},
abstract = {Recently, Brain–Computer Interface (BCI) has emerged as a potential modality that utilizes natural and intuitive human mechanisms of thinking process to enable interactions in CAD interfaces. Before BCI could become a mainstream mode of HCI for CAD interfaces; fundamental studies directed towards understanding how humans mentally represent and process the geometry are needed. The outlined work in this paper presents an objective user study to understand shape recognition process in the humans. Specifically, we focus on the fundamental task of fragmentary shape identification. The problem of fragmentary shape recognition can be defined as follows: given a partial and incomplete minimalistic representation of a given shape, can one recognize the actual complete shape or object? In user studies, each subject was progressively (in stages) shown more informative fragmented images of an object to be recognized. During each stage of the experiment, the brain activity of users in the form of electroencephalogram (EEG) signals was recorded with a BCI headset. The recorded signals are then processed to objectively study the fragmentary shape recognition process. The results of user studies conclusively show that the measured brain activities of subjects can serve as a very accurate proxy to estimate subjects fragmentary shape recognition process.}
}
@article{BIRJALI201765,
title = {Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks},
journal = {Procedia Computer Science},
volume = {113},
pages = {65-72},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091731699X},
author = {Marouane Birjali and Abderrahim Beni-Hssane and Mohammed Erritali},
keywords = {Sentiment Analysis, Machine Learning, Suicide, Social Networks, Tweets, Semantic Sentiment Analysis},
abstract = {Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.}
}