@incollection{ERICSSON200112256,
title = {Protocol Analysis in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12256-12262},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01598-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015989},
author = {K.A. Ericsson},
abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.}
}
@incollection{DIX2003381,
title = {CHAPTER 14 - Upside-Down ∀s and Algorithms—Computational Formalisms and Theory},
editor = {John M. Carroll},
booktitle = {HCI Models, Theories, and Frameworks},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {381-429},
year = {2003},
series = {Interactive Technologies},
isbn = {978-1-55860-808-5},
doi = {https://doi.org/10.1016/B978-155860808-5/50014-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608085500149},
author = {Alan Dix}
}
@article{CAI2004135,
title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {2},
pages = {135-167},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000124},
author = {Jinfa Cai},
abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.}
}
@article{LOTURCO2022104059,
title = {The knowledge and skill content of production complexity},
journal = {Research Policy},
volume = {51},
number = {8},
pages = {104059},
year = {2022},
note = {Special Issue on Economic Complexity},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2020.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0048733320301372},
author = {Alessia {Lo Turco} and Daniela Maggioni},
keywords = {Occupational complexity, Services, Regional growth, STEM},
abstract = {In this paper we investigate the labour content of complex products. By exploiting O*NET information on the skill and knowledge required by occupations, we find that the product complexity measure suggested by Hausmann and Hidalgo (2009) is highly intensive in STEM knowledge and in Science, Mathematics and Critical Thinking skill requirements. We then propose a new measure of occupational complexity based on these occupational features. Among other advantages, this indicator has the merit to measure complexity for service industries that, so far, has never been measured. In an empirical model of the growth of USA Metropolitan Areas (MSAs), we find that MSAs whose initial industrial structure embeds a higher level of occupational complexity experience higher real per capita GDP growth over the 2001–2017 period. The occupational complexity measure is a stronger predictor of growth than other metrics of industries’ occupational and task content as well as compared to indicators of local occupational and industrial composition. When we separately compute occupational complexity of service and manufacturing industries and delve into their specific role for long run growth, we find a prominent role of the occupation complexity embedded in local services with respect to the one embedded in local manufacturing. Our baseline evidence is corroborated in the context of the NUTS3 regions of France over the period 2010–2017.}
}
@article{BERNALMANRIQUE202086,
title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
journal = {Journal of Contextual Behavioral Science},
volume = {17},
pages = {86-94},
year = {2020},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144720301551},
author = {Koryn N. Bernal-Manrique and María B. García-Martín and Francisco J. Ruiz},
keywords = {Acceptance and commitment therapy, Interpersonal skills, Emotional disorders, Psychological flexibility, Repetitive negative thinking},
abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{AMADEI2020120149,
title = {Revisiting positive peace using systems tools},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120149},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120149},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309756},
author = {Bernard Amadei},
keywords = {Complex systems, Systems thinking, System dynamics, Cross-impact analysis, Network analysis, Positive peace, Peace geometry},
abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.}
}
@article{HERNIMAN2021373,
title = {Interrelationships between depressive symptoms and positive and negative symptoms of recent onset schizophrenia spectrum disorders: A network analytical approach},
journal = {Journal of Psychiatric Research},
volume = {140},
pages = {373-380},
year = {2021},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2021.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0022395621003071},
author = {Sarah E. Herniman and Lisa J. Phillips and Stephen J. Wood and Sue M. Cotton and Edith J. Liemburg and Kelly A. Allott},
keywords = {Comorbidity, Co-occurrence, Early psychosis, Depression, Affect},
abstract = {Objective
There is a need to better understand the interrelationships between positive and negative symptoms of recent-onset schizophrenia spectrum disorders (SSD) and co-occurring depressive symptoms. Aims were to determine: (1) whether depressive symptoms are best conceptualised as distinct from, or intrinsic to, positive and negative symptoms; and (2) bridging symptoms.
Methods
Network analysis was applied to data from 198 individuals with depressive and psychotic symptoms in SSD from the Psychosis Recent Onset GRoningen Survey (PROGR-S). Measures were: Montgomery–Åsberg Depression Rating Scale and Positive and Negative Syndrome Scale.
Results
Positive symptoms were just as likely to be associated with depressive and negative symptoms, and had more strong associations with depressive than negative symptoms. Negative symptoms were more likely to be associated with depressive than positive symptoms, and had more strong associations with depressive than positive symptoms. Suspiciousness and stereotyped thinking bridged between positive and depressive symptoms, and apparent sadness and lassitude between negative and depressive symptoms.
Conclusions
Depressive symptoms might be best conceptualised as intrinsic to positive and negative symptoms pertaining to deficits in motivation and interest in the psychotic phase of SSD. Treatments targeting bridges between depressive and positive symptoms, and depressive and such negative symptoms, might prevent or improve co-occurring depressive symptoms, or vice-versa, in the psychotic phase of SSD.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{HEILMAN20041,
title = {Computational models of epileptiform activity in single neurons},
journal = {Biosystems},
volume = {78},
number = {1},
pages = {1-21},
year = {2004},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2004.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264704000978},
author = {Avram D. Heilman and James Quattrochi},
keywords = {Paroxysmal depolarizing shifts (PDS), Sustained depolarizations (SD), Hippocampus, Autaptic CA1/CA3 pyramidal neuron, Voltage-gated Ca channels, Ca-dependent K channels},
abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.}
}
@article{GREGORY198254,
title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
journal = {Design Studies},
volume = {3},
number = {1},
pages = {54},
year = {1982},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(82)90084-9},
url = {https://www.sciencedirect.com/science/article/pii/0142694X82900849},
author = {Sydney Gregory}
}
@article{LI202231,
title = {Application Analysis of Artificial Intelligent Neural Network Based on Intelligent Diagnosis},
journal = {Procedia Computer Science},
volume = {208},
pages = {31-35},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014478},
author = {Yukun Li},
keywords = {Intelligent diagnosis, artificial intelligence network, automobile fault diagnosis, the neural network},
abstract = {In recent years, the continuous development of computer science and AI technology makes the application prospect of artificial intelligence in fault diagnosis emerge. As a simulation technology of human thinking pattern, intelligent diagnosis technology can check and manage the monitoring target in real time to ensure the accuracy of data information. This paper introduces the basic principles of key artificial intelligence technologies in the field of sports, such as convolutional neural network, object detection, object tracking and action recognition. Then it analyzes the application status of intelligent diagnosis technology and artificial intelligence network under intelligent diagnosis, and puts forward the application of artificial intelligence neural network in automobile fault diagnosis based on examples. In the construction of the neural network system, the real-time collection of vehicle operation data can be analyzed, once the fault is found, the driver can be notified in time to avoid safety accidents. The author summarizes the existing research results on the application of artificial intelligence algorithm in intelligent diagnosis, in order to provide help for the subsequent research.}
}
@incollection{MILLER2023125,
title = {Chapter 7 - Graduate and postgraduate education at a crossroads},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {125-155},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000092},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Pharmaceutical, Pharmacology, Postdoctoral, Postgraduate, Master’s degree, Doctorate},
abstract = {In this chapter, we introduce a proverbial crossroads in graduate and postgraduate education and jobs. We use medicinal chemistry as a core example for many topics, representative of what could also be said about pharmacology and other critical disciplines involved in drug discovery. Many factors are at play today for drug hunters, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has in the past been called “the gathering storm.” Core drug discovery disciplines are discussed, along with the importance of diversity and interdisciplinary skills and the value of academia-industry symbiosis. Challenges in making sure we continue to educate and engage the best and the brightest to tackle important biomedical problems are considered, especially in the context of personalized medicine and its interfaces with big data, bioinformatics, pharmacogenomics, and more. Finally, we scratch the surface on how to navigate graduate school, postdocs, employers, and careers.}
}
@article{KONG20241462,
title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
journal = {Chinese Journal of Physics},
volume = {89},
pages = {1462-1473},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324001655},
author = {Otto C.W. Kong},
keywords = {Quantum information, Quantum locality, Deutsch–Hayden descriptors, Noncommutative values of observables},
abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.}
}
@article{BADIA2024101049,
title = {Analysing the radiation reliability, performance and energy consumption of low-power SoC through heterogeneous parallelism},
journal = {Sustainable Computing: Informatics and Systems},
volume = {44},
pages = {101049},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2024.101049},
url = {https://www.sciencedirect.com/science/article/pii/S2210537924000945},
author = {Jose M. Badia and German Leon and Mario Garcia-Valderas and Jose A. Belloch and Almudena Lindoso and Luis Entrena},
keywords = {Heterogeneous parallelism, System-on-Chip, Fault tolerance, Energy consumption, Neutron irradiation},
abstract = {This study focuses on the low-power Tegra X1 System-on-Chip (SoC) from the Jetson Nano Developer Kit, which is increasingly used in various environments and tasks. As these SoCs grow in prevalence, it becomes crucial to analyse their computational performance, energy consumption, and reliability, especially for safety-critical applications. A key factor examined in this paper is the SoC’s neutron radiation tolerance. This is explored by subjecting a parallel version of matrix multiplication, which has been offloaded to various hardware components via OpenMP, to neutron irradiation. Through this approach, this researcher establishes a correlation between the SoC’s reliability and its computational and energy performance. The analysis enables the identification of an optimal workload distribution strategy, considering factors such as execution time, energy efficiency, and system reliability. Experimental results reveal that, while the GPU executes matrix multiplication tasks more rapidly and efficiently than the CPU, using both components only marginally reduces execution time. Interestingly, GPU usage significantly increases the SoC’s critical section, leading to an escalated error rate for both Detected Unrecoverable Errors (DUE) and Silent Data Corruptions (SDC), with the CPU showing a higher average number of affected elements per SDC.}
}
@article{M2023120604,
title = {Design of a Cognitive Knowledge Representation Model to Assess the Reasoning Levels of Primary School Children},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120604},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120604},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011065},
author = {Srivani M. and Abirami Murugappan},
keywords = {Customized AI based teaching, Cognitive performance test, Reasoning coefficient, Cognition level, Knowledge representation, Cognitive metrics},
abstract = {Background and aim:
In recent days, the research on student’s intelligence level modelling is a challenging Artificial Intelligence (AI) task, which gains more attraction because it provides actionable insights to the tutor by analysing the intelligence level of the learners. Each learner’s knowledge, comprehension, and intellectual capacities are unique. It is critical to identify these capacities and provide learners, particularly slow learners, with the necessary knowledge. Cognitive Performance Test (CPT) is an essential component for assessing the knowledge level of students. The reasoning level or coefficient deals with the analysis of the thinking capability in a logical way. It also reflects the child’s learning potential. The main aim of the proposed system is to design a Cognitive Knowledge Representation Model (CKRM), which fuses Cognitive Performance Metrics (CPM) calculation and Reasoning Coefficient Calculation (RCC) algorithms to assess the student’s intelligence level. The result of the proposed system is stratification of students to three different ranges of reasoning coefficient.
Methods:
The CKRM consists of the following phases: data collection, statistical Exploratory Data Analysis (EDA), model building and analysis, which involve the assessment of the knowledge level using CPT and calculation of reasoning coefficient using First Order Logic (FOL), and finally model evaluation using cognitive evaluation metrics. CPM and RCC algorithms have been proposed in this paper to calculate the student’s reasoning coefficient by using the forward chaining FOL inference engine. The dataset is a real time data which consists of the academic and cognitive performance details of school students from classes 1 to 6 for the year 2019 to 2020. The academic data are collected from the Educational Management Information System (EMIS) maintained by the school. The cognitive performance data are collected by conducting the tests for the students using the memory training application called Lumosity.
Results:
The proposed system’s performance is evaluated using ten Machine Learning (ML) algorithms in which the Quadratic Discriminant Analysis achieved an accuracy of 0.97 for classes 1, 2, and 3. For classes 4, 5, and 6, nearly twelve ML algorithms are evaluated in which Random Forest (RF) Classifier achieved an accuracy of 0.98. Six math expert committee teachers concluded that the reasoning coefficient value was acceptable with an average accuracy of 0.92 for classes 1, 2, 3 and 0.9 for classes 4, 5, 6. In comparison to the pre-existing models employed in the prior research, it was determined that the created CKRM (academic and cognitive) was superior. The cognitive metrics such as taskability, Response Time (RT), knowledge capacity and utilization has also been evaluated. The average values of taskability, RT, knowledge capacity and knowledge utilization are 0.85, 0.81, 0.55, and 0.44.
Conclusion:
The ultimate goal is to make customized teaching easier; hence, this article involves determining a student’s cognitive level by estimating their reasoning coefficient. The suggested approach analyses and categorizes students’ cognitive abilities, such as memory, reasoning, problem solving, thinking, and logical reasoning, using three different reasoning coefficients. This approach assists teachers in determining the degree of intelligence of their students.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{LIU2024105391,
title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
journal = {Automation in Construction},
volume = {162},
pages = {105391},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001274},
author = {Yuming Liu and Aidi Hizami Bin Alias and Nuzul Azam Haron and Nabilah Abu Bakar and Hao Wang},
keywords = {Construction robotics, BERTopic model, BIM, Human–robot collaboration, Deep reinforcement learning, Dual-track quantitative analysis},
abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.}
}
@article{LI2021104369,
title = {Elementary effects analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of social distancing and mask usage},
journal = {Computers in Biology and Medicine},
volume = {134},
pages = {104369},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104369},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001633},
author = {Kelvin K.F. Li and Stephen A. Jarvis and Fayyaz Minhas},
keywords = {COVID-19, Agent-based modelling, Coronavirus, Simulation, SARS-COV-2, netlogo, Python, Epidemiology, Survival, Infectious diseases, VIRUS, Stochastic processes, Stochasticity, Social distancing, Masks, Isolation, Lockdown},
abstract = {COVID-19 was declared a pandemic by the World Health Organisation (WHO) on March 11th, 2020. With half of the world's countries in lockdown as of April due to this pandemic, monitoring and understanding the spread of the virus and infection rates and how these factors relate to behavioural and societal parameters is crucial for developing control strategies. This paper aims to investigate the effectiveness of masks, social distancing, lockdown and self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings from an agent-based simulation modelling showed that whilst requiring a lockdown is widely believed to be the most efficient method to quickly reduce infection numbers, the practice of social distancing and the usage of surgical masks can potentially be more effective than requiring a lockdown. Our multivariate analysis of simulation results using the Morris Elementary Effects Method suggests that if a sufficient proportion of the population uses surgical masks and follows social distancing regulations, then SARS-CoV-2 infections can be controlled without requiring a lockdown.}
}
@incollection{ALISEDA2007431,
title = { - Logical, Historical and Computational Approaches},
editor = {Theo A.F. Kuipers},
booktitle = {General Philosophy of Science},
publisher = {North-Holland},
address = {Amsterdam},
pages = {431-513},
year = {2007},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-044451548-3/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444515483500100},
author = {Atocha Aliseda and Donald Gillies},
abstract = {Publisher Summary
This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the ﬁrst time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important inﬂuences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientiﬁc revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the ﬁrst time.}
}
@article{FATTAHITABASI20221151,
title = {Design and mechanism of building responsive skins: State-of-the-art and systematic analysis},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {6},
pages = {1151-1176},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095263522000565},
author = {Saba {Fattahi Tabasi} and Saeed Banihashemi},
keywords = {Responsive skin, Architectural design, Mechanism design},
abstract = {The demand to satisfy environmental and economic performance requirements of buildings highlights the application of the responsive skin facades in offering superior performance, as compared to conventional façades. With this respect, responsive skins have become a growing field of research during the recent decade while a thorough review of studies investigating their design and technology aspects is still missing. To fill the identified gap, this study aims to present a systematic literature review and state of the art in an untouched research area of the responsive skins, integrated with their geometric and mechanism design approaches. To this end, a total of 89 studies, collected from two major bibliographic databases of Scopus and Google Scholar from the first of 2010 to the mid of 2021, were reviewed and several classifications and analyses on the associated design thinking, skin systems and responsive mechanisms were presented. The gap analysis of the findings indicates that the lack of controllable substitution design for mechanical skins is one of the reasons preventing the application of responsive skins in construction industry. Furthermore, the gap between simulation and constructability and the relationship between the designed skin geometry with climatic analysis and performance provide basis for future studies.}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{MASHALEH20242245,
title = {IoT Smart Devices Risk Assessment Model Using Fuzzy Logic and PSO},
journal = {Computers, Materials and Continua},
volume = {78},
number = {2},
pages = {2245-2267},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.047323},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001267},
author = {Ashraf S. Mashaleh and Noor Farizah Binti Ibrahim and Mohammad Alauthman and Mohammad Almseidin and Amjad Gawanmeh},
keywords = {IoT botnet detection, risk assessment, fuzzy logic, particle swarm optimization (PSO), cybersecurity, interconnected devices},
abstract = {Increasing Internet of Things (IoT) device connectivity makes botnet attacks more dangerous, carrying catastrophic hazards. As IoT botnets evolve, their dynamic and multifaceted nature hampers conventional detection methods. This paper proposes a risk assessment framework based on fuzzy logic and Particle Swarm Optimization (PSO) to address the risks associated with IoT botnets. Fuzzy logic addresses IoT threat uncertainties and ambiguities methodically. Fuzzy component settings are optimized using PSO to improve accuracy. The methodology allows for more complex thinking by transitioning from binary to continuous assessment. Instead of expert inputs, PSO data-driven tunes rules and membership functions. This study presents a complete IoT botnet risk assessment system. The methodology helps security teams allocate resources by categorizing threats as high, medium, or low severity. This study shows how CICIoT2023 can assess cyber risks. Our research has implications beyond detection, as it provides a proactive approach to risk management and promotes the development of more secure IoT environments.}
}
@article{COELHOLOPES2023103555,
title = {The structure of a strategic crisis management model: The context and characteristics of a brazilian community college},
journal = {International Journal of Disaster Risk Reduction},
volume = {87},
pages = {103555},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.103555},
url = {https://www.sciencedirect.com/science/article/pii/S2212420923000353},
author = {Gisele Silveira {Coelho Lopes} and Carlos Ricardo Rossetto and Micheline {Ramos de Oliveira} and Jorge Oneide Sausen and Rudimar {Antunes da Rocha}},
keywords = {Crisis management, Organizational strategy, Community university},
abstract = {This study presents the structure of a strategic crisis management model, considering the context and characteristics of a Brazilian community college. Strategic crisis management associated with coordination and control mechanisms theoretically underpinned the problem under study. It is a single case study, with grounded theory as the strategy for data treatment and content analysis to interpret and present the findings. In the model, the strategic and tactical stages systematized the dynamics of crisis management in a coordinated manner. Strategic crisis management was considered a continuous process rather than a strictly punctual one. The dynamism of the model's operationalization considered some premises that guided the behavior of the leadership and the crisis management team: i) pragmatic strategic thinking shaped by rationality; ii) quick responses in facing the crisis; iii) simplicity in actions; iv) reversible decisions susceptible to flexibilization; v) creativity and boldness to innovate and set new standards; v) collaboration for common causes.}
}
@article{LI2025e42756,
title = {Classifying breast intraductal proliferative lesions via a knowledge distillation framework using convolutional neural network-based nuclei-segmentation-assisted classification (KDCNN-NSAC)},
journal = {Heliyon},
pages = {e42756},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42756},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025011375},
author = {Xiangmin Li and Jiamei Chen and Bo Luo and Minyan Xia and Xu Zhang and Hangjia Zhu and Yutian Zhang-Cai and Yongshun Chen and Yang Yang and Yaofeng Wen},
keywords = {Breast intraductal proliferative lesions, Breast cancer, Knowledge distillation, Convolutional neural network, Nuclei segmentation, Classification},
abstract = {ABSTRACT
Background and objective
Diagnosis of breast intraductal proliferative lesions (BIDPLs) in hematoxylin-eosin (HE) images remains a time-consuming and intractable topic because of subjective processes and subtle morphological differences. Convolutional neural networks (CNNs) show great potential for providing objective analysis strategies for HE images. In this study, we proposed a novel knowledge distillation (KD) framework using CNN-based nuclei segmentation-assisted classification (KDCNN-NSAC).
Methods
The diagnosis of BIDPLs is treated as multiple class classification tasks in the BReAst Carcinoma Subtyping dataset. The KDCNN-NSAC fully leveraged the epithelial and stromal nuclei-level features in training phases and performed region-of-interest (ROI)-level classifications in predicting phases. Then, the whole slide image (WSI) was diagnosed based on the risk ratings of the ROIs within it, instead of processing a WSI.
Results
The principal results showed that in ROI-level classifications, KDCNN-NSAC outperformed the state-of-the-art methods for 7-class classification with an average F1 score of 63.26% and achieves F1 score of 98.36% and 94.21%, respectively, in distinguishing BIDPLs from invasive cancer and normal tissue. The WSI-level predictions obtained a high degree of consistency with the pathologists’ annotation (kappa value of 0.88). Ablation experiments showed that nuclei segmentation and classification components improve the performance of the baseline model in KDCNN-NSAC by 3%.
Conclusions
The KDCNN-NSAC makes the model focus on important cellular information and predicts the WSI in accordance with the pathologists’ diagnostic thinking, thus improving model explainability. Moreover, the introduce of KDCNN-NSAC will help achieve superior performance in diagnosing BIDPLs.}
}
@article{PARTTO2012442,
title = {Explaining failures in innovative thought processes in engineering design},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {442-449},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009330},
author = {Minna Pärttö and Pertti Saariluoma},
keywords = {Microinnovation processes, engineering design, thought errors, thought failures},
abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.}
}
@article{MIASNIKOVA202126,
title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
journal = {Neuroscience Research},
volume = {166},
pages = {26-33},
year = {2021},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2020.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010219305772},
author = {Aleksandra Miasnikova and Gleb Perevoznyuk and Olga Martynova and Mikhail Baklushev},
keywords = {Abstract reasoning, Salience, Phase synchronization, Cross-frequency coupling, Phase-to-phase coupling, EEG},
abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.}
}
@article{AIGBAVBOA20173003,
title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
journal = {Energy Procedia},
volume = {105},
pages = {3003-3010},
year = {2017},
note = {8th International Conference on Applied Energy, ICAE2016, 8-11 October 2016, Beijing, China},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.743},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217308068},
author = {Clinton Aigbavboa and Ifije Ohiomah and Thulisile Zwane},
keywords = {Climate change, sustainable thinking, sustainable construction practices, South Africa},
abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{GAO20241233,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
volume = {92},
pages = {1233-1239},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.}
}
@incollection{PAUL2005431,
title = {Chapter 15 - Models of Computation for Systems-on-Chips},
editor = {Ahmed Amine Jerraya and Wayne Wolf},
booktitle = {Multiprocessor Systems-on-Chips},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {431-463},
year = {2005},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012385251-9/50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852519500311},
author = {JoAnn M. Paul and Donald E. Thomas},
abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.}
}
@article{CASTELLO202354,
title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {54-62},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000210},
author = {E. Castelló and C. Santiviago and J. Ferreira and R. Coniglio and E. Budelli and V. Larnaudie and M. Passeggi and I. López},
keywords = {Engineering education, Professional skills, Unconventional laboratory practice, Industrial Internships, Autonomous learning},
abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.}
}
@article{CHEN1998475,
title = {A computationally attractive nonlinear predictive control scheme with guaranteed stability for stable systems},
journal = {Journal of Process Control},
volume = {8},
number = {5},
pages = {475-485},
year = {1998},
note = {ADCHEM '97 IFAC Symposium: Advanced Control of Chemical Processes},
issn = {0959-1524},
doi = {https://doi.org/10.1016/S0959-1524(98)00021-3},
url = {https://www.sciencedirect.com/science/article/pii/S0959152498000213},
author = {H. Chen and F. Allgöwer},
keywords = {nonlinear predictive control, constraints, stability, terminal conditions},
abstract = {We introduce in this paper a nonlinear model predictive control scheme for open-loop stable systems subject to input and state constraints. Closed-loop stability is guaranteed by an appropriate choice of the finite prediction horizon, independent of the specification of the desired control performance. In addition, this control scheme is likely to allow ‘real time’ implementation, because of its computational attractiveness. The theoretical results are demonstrated and discussed with a CSTR control application.}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{HO1993567,
title = {Recent Applications of Symbolic Computation in Control System Design},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 2},
pages = {567-570},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 2 Robust Control, Design and Software, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49006-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017490069},
author = {D.W.C. Ho and J. Lam and S.K. Tin and C.Y. Han},
keywords = {Symbolic Computation, Computer-aided Control System Design},
abstract = {In this paper we describe several recent applications of symbolic computation to control system design based on MACSYMA. These include routines to calculate the transfer function of a control system in block diagram representation and to compute and simplify state space realizations of multivariable control systems. The programs provide a quick way to formulate design problem and automate the calculation in the initial stage of a control system design process. An example on the application of these routines in the setting up of generalized plant state space realization for use in H∞/H2 optimial control is provided}
}
@incollection{RUBIN2023125,
title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {125-145},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000062},
author = {Matthew Rubin and Ella Miron-Spektor and Joshua Keller},
keywords = {Creativity, Innovation, Paradox, Mindset, Paradoxical leadership, Culture},
abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.}
}
@incollection{JAIN2015181,
title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
editor = {Sarah A. Woodson and Frédéric H.T. Allain},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {558},
pages = {181-212},
year = {2015},
booktitle = {Structures of Large RNA Molecules and Their Complexes},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915000208},
author = {Swati Jain and David C. Richardson and Jane S. Richardson},
keywords = {RNA crystallography, RNA backbone conformers, Ribose pucker, Clash correction, MolProbity, PHENIX, ERRASER, wwPDB validation},
abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{ALKABI202368,
title = {Proposed artificial intelligence algorithm and deep learning techniques for development of higher education},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {68-73},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000039},
author = {Amin {Al Ka'bi}},
keywords = {Artificial intelligence (AI), Communication systems, Higher education, Neural networks, Attention process, Deep learning},
abstract = {Artificial intelligence (AI) has been increasingly impacting various aspects of our daily lives, including education. With the rise of digital technologies, higher education has also been experiencing a transformation, and AI has been playing a crucial role in this transformation. The application of AI in higher education has been rapidly increasing, with a focus on improving student engagement, increasing efficiency, and enhancing the learning experience. The use of AI in higher education is not without its challenges and ethical considerations. One of the biggest challenges is ensuring the accuracy and fairness of AI algorithms, as well as avoiding potential biases. In addition, there are concerns about the privacy of student data, as well as the potential for AI to replace human instructors and support staff. Another challenge is ensuring that AI is used in a way that supports the overall goals of higher education, such as promoting critical thinking and creativity, rather than just being used as a tool for automating tasks and increasing efficiency. In this article, we will discuss the various ways in which AI is being applied in higher education where a proposed model for improving the cognitive capability of students is proposed and compared to other existing algorithms. It will be shown that the proposed model shows better performance compared to other models.}
}
@article{JORAJURIA2022664,
title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
journal = {Neurocomputing},
volume = {492},
pages = {664-675},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018956},
author = {Tania Jorajuría and Mina {Jamshidi Idaji} and Zafer İşcan and Marisol Gómez and Vadim V. Nikulin and Carmen Vidaurre},
keywords = {Brain-computer interface, Steady-state visual evoked potential, Spatio-spectral decomposition, Higher order discriminant analysis, Analytical regularization, Tensor-based feature reduction},
abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.}
}
@article{KLIMUSOVA2016652,
title = {Psychometric Properties of the Learning Potential Test},
journal = {Procedia - Social and Behavioral Sciences},
volume = {217},
pages = {652-656},
year = {2016},
note = {Future Academy Multidisciplinary Conference “ICEEPSY & CPSYC & icPSIRS & BE-ci” 13–17 October 2015 Istanbul},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2016.02.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877042816001142},
author = {Helena Klimusová and Petr Květon},
keywords = {psychometrics, admission test},
abstract = {The use of cognitive ability tests to help select highly performing students is becoming a standard in most major Czech universities. Such tests need to show good psychometric properties. To highlight the importance of these properties to the process of selection, this study explores the psychometric properties of the Learning Potential test, which is used as a selection criterion within the admission procedure at a major Czech university. This study's objective is to assess the psychometric properties of the Learning Potential Test and provide an insight into its structure. The Cronbach's alpha were computed to assess the internal consistency of the test. The structure of the items was explored by the factor analysis methods. Factor analysis indicated the anticipated structure of the test with two major factors - critical thinking/verbal reasoning abilities and numerical/spatial/analytical abilities. Since the role of admission tests in the process of selection new university students is crucial, it is essential to periodically reassess its psychometric characteristics to ensure that our test remain relevant and applicable.}
}
@article{CHAN2022102109,
title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
journal = {Learning and Individual Differences},
volume = {93},
pages = {102109},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102109},
url = {https://www.sciencedirect.com/science/article/pii/S1041608021001461},
author = {Jenny Yun-Chen Chan and Erin R. Ottmar and Ji-Eun Lee},
keywords = {Pause time, Strategy efficiency, Algebra problem-solving, Online learning environment, Metacognitive skills},
abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.}
}
@article{CARLISLE1996248,
title = {Software Caching and Computation Migration in Olden},
journal = {Journal of Parallel and Distributed Computing},
volume = {38},
number = {2},
pages = {248-255},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0145},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901458},
author = {Martin C. Carlisle and Anne Rogers},
abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.}
}
@article{KING1980313,
title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
journal = {Artificial Intelligence},
volume = {13},
number = {3},
pages = {313-322},
year = {1980},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(80)90005-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370280900053},
author = {Margaret King}
}
@article{ZORAN2025101135,
title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
journal = {International Journal of Gastronomy and Food Science},
volume = {39},
pages = {101135},
year = {2025},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2025.101135},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X25000368},
author = {Amit Raphael Zoran},
abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.}
}
@article{HAGSTROM1998385,
title = {Experiments with approximate radiation boundary conditions for computational aeroacoustics},
journal = {Applied Numerical Mathematics},
volume = {27},
number = {4},
pages = {385-402},
year = {1998},
note = {Special Issue on Absorbing Boundary Conditions},
issn = {0168-9274},
doi = {https://doi.org/10.1016/S0168-9274(98)00021-X},
url = {https://www.sciencedirect.com/science/article/pii/S016892749800021X},
author = {Thomas Hagstrom and John Goodrich},
keywords = {Nonreflecting Boundary Conditions, Aeroacoustics},
abstract = {We present a series of numerical experiments on the accuracy of approximate radiation boundary conditions for computational aeroacoustics based on Padé approximants. Our test problem is described by an infinite periodic array of pressure pulses, for which we can independently evaluate the exact solution by numerical quadrature. It is demonstrated that acceptable long time accuracy can be achieved, but only if conditions of high order are employed. As predicted by theory, the order required for a given accuracy is proportional to the time of the simulation.}
}
@article{TARMIZI2010384,
title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {384-392},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810021592},
author = {Rohani Ahmad Tarmizi and Sahar Bayat},
keywords = {Problem-based learning, E-learning, Statistic learning performance, Mental load},
abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{CUI2024124662,
title = {Cooperative interference to achieve interval many-objective evolutionary algorithm for association privacy secure computing migration},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124662},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124662},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401529X},
author = {Zhihua Cui and Zhenyu Shi and Qi Li and Tianhao Zhao and Wensheng Zhang and Jinjun Chen},
keywords = {Mobile edge computing, Computing migration, Physical layer security(PLS), Interval many-objective optimization, Evolutionary algorithm},
abstract = {In this paper, we study secure computing migration scenarios in uncertain environments with the presence of multiple malicious eavesdroppers (MEs). Specifically, when edge servers (ESs) execute tasks delivered by smart devices (SDs), SDs may move beyond the coverage of ESs, and computing migration (CM) of unfinished tasks is required to ensure service continuity. There is a risk of privacy leakage during task migration, and MEs use colluding eavesdropping to eavesdrop on the migrated tasks, and we consider eavesdropping on the associated tasks through data sharing among MEs to improve the eavesdropping efficiency. For eavesdropping in MEs, we achieve eavesdropping strikes using cooperative interference by jammers, which benefit by providing jamming services. In addition, uncertain computational scenarios directly affect the efficiency of task execution, and we consider the uncertainty factor in the malicious eavesdropping environment. To this end, this paper proposes the secure computational migration of associative privacy in uncertain environments (SCMAPUE) model, which transforms uncertainties into interval parameters, and optimizes the five objectives of migration delay, maximum completion time, energy consumption, load balancing and migration reliability to achieve efficient task execution and reliable migration. Aiming at the model characteristics, this paper designs an interval many-objective evolutionary algorithm for reliable migration (IMaOEA-RM), which employs a condition-based interval confidence strategy and a multi-access secure migration selection strategy to improve the convergence of the algorithm, and utilizes a dual-migration crossover strategy in order to adjust the jammer partners and improve the population diversity. Simulation results show that our proposed IMaOEA-RM algorithm can provide a more reliable and efficient migration scheme than existing algorithms.}
}
@article{CHAKRAVARTY2010606,
title = {The creative brain – Revisiting concepts},
journal = {Medical Hypotheses},
volume = {74},
number = {3},
pages = {606-612},
year = {2010},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2009.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0306987709006926},
author = {Ambar Chakravarty},
abstract = {Summary
Creativity is a complex neuro-psycho-philosophical phenomenon which is difficult to define literally. Fundamentally it involves the ability to understand and express novel orderly relationships. The creative process involves four stages – preparation, incubation, illumination and verification. A high level of general intelligence, domain specific knowledge and special skills are necessary pre-requisites. It is possible that in addition, some creative people might have architectural alternations of specific portions of the posterior neocortex. Associated with such pre-requisites, the process of creative innovation (incubation and illumination stages) necessitates the need for an ability of divergent thinking, a novelty seeking behavior, some degree of suppression of latent inhibition and a subtle degree of frontal dysfunction. The author hypothesizes that these features are often inter-linked and subtle frontally disinhibited behavior is conducive towards creativity by allowing uninterrupted flow of creative thought possessing and opening up new avenues towards problem solving. Perhaps the most essential feature of the creative brain is its degree of connectivity – both inter-hemispheric and intra-hemispheric. Connectivity correlates or binds together functions of apparently structurally isolated domains on brain modules sub-serving different functions. It is felt that creative cognition is a self rewarding process where divergent thinking would promote connectivity through development of new synapses. In addition, the phenomenon of synaesthesia has often been observed in creative visual artists. Creative innovation often occurs during low arousal states and creative people often manifests features of affective disorders. This suggests a role of neurotransmitters in creative innovation. Dopaminergic pathways are involved in the novelty seeking attitude of creative people while norepinephrine levels are depressed during discovery of novel orderly relationships. The relationship between mood and catecholamines and that of creative cognition is often in an inverted U-shaped form. It is hypothesized that that subtle frontal dysfunction is a pre-requisite for creative cognition but here again the relationship is also in an inverted U-form.}
}
@article{BOYDDAVIS2018185,
title = {‘A dialogue between the real-world and the operational model’ – The realities of design in Bruce Archer’s 1968 doctoral thesis},
journal = {Design Studies},
volume = {56},
pages = {185-204},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300893},
author = {Stephen {Boyd Davis} and Simone Gristwood},
keywords = {design history, philosophy of design, science of design, design research, systematic method},
abstract = {The article centres on a single document, the 1968 doctoral thesis of L. Bruce Archer. It traces Archer’s earlier publications and the sources that informed and inspired his thinking as a way of understanding his influential work at the Royal College of Art from 1962. Analysis suggests that Archer’s ambition for a rigorous ‘science of design’ inspired by linear algorithmic approaches was increasingly threatened with disruption by his experience of large, complex design projects. Reflecting on Archer's engagement with other models of designing, the article ends with Archer’s retrospective view and an account of his significantly altered opinions. Archer is located as both a theorist and someone fascinated by the commercial and practical aspects of designing.}
}
@article{KUAI2020101103,
title = {The extensible Data-Brain model: Architecture, applications and directions},
journal = {Journal of Computational Science},
volume = {46},
pages = {101103},
year = {2020},
note = {20 years of computational science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320300752},
author = {Hongzhi Kuai and Ning Zhong},
keywords = {Artificial intelligence (AI), Brain informatics, Brain computing, Data-Brain, Brain big data, Web intelligence (WI), Intelligence systems},
abstract = {One of the key ideas in realizing human-like intelligence is to understand information-processing mechanisms in the human brain. Brain Informatics is a rapidly expanding interdisciplinary field to systematically utilize brain-related data, information and knowledge coming from the entire research process for in-depth brain investigation. In the past few years, a data-centric conceptual brain model, namely Data-Brain, has been proposed, providing the foundation for the systematic Brain Informatics methodology. The Data-Brain model constitutes a conceptual framework and detailed guideline for managing and analyzing brain big data. The development of Data-Brain model also demands the support from advanced technologies. This paper presents an extensible version of the Data-Brain with advanced computing techniques in the connected world. It provides a global understanding of how multidisciplinary techniques work together to tackle brain computing challenges. Particularly, the integrated K-I-D (Knowledge-Information-Data) loop is proposed, constructing a cycle as the thinking space to help pursue the systematic brain investigation, by which the extensible Data-Brain model continuously iterates and evolves through the never-ending learning. Such synergistic evolvement will power future progress for building intelligence systems and applications connected with the study of complex human brain.}
}
@article{KING2023104028,
title = {Identifying risk controls for future advanced brain-computer interfaces: A prospective risk assessment approach using work domain analysis},
journal = {Applied Ergonomics},
volume = {111},
pages = {104028},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104028},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023000662},
author = {Brandon J. King and Gemma J.M. Read and Paul M. Salmon},
keywords = {Brain-computer interfaces, Risk assessment, System modelling},
abstract = {Brain-computer interface (BCI) technologies are progressing rapidly and may eventually be implemented widely within society, yet their risks have arguably not yet been comprehensively identified, nor understood. This study analysed an anticipated invasive BCI system lifecycle to identify the individual, organisational, and societal risks associated with BCIs, and controls that could be used to mitigate or eliminate these risks. A BCI system lifecycle work domain analysis model was developed and validated with 10 subject matter experts. The model was subsequently used to undertake a systems thinking-based risk assessment approach to identify risks that could emerge when functions are either undertaken sub-optimally or not undertaken at all. Eighteen broad risk themes were identified that could negatively impact the BCI system lifecycle in a variety of unique ways, while a larger number of controls for these risks were also identified. The most concerning risks included inadequate regulation of BCI technologies and inadequate training of BCI stakeholders, such as users and clinicians. In addition to specifying a practical set of risk controls to inform BCI device design, manufacture, adoption, and utilisation, the results demonstrate the complexity involved in managing BCI risks and suggests that a system-wide coordinated response is required. Future research is required to evaluate the comprehensiveness of the identified risks and the practicality of implementing the risk controls.}
}
@article{DALLAQUA2021422,
title = {ForestEyes Project: Conception, enhancements, and challenges},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {422-435},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001965},
author = {Fernanda B.J.R. Dallaqua and Álvaro L. Fazenda and Fabio A. Faria},
keywords = {Citizen Science, Deforestation area detection, Rainforest, Tropical forest, Volunteered Thinking},
abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.}
}
@article{CHOI199617,
title = {Computation and semiotic practice as compositional process},
journal = {Computers & Mathematics with Applications},
volume = {32},
number = {1},
pages = {17-35},
year = {1996},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(96)00084-3},
url = {https://www.sciencedirect.com/science/article/pii/0898122196000843},
author = {I. Choi},
keywords = {Dynamical systems, Semiotics, Computer music, Synergetics, Cognitive systems, Music composition, Chaos, Music synthesis},
abstract = {In sound computation, computational processes are brought into the acoustic domain by a set of formalized instructions for controlling parameters in synthesis engines and compositional algorithms. From the acoustic events, listeners often extract patterns or “musical objects” in their perception to the extent that certain associations are made external to the computational process. Perceived, musical objects rapidly become immutable, and that immutability may be considered a compositional problem. The problem is how to approach a compositional project for bringing new insights into play while, on one side, using the existing representational system such as symbolic language in computation, and on the other side, facing listeners' perceptual tendency to make external associations. For composers, the problem requires technical solutions as well as an ability to articulate the philosophical issues. This problem also exists in semiotics, a general study of signs, when one has to borrow language from existing linguistic systems in order to express a new thought without being trapped within the immutability of given linguistic sources. Semiotic practice is a discipline which emphasizes the function of semiotics to generate necessary discourse to examine the linguistic system in use and its logocentric tendency—the tendency towards known signs. We define semiotic practice as a signifying process in which meaning may be generated during that particular process under study; thus, meaning in semiotic practice is temporal context-dependent as a function of signifiers. The compositional problems involving sound computation for generating cases to support semiotic practice inquires about two tasks: 1.(1) how to design software which enables acoustic events to be observed as processes rather than observing sounds only as familiar objects or transformations featuring the recognition of objects, and2.(2) how to compose a piece of music so mutability of signs can be observed. To meet these problems, this paper examines perspectives on systems and cognition from multiple views such as semiotics, computation theories, and synergetics. We also discuss software designed for composition in terms of semiotic practice.}
}
@article{DECARO200758,
title = {Methodologies for examining problem solving success and failure},
journal = {Methods},
volume = {42},
number = {1},
pages = {58-67},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002982},
author = {Marci S. DeCaro and Mareike Wieth and Sian L. Beilock},
keywords = {Working memory, Performance, Pressure, Individual differences, Problem solving, Creativity, Short term memory, Stress, Math},
abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.}
}
@article{PELAEZ2025109363,
title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
journal = {Computer Physics Communications},
volume = {306},
pages = {109363},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109363},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524002868},
author = {Raúl P. Peláez and Pablo Ibáñez-Freire and Pablo Palacios-Alonso and Aleksandar Donev and Rafael Delgado-Buscalioni},
keywords = {Molecular dynamics, Hydrodynamics, C++, CUDA, Soft matter},
abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.}
}
@article{HULME2017345,
title = {From control to causation: Validating a ‘complex systems model’ of running-related injury development and prevention},
journal = {Applied Ergonomics},
volume = {65},
pages = {345-354},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730159X},
author = {A. Hulme and P.M. Salmon and R.O. Nielsen and G.J.M. Read and C.F. Finch},
keywords = {Systems ergonomics, STAMP, Sports injury prevention, Running injury},
abstract = {Introduction
There is a need for an ecological and complex systems approach for better understanding the development and prevention of running-related injury (RRI). In a previous article, we proposed a prototype model of the Australian recreational distance running system which was based on the Systems Theoretic Accident Mapping and Processes (STAMP) method. That model included the influence of political, organisational, managerial, and sociocultural determinants alongside individual-level factors in relation to RRI development. The purpose of this study was to validate that prototype model by drawing on the expertise of both systems thinking and distance running experts.
Materials and methods
This study used a modified Delphi technique involving a series of online surveys (December 2016- March 2017). The initial survey was divided into four sections containing a total of seven questions pertaining to different features associated with the prototype model. Consensus in opinion about the validity of the prototype model was reached when the number of experts who agreed or disagreed with survey statement was ≥75% of the total number of respondents.
Results
A total of two Delphi rounds was needed to validate the prototype model. Out of a total of 51 experts who were initially contacted, 50.9% (n = 26) completed the first round of the Delphi, and 92.3% (n = 24) of those in the first round participated in the second. Most of the 24 full participants considered themselves to be a running expert (66.7%), and approximately a third indicated their expertise as a systems thinker (33.3%). After the second round, 91.7% of the experts agreed that the prototype model was a valid description of the Australian distance running system.
Conclusion
This is the first study to formally examine the development and prevention of RRI from an ecological and complex systems perspective. The validated model of the Australian distance running system facilitates theoretical advancement in terms of identifying practical system-wide opportunities for the implementation of sustainable RRI prevention interventions. This ‘big picture’ perspective represents the first step required when thinking about the range of contributory causal factors that affect other system elements, as well as runners' behaviours in relation to RRI risk.}
}
@article{RYDER2022100703,
title = {Rethinking reflective practice: John Boyd’s OODA loop as an alternative to Kolb},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100703},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100703},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722001057},
author = {Mike Ryder and Carolyn Downs},
keywords = {OODA, Reflective practice, Experiential learning, Work-based learning, Employability, Business education},
abstract = {The world is changing and business schools are struggling to keep up. Theories of reflective practice developed by the likes of Schon (1983), Gibbs (1988), Driscoll (1994, 2007) and Kolb (1984, 2015) are outdated and unfit for current purposes. Problems include the chronology of events, the orientation of the observer, the impact of external inputs, and the fact that neither education nor the workplace follow a structured, linear path. In response to these challenges, we propose a new ‘solution’: John Boyd's OODA loop. We argue that OODA loops offer the chance to reshape reflective practice and work-based learning for a world in which individuals must cope with ‘an unfolding evolving reality that is uncertain, ever changing and unpredictable’ (Boyd, 1995, slide 1). By embracing the philosophy of John Boyd and his OODA loop theory, business schools can develop greater resilience and employability in graduates, preparing them to embrace change while also embedding the concept of life-long learning to make them better equipped to face the uncertainty that the modern world brings.}
}
@article{TIAN2018104,
title = {The association between visual creativity and cortical thickness in healthy adults},
journal = {Neuroscience Letters},
volume = {683},
pages = {104-110},
year = {2018},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2018.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0304394018304397},
author = {Fang Tian and Qunlin Chen and Wenfeng Zhu and Yongming Wang and Wenjing Yang and Xingxing Zhu and Xue Tian and Qinglin Zhang and Guikang Cao and Jiang Qiu},
keywords = {Visual creativity, Cortical thickness, Prefrontal cortex, Supplementary motor cortex, Insula},
abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.}
}
@article{PERCHTOLDSTEFAN202398,
title = {Functional EEG Alpha Activation Patterns During Malevolent Creativity},
journal = {Neuroscience},
volume = {522},
pages = {98-108},
year = {2023},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306452223002178},
author = {Corinna M. Perchtold-Stefan and Christian Rominger and Ilona Papousek and Andreas Fink},
keywords = {malevolent creativity, EEG, alpha power, time-course, divergent thinking},
abstract = {On the dark side of creativity, creative ideation is intentionally used to damage others. This first electroencephalogram (EEG) study on malevolent creativity investigated task-related power (TRP) changes in the alpha band while n = 89 participants (52 women, 37 men) generated original ideas for revenge in the psychometric Malevolent Creativity Test. TRP changes were assessed for different stages of the idea generation process and linked to performance indicators of malevolent creativity. This study revealed three crucial findings: 1) Malevolent creativity yielded topographically distinct alpha power increases similar to conventional creative ideation. 2) Time-related activity changes during malevolent creative ideation were reflected in early prefrontal and mid-stage temporal alpha power increases in individuals with higher malevolent creativity performance. This performance-related, time-sensitive pattern of TRP changes during malevolent creativity may reflect early conceptual expansion from prosocial to antisocial perspectives, and subsequent inhibition of dominant semantic associations in favor of novel revenge ideas. 3) The observed, right-lateralized alpha power increases over the entire ideation phase may denote an additional emotional load of creative ideation. Our study highlights the seminal role of EEG alpha oscillations as a biomarker for creativity, also when creative processes operate in a malevolent context.}
}
@article{DELLACQUA20241727,
title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
journal = {Procedia Computer Science},
volume = {239},
pages = {1727-1735},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015953},
author = {Pierangelo Dell’Acqua and Stefania Costantini and Abeer Dyoub and Giovanni De Gasperis and Andrea Monaldini and Andrea Rafanelli},
keywords = {Behavior Trees, Affective Computing, Decision Making},
abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.}
}
@article{KARIMI199745,
title = {A Parallel Algorithm for Routing: Best Solutions at Low Computational Costs},
journal = {Geomatica},
volume = {51},
number = {1},
pages = {45-51},
year = {1997},
issn = {1195-1036},
doi = {https://doi.org/10.5623/geomat-1997-0006},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624002969},
author = {Hassan A. Karimi and Dongming Hwang},
abstract = {Routing is a common activity in network analysis. Of the many routing algorithms available, Dijkstra’s algorithm is one of the most widely used for computing best paths in networks. Dijkstra’s algorithm guarantees best solutions with a time complexity ofO(N2) in the worst case. This time complexity results in very long processing times for computing best paths in large networks, which can be a serious problem for real-time applications. Alternative approaches that give a better time complexity have been suggested, most of which are heuristics. However, these heuristics do not guarantee best solutions. To achieve best solutions at low computational costs, a parallel algorithm for parallel machines is suggested.
L’établissement de parcours s’emploie fréquemment dans l’analyse de réseau. De tous les algorithmes d’établissement de parcours disponibles, l’algorithme de Dijkstra est parmi ceux qu’on emploie le plus souvent afin de calculer les meilleurs itinéraires de réseaux. Cet algorithme assure les meilleures solutions avec une complexité temporelle de 0(N2) dans la pire éventualité. Cette complexité temporelle entraîne des temps de traitement très longs lors du calcul des meilleurs itinéraires dans les grands réseaux, ce qui peut constituer un problème sérieux pour les applications en temps réel. On a suggéré d’autres méthodes qui offrent une meilleure complexité temporelle, la plupart étant des heuristiques. Toutefois, ces heuristiques ne garantissent pas les meilleures solutions. Afin d’en arriver aux meilleurs solutions tout en réduisant les coûts de calcul, on suggère un algorithme parallèle pour des machines parallèles.}
}
@incollection{DRYGAS20201,
title = {1 - Introduction to computational methods and theory of composites},
editor = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
booktitle = {Applied Analysis of Composite Media},
publisher = {Woodhead Publishing},
pages = {1-56},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102670-0},
doi = {https://doi.org/10.1016/B978-0-08-102670-0.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102670000010X},
author = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
keywords = {Self-consistent approximation, structural sum, statistical mechanics methods, self-Similarity and renormalization-group},
abstract = {Overview of traditional approaches based on self-consistent approximations in composite materials is presented. Their restrictions are underlined. Neoclassical approach previously introduced in the Preface, is illustrated and compared to methods applied in statistical mechanics. The structural sums, the key construction of the neoclassical approach, are outlined. Method of series and asymptotic method of approximants, Padé approximants, DLog Padé approximants, Factor, Root, Additive approximants are briefly discussed. Notion of Self-Similarity and renormalization-group is introduced. Minimal difference and minimal derivative methods of calculation for short series are discussed in detail. Critical Index is calculated from various short series. DLog root approximants are introduced and illustrate by several examples, where the DLog Padé approximants fail. DLog additive approximants are introduced and presented iteratively. Multiple examples are presented in the chapter and in the appendix. Method of Log Padé approximants is suggested.}
}
@article{WANG2025102570,
title = {ST-TNet: An spatio-temporal joint transformer network for CSI feedback in FDD-MIMO systems},
journal = {Physical Communication},
volume = {68},
pages = {102570},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102570},
url = {https://www.sciencedirect.com/science/article/pii/S187449072400288X},
author = {Linyu Wang and Yize Cao and Jianhong Xiang},
keywords = {Massive MIMO, CSI feedback, Deep learning, Lightweighting, Transformer, Attention},
abstract = {In recent years, deep learning methods have been shown to have strong potential and superiority in reducing channel state information (CSI) feedback overhead and further improving feedback accuracy to maximize the performance benefits of massive Multiple-Input Multiple-Output (MIMO) in frequency division duplex (FDD) mode. As the CSI matrices are transformed into sequences for input to the Transformer model, the rearrangement leads to the loss of the original physical location relationships. Based on this problem, this paper proposes a transformer decoder based on spatio-temporal joint (ST-T). We employ a spatial attention mechanism to compensate for this information loss and focus on key spatial features more accurately, further exploiting the potential of single- and two-layer transformers in reconstructing CSI matrices. The results are validated by simulations based on DCRNet and CLNet encoders, which show that higher performance can be achieved with lower computational load compared to other lightweight models.}
}
@incollection{NIEVERGELT1993167,
title = {Experiments in Computational Heuristics and Their Lessons for Software and Knowledge Engineering},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {37},
pages = {167-205},
year = {1993},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60405-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808604052},
author = {Jurg Nievergelt},
abstract = {Publisher Summary
This chapter presents examples that illustrate a type of programming project increasingly prominent in the field of knowledge engineering. The examples are chosen on grounds of familiarity, without any claim to represent the field of heuristic programming at large. The chapter begins by describing some software projects in computational heuristics. These projects are presented as case studies of the interaction between software engineering and knowledge engineering that illustrate the decisive importance of a powerful software environment. The chapter presents the case of the smart game board and describes the main software tool for rapid prototyping of game implementations, needed to conduct experiments. It also describes a project involving heuristics and knowledge engineering that has been evolving without interruption for the past five years. It presents paradigms of software development favored by system designers and implementers of exploratory development projects, and explains why these are often diametrically opposed to the conventional software engineering lore.}
}
@article{METTLER2024101932,
title = {Same same but different: How policies frame societal-level digital transformation},
journal = {Government Information Quarterly},
volume = {41},
number = {2},
pages = {101932},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101932},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000248},
author = {Tobias Mettler and Gianluca Miscione and Claus D. Jacobs and Ali A. Guenduez},
keywords = {Digital transformation, IS policy research, Computational content analysis, Narratives},
abstract = {The digital transformation (DT) is not only forcing companies to rethink their business models but is also challenging governments to address the question of how information technology will change society today and in the future. By setting the legal boundaries and acting as an investor and promoter of the domestic digital economy, governments actively influence in which ways this transformational process takes place. The vision and objectives how DT should be realized on state level is portrayed in well-crafted DT policies. Yet, little is known how governments, as strategic actors, see their role in the DT and how they frame these documents. In this paper, we argue that policymaking about DT is isomorphic in the global context, rather than a differentiator for countries to gain a competitive edge. Using machine learning to analyze a vast text corpus of policy documents, we identify the common repertoire of narratives used by governments from all around the globe to picture their vision of the DT and show that DT policies appear to be almost context-free due to their high similarity.}
}
@article{EGOROV2007293,
title = {Neural logic molecular, counter-intuitive},
journal = {Biomolecular Engineering},
volume = {24},
number = {3},
pages = {293-299},
year = {2007},
note = {6th Atlantic Symposium on Computational Biology},
issn = {1389-0344},
doi = {https://doi.org/10.1016/j.bioeng.2007.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389034407000342},
author = {Igor K. Egorov},
keywords = {Creative thinking, Boolean logic gates, Molecular mechanism, Transcription regulation, Somatic hypermutation, Neuron},
abstract = {A hypothesis is proposed that multiple “LOGIC” genes control Boolean logic in a neuron. Each hypothetical LOGIC gene encodes a transcription factor that regulates another LOGIC gene(s). Through transcription regulation, LOGIC genes connect into a complex circuit, such as a XOR logic gate or a two-input flip–flop logic circuit capable of retaining information. LOGIC gene duplication, mutation and recombination may result in the diversification of Boolean logic gates. Creative thinking may sometimes require counter-intuitive reasoning, rather than common sense. Such reasoning is likely to engage novel logic circuits produced by LOGIC somatic mutations. An individual's logic maturates by a mechanism of somatic hypermutation, gene conversion and recombination of LOGIC genes in precursor cells followed by selection of neurons in the brain for functional competence. In this model, a single neuron among billions in the brain may contain a unique logic circuit being the key to a hard intellectual problem. The output of a logic neuron is likely to be a neurotransmitter. This neuron is connected to other neurons in the spiking neural network. The LOGIC gene hypothesis is testable by molecular techniques. Understanding mechanisms of authentic human ingenuity may help to invent digital systems capable of creative thinking.}
}
@article{SUDDENDORF201826,
title = {Prospection and natural selection},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {26-31},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302449},
author = {T Suddendorf and A Bulley and B Miloyan},
abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.}
}
@article{RODRIGUEZ2022104446,
title = {Using scaffolded feedforward and peer feedback to improve problem-based learning in large classes},
journal = {Computers & Education},
volume = {182},
pages = {104446},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000173},
author = {María Fernanda Rodríguez and Miguel Nussbaum and Leyla Yunis and Tomás Reyes and Danilo Alvares and Jean Joublan and Patricio Navarrete},
abstract = {The growing demand for access to higher education has seen institutions turn increasingly towards large classes. Implementing active, problem-based learning in this context can be difficult as it requires the lecturer to attend to every student's individual needs. Given the lack of tools for providing personalized feedback, this represents a significant challenge. The aim of this study is to see how best to support lecturers in giving timely feedback to students in a large class during problem-based learning. To meet this goal, we propose a model that combines feedforward, scaffolded using an automated summarization tool, with peer feedback. In this sense, the lecturer first provides feedforward through a series of general comments before an anonymous peer gives personalized feedback. The results show that, despite not giving personalized feedback, the lecturer is able to provide enriched formative feedforward thanks to the summary generated by the automated system. Furthermore, in more qualitative terms, the students show that they appreciate the opportunity to both give and receive feedback. Finally, the students' critical thinking skills are also shown to improve progressively from one activity to the next. Given the research gap regarding how lecturers use the reports generated by automated summarization tools, our study contributes to the literature by proposing a strategy for lecturers to use such reports to provide feedforward. Additionally, this study also contributes to the literature by proposing a model that can be fully integrated in both synchronous and asynchronous online learning.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{EGIDI2020155,
title = {Desertification risk, economic resilience and social issues: From theory to practice},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {18},
number = {2},
pages = {155-163},
year = {2020},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S2325426221000310},
author = {Gianluca Egidi and Luca Salvati},
keywords = {Population dynamics, Ecosystem functioning, Socio-ecological resilience, Complex adaptive systems, Interpretative framework},
abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.}
}
@article{HARTWIGSEN20212075,
title = {How does hemispheric specialization contribute to human-defining cognition?},
journal = {Neuron},
volume = {109},
number = {13},
pages = {2075-2090},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321002907},
author = {Gesa Hartwigsen and Yoshua Bengio and Danilo Bzdok},
keywords = {human intelligence, artificial general intelligence, computational design principles, deep learning, language, global workspace theory},
abstract = {Summary
Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman’s System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{POLZER2022100181,
title = {The rise of people analytics and the future of organizational research},
journal = {Research in Organizational Behavior},
volume = {42},
pages = {100181},
year = {2022},
issn = {0191-3085},
doi = {https://doi.org/10.1016/j.riob.2023.100181},
url = {https://www.sciencedirect.com/science/article/pii/S0191308523000011},
author = {Jeffrey T. Polzer},
keywords = {People analytics, Algorithms, Decision-making, Networks, Teams, Meetings, Culture, Monitoring, Computational social science, Organizational behavior},
abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{SHUKLA2024117388,
title = {Association of road traffic noise exposure and school childrens’ cognition: A structural equation model approach},
journal = {Environmental Research},
volume = {240},
pages = {117388},
year = {2024},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2023.117388},
url = {https://www.sciencedirect.com/science/article/pii/S0013935123021928},
author = {Avnish Shukla and Bhaven N. Tandel},
keywords = {School children, Cognition, Traffic noise index (TNI), Exploratory factor analysis (EFA), Structural equation modeling (SEM)},
abstract = {This study explores the complex relationship between traffic noise and school children's cognition, acknowledging existing empirical inconsistencies and aiming to contribute to a richer understanding of this pivotal issue. Schools adjacent to noisy roads were selected, and outdoor noise levels were measured employing a Kimo dB300 sound level meter, focusing on noise level indices LAeq, L10, and L90. Subsequent calculations were performed to determine the noise pollution level (Lnp), noise climate (NC), and traffic noise index (TNI), revealing a severe noise exposure when compared to standard guidelines. A perception questionnaire for various noise and acoustic factors influencing cognition was developed, and 1524 student responses were collected. Data analysis incorporated Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) for dimension reduction, revealing three latent factors labelled 'annoyance,' 'behaviour,' and 'cognition'. Further, Structural Equation Modeling (SEM) was utilized to explore multivariate relationships between variables and latent factors. Resultant path coefficients were obtained as 0.12, 0.98, and 0.10 for the impact of 'behaviour' and 'annoyance' on 'cognition' and the correlation between 'annoyance' and 'behaviour', respectively. Findings underscore a potent positive impact of annoyance, stemming from acute ambient noise exposure, on the deterioration of children's cognition. While suggesting that ambient noise may be correlated with adverse health impacts due to its influence on cognition, this study emphasizes the pressing necessity for noise mitigation in roadside schools and stringent enforcement of noise pollution guidelines in academic zones.}
}
@article{SEWALL2020,
title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
journal = {Journal of Microbiology & Biology Education},
volume = {21},
number = {1},
year = {2020},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v21i1.1991},
url = {https://www.sciencedirect.com/science/article/pii/S1935787720000660},
author = {Julia Massimelli Sewall and Andrew Oliver and Kameryn Denaro and Alexander B. Chase and Claudia Weihe and Mi Lay and Jennifer B. H. Martiny and Katrine Whiteson},
abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.}
}
@article{TWORZYDLO1995759,
title = {Knowledge-based methods and smart algorithms in computational mechanics},
journal = {Engineering Fracture Mechanics},
volume = {50},
number = {5},
pages = {759-800},
year = {1995},
issn = {0013-7944},
doi = {https://doi.org/10.1016/0013-7944(94)E0060-T},
url = {https://www.sciencedirect.com/science/article/pii/0013794494E0060T},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. These include algorithmic approaches, based on error estimation, adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object-oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here.}
}
@article{CIPRIANI2024102277,
title = {Personality traits and climate change denial, concern, and proactivity: A systematic review and meta-analysis},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102277},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102277},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000501},
author = {Enrico Cipriani and Sergio Frumento and Angelo Gemignani and Danilo Menicucci},
keywords = {Climate change, Personality, Communication, Big five, Climate change denial, Climate change concern},
abstract = {Climate Change is a global issue which touches the lives of all human beings, each of whom have their own unique outlooks and motivations. Hence, the high degree of complexity which emerges from the involvement of such a large number of people might be better understood through the lenses of their individual differences. We performed a systematic review and meta-analysis following PRISMA guidelines. We searched keywords on Web of Science™ and Scopus®, and included peer-reviewed articles which quantitatively examined correlations between personality and climate attitudes. After screening, 74 papers were included in our review. From these articles, k = 100 samples were extracted and included in meta-analysis models. Our results show that Climate Change Denial is positively correlated with Social Dominance Orientation (r = 0.39) and Right-Wing Authoritarianism (r = 0.42), and negatively with Openness (r = −0.14), Conscientiousness (r = −0.05), Agreeableness (r = −0.11), Consideration of Future Consequences (r = −0.38), and Actively Open-Minded Thinking (r = −0.38). Concern for Climate Change correlates with Openness (r = 0.10), Neuroticism (r = 0.12), Consideration of Future Consequences (r = 0.34), and negatively with Social Dominance Orientation (r = -0.36) and Right-Wing Authoritarianism (r = −0.22). Finally, Proactivity towards Climate Change correlates positively with Openness (r = 0.17), Extraversion (r = 0.09), Agreeableness (r = 0.05), Neuroticism (r = 0.10), Consideration of Future Consequences (r = 0.39), and negatively with Social Dominance Orientation (r = -0.25) and Right-Wing Authoritarianism (r = -0.31). Moderation analysis shows geographical variations in the Social Dominance Orientation and Climate Denial relationship. We conclude that some personality traits – such as Openness – transversally affect climate change attitudes. Moreover, meta-analytic data suggest that the personality involvement in Climate Change may be dependent on the socio-political context of different countries. Future research, policies, and communication campaigns should take these peculiarities into account.}
}
@article{FARAJ2021100337,
title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
journal = {Information and Organization},
volume = {31},
number = {1},
pages = {100337},
year = {2021},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2021.100337},
url = {https://www.sciencedirect.com/science/article/pii/S1471772721000038},
author = {Samer Faraj and Wadih Renno and Anand Bhardwaj},
keywords = {COVID, Digitalization, Technology, Organizing, Breaching experiment},
abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.}
}
@article{BARKER2023100569,
title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
journal = {Molecular & Cellular Proteomics},
volume = {22},
number = {7},
pages = {100569},
year = {2023},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2023.100569},
url = {https://www.sciencedirect.com/science/article/pii/S1535947623000804},
author = {Anna D. Barker and Mario M. Alba and Parag Mallick and David B. Agus and Jerry S.H. Lee},
keywords = {proteomics, cancer biomarkers, protein biomarkers, complex adaptive systems, clinical proteomics},
abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.}
}
@article{IVANOV2023108938,
title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
journal = {International Journal of Production Economics},
volume = {263},
pages = {108938},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108938},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323001706},
author = {Dmitry Ivanov},
keywords = {Supply chain resilience, Intelligent digital twin, Data analytics, Stress-test, Ripple effect, anyLogistix},
abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{LUO202571,
title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
journal = {Methods},
volume = {235},
pages = {71-80},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000349},
author = {Rentao Luo and Jiawei Liu and Lixin Guan and Mengshan Li},
keywords = {Promoter, Deep learning, Attention, Gene sequences, Bioinformatics},
abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.}
}
@article{TALWAR2021102341,
title = {Has financial attitude impacted the trading activity of retail investors during the COVID-19 pandemic?},
journal = {Journal of Retailing and Consumer Services},
volume = {58},
pages = {102341},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102341},
url = {https://www.sciencedirect.com/science/article/pii/S0969698920313497},
author = {Manish Talwar and Shalini Talwar and Puneet Kaur and Naliniprava Tripathy and Amandeep Dhir},
keywords = {Artificial neural network, COVID-19, Financial behavior, Financial attitude, Financial anxiety, Pandemic},
abstract = {Financial attitude influences the financial behavior of retail investors. Although the extant research has acknowledged and examined this relationship, the measures of financial attitude and behavior still vary widely and are generally posed as a series of questions rather than statements. In addition to this, there is insufficient knowledge regarding retail investors' behavior in the face of a health crisis, such as the current COVID-19 pandemic. This study addresses these gaps in the prior literature by examining the relative influence of six dimensions of financial attitude, namely, financial anxiety, optimism, financial security, deliberative thinking, interest in financial issues, and needs for precautionary savings, on the trading activity of retail investors during the pandemic. Data were collected from 404 respondents and analyzed using the artificial neural network (ANN) method. The results revealed that all six dimensions had a positive influence on trading activity, with interest in financial issues exerting the strongest influence, followed by deliberative thinking. The study thus contributes important inferences for researchers and managers.}
}
@article{FALBEN2023105386,
title = {The power of the unexpected: Prediction errors enhance stereotype-based learning},
journal = {Cognition},
volume = {235},
pages = {105386},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105386},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000203},
author = {Johanna K. Falbén and Marius Golubickis and Dimitra Tsamadi and Linn M. Persson and C. Neil Macrae},
keywords = {Stereotyping, Person perception, Reinforcement learning, Prediction errors, Drift diffusion model},
abstract = {Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced.}
}
@article{DAVIS20111046,
title = {Homogeneous steady deformation: A review of computational techniques},
journal = {Journal of Structural Geology},
volume = {33},
number = {6},
pages = {1046-1062},
year = {2011},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191814111000447},
author = {Joshua R. Davis and Sarah J. Titus},
keywords = {Kinematic model, Homogeneous deformation, Velocity gradient, Transpression, Vorticity},
abstract = {Homogeneous steady models are frequently used in the structural geology community to describe rock deformation. We review the literature on these models in a streamlined, coordinate-free framework based on matrix exponentials and logarithms. These mathematical tools allow us to compute progressive and simultaneous deformations easily. As an application, we develop transpression with triclinic symmetry in two ways. The tools let us integrate field data related to position and velocity in computing best-fit models with many degrees of freedom. As an application, we reanalyze a published study to demonstrate the extent to which kinematic vorticity is sensitive to modeling assumptions. The tools also open the door to an increased role for the mathematics of Lie groups (spaces of deformations) in structural geology. We suggest two topics for further study: numerical methods for non-steady deformations, and statistics of deformation tensors.}
}
@article{STOLOWY2022101334,
title = {Competing for narrative authority in capital markets: Activist short sellers vs. financial analysts},
journal = {Accounting, Organizations and Society},
volume = {100},
pages = {101334},
year = {2022},
issn = {0361-3682},
doi = {https://doi.org/10.1016/j.aos.2022.101334},
url = {https://www.sciencedirect.com/science/article/pii/S0361368222000010},
author = {Hervé Stolowy and Luc Paugam and Yves Gendron},
keywords = {Activist short sellers, Expertise, Financial analysts, Framing, Narrative authority},
abstract = {Activist short sellers (AShSs) and financial analysts are information intermediaries who analyze firm disclosures as well as produce and disseminate influential investment narratives. This study aims to better understand narrative challenges surrounding the legitimate expertise of financial analysts. Specifically, we examine how AShSs challenge sell-side financial analysts' narrative authority (i.e., the perception that they produce expert knowledge) in interpreting firms' performance and future prospects. We investigate how analysts respond (or do not respond) to this challenge. We use 442 AShS reports, 12 interviews with AShSs and analysts, and analysts' stock recommendations and target prices. In their criticisms of analysts (found in one-third of reports), AShSs frequently frame analysts as lacking market expertise and critical thinking – two core dimensions of analysts' narrative authority. Sixty-six percent of analysts, although explicitly criticized in AShS reports, do not engage in written responses in their equity research reports because they reportedly either adopt a renunciation attitude to the challenge or they engage in off-the-record discussions with certain market participants. However, 34% of analysts respond overtly by counter-framing AShSs as lacking market expertise and objectivity. After the dissemination of AShS reports, analysts, on average, do not revise their highly visible stock recommendations but they revise target prices downward. Theoretically, this study extends our understanding of the construction of narrative authority in capital markets as we examine a challenge to the expertise of influential information intermediaries.}
}
@article{BIELZA2000725,
title = {Structural, elicitation and computational issues faced when solving complex decision making problems with influence diagrams},
journal = {Computers & Operations Research},
volume = {27},
number = {7},
pages = {725-740},
year = {2000},
issn = {0305-0548},
doi = {https://doi.org/10.1016/S0305-0548(99)00113-6},
url = {https://www.sciencedirect.com/science/article/pii/S0305054899001136},
author = {C. Bielza and M. Gómez and S. Rı́os-Insua and J.A.Fernández {del Pozo}},
keywords = {Decision analysis, Influence diagrams, Implementation issues, Medical decision making, Neonatal jaundice},
abstract = {Influence diagrams have become a popular tool for representing and solving decision making problems under uncertainty (Shachter, Operations Research 1986;34:871–82). We show here some practical difficulties when using them to construct a medical decision support system. Specifically, it is hard to tackle issues related to the problem structuring, like the existence of constraints on the sequence of decisions, and the time evolution modeling; related to the knowledge-acquisition, like probability and utility assignment; and related to computational limitations, in memory storage and evaluation phases, as well as the explanation of results. We have recently developed a complex decision support system for neonatal jaundice management — a very common medical problem — , encountering all these difficulties. In this paper, we describe them and how they have been undertaken, providing insights into the community involved in the design and solution of decision models by means of influence diagrams.
Scope and purpose
Decision Analysis is a very well-known discipline that deals with the practice of Decision Theory (Clemen, Making hard decisions: an introduction to decision analysis, 2nd ed. Pacific Grove, CA: Duxbury, 1996). It comprises various steps usually implemented in a decision support system: definition of the alternatives and objectives, modelization of the structure of the decision problem, as well as the beliefs and preferences of the decision maker. The recommended alternative is the one with maximum expected utility, once all the assignments have been refined via sensitivity analyses. However, there are a number of difficulties faced in practice when solving large problems, that require an attentive study.}
}
@article{PRONK202443,
title = {Qualitative systems mapping in promoting physical activity and cardiorespiratory fitness: Perspectives and recommendations},
journal = {Progress in Cardiovascular Diseases},
volume = {83},
pages = {43-48},
year = {2024},
note = {Cardiorespiratory Fitness and Physical Activity: An Update of Evidence, Global Status and Recommendations},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2024.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033062024000355},
author = {Nicolaas P. Pronk and Bruce Y. Lee},
keywords = {Systems mapping, Causal loop diagram, Physical activity, Cardiorespiratory fitness, Complexity},
abstract = {The purpose of this report is to provide a perspective on the use of qualitative systems mapping, provide examples of physical activity (PA) systems maps, discuss the role of PA systems mapping in the context of iterative learning to derive breakthrough interventions, and provide actionable recommendations for future work. Systems mapping methods and applications for PA are emerging in the scientific literature in the study of complex health issues and can be used as a prelude to mathematical/computational modeling where important factors and relationships can be elucidated, data needs can be prioritized and guided, interventions can be tested and (co)designed, and metrics and evaluations can be developed. Examples are discussed that describe systems mapping based on Group Model Building or literature reviews. Systems maps are highly informative, illustrate multiple components to address PA and physical inactivity issues, and make compelling arguments against single intervention action. No studies were identified in the literature scan that considered cardiorespiratory fitness the focal point of a systems maps. Recommendations for future research and education are presented and it is concluded that systems mapping represents a valuable yet underutilized tool for visualizing the complexity of PA promotion.}
}
@article{KLOOSTER2024110771,
title = {A systematic review on eHealth technology personalization approaches},
journal = {iScience},
volume = {27},
number = {9},
pages = {110771},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019965},
author = {Iris ten Klooster and Hanneke Kip and Lisette {van Gemert-Pijnen} and Rik Crutzen and Saskia Kelders},
keywords = {Health sciences, Health technology},
abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.}
}
@article{GOLTZ2021103417,
title = {Do you listen to music while studying? A portrait of how people use music to optimize their cognitive performance},
journal = {Acta Psychologica},
volume = {220},
pages = {103417},
year = {2021},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S0001691821001670},
author = {Franziska Goltz and Makiko Sadakata},
keywords = {Background music, Cognitive performance, Music perception},
abstract = {The effect of background music (BGM) on cognitive task performance is a popular topic. However, the evidence is not converging: experimental studies show mixed results depending on the task, the type of music used and individual characteristics. Here, we explored how people use BGM while optimally performing various cognitive tasks in everyday life, such as reading, writing, memorizing, and critical thinking. Specifically, the frequency of BGM usage, preferred music types, beliefs about the scientific evidence on BGM, and individual characteristics, such as age, extraversion and musical background were investigated. Although the results confirmed highly diverse strategies among individuals regarding when, how often, why and what type of BGM is used, we found several general tendencies: people tend to use less BGM when engaged in more difficult tasks, they become less critical about the type of BGM when engaged in easier tasks, and there is a negative correlation between the frequency of BGM and age, indicating that younger generations tend to use more BGM than older adults. The current and previous evidence are discussed in light of existing theories. Altogether, this study identifies essential variables to consider in future research and further forwards a theory-driven perspective in the field.}
}
@article{FAN2025126317,
title = {Deep dive into clarity: Leveraging signal-to-noise ratio awareness and knowledge distillation for underwater image enhancement},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126317},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126317},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031841},
author = {Guodong Fan and Jingchun Zhou and Chengpei Xu and Zheng Cheng},
keywords = {Underwater image enhancement, SNR-Aware Transformer, Knowledge distillation},
abstract = {This paper presents an innovative dual-branch solution designed for underwater image enhancement (UIE), leveraging the synergistic combination of Signal-to-Noise Ratio (SNR) aware transformers and convolutional models. SNR-Net dynamically enhances pixel quality through spatial-varying operations. While transformers excel in capturing long-range dependencies, they face challenges in weak local relation learning. To address this, we introduce a SNR prior to guide transformer learning, incorporating a novel self-attention mechanism that avoids tokens from regions with very low SNR. Conversely, CNNs, optimized for exploiting local patterns, suffer from limited receptive fields and weak diversity representation. To overcome this limitation, we enhance the receptive field and multi-scale perception of CNNs by introducing a MR-ResNet module. Additionally, we incorporate a Selective Kernel Merging Module (SKMM), an attention-based feature merging module. These enhancements empower our approach to learn an enriched set of features that selectively combine contextual information from both branches while preserving high-quality spatial details. Finally, through knowledge distillation and contrastive learning, SNR-KD significantly reduces the number of parameters and computations of SNR-Net with minimal impact on performance. Extensive experiments validate the effectiveness of our methods, namely SNR-Net and SNR-KD, demonstrating their state-of-the-art performance compared to other recent UIE methods. The code of our model is publicly available at: https://github.com/Alexande-rChan/SNR-UIE.}
}
@article{SEEMAN202211461,
title = {Understanding chemistry: from “heuristic (soft) explanations and reasoning by analogy” to “quantum chemistry”††Dedicated to Dudley Herschbach in celebration of his 90th year who, when asked whether he was a theoretician or an experimentalist, responded, “The molecules don't know and don't care.”},
journal = {Chemical Science},
volume = {13},
number = {39},
pages = {11461-11486},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc02535c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023014347},
author = {Jeffrey I. Seeman and Dean J. Tantillo},
abstract = {ABSTRACT
“Soft theories,” i.e., “heuristic models based on reasoning by analogy” largely drove chemistry understanding for 150 years or more. But soft theories have their limitations and with the expansion of chemistry in the mid-20th century, more and more inexplicable (by soft theory) experimental results were being obtained. In the past 50 years, quantum chemistry, most often in the guise of applied theoretical chemistry including computational chemistry, has provided (a) the underlying “hard evidence” for many soft theories and (b) the explanations for chemical phenomena that were unavailable by soft theories. In this publication, we define “hard theories” as “theories derived from quantum chemistry.” Both soft and hard theories can be qualitative and quantitative, and the “Houk quadrant” is proposed as a helpful categorization tool. Furthermore, the language of soft theories is often used appropriately to describe quantum chemical results. A valid and useful way of doing science is the appropriate use and application of both soft and hard theories along with the best nomenclature available for successful communication of results and ideas.}
}
@incollection{RUNCO20141,
title = {Chapter 1 - Cognition and Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-38},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000011},
author = {Mark A. Runco},
keywords = {Threshold theory, IQ, Structure of intellect, Associative theory, Problem solving, Problem finding, Incubation, Insight, Intuition, Meta-cognition, Mindfulness, Overinclusive thinking},
abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.}
}
@article{XIA2025108857,
title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
journal = {Optics and Lasers in Engineering},
volume = {186},
pages = {108857},
year = {2025},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2025.108857},
url = {https://www.sciencedirect.com/science/article/pii/S0143816625000442},
author = {Xuhui Xia and Jiale Guo and Zelin Zhang and Lei Wang and Yuyao Guo},
keywords = {Cold-rolled strip steel, Defect classification, Lightweight network, Feature extraction},
abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.}
}
@article{FADLALLA1995987,
title = {Improving the performance of enumerative search methods—part II: Computational experiments},
journal = {Computers & Operations Research},
volume = {22},
number = {10},
pages = {987-994},
year = {1995},
issn = {0305-0548},
doi = {https://doi.org/10.1016/0305-0548(95)00016-F},
url = {https://www.sciencedirect.com/science/article/pii/030505489500016F},
author = {Adam Fadlalla and James R. Evans and Martin S. Levy},
abstract = {Generally, branch and bound algorithms typically use mechanistic search strategies and generally do not fully exploit “local” information inherent in problem structures; that is, specific problem-domain knowledge. Some exceptions are found in [2–5]. Incorporatiing intelligence in branch and bound algorithms has been suggested by Glover [1], but not studied in a rigorous experimental framework. We use the mean tardiness job sequencing problem to explore these issues. This paper is divided into two Parts. In Part I [9], we provided the intuitive motivation for this investigation and an experimental framework. In Part II, we present detailed computational results and statistical analysis. The results indicate that branch and bound algorithms can be enhanced significantly by exploiting local knowledge of problem structure and more judicious search strategies.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}
@article{AGRAWAL2022101673,
title = {Spectrum sensing in cognitive radio networks and metacognition for dynamic spectrum sharing between radar and communication system: A review},
journal = {Physical Communication},
volume = {52},
pages = {101673},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101673},
url = {https://www.sciencedirect.com/science/article/pii/S187449072200043X},
author = {Sumit Kumar Agrawal and Abhay Samant and Sandeep Kumar Yadav},
keywords = {Cognitive radio, Spectrum sensing, Spectrum sharing, Cognitive radar, Metacognition, Metacognitive radar},
abstract = {The massive growth in mobile users and wireless technologies has resulted in increased data traffic and created demand for additional radio spectrum. This growing demand for radio spectrum has resulted in spectrum congestion and mandated the need for coexistence between radar and interfering communication emitters. To address the aforementioned issues, it is critical to review existing policies and evaluate new technologies that can utilize spectrum in an efficient and intelligent manner. Cognitive radio and cognitive radar are two promising technologies that exploit spectrum using dynamic spectrum access techniques. Additionally, introducing the bio-inspired concept ‘metacognition’ in a cognitive process has shown to increase the effectiveness and robustness of the cognitive radio and cognitive radar system. Metacognition is a high-order thinking agent that monitors and regulates the cognition process through a feedback and control process called the perception–action cycle. Extensive research has been done in the field of spectrum sensing in cognitive radio and spectral coexistence between radar and communication systems. This paper provides a detailed classification of spectrum sensing schemes and explains how dynamic spectrum access strategies share the spectrum between radar and communication systems. In addition to this, the fundamentals of cognitive radio, its architecture, spectrum management framework, and metacognition concept in radar are discussed. Furthermore, this paper presents various research issues, challenges, and future research directions associated with spectrum sensing in cognitive radar and dynamic spectrum access strategies in cognitive radar.}
}