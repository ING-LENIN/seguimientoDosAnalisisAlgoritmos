@article{SANKARANARAYANAN20151,
title = {Genome-based, mechanism-driven computational modeling of risks of ionizing radiation: The next frontier in genetic risk estimation?},
journal = {Mutation Research/Reviews in Mutation Research},
volume = {764},
pages = {1-15},
year = {2015},
issn = {1383-5742},
doi = {https://doi.org/10.1016/j.mrrev.2014.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S138357421400091X},
author = {K. Sankaranarayanan and H. Nikjoo},
keywords = {Radiation risk, DNA damage, DNA repair, Biophysical models},
abstract = {Research activity in the field of estimation of genetic risks of ionizing radiation to human populations started in the late 1940s and now appears to be passing through a plateau phase. This paper provides a background to the concepts, findings and methods of risk estimation that guided the field through the period of its growth to the beginning of the 21st century. It draws attention to several key facts: (a) thus far, genetic risk estimates have been made indirectly using mutation data collected in mouse radiation studies; (b) important uncertainties and unsolved problems remain, one notable example being that we still do not know the sensitivity of human female germ cells to radiation-induced mutations; and (c) the concept that dominated the field thus far, namely, that radiation exposures to germ cells can result in single gene diseases in the descendants of those exposed has been replaced by the concept that radiation exposure can cause DNA deletions, often involving more than one gene. Genetic risk estimation now encompasses work devoted to studies on DNA deletions induced in human germ cells, their expected frequencies, and phenotypes and associated clinical consequences in the progeny. We argue that the time is ripe to embark on a human genome-based, mechanism-driven, computational modeling of genetic risks of ionizing radiation, and we present a provisional framework for catalyzing research in the field in the 21st century.}
}
@incollection{DASILVASOARES202349,
title = {Chapter Three - Exploring the potential of eye tracking on personalized learning and real-time feedback in modern education},
editor = {Mariuche Gomides and Isabela Starling-Alves and Flávia H. Santos},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {282},
pages = {49-70},
year = {2023},
booktitle = {Brain and Maths in Ibero-America},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079612323000936},
author = {Raimundo {da Silva Soares} and Amanda Yumi Ambriola Oku and Cândida da Silva Ferreira Barreto and João Ricardo Sato},
keywords = {Mathematics education, Eye tracking, Teaching practice, Student gaze},
abstract = {Eye tracking is one of the techniques used to investigate cognitive mechanisms involved in the school context, such as joint attention and visual perception. Eye tracker has portability, straightforward application, cost-effectiveness, and infant-friendly neuroimaging measures of cognitive processes such as attention, engagement, and learning. Furthermore, the ongoing software enhancements coupled with the implementation of artificial intelligence algorithms have improved the precision of collecting eye movement data and simplified the calibration process. These characteristics make it plausible to consider eye-tracking technology a promising tool to assist the teaching-learning process in school routines. However, eye tracking needs to be explored more as an educational instrument for real-time classroom activities and teachers' feedback. This perspective article briefly presents the fundamentals of the eye-tracking technique and four illustrative examples of employing this method in everyday school life. The first application shows how eye tracker information may contribute to teacher assessment of students' computational thinking in coding classes. In the second and third illustrations, we discuss the additional information provided by the eye-tracker to the teacher assessing the student's strategies to solve fraction problems and chart interpretation. The last illustration demonstrates the potential of eye tracking to provide Real-time feedback on learning difficulties/disabilities. Thus, we highlight the potential of the eye tracker as a complementary tool to promote personalized education and discuss future perspectives. In conclusion, we suggest that an eye-tracking system could be helpful by providing real-time student gaze leading to immediate teacher interventions and metacognition strategies.}
}
@article{ANDRADE2017111,
title = {Exact posterior computation in non-conjugate Gaussian location-scale parameters models},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {53},
pages = {111-129},
year = {2017},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2017.04.036},
url = {https://www.sciencedirect.com/science/article/pii/S100757041730151X},
author = {J.A.A. Andrade and P.N. Rathie},
keywords = {Bayesian computation, Exact posterior distribution, Non-conjugate models, Special functions, H-function},
abstract = {In Bayesian analysis the class of conjugate models allows to obtain exact posterior distributions, however this class quite restrictive in the sense that it involves only a few distributions. In fact, most of the practical applications involves non-conjugate models, thus approximate methods, such as the MCMC algorithms, are required. Although these methods can deal with quite complex structures, some practical problems can make their applications quite time demanding, for example, when we use heavy-tailed distributions, convergence may be difficult, also the Metropolis-Hastings algorithm can become very slow, in addition to the extra work inevitably required on choosing efficient candidate generator distributions. In this work, we draw attention to the special functions as a tools for Bayesian computation, we propose an alternative method for obtaining the posterior distribution in Gaussian non-conjugate models in an exact form. We use complex integration methods based on the H-function in order to obtain the posterior distribution and some of its posterior quantities in an explicit computable form. Two examples are provided in order to illustrate the theory.}
}
@article{HUANG2022209,
title = {A Framework for Collaborative Artificial Intelligence in Marketing},
journal = {Journal of Retailing},
volume = {98},
number = {2},
pages = {209-223},
year = {2022},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022435921000142},
author = {Ming-Hui Huang and Roland T. Rust},
keywords = {Artificial intelligence, Collaborative AI, Collaborative intelligence, Augmentation, Replacement},
abstract = {We develop a conceptual framework for collaborative artificial intelligence (AI) in marketing, providing systematic guidance for how human marketers and consumers can team up with AI, which has profound implications for retailing, which is the interface between marketers and consumers. Drawing from the multiple intelligences view that AI advances from mechanical, to thinking, to feeling intelligence (based on how difficult for AI to mimic human intelligences), the framework posits that collaboration between AI and HI (human marketers and consumers) can be achieved by 1) recognizing the respective strengths of AI and HI, 2) having lower-level AI augmenting higher-level HI, and 3) moving HI to a higher intelligence level when AI automates the lower level. Implications for marketers, consumers, and researchers are derived. Marketers should optimize the mix and timing of AI-HI marketing team, consumers should understand the complementarity between AI and HI strengths for informed consumption decisions, and researchers can investigate innovative approaches to and boundary conditions of collaborative intelligence.}
}
@article{RUBENSTEIN2022101030,
title = {Exploring creativity's complex relationship with learning in early elementary students},
journal = {Thinking Skills and Creativity},
volume = {44},
pages = {101030},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101030},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000335},
author = {Lisa DaVia Rubenstein and Jenna Thomas and W. Holmes Finch and Lisa M. Ridgley},
keywords = {Creativity, Learning, Early elementary, Academic achievement, Kindergarten},
abstract = {The purpose of this study was to examine the relationship between learning and creativity in early elementary students using both static and growth achievement scores in reading and mathematics. Participants were kindergarten and first grade students from the Midwestern United States. Initial correlations demonstrated significant positive relationships between students’ performance on the Torrance Test of Creative Thinking –Figural (TTCT-F) and static academic achievement scores in both reading and mathematics, but that same relationship did not exist with academic growth scores. Specifically, when academic growth was examined further using Generalized Additive Models (GAMs), a complex picture emerged, such that grade level (i.e., kindergarten v. first grade) and subscale type (e.g., Fluency v. Originality) influenced the significance and nature of the relationship (i.e., linear v. nonlinear). In general, as students increased in creativity performance, they demonstrated less academic growth. Future work should explore the underlying mechanisms explaining these relationships to better help students leverage their creative abilities for positive academic gains in the classroom setting.}
}
@article{YOUSIF201880,
title = {Fuzzy logic computational model for performance evaluation of Sudanese Universities and academic staff},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {1},
pages = {80-119},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1319157816300556},
author = {Mohamed Khalid Yousif and Adnan Shaout},
keywords = {Evaluation criteria, Performance evaluation, Sudanese universities, Survey design, Fuzzy computational model, Consistency checking},
abstract = {The excellence of a Sudanese universities and academic staff member can be effectively classified by systematic and objective design criteria, which participates in developing the learning outcomes in Sudan. In the first phase of this study, we reviewed the literatures, determined and defined the suitable quantitative and qualitative criteria and then designed & exploited pairwise comparison and evaluation forms through a survey to get experts opinions/preference on the evaluation criteria that are used to measure the universities and academic staff performance. This paper presents a fuzzy logic computational model based on this survey to measure and classify the performance of Sudanese universities and academic staff, which includes computation of criteria weights and overall evaluation of Sudanese universities and academic staff using AHP and TOPSIS techniques.}
}
@article{JIA2024101456,
title = {Memory backtracking strategy: An evolutionary updating mechanism for meta-heuristic algorithms},
journal = {Swarm and Evolutionary Computation},
volume = {84},
pages = {101456},
year = {2024},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2023.101456},
url = {https://www.sciencedirect.com/science/article/pii/S2210650223002286},
author = {Heming Jia and Chenghao Lu and Zhikai Xing},
keywords = {Memory backtracking strategies, New evolutionary updating strategy, Meta-heuristic optimization algorithm},
abstract = {The search domain of meta-heuristic algorithms is always constantly changing, which make it difficult to adapt the diverse optimization issues. To overcome above issue, an evolutionary updating mechanism called Memory Backtracking Strategy (MBS) is proposed, which contains thinking stage, recall stage, and memory stage. Overall, the adoption of the MBS enhances the efficiency of MHSs by incorporating group memory, clue recall, and memory forgetting mechanisms. These strategies improve the algorithm's ability to explore the search space, optimize the search process, and escape local optima. MBS will be applied to three different types of MHS algorithms: evolutionary based (LSHADE_SPACMA), physical based (Stochastic Fractal Search, SFS), and biological based (Marine Predators Algorithmnm, MPA) to demonstrate the universality of MBS. In the experimental section including 57 engineering problems, algorithm complexity analysis, CEC2020 Friedman ranking, convergence curve, Wilcoxon statistical, and box plot. Among them, 21 algorithms participated in the Friedman experiment, including MBS_LSHADE_SPACMA ranked first, LSHADE_SPACMA ranked second, MBS_MPA ranked 6th, MPA ranked 8th, MBS_SFS ranked 9th and SFS ranked 12th. Combined with the analysis of "MBS testing analysis" and the experimental results of engineering problems, it has proven that MBS has universality and good ability to improve optimization algorithm performance. The source codes of the proposed MBS (MBS_MPA) can be accessed by https://github.com/luchenghao2022/Memory-Backtracking-Strategy}
}
@article{ARJMANDI202350,
title = {Embedding computer programming into a chemical engineering course: The impact on experiential learning},
journal = {Education for Chemical Engineers},
volume = {43},
pages = {50-57},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000064},
author = {Mohammadreza Arjmandi and Meng Wai Woo and Cody Mankelow and Thomas Loho and Kaveh Shahbaz and Amar Auckaili and Ashvin Thambyah},
keywords = {Engineering education, Qualitative study, Programming with MATLAB, Problem-based learning, Student experience},
abstract = {The need for autonomous engineering graduates who demonstrate hands-on skills has increased in the industry. Computer programming helps engineering students solve real-world problems systematically and accurately by applying governing physical and mathematical models into a format that a computer can read and execute. This study describes the pedagogical approach of incorporating programming workshops and assessments into a second-year chemical engineering course. The impact of this intervention on experiential learning amongst the students was then evaluated by analysing the feedback provided by voluntary participants during several focus group sessions. The feedback gave further insight into teaching pedagogy with respect to Kolb's experiential learning cycle. It was found the programming background of an individual clearly affects the phase of the learning cycle they predominantly experience during the workshops. Furthermore, programming background affected an individual's critical thinking while approaching an engineering problem. Constructive feedback provided by the student participants offered an invaluable opportunity for the teaching team to reflect on what went well and the areas for improvement in future iterations. The findings of this study can advance knowledge around design and implementation of a programming module within an engineering course.}
}
@article{KONDINSKI20226397,
title = {Composition-driven archetype dynamics in polyoxovanadates††Electronic supplementary information (ESI) available. CCDC 2128841 and 2130016. For ESI and crystallographic data in CIF or other electronic format see https://doi.org/10.1039/d2sc01004f},
journal = {Chemical Science},
volume = {13},
number = {21},
pages = {6397-6412},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc01004f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023011690},
author = {Aleksandar Kondinski and Maren Rasmussen and Sebastian Mangelsen and Nicole Pienack and Viktor Simjanoski and Christian Näther and Daniel L. Stares and Christoph A. Schalley and Wolfgang Bensch},
abstract = {ABSTRACT
Molecular metal oxides often adopt common structural frameworks (i.e. archetypes), many of them boasting impressive structural robustness and stability. However, the ability to adapt and to undergo transformations between different structural archetypes is a desirable material design feature offering applicability in different environments. Using systems thinking approach that integrates synthetic, analytical and computational techniques, we explore the transformations governing the chemistry of polyoxovanadates (POVs) constructed of arsenate and vanadate building units. The water-soluble salt of the low nuclearity polyanion [V6As8O26]4− can be effectively used for the synthesis of the larger spherical (i.e. kegginoidal) mixed-valent [V12As8O40]4− precipitate, while the novel [V10As12O40]8− POVs having tubular cyclic structures are another, well soluble product. Surprisingly, in contrast to the common observation that high-nuclearity polyoxometalate (POM) clusters are fragmented to form smaller moieties in solution, the low nuclearity [V6As8O26]4− anion is in situ transformed into the higher nuclearity cluster anions. The obtained products support a conceptually new model that is outlined in this article and that describes a continuous evolution between spherical and cyclic POV assemblies. This new model represents a milestone on the way to rational and designable POV self-assemblies.}
}
@article{VALLEETOURANGEAU2020100812,
title = {Mapping systemic resources in problem solving},
journal = {New Ideas in Psychology},
volume = {59},
pages = {100812},
year = {2020},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2020.100812},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X17300272},
author = {Frédéric Vallée-Tourangeau and Gaëlle Vallée-Tourangeau},
abstract = {In the wild, thinking demonstrably uses interactive processes that draw on a wide range of external resources, spanning multiple time scales. As Malafouris (2015, p. 361) puts it, “cognition is not a within property; it is an in-between process”. Interactive processes configure extended systems within which each human agent is embedded. Yet much research on higher cognition, such as problem solving, reflects an implicit but deep commitment to methodological individualism that casts the agent as the ontological locus of cognition, and largely dictates the nature of the research enterprise. Thus, tasks to measure capacities and gauge reasoning performance are designed in a manner that reduces or eliminates the possibility of interacting with the problem presentation; if thinking takes place in the head, there is no need or reason to engineer procedures wherein agents can interact with the task's physical constituents. Conversely, a methodological interactivism forces one to acknowledge the participative yet not all-encompassing role of capacities such as working memory and thinking dispositions; it also encourages the granular mapping of the cognitive ecosystem from which new ideas emerge. To adopt an interactivist perspective is thus to focus on the cognitive resources of the extended system inviting a careful description of how these resources are dynamically configured over time and space to promote the development of new ideas in problem solving. In turn, a systemic perspective encourages the development of interventions that promote cognitive performance through the optimisation of systemic rather than individualist cognitive resources.}
}
@article{CHEN20121,
title = {Varieties of agents in agent-based computational economics: A historical and an interdisciplinary perspective},
journal = {Journal of Economic Dynamics and Control},
volume = {36},
number = {1},
pages = {1-25},
year = {2012},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2011.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0165188911001692},
author = {Shu-Heng Chen},
keywords = {Cellular automata, Autonomous agents, Tournaments, Genetic algorithms, Genetic programming, Cognitive capacity},
abstract = {In this paper, we trace four origins of agent-based computational economics (ACE), namely, the markets origin, the cellular-automata origin, the tournaments origin, and the experiments origin. Along with this trace, we examine how these origins have motivated different concepts and designs of agents in ACE, which starts from the early work on simple programmed agents, randomly behaving agents, zero-intelligence agents, human-written programmed agents, autonomous agents, and empirically calibrated agents, and extends to the newly developing cognitive agents, psychological agents, and culturally sensitive agents. The review also shows that the intellectual ideas underlying these varieties of agents cross several disciplines, which may be considered as a part of a general attempt to study humans (and their behavior) with an integrated interdisciplinary foundation.}
}
@article{SILAGHI20121303,
title = {A time-constrained SLA negotiation strategy in competitive computational grids},
journal = {Future Generation Computer Systems},
volume = {28},
number = {8},
pages = {1303-1315},
year = {2012},
note = {Including Special sections SS: Trusting Software Behavior and SS: Economics of Computing Services},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2011.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X11002251},
author = {Gheorghe Cosmin Silaghi and Liviu Dan Şerban and Cristian Marius Litan},
keywords = {SLA negotiation, Intelligent strategies, Bayesian learning, Time constraints},
abstract = {Automated and intelligent negotiation solutions for reaching service level agreements (SLA) represent a hot research topic in computational grids. Previous work regarding SLA negotiation in grids focuses on devising bargaining models where service providers and consumers can meet and exchange SLA offers and counteroffers. Recent developments in agent research introduce strategies based on opponent learning for contract negotiation. In this paper we design a generic framework for strategical negotiation of service level values under time constraints and exemplify the usage of our framework by extending the Bayesian learning agent to cope with the limited duration of a negotiation session. We prove that opponent learning strategies are worth for consideration in open competitive computational grids, leading towards an optimal allocation of resources and fair satisfaction of participants.}
}
@article{LOHSE2012236,
title = {Thinking about muscles: The neuromuscular effects of attentional focus on accuracy and fatigue},
journal = {Acta Psychologica},
volume = {140},
number = {3},
pages = {236-245},
year = {2012},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2012.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0001691812000807},
author = {Keith R. Lohse and David E. Sherwood},
keywords = {Attention, Force production, Motor control, Fatigue},
abstract = {Although the effects of attention on movement execution are well documented behaviorally, much less research has been done on the neurophysiological changes that underlie attentional focus effects. This study presents two experiments exploring effects of attention during an isometric plantar-flexion task using surface electromyography (sEMG). Participants' attention was directed either externally (towards the force plate they were pushing against) or internally (towards their own leg, specifically the agonist muscle). Experiment 1 tested the effects of attention on accuracy and efficiency of force produced at three target forces (30, 60, and 100% of the maximum voluntary contraction; MVC). An internal focus of attention reduced the accuracy of force being produced and increased cocontraction of the antagonist muscle. Error on a given trial was positively correlated with the magnitude of cocontraction on that trial. Experiment 2 tested the effects of attention on muscular fatigue at 30, 60 and 100%MVC. An internal focus of attention led to less efficient intermuscular coordination, especially early in the contraction. These results suggest that an internal focus of attention disrupts efficient motor control in force production resulting in increased cocontraction, which potentially explains other neuromechanical findings (e.g. reduced functional variability with an internal focus).}
}
@incollection{DIBBLE20061511,
title = {Chapter 31 Computational Laboratories for Spatial Agent-Based Models},
editor = {L. Tesfatsion and K.L. Judd},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {2},
pages = {1511-1548},
year = {2006},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(05)02031-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574002105020319},
author = {Catherine Dibble},
keywords = {agent-based simulation, computational laboratory, computational social science, computational economics, spatial economics, spatial social science, spatial networks, small-world networks, scale-free networks, synthetic landscape, inference},
abstract = {An agent-based model is a virtual world comprising distributed heterogeneous agents who interact over time. In a spatial agent-based model the agents are situated in a spatial environment and are typically assumed to be able to move in various ways across this environment. Some kinds of social or organizational systems may also be modeled as spatial environments, where agents move from one group or department to another and where communications or mobility among groups may be structured according to implicit or explicit channels or transactions costs. This chapter focuses on the potential usefulness of computational laboratories for spatial agent-based modeling. Speaking broadly, a computational laboratory is any computational framework permitting the exploration of the behaviors of complex systems through systematic and replicable simulation experiments. By that definition, most of the research discussed in this handbook would be considered to be work with computational laboratories. A narrower definition of computational laboratory (or comp lab for short) refers specifically to specialized software tools to support the full range of agent-based modeling and complementary tasks. These tasks include model development, model evaluation through controlled experimentation, and both the descriptive and normative analysis of model outcomes. The objective of this chapter is to explore how comp lab tools and activities facilitate the systematic exploration of spatial agent-based models embodying complex social processes critical for social welfare. Examples include the spatial and temporal coordination of human activities, the diffusion of new ideas or of infectious diseases, and the emergence and ecological dynamics of innovative ideas or of deadly new diseases.}
}
@article{WELLS1998269,
title = {Turing's analysis of computation and theories of cognitive architecture},
journal = {Cognitive Science},
volume = {22},
number = {3},
pages = {269-294},
year = {1998},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(99)80041-X},
url = {https://www.sciencedirect.com/science/article/pii/S036402139980041X},
author = {A.J. Wells},
abstract = {Turing's analysis of computation is a fundamental part of the background of cognitive science. In this paper it is argued that a re-interpretation of Turing's work is required to underpin theorizing about cognitive architecture. It is claimed that the symbol systems view of the mind, which is the conventional way of understanding how Turing's work impacts on cognitive science, is deeply flawed. There is an alternative interpretation that is more faithful to Turing's original insights, avoids the criticisms made of the symbol systems approach and is compatible with the growing interest in agent-environment interaction. It is argued that this interpretation should form the basis for theories of cognitive architecture.}
}
@article{GOLDSMITH198815,
title = {Idiots savants — Thinking about remembering: A response to White},
journal = {New Ideas in Psychology},
volume = {6},
number = {1},
pages = {15-23},
year = {1988},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(88)90020-7},
url = {https://www.sciencedirect.com/science/article/pii/0732118X88900207},
author = {Lynn T. Goldsmith and David Henry Feldman}
}
@article{ZHANG2024109147,
title = {Three-phase multi-criteria ranking considering three-way decision framework and criterion fuzzy concept},
journal = {International Journal of Approximate Reasoning},
volume = {168},
pages = {109147},
year = {2024},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109147},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24000343},
author = {Kai Zhang and Jianhua Dai},
keywords = {Three-way decision, Criterion fuzzy concept, Three-phase ranking, Multi-criteria ranking},
abstract = {The criterion fuzzy concept refers to a fuzzy set that represents the decision-maker's subjective preference for each criterion within the universe of criteria. Addressing the challenge of ranking all alternatives based on a given criterion fuzzy concept is a novel research direction in the field of fuzzy multi-criteria ranking issues. This paper proposes a three-phase approach for multi-criteria ranking in fuzzy environments, which combines the criterion fuzzy concept and three-way decision thinking. The proposed approach not only analyzes the decision-making characteristics of all alternatives but also facilitates their ranking. During the first phase, a qualitative classification method based on the criterion fuzzy concept and ideal solutions is defined, which divides all alternatives into three independent decision sub-regions. During the second phase, by analyzing the priority relationships among the alternatives within every sub-region, three local ranking rules for alternatives are proposed to determine the ranking of alternatives in each classification region. During the third phase, the semantic relations among three classification regions are considered to give an overall ranking of all alternatives. Finally, combined with two existing quantitative ranking indicators, multiple data sets are employed to verify the feasibility and superiority of the proposed three-phase multi-criteria ranking approach.}
}
@article{SIMMONS2012311,
title = {Bats use a neuronally implemented computational acoustic model to form sonar images},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {2},
pages = {311-319},
year = {2012},
note = {Neuroethology},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812000293},
author = {James A Simmons},
abstract = {This paper reexamines neurophysiological results from echolocating big brown bats to propose a new perspective on FM biosonar processing in the auditory system. Individual auditory neurons are frequency-tuned and respond to brief, 2–10ms FM sweeps with an average of one spike per sound to register their tuned frequencies, to detect echo arrival, or to register a local null in the echo spectrum. When initiated by the broadcast, these responses comprise a cascade of single spikes distributed across time in neurons tuned to different frequencies that persists for 30–50ms, long after the sound has ended. Their progress mirrors the broadcast's propagation away from the bat and the return of echoes for distances out to 5–8m. Each returning echo evokes a similar pattern of single spikes that coincide with ongoing responses to the broadcast to register the target's distance and shape. The hypothesis advanced here is that this flow of responses over time acts as an internal model of sonar acoustics that the bat executes using neuronal computations distributed across many neurons to accumulate a dynamic image of the bat's surroundings.}
}
@article{OXMAN1999105,
title = {Educating the designerly thinker},
journal = {Design Studies},
volume = {20},
number = {2},
pages = {105-122},
year = {1999},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(98)00029-5},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X98000295},
author = {Rivka Oxman},
keywords = {design education, design cognition, design knowledge, conceptual design, computational models},
abstract = {This paper presents a hypothesis about design education that is framed within and derived from cognitive theories of learning. The relevance of design thinking and cognitive approaches to the development of pedagogical approaches in design education is presented and discussed. A conceptual model for design education that emphasizes the acquisition of explicit knowledge of design is proposed. The acquisition of knowledge is achieved through the explication of cognitive structures and strategies of design thinking. The explication process is constructed by exploiting a representational formalism, and a computational medium which supports both the learning process as well as the potential re-use of this knowledge. Finally, an argument is presented that the measure of learning, generally equated with the evaluation of the product of designing, can instead be based upon evaluating learning increments of acquired knowledge.}
}
@article{SCHULZ2024210,
title = {Political reinforcement learners},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {3},
pages = {210-222},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002875},
author = {Lion Schulz and Rahul Bhui},
keywords = {computational models, reinforcement learning, political psychology},
abstract = {Politics can seem home to the most calculating and yet least rational elements of humanity. How might we systematically characterize this spectrum of political cognition? Here, we propose reinforcement learning (RL) as a unified framework to dissect the political mind. RL describes how agents algorithmically navigate complex and uncertain domains like politics. Through this computational lens, we outline three routes to political differences, stemming from variability in agents’ conceptions of a problem, the cognitive operations applied to solve the problem, or the backdrop of information available from the environment. A computational vantage on maladies of the political mind offers enhanced precision in assessing their causes, consequences, and cures.}
}
@article{GOLIGHER20241067,
title = {Bayesian statistics for clinical research},
journal = {The Lancet},
volume = {404},
number = {10457},
pages = {1067-1076},
year = {2024},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(24)01295-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140673624012959},
author = {Ewan C Goligher and Anna Heath and Michael O Harhay},
abstract = {Summary
Frequentist and Bayesian statistics represent two differing paradigms for the analysis of data. Frequentism became the dominant mode of statistical thinking in medical practice during the 20th century. The advent of modern computing has made Bayesian analysis increasingly accessible, enabling growing use of Bayesian methods in a range of disciplines, including medical research. Rather than conceiving of probability as the expected frequency of an event (purported to be measurable and objective), Bayesian thinking conceives of probability as a measure of strength of belief (an explicitly subjective concept). Bayesian analysis combines previous information (represented by a mathematical probability distribution, the prior) with information from the study (the likelihood function) to generate an updated probability distribution (the posterior) representing the information available for clinical decision making. Owing to its fundamentally different conception of probability, Bayesian statistics offers an intuitive, flexible, and informative approach that facilitates the design, analysis, and interpretation of clinical trials. In this Review, we provide a brief account of the philosophical and methodological differences between Bayesian and frequentist approaches and survey the use of Bayesian methods for the design and analysis of clinical research.}
}
@article{VAZQUEZ2017550,
title = {Price computation in electricity auctions with complex rules: An analysis of investment signals},
journal = {Energy Policy},
volume = {105},
pages = {550-561},
year = {2017},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0301421517300770},
author = {Carlos Vazquez and Michelle Hallack and Miguel Vazquez},
keywords = {Electricity auctions, Investment signals, Side payments, Integer decisions, Marginal cost},
abstract = {This paper discusses the problem of defining marginal costs when integer variables are present, in the context of short-term power auctions. Most of the proposals for price computation existing in the literature are concerned with short-term competitive equilibrium (generators should not be willing to change the dispatch assigned to them by the auctioneer), which implies operational-cost recovery for all of the generators accepted in the auction. However, this is in general not enough to choose between the different pricing schemes. We propose to include an additional criterion in order to discriminate among different pricing schemes: prices have to be also signals for generation expansion. Using this condition, we arrive to a single solution to the problem of defining prices, where they are computed as the shadow prices of the balance equations in a linear version of the unit commitment problem. Importantly, not every linearization of the unit commitment is valid; we develop the conditions for this linear model to provide adequate investment signals. Compared to other proposals in the literature, our results provide a strong motivation for the pricing scheme and a simple method for price computation.}
}
@article{TIBURU201836,
title = {Investigating the Conformation of S100β Protein Under Physiological Parameters Using Computational Modeling: A Clue for Rational Drug Design},
journal = {The Open Biomedical Engineering Journal},
volume = {12},
pages = {36-50},
year = {2018},
issn = {1874-1207},
doi = {https://doi.org/10.2174/1874120701812010036},
url = {https://www.sciencedirect.com/science/article/pii/S1874120718000036},
author = {Elvis K. Tiburu and Ibrahim Issah and Mabel Darko and Robert E. Armah-Sekum and Stephen O. A. Gyampo and Nadia K. Amoateng and Samuel K. Kwofie and Gordon Awandare},
keywords = {S100β Protein, Molecular Dynamics, Cofactors, Energy Minimization, Physiological Parameters, Alzheimer's},
abstract = {Background
Physiochemical factors such as temperature, pH and cofactors are well known parameters that confer conformational changes in a protein structure. With S100β protein being a metal binding brain-specific receptor for both extracellular and intracellular functions, a change in conformation due to the above-mentioned factors, can compromise their cellular functions and therefore result in several pathological conditions such as Alzheimer’s disease, Ischemic stroke, as well as Myocardial Infarction.
Objective
The studies conducted sought to elucidate the effect of these physiological factors on the conformational dynamics of S100β protein using computational modeling approaches.
Method
Temperature-dependent and protein-cofactor complexes molecular dynamics simulations were conducted by varying the temperature from 100 to 400K using GROMACS 5.0.3. Additionally, the conformational dynamics of the protein was studied by varying the pH at 5.0, 7.4 and 9.0 using Ambertools17. This was done by preparing the protein molecule, solvating and minimizing its energy level as well as heating it to the required temperature, equilibrating and simulating under desired conditions (NVT and NPT ensembles).
Results
The results show that the protein misfolds as a function of increasing temperature with alpha helical content at 100K and 400K being 57.8% and 43.3%, respectively. However, the binding sites of the protein were not appreciably affected by temperature variations. The protein displayed high conformational instability in acidic medium (pH ~5.0). The binding sites of Ca2+, Mg2+ and Zn2+ were identified and each exhibited different groupings of the secondary structural elements (binding motifs). The secondary structure analysis revealed different conformational changes with the characteristic appearance of two beta hairpins in the presence of Zn2+and Mg2+.
Conclusion
High temperatures, different cofactors and acidic pH confer conformational changes to the S100β structure and these results may indicate the design of novel drugs against the protein.}
}
@article{SAHADEVAN2025103141,
title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103141},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000345},
author = {Vijayalaxmi Sahadevan and Rohin Joshi and Kane Borg and Vishal Singh and Abhishek Raj Singh and Bilal Muhammed and Soban Babu Beemaraj and Amol Joshi},
abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages.}
}
@article{KALPOKIENE2023102197,
title = {Creative encounters of a posthuman kind – anthropocentric law, artificial intelligence, and art},
journal = {Technology in Society},
volume = {72},
pages = {102197},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102197},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23000027},
author = {Julija Kalpokiene and Ignas Kalpokas},
keywords = {Anthropocentrism, Artificial intelligence, Creativity, Copyright},
abstract = {Artificial Intelligence (AI) is becoming an increasingly transformative force in human life. Crucially, its impact is already extending beyond automation of routine tasks and encroaching on creativity – a domain once seen as exclusively human. Hence, this article first surveys the discriminatory and exploitative underpinnings of the anthropocentric thinking that lies beyond attempts at sidelining the creative capacities of AI. Next, four different approaches to creativity and art are analyzed, ultimately conceptualizing art-ness as externally ascribed. Ultimately, the article moves to one way of such ascription – copyrightability – demonstrating the anthropocentric thinking behind attempts to both deny and award copyright protection to AI-generated content. Moreover, it transpires that human authors are under threat whichever of such strategies ends up dominant.}
}
@article{EGRINAGY2008135,
title = {Algebraic properties of automata associated to Petri nets and applications to computation in biological systems},
journal = {Biosystems},
volume = {94},
number = {1},
pages = {135-144},
year = {2008},
note = {Seventh International Workshop on Information Processing in Cells and Tissues},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2008.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0303264708001366},
author = {Attila Egri-Nagy and Chrystopher L. Nehaniv},
keywords = {Algebraic automata theory, Petri nets, Krohn-Rhodes theorem, Algebraic biology},
abstract = {Biochemical and genetic regulatory networks are often modeled by Petri nets. We study the algebraic structure of the computations carried out by Petri nets from the viewpoint of algebraic automata theory. Petri nets comprise a formalized graphical modeling language, often used to describe computation occurring within biochemical and genetic regulatory networks, but the semantics may be interpreted in different ways in the realm of automata. Therefore, there are several different ways to turn a Petri net into a state-transition automaton. Here, we systematically investigate different conversion methods and describe cases where they may yield radically different algebraic structures. We focus on the existence of group components of the corresponding transformation semigroups, as these reflect symmetries of the computation occurring within the biological system under study. Results are illustrated by applications to the Petri net modelling of intermediary metabolism. Petri nets with inhibition are shown to be computationally rich, regardless of the particular interpretation method. Along these lines we provide a mathematical argument suggesting a reason for the apparent all-pervasiveness of inhibitory connections in living systems.}
}
@article{KENNEDY198538,
title = {Thinking of opening your own business? Be prepared!},
journal = {Business Horizons},
volume = {28},
number = {5},
pages = {38-42},
year = {1985},
issn = {0007-6813},
doi = {https://doi.org/10.1016/0007-6813(85)90066-7},
url = {https://www.sciencedirect.com/science/article/pii/0007681385900667},
author = {Carson R. Kennedy},
abstract = {The good news is that new businesses are booming. The bad news is that many are going bust. Careful preparation prior to opening your own business is the best way to forestall failure.}
}
@incollection{MOLE2022367,
title = {Executive/Cognitive Control},
editor = {Sergio {Della Sala}},
booktitle = {Encyclopedia of Behavioral Neuroscience, 2nd edition (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {367-376},
year = {2022},
isbn = {978-0-12-821636-1},
doi = {https://doi.org/10.1016/B978-0-12-819641-0.00111-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196410001110},
author = {Joseph Mole and Lisa Cipolotti},
keywords = {Frontal lobes, Active thought, Executive functioning, Fluid intelligence, Language, Focal lesions, Neuropsychology, Supervisory system, Reasoning, Lateralization of function},
abstract = {The capacity for active thought is arguably one of humanity's defining features. The frontal lobes are critically involved in active thinking. In this article we will consider what can be learned from the effects of frontal lobe lesions about: (1) the relationship between active thought and intelligence, (2) whether active thought can occur without language, and (3) the processes involved in active thinking. The evidence reviewed reveals that different forms of active thought and their essential pre-requisites can be fractionated and appear to be underpinned by different frontal areas. Hence, active thinking may be achieved by distinct, interacting cognitive processes.}
}
@article{BATISTA2003189,
title = {A Computational Basis to Object?},
journal = {Neuron},
volume = {37},
number = {2},
pages = {189-190},
year = {2003},
issn = {0896-6273},
doi = {https://doi.org/10.1016/S0896-6273(03)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0896627303000291},
author = {Aaron P. Batista},
abstract = {To use an object, we must be able to perceive the spatial relationship between the object's parts. The accepted view of how the brain coherently encodes an object is that some neurons in the frontal cortex employ an object-centered coordinate frame. A new computational model challenges this view, using the rich conceptual framework of neural basis functions.}
}
@article{MOORE202542,
title = {Considerations for using participatory systems modeling as a tool for implementation mapping in chronic disease prevention},
journal = {Annals of Epidemiology},
volume = {101},
pages = {42-51},
year = {2025},
issn = {1047-2797},
doi = {https://doi.org/10.1016/j.annepidem.2024.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047279724002758},
author = {Travis R. Moore and Erin Hennessy and Yuilyn Chang Chusan and Laura Ellen Ashcraft and Christina D. Economos},
keywords = {Implementation science, Implementation mapping, Community-engaged research, Systems thinking, Epidemiology, Systems science},
abstract = {Effective chronic disease prevention requires a systems approach to the design, implementation, and refinement of interventions that account for the complexity and interdependence of factors influencing health outcomes. This paper proposes the Participatory Implementation Systems Mapping (PISM) process, which combines participatory systems modeling with implementation strategy development to enhance intervention design and implementation planning. PISM leverages the collaborative efforts of researchers and community partners to analyze complex health systems, identify key determinants, and develop tailored interventions and strategies that are both adaptive and contextually relevant. The phases of the PISM process include strategize, innovate, operationalize, and assess. We describe and demonstrate how each phase contributes to the overall goal of effective and sustainable intervention implementation. We also address the challenges of data availability, model complexity, and resource constraints. We offer solutions such as innovative data collection methods and participatory model development to enhance the robustness and applicability of systems models. Through a case study on the development of a chronic disease prevention intervention, the paper illustrates the practical application of PISM and highlights its potential to guide epidemiologists and implementation scientists in developing interventions that are responsive to the complexities of real-world health systems. The conclusion calls for further research to refine participatory systems modeling techniques, overcome existing challenges in data availability, and expand the use of PISM in diverse public health contexts.}
}
@article{VIGNONCLEMENTEL20103,
title = {A primer on computational simulation in congenital heart disease for the clinician},
journal = {Progress in Pediatric Cardiology},
volume = {30},
number = {1},
pages = {3-13},
year = {2010},
note = {Proceedings of the 1st International Conference on Computational Simulation in Congenital Heart Disease},
issn = {1058-9813},
doi = {https://doi.org/10.1016/j.ppedcard.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1058981310000767},
author = {Irene E. Vignon-Clementel and Alison L. Marsden and Jeffrey A. Feinstein},
keywords = {Hemodynamics, Computer modeling, Boundary conditions, Clinical data, Congenital heart disease},
abstract = {Interest in the application of engineering methods to problems in congenital heart disease has gained increased popularity over the past decade. The use of computational simulation to examine common clinical problems including single ventricle physiology and the associated surgical approaches, the effects of pacemaker implantation on vascular occlusion, or delineation of the biomechanical effects of implanted medical devices is now routinely appearing in clinical journals within all pediatric cardiovascular subspecialties. In practice, such collaboration can only work if both communities understand each other's methods and their limitations. This paper is intended to facilitate this communication by presenting in the context of congenital heart disease (CHD) the main steps involved in performing computational simulation—from the selection of an appropriate clinical question/problem to understanding the computational results, and all of the “black boxes” in between. We examine the current state of the art and areas in need of continued development. For example, medical image-based model-building software has been developed based on numerous different methods. However, none of them can be used to construct a model with a simple “click of a button.” The creation of a faithful, representative anatomic model, especially in pediatric subjects, often requires skilled manual intervention. In addition, information from a second imaging modality is often required to facilitate this process. We describe the technical aspects of model building, provide a definition of some of the most commonly used terms and techniques (e.g. meshes, mesh convergence, Navier-Stokes equations, and boundary conditions), and the assumptions used in running the simulations. Particular attention is paid to the assignment of boundary conditions as this point is of critical importance in the current areas of research within the realm of congenital heart disease. Finally, examples are provided demonstrating how computer simulations can provide an opportunity to “acquire” data currently unobtainable by other modalities, with essentially no risk to patients. To illustrate these points, novel simulation examples of virtual Fontan conversion (from preoperative data to predicted postoperative state) and outcomes of different surgical designs are presented. The need for validation of the currently employed techniques and predicted results are required and the methods remain in their infancy. While the daily application of these technologies to patient-specific clinical scenarios likely remains years away, the ever increasing interest in this area among both clinicians and engineers makes its eventual use far more likely than ever before and, some could argue, only a matter of [computing] time.}
}
@article{JOHNSON20241037,
title = {Minds and markets as complex systems: an emerging approach to cognitive economics},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {11},
pages = {1037-1050},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324001748},
author = {Samuel G.B. Johnson and Patrick R. Schotanus and J.A. Scott Kelso},
keywords = {decision-making, behavioral economics, narratives, agent-based models, extended mind, Coordination Dynamics},
abstract = {Cognitive economics is an emerging interdisciplinary field that uses the tools of cognitive science to study economic and social decision-making. Although most strains of cognitive economics share commitments to bridging levels of analysis (cognitive, behavioral, and systems) and embracing interdisciplinary approaches, we review a newer strand of cognitive economic thinking with a further commitment: conceptualizing minds and markets each as complex adaptive systems. We describe three ongoing research programs that strive toward these goals: (i) studying narratives as a cognitive and social representation used to guide decision-making; (ii) building cognitively informed agent-based models; and (iii) understanding markets as an extended mind – the Market Mind Hypothesis – analyzed using the concepts, methods, and tools of Coordination Dynamics.}
}
@article{HOGENDOORN2022128,
title = {Perception in real-time: predicting the present, reconstructing the past},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {2},
pages = {128-141},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321002886},
author = {Hinze Hogendoorn},
keywords = {perception, time, prediction, real-time, neural delays},
abstract = {We feel that we perceive events in the environment as they unfold in real-time. However, this intuitive view of perception is impossible to implement in the nervous system due to biological constraints such as neural transmission delays. I propose a new way of thinking about real-time perception: at any given moment, instead of representing a single timepoint, perceptual mechanisms represent an entire timeline. On this timeline, predictive mechanisms predict ahead to compensate for delays in incoming sensory input, and reconstruction mechanisms retroactively revise perception when those predictions do not come true. This proposal integrates and extends previous work to address a crucial gap in our understanding of a fundamental aspect of our everyday life: the experience of perceiving the present.}
}
@article{WANG2013226,
title = {A Computational Knowledge Elicitation and Sharing System for mental health case management of the social service industry},
journal = {Computers in Industry},
volume = {64},
number = {3},
pages = {226-234},
year = {2013},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2012.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361512001777},
author = {W.M. Wang and C.F. Cheung},
keywords = {Narratives, Knowledge management, Concept mapping, Knowledge-based system, Natural language processing},
abstract = {Narrative data provide rich information and knowledge to the workers. However, existing systems mainly served as a workflow system, a reporting system, or a database system for storing this kind of information. The massive amount of unstructured narrative data makes it extremely difficult to be shared and reused. Actual knowledge sharing and reuse among the workers is still limited. This paper presents a Computational Knowledge Elicitation and Sharing System which attempts to elicit knowledge from individuals as well as a team and converts it into a structured format and shared among the team. The proposed system accomplishes several current technologies in knowledge-based system, artificial intelligence and natural language processing, which converts the narrative knowledge of knowledge workers into a concept mapping representation. With a sufficient number of narratives, patterns are revealed and an aggregate concept map for all participating members is produced. It converts the unstructured text into a more structured format which helps to summarize and share the knowledge that can be taken in handling different case management issues. Such integration is considered to be novel. A prototype system has been implemented based on the method successfully in the mental healthcare of a social service organization for handling their case management issues. An experiment has been carried out for measuring the accuracy for converting the unstructured data into the structured format. The theoretical results are found to agree well with the experimental results.}
}
@article{RUSSO2020745,
title = {Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation},
journal = {Neuron},
volume = {107},
number = {4},
pages = {745-758.e6},
year = {2020},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2020.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0896627320303664},
author = {Abigail A. Russo and Ramin Khajeh and Sean R. Bittner and Sean M. Perkins and John P. Cunningham and L.F. Abbott and Mark M. Churchland},
keywords = {supplementary motor area, motor control, motor cortex, population coding, recurrent neural network, neural dynamics, neural computation, population geometry},
abstract = {Summary
The supplementary motor area (SMA) is believed to contribute to higher order aspects of motor control. We considered a key higher order role: tracking progress throughout an action. We propose that doing so requires population activity to display low "trajectory divergence": situations with different future motor outputs should be distinct, even when present motor output is identical. We examined neural activity in SMA and primary motor cortex (M1) as monkeys cycled various distances through a virtual environment. SMA exhibited multiple response features that were absent in M1. At the single-neuron level, these included ramping firing rates and cycle-specific responses. At the population level, they included a helical population-trajectory geometry with shifts in the occupied subspace as movement unfolded. These diverse features all served to reduce trajectory divergence, which was much lower in SMA versus M1. Analogous population-trajectory geometry, also with low divergence, naturally arose in networks trained to internally guide multi-cycle movement.}
}
@article{SNAIDER201259,
title = {Time production and representation in a conceptual and computational cognitive model},
journal = {Cognitive Systems Research},
volume = {13},
number = {1},
pages = {59-71},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041710000781},
author = {Javier Snaider and Ryan McCall and Stan Franklin},
keywords = {Time, Time perception, Cognitive architecture, Event, Duration},
abstract = {Time perception and inferences there from are of critical importance to many autonomous agents. But time is not perceived directly by any sensory organ. We argue that time is constructed by cognitive processes. Here we present a model for time perception that concentrates on succession and duration, and that generates these concepts and others, such as continuity, immediate present duration, and lengths of time. These concepts are grounded through the perceptual process itself. We also address event representation, event hierarchy and expectations, as issues intimately related with time. The LIDA cognitive model is used to illustrate these ideas.}
}
@incollection{RUNCO2023115,
title = {Chapter 4 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {115-154},
year = {2023},
isbn = {978-0-08-102617-5},
doi = {https://doi.org/10.1016/B978-0-08-102617-5.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026175000059},
author = {Mark A. Runco},
keywords = {Adoption studies, Altered states of consciousness, Cerebellum, Corpus callosum, Dopamine, Dreams, Drugs, Exercise, Genealogies, Genetics, Prefrontal cortex, Split brain, Stress},
abstract = {This chapter discusses biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness has long been used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared with left-handed people. There are several reports of left-handed persons outnumbering the right-handed ones in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have more recently been studied with electroencephalogram (EEG), positron emission topography (PET), cerebral blood flow, and magnetic resonance imaging (MRI) techniques. Numerous EEG studies suggest that there are particular brain wave patterns and brain structures that are associated with creative problem-solving or at least with specific phases within the problem-solving process. EEGs suggest a complex kind of activity while individuals work on tasks indicative of creative potential. Much of the complexity disappears when those same individuals work on convergent thinking tasks. Research suggests that the prefrontal cortex plays an important role in creative thinking and behaviour.}
}
@incollection{MADIAJAGAN20191,
title = {Chapter 1 - Parallel Computing, Graphics Processing Unit (GPU) and New Hardware for Deep Learning in Computational Intelligence Research},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {1-15},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000087},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Deep learning, Parallelization, Graphics processing unit, Hardware architecture, Memory optimization, Computational intelligence},
abstract = {Graphics processing unit (GPU) is an electronic circuit which manipulates and modifies the memory for better image output. Deep learning involves huge amounts of matrix multiplications and other operations which can be massively parallelized and thus sped up on GPUs. A single GPU might have thousands of cores while a CPU usually has no more than 12 cores. GPU's practical applicability is affected by two issues: long training time and limited GPU memory, which is greatly influenced as the neural network size grows. In order to overcome these issues, this chapter presents various technologies in distributed parallel processing which improve the training time and optimize the memory, and hardware engine architectures will be explored for data size reduction. The GPUs generally used for deep learning are limited in memory size compared to CPUs, so even the latest Tesla GPU has only 16 GB of memory. Therefore, GPU memory cannot be increased to that extent easily, so networks must be designed to fit within the available memory. This could be a factor limiting progress, overcoming which would be highly beneficiary to the computational intelligence area.}
}
@incollection{VERDICCHIO2025,
title = {Language of Artificial Intelligence Discourses},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00392-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041003926},
author = {Mario Verdicchio},
keywords = {Artificial intelligence, Figures of speech, Machine learning, Philosophy of mind},
abstract = {Language has played a fundamental role in Artificial Intelligence discourses from the very beginning of the establishment of the field. An early assumption was that every aspect of intelligence could be described in a manner compatible with machine operation. This assumption is critical for comparing humans and machines, given that, on the one hand, our understanding of how human brains work is insufficient to define intelligence clearly, and on the other hand, a focus on computational artifacts may lead to a limited conceptualization that overlooks significant aspects of what it means to be conscious and conscientious humans. A mindful analysis of the metaphors used to describe AI systems is key to navigating the intricate entanglements between society and technology that contribute to this endeavor.}
}
@article{CLEMENTZ2023143,
title = {Clinical characterization and differentiation of B-SNIP psychosis Biotypes: Algorithmic Diagnostics for Efficient Prescription of Treatments (ADEPT)-1},
journal = {Schizophrenia Research},
volume = {260},
pages = {143-151},
year = {2023},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2023.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920996423002645},
author = {Brett A. Clementz and Ishanu Chattopadhyay and Rebekah L. Trotti and David A. Parker and Elliot S. Gershon and S. Kristian Hill and Elena I. Ivleva and Sarah K. Keedy and Matcheri S. Keshavan and Jennifer E. McDowell and Godfrey D. Pearlson and Carol A. Tamminga and Robert D. Gibbons},
abstract = {Clinically defined psychosis diagnoses are neurobiologically heterogeneous. The B-SNIP consortium identified and validated more neurobiologically homogeneous psychosis Biotypes using an extensive battery of neurocognitive and psychophysiological laboratory measures. However, typically the first step in any diagnostic evaluation is the clinical interview. In this project, we evaluated if psychosis Biotypes have clinical characteristics that can support their differentiation in addition to obtaining laboratory testing. Clinical interview data from 1907 individuals with a psychosis Biotype were used to create a diagnostic algorithm. The features were 58 ratings from standard clinical scales. Extremely randomized tree algorithms were used to evaluate sensitivity, specificity, and overall classification success. Biotype classification accuracy peaked at 91 % with the use of 57 items on average. A reduced feature set of 28 items, though, also showed 81 % classification accuracy. Using this reduced item set, we found that only 10–11 items achieved a one-vs-all (Biotype-1 or not, Biotype-2 or not, Biotype-3 or not) area under the sensitivity-specificity curve of .78 to .81. The top clinical characteristics for differentiating psychosis Biotypes, in order of importance, were (i) difficulty in abstract thinking, (ii) multiple indicators of social functioning, (iii) conceptual disorganization, (iv) severity of hallucinations, (v) stereotyped thinking, (vi) suspiciousness, (vii) unusual thought content, (viii) lack of spontaneous speech, and (ix) severity of delusions. These features were remarkably different from those that differentiated DSM psychosis diagnoses. This low-burden adaptive algorithm achieved reasonable classification accuracy and will support Biotype-specific etiological and treatment investigations even in under-resourced clinical and research environments.}
}
@incollection{TOPLAK20221,
title = {1 - Defining cognitive sophistication in the development of judgment and decision-making},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {1-22},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000104},
author = {Maggie E. Toplak},
keywords = {Judgment and decision-making, Children and youth, Development, Cognitive sophistication, Critical thinking, Rationality, Stimulus equivalence, Miserly processing},
abstract = {Judgment and decision-making paradigms have been relatively well-studied in developmental samples. The measurement of these competencies in developmental samples has been of scientific interest. They have been recognized as having important implications for defining rational thinking in children and youth but also for teaching and training (such as, critical thinking in education). The origin of the theories and paradigms come from the adult literature, which has also undergone considerable progress in theoretical advancements and empirical studies over the last several years. The integration of our understanding from the work conducted in adults with consideration of developmental factors provides a way to advance our understanding of judgment and decision-making in children and youth. To accomplish this, establishing stimulus equivalence will be important given that these paradigms were first designed for adult samples. In addition, taking into account the rapid growth and change in cognitive capacities, that happen in development, are central for understanding performance on these paradigms. Using a working taxonomy of rational thinking based on adult samples, data from a longitudinal developmental study were used to empirically examine performance patterns on these paradigms.}
}
@article{SAIDI20091467,
title = {PLR-based heuristic for backup path computation in MPLS networks},
journal = {Computer Networks},
volume = {53},
number = {9},
pages = {1467-1479},
year = {2009},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2009.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389128609000292},
author = {Mohand Yazid Saidi and Bernard Cousin and Jean-Louis {Le Roux}},
keywords = {Recovery, Local protection, Backup LSP, Failure risk, SRLG, MPLS, Bandwidth sharing, Path computation, Network},
abstract = {To ensure service continuity in networks, local protection pre-configuring the backup paths is preferred to global protection. Under the practical hypothesis of single physical failures in the network, the backup paths which protect against different logical failure risks (node, link and shared risk link group (SRLG)) cannot be active at the same time. Thus, sharing bandwidth between such backup paths is crucial to increase the bandwidth availability. In this article, we focus on the optimal on-line distributed computation of the bandwidth-guaranteed backup paths in MPLS networks. As the requests for connection establishment and release arrive dynamically without knowledge of future arrivals, we choose to use the on-line mode to avoid LSP reconfigurations. We also selected a distributed computation to offer scalability and decrease the LSP setup time. Finally, the optimization of bandwidth utilization can be achieved thanks to the flexibility of the path choice offered by MPLS and to the bandwidth sharing. For a good bandwidth sharing, the backup path computation entities (BPCEs) require the knowledge and maintenance of a great quantity of bandwidth information (e.g. non aggregated link information or per path information) which is undesirable in distributed environments. To get around this problem, we propose here a PLR (point of local repair)-based heuristic (PLRH) which aggregates and noticeably decreases the size of the bandwidth information advertised in the network while offering a high bandwidth sharing. PLRH permits an efficient computation of backup paths. It is scalable, easy to be deployed and balances equitably computations on the network nodes. Simulations show that with the transmission of a small quantity of aggregated information per link, the ratio of rejected backup paths is low and close to the optimum.}
}
@article{MILOVANOVIC2021101044,
title = {Characterization of concept generation for engineering design through temporal brain network analysis},
journal = {Design Studies},
volume = {76},
pages = {101044},
year = {2021},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2021.101044},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X21000557},
author = {Julie Milovanovic and Mo Hu and Tripp Shealy and John Gero},
keywords = {design cognition, design process, problem solving, conceptual design, design neurocognition},
abstract = {This research explores the effect of the structuredness of design concept generation techniques on temporal network neurocognition. Engineering graduate students (n = 30) completed three concept generation tasks using techniques with different levels of structuredness: brainstorming, morphological analysis, and TRIZ. Students’ brain activation in their prefrontal cortex (PFC) was measured using functional near-infrared spectroscopy (fNIRS). The temporal dynamic of central regions in brain networks were compared between tasks. Central regions facilitate functional interaction and imply information flow through the brain. A consistent central region appears in the medial PFC. Consistent network connections occurred across both hemispheres suggesting a concurrent dual processing of divergent and convergent thinking. This study offers novel insights into the underlying neurophysiological mechanism when using these concept generation techniques.}
}
@article{BALAHUR20141,
title = {Computational approaches to subjectivity and sentiment analysis: Present and envisaged methods and applications},
journal = {Computer Speech & Language},
volume = {28},
number = {1},
pages = {1-6},
year = {2014},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2013.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885230813000697},
author = {Alexandra Balahur and Rada Mihalcea and Andrés Montoyo},
keywords = {Subjectivity analysis, Sentiment analysis, Multilingual resources, Social Media mining, Chat analysis},
abstract = {Recent years have witnessed a surge of interest in computational methods for affect, ranging from opinion mining, to subjectivity detection, to sentiment and emotion analysis. This article presents a brief overview of the latest trends in the field and describes the manner in which the articles contained in the special issue contribute to the advancement of the area. Finally, we comment on the current challenges and envisaged developments of the subjectivity and sentiment analysis fields, as well as their application to other Natural Language Processing tasks and related domains.}
}
@article{KUO2010307,
title = {Conceptual study of micro-tab device in airframe noise reduction: (I) 2D computation},
journal = {Aerospace Science and Technology},
volume = {14},
number = {5},
pages = {307-315},
year = {2010},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2010.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1270963810000210},
author = {Brian C. Kuo and Nesrin Sarigul-Klijn},
keywords = {Computational aeroacoustics, High-lift device, Micro-tab, Airframe noise},
abstract = {A two-dimensional numerical study was performed to investigate the acoustic effects of micro-tab device on airframe noise reduction. As the noise generated by leading-edge slat and trailing-edge flap rise with their increased deflection angles, it is possible to mitigate such high-lift noise by using reduced settings without sacrificing the aerodynamic performance during approach. In this paper, micro-tab device attached to the pressure side of the flap surface is envisioned as a mean to achieve this goal. Hybrid method involving Computational Fluid Dynamics and acoustic analogy was used to predict the far-field noise spectrum. Results illustrate that the micro-tab device with reduced deflection angles of the high-lift settings provides lower noise signature at far-field positions, comparing to the baseline configuration, while the aerodynamic performance is maintained. In addition, two parametric studies which investigated the effects of micro-tab location and micro-tab height on acoustic spectra were also included.}
}
@article{LOURENCO2020258,
title = {Synaptic inhibition in the neocortex: Orchestration and computation through canonical circuits and variations on the theme},
journal = {Cortex},
volume = {132},
pages = {258-280},
year = {2020},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2020.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S001094522030318X},
author = {Joana Lourenço and Fani Koukouli and Alberto Bacci},
keywords = {Neocortex, Inhibition, Interneurons, Cortical circuits, Synaptic transmission},
abstract = {The neocortex plays a crucial role in all basic and abstract cognitive functions. Conscious mental processes are achieved through a correct flow of information within and across neocortical networks, whose particular activity state results from a tight balance between excitation and inhibition. The proper equilibrium between these indissoluble forces is operated with multiscale organization: along the dendro–somatic axis of single neurons and at the network level. Fast synaptic inhibition is assured by a multitude of inhibitory interneurons. During cortical activities, these cells operate a finely tuned division of labor that is epitomized by their detailed connectivity scheme. Recent results combining the use of mouse genetics, cutting-edge optical and neurophysiological approaches have highlighted the role of fast synaptic inhibition in driving cognition-related activity through a canonical cortical circuit, involving several major interneuron subtypes and principal neurons. Here we detail the organization of this cortical blueprint and we highlight the crucial role played by different neuron types in fundamental cortical computations. In addition, we argue that this canonical circuit is prone to many variations on the theme, depending on the resolution of the classification of neuronal types, and the cortical area investigated. Finally, we discuss how specific alterations of distinct inhibitory circuits can underlie several devastating brain diseases.}
}
@article{DAW2006199,
title = {The computational neurobiology of learning and reward},
journal = {Current Opinion in Neurobiology},
volume = {16},
number = {2},
pages = {199-204},
year = {2006},
note = {Cognitive neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2006.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959438806000316},
author = {Nathaniel D Daw and Kenji Doya},
abstract = {Following the suggestion that midbrain dopaminergic neurons encode a signal, known as a ‘reward prediction error’, used by artificial intelligence algorithms for learning to choose advantageous actions, the study of the neural substrates for reward-based learning has been strongly influenced by computational theories. In recent work, such theories have been increasingly integrated into experimental design and analysis. Such hybrid approaches have offered detailed new insights into the function of a number of brain areas, especially the cortex and basal ganglia. In part this is because these approaches enable the study of neural correlates of subjective factors (such as a participant's beliefs about the reward to be received for performing some action) that the computational theories purport to quantify.}
}
@article{FAUL2024255,
title = {Update on “Emotion and autobiographical memory”: 14 years of advances in understanding functions, constructions, and consequences},
journal = {Physics of Life Reviews},
volume = {51},
pages = {255-272},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001301},
author = {Leonard Faul and Jaclyn H. Ford and Elizabeth A. Kensinger},
abstract = {Holland and Kensinger (2010) reviewed the literature on “Emotion and autobiographical memory.” They focused on two broad ways that emotions influence memory: (1) emotion during an event influences how the event is remembered, and (2) emotion and emotional goals during memory retrieval influence how past events are remembered. We begin by providing a brief update on the key points from that review. Holland and Kensinger (2010) also had noted a number of important avenues for future work. Here, we describe what has been learned about the functions of autobiographical memory and their reconstructive nature. Relatedly, we review more recent research on memory reconstruction in the context of visual perspective shifts, counterfactual thinking, nostalgia, and morality. This research has emphasized the reciprocal nature of the interactions between emotion and autobiographical memory: Not only do emotions influence memory, memories influence emotions. Next, we discuss advances that have been made in understanding the reciprocal relations between stress, mood, and autobiographical memory. Finally, we discuss the research that is situating emotional autobiographical memories within a social framework, providing a bedrock for collective memories. Despite the many advances of the past 14 years, many open questions remain; throughout the review we note domains in which we hope to see advances over the next decades.}
}
@article{LONG201855,
title = {Data-driven decision making for supply chain networks with agent-based computational experiment},
journal = {Knowledge-Based Systems},
volume = {141},
pages = {55-66},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117305294},
author = {Qingqi Long},
keywords = {Data-driven decision making, Supply chain network, Business analytics, Data-granularity model, Four-dimensional-flow model, Agent-based computational experiment},
abstract = {The complicated micro structures, macro emergences and dynamic evolutions in a supply chain network pose challenges to decision making for solving operational problems for the network's performance improvement. Most of these problems are complicated since various factors and their complicated relationships are involved. Success of this decision making relies on efficient business analytics based on the comprehensive and multi-dimensional data related to the static attributes and dynamic operations of the network. To confront the challenges, this paper proposes to explore a methodology of data-driven decision making for supply chain networks. In this methodology, a data-granularity model of a supply chain network is developed to standardize the data form for decision making. A four-dimensional-flow model of a supply chain network is proposed to satisfy the data requirements for decision making that are defined in the data-granularity model. Agent-based computational experiment is employed to support the generation of a comprehensive operational dataset of a supply chain network and to verify the solutions generated in decision making. Integrating these models, a data-driven decision-making framework for supply chain networks is proposed. In the framework, a new decision-making mode of “problem definition - business analytics - solution verification - parameter adjustment” is proposed. Oriented towards domain knowledge in supply chain networks, two approaches of business analytics—mapping analysis and correlation analysis—are presented. Finally, a case of a five-echelon manufacturing supply chain network is studied with the methodology. The findings indicate that the proposed methodology, models and framework are effective in supporting the data-centric decision making for solving complicated operational problems in supply chain networks and provide the networks’ managers or member enterprises with an effective tool to generate unbiased and efficient decisions for the networks’ performance improvement.}
}
@incollection{WILLETT2018231,
title = {Chapter 8 - Application of Mathematical Models and Computation in Plant Metabolomics},
editor = {Satyajit D. Sarker and Lutfun Nahar},
booktitle = {Computational Phytochemistry},
publisher = {Elsevier},
pages = {231-254},
year = {2018},
isbn = {978-0-12-812364-5},
doi = {https://doi.org/10.1016/B978-0-12-812364-5.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128123645000080},
author = {Denis S. Willett and Caitlin C. Rering and Dominique A. Ardura and John J. Beck},
keywords = {Big data, Machine learning, Data science, Agriculture},
abstract = {The investigation and reporting of plant chemical constituents has greatly evolved over the course of natural products and phytochemical research. Starting from the extraction and identification of plant-based bioactive components, such as historical salicin or more recent paclitaxel, phytochemistry-based research now includes plant metabolomics that help delineate chemotaxonomy, phylogenetic biomarkers and the functional genetics of a plant’s response to biotic or abiotic stressors. Here, we examine the invaluable contributions of mathematical models and computation for analysing plant metabolomics data and discuss the analytics mindset, highlight best practices, provide example workflows, as well as introduce future opportunities. Important in this chapter is the application of statistical methods for the improved visualization and interpretation of plant metabolomics data and their relevance for future project planning.}
}
@article{FEKETE2011807,
title = {Towards a computational theory of experience},
journal = {Consciousness and Cognition},
volume = {20},
number = {3},
pages = {807-827},
year = {2011},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2011.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1053810011000365},
author = {Tomer Fekete and Shimon Edelman},
keywords = {Representation, Experience, Qualia, Computation, State space, Trajectory, Dynamics, Brain activation, Concept, Clustering},
abstract = {A standing challenge for the science of mind is to account for the datum that every mind faces in the most immediate – that is, unmediated – fashion: its phenomenal experience. The complementary tasks of explaining what it means for a system to give rise to experience and what constitutes the content of experience (qualia) in computational terms are particularly challenging, given the multiple realizability of computation. In this paper, we identify a set of conditions that a computational theory must satisfy for it to constitute not just a sufficient but a necessary, and therefore naturalistic and intrinsic, explanation of qualia. We show that a common assumption behind many neurocomputational theories of the mind, according to which mind states can be formalized solely in terms of instantaneous vectors of activities of representational units such as neurons, does not meet the requisite conditions, in part because it relies on inactive units to shape presently experienced qualia and implies a homogeneous representation space, which is devoid of intrinsic structure. We then sketch a naturalistic computational theory of qualia, which posits that experience is realized by dynamical activity-space trajectories (rather than points) and that its richness is measured by the representational capacity of the trajectory space in which it unfolds.}
}
@article{RONEZRA2021100896,
title = {Engaging a third-grade student with autism spectrum disorder in an error finding activity},
journal = {The Journal of Mathematical Behavior},
volume = {63},
pages = {100896},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100896},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000572},
author = {Maya Ron-Ezra and Esther S. Levenson},
keywords = {Autism spectrum disorder, Two-digit addition, Error analysis, Mathematical explanations},
abstract = {This paper describes a case study of one mainstreamed third grade student with autism spectrum disorder (ASD) and his ability to explain his solutions for two-digit addition problems, and find and explain the mistake when presented with incorrectly solved addition problems. The study is presented as a counterexample to deficit views of ASD, views that focus on lack of communication skills, not being able to see someone else’s point of view, and poor executive functions. Each encounter with the student is analyzed in two ways, first analyzing his mathematical knowledge, and then analyzing obstacles the student faces that are associated with ASD. Some obstacles are overcome by the student on his own and others are overcome with the help of the researcher, who responds to the student’s thinking, and supports his endeavor to engage with a challenging activity.}
}
@article{KISS2020106823,
title = {Process systems engineering developments in Europe from an industrial and academic perspective},
journal = {Computers & Chemical Engineering},
volume = {138},
pages = {106823},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106823},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420303069},
author = {Anton A. Kiss and Johan Grievink},
keywords = {Process systems engineering, Industry, Education, Research, Interface, Perspectives},
abstract = {Process Systems Engineering (PSE) is a discipline that deals with decision-making, at all levels and scales, by understanding any complex process system using a holistic view and a systems thinking framework. A closely related discipline (considered usually a part of PSE) is the Computer Aided Process Engineering (CAPE) which is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. Nowadays, the ‘PSE’ term suffers from a branding issue to the point that PSE no longer gets the recognition that it deserves. In chemical engineering education the integrative systems frame for process design, control and operations is virtually absent. Its application potential in process industry lags relative to academic research progress and results. This work aims to provide an informative industrial and academic perspective on PSE (focused on the European region), arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be given priority over just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces to create win-win situations.}
}
@article{TARIM2011563,
title = {An efficient computational method for a stochastic dynamic lot-sizing problem under service-level constraints},
journal = {European Journal of Operational Research},
volume = {215},
number = {3},
pages = {563-571},
year = {2011},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2011.06.034},
url = {https://www.sciencedirect.com/science/article/pii/S0377221711005637},
author = {S. Armagan Tarim and Mustafa K. Dogˇru and Ulaş Özen and Roberto Rossi},
keywords = {Inventory, Relaxation, Stochastic non-stationary demand, Mixed integer programming, Service level, Static–dynamic uncertainty},
abstract = {We provide an efficient computational approach to solve the mixed integer programming (MIP) model developed by Tarim and Kingsman [8] for solving a stochastic lot-sizing problem with service level constraints under the static–dynamic uncertainty strategy. The effectiveness of the proposed method hinges on three novelties: (i) the proposed relaxation is computationally efficient and provides an optimal solution most of the time, (ii) if the relaxation produces an infeasible solution, then this solution yields a tight lower bound for the optimal cost, and (iii) it can be modified easily to obtain a feasible solution, which yields an upper bound. In case of infeasibility, the relaxation approach is implemented at each node of the search tree in a branch-and-bound procedure to efficiently search for an optimal solution. Extensive numerical tests show that our method dominates the MIP solution approach and can handle real-life size problems in trivial time.}
}
@incollection{SILVA20203,
title = {Chapter 1 - Introduction and overview of using computational fluid dynamics tools},
editor = {Valter Bruno Reis E. Silva and João Cardoso},
booktitle = {Computational Fluid Dynamics Applied to Waste-to-Energy Processes},
publisher = {Butterworth-Heinemann},
pages = {3-28},
year = {2020},
isbn = {978-0-12-817540-8},
doi = {https://doi.org/10.1016/B978-0-12-817540-8.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128175408000017},
author = {Valter Bruno Reis E. Silva and João Cardoso},
keywords = {Computer fluid dynamics, Waste-to-energy, Simulation workflow, Fluid dynamics history},
abstract = {Over the last decades, with the increasing computational power and numerical solvers efficiency, computational fluid dynamics (CFD) is broadly used to design, optimize, and predict the physical-chemical phenomena regarding energy-related processes. A set of elaborate mathematical models is governed by partial differential equations representing conservation laws for mass, momentum, and energy, alongside with theoretical and empirical correlation. Therefore, CFD simulation is a crucial asset to understand the influence of parameters of interest in these processes and related operation and optimization of the technology involved. This chapter discusses how CFD can be used advantageously over waste-to-energy processes, also outlining advantages, disadvantages, and main setbacks with such an approach.}
}
@article{ALPUENTE20153,
title = {Exploring conditional rewriting logic computations},
journal = {Journal of Symbolic Computation},
volume = {69},
pages = {3-39},
year = {2015},
note = {Symbolic Computation in Software Science},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2014.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0747717114000960},
author = {M. Alpuente and D. Ballis and F. Frechina and J. Sapiña},
keywords = {Rewriting logic, Trace exploration, Maude, Conditional rewrite theories},
abstract = {Trace exploration is concerned with techniques that allow computation traces to be dynamically searched for specific contents. Depending on whether the exploration is carried backward or forward, trace exploration techniques allow provenance tracking or impact tracking to be done. The aim of provenance tracking is to show how (parts of) a program output depends on (parts of) its input and to help estimate which input data need to be modified to accomplish a change in the outcome. The aim of impact tracking is to identify the scope and potential consequences of changing the program input. Rewriting Logic (RWL) is a logic of change that supplements (an extension of) the equational logic by adding rewrite rules that are used to describe (nondeterministic) transitions between states. In this paper, we present a rich and highly dynamic, parameterized technique for the forward inspection of RWL computations that allows the nondeterministic execution of a given conditional rewrite theory to be followed up in different ways. With this technique, an analyst can browse, slice, filter, or search the traces as they come to life during the program execution. The navigation of the trace is driven by a user-defined, inspection criterion that specifies the required exploration mode. By selecting different inspection criteria, one can automatically derive a family of practical algorithms such as program steppers and more sophisticated dynamic trace slicers that compute summaries of the computation tree, thereby facilitating the dynamic detection of control and data dependencies across the tree. Our methodology, which is implemented in the Anima graphical tool, allows users to evaluate the effects of a given statement or instruction in isolation, track input change impact, and gain insight into program behavior (or misbehavior).}
}
@article{BASU2021135660,
title = {Integrative STEM education for undergraduate neuroscience: Design and implementation},
journal = {Neuroscience Letters},
volume = {746},
pages = {135660},
year = {2021},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2021.135660},
url = {https://www.sciencedirect.com/science/article/pii/S0304394021000380},
author = {Alo C. Basu and Alexis S. Hill and André K. Isaacs and Michelle A. Mondoux and Ryan E.B. Mruczek and Tomohiko Narita},
keywords = {Integrative thinking, Spiral curriculum, Active learning, Inclusive pedagogy, Inclusive excellence, Anti-deficit},
abstract = {As an integrative discipline, neuroscience can serve as a vehicle for the development of integrative thinking skills and broad-based scientific proficiency in undergraduate students. Undergraduate neuroscience curricula incorporate fundamental concepts from multiple disciplines. Deepening the explicit exploration of these connections in a neuroscience core curriculum has the potential to support more meaningful and successful undergraduate STEM learning for neuroscience students. Curriculum and faculty development activities related to an integrative core curriculum can provide opportunities for faculty across disciplines and departments to advance common goals of inclusive excellence in STEM. These efforts facilitate analysis of the institutional STEM curriculum from the student perspective, and assist in creating an internal locus of accountability for diversity, equity, and inclusion within the institution. Faculty at the College of the Holy Cross have undertaken the collaborative design and implementation of an integrative core curriculum for neuroscience that embraces principles of inclusive pedagogy, emphasizes the connections between neuroscience and other disciplines, and guides students to develop broad proficiency in fundamental STEM concepts and skills.}
}
@article{CSILLERY2010410,
title = {Approximate Bayesian Computation (ABC) in practice},
journal = {Trends in Ecology & Evolution},
volume = {25},
number = {7},
pages = {410-418},
year = {2010},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2010.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534710000662},
author = {Katalin Csilléry and Michael G.B. Blum and Oscar E. Gaggiotti and Olivier François},
abstract = {Understanding the forces that influence natural variation within and among populations has been a major objective of evolutionary biologists for decades. Motivated by the growth in computational power and data complexity, modern approaches to this question make intensive use of simulation methods. Approximate Bayesian Computation (ABC) is one of these methods. Here we review the foundations of ABC, its recent algorithmic developments, and its applications in evolutionary biology and ecology. We argue that the use of ABC should incorporate all aspects of Bayesian data analysis: formulation, fitting, and improvement of a model. ABC can be a powerful tool to make inferences with complex models if these principles are carefully applied.}
}
@article{GROTH20211712,
title = {A systems-based framework to computationally describe putative transcription factors and signaling pathways regulating glycan biosynthesis},
journal = {Beilstein Journal of Organic Chemistry},
volume = {17},
pages = {1712-1724},
year = {2021},
issn = {1860-5397},
doi = {https://doi.org/10.3762/bjoc.17.119},
url = {https://www.sciencedirect.com/science/article/pii/S186053972102209X},
author = {Theodore Groth and Rudiyanto Gunawan and Sriram Neelamegham},
keywords = {ChIP-Seq, glycoinformatics, glycosylation, TCGA transcription factor},
abstract = {Glycosylation is a common posttranslational modification, and glycan biosynthesis is regulated by a set of glycogenes. The role of transcription factors (TFs) in regulating the glycogenes and related glycosylation pathways is largely unknown. In this work, we performed data mining of TF–glycogene relationships from the Cistrome Cancer database (DB), which integrates chromatin immunoprecipitation sequencing (ChIP-Seq) and RNA-Seq data to constitute regulatory relationships. In total, we observed 22,654 potentially significant TF–glycogene relationships, which include interactions involving 526 unique TFs and 341 glycogenes that span 29 the Cancer Genome Atlas (TCGA) cancer types. Here, TF–glycogene interactions appeared in clusters or so-called communities, suggesting that changes in single TF expression during both health and disease may affect multiple carbohydrate structures. Upon applying the Fisher’s exact test along with glycogene pathway classification, we identified TFs that may specifically regulate the biosynthesis of individual glycan types. Integration with Reactome DB knowledge provided an avenue to relate cell-signaling pathways to TFs and cellular glycosylation state. Whereas analysis results are presented for all 29 cancer types, specific focus is placed on human luminal and basal breast cancer disease progression. Overall, the article presents a computational approach to describe TF–glycogene relationships, the starting point for experimental system-wide validation.}
}
@article{SCHERER2020106349,
title = {A meta-analysis of teaching and learning computer programming: Effective instructional approaches and conditions},
journal = {Computers in Human Behavior},
volume = {109},
pages = {106349},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106349},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220301023},
author = {Ronny Scherer and Fazilat Siddiq and Bárbara {Sánchez Viveros}},
keywords = {Computational thinking, Computer programming, Intervention studies, Multilevel meta-analysis, Scratch programming},
abstract = {This meta-analysis maps the evidence on the effectiveness of instructional approaches and conditions for learning computer programming under three study conditions: (a) Studies focusing on the effectiveness of programming interventions per se, (b) studies focusing on the effectiveness of visualization and physicality, and (c) studies focusing on the effectiveness of dominant instructional approaches. Utilizing the data from 139 interventions and 375 effect sizes, we found (a) a strong effect of learning computer programming per se (Hedges’ g‾ = 0.81, 95% CI [0.42, 1.21]), (b) moderate to large effect sizes of visualization (g‾ = 0.44, 95% CI [0.29, 0.58]) and physicality interventions (g‾ = 0.72, 95% CI [0.23, 1.21]), and (c) moderate to large effect sizes for studies focusing on dominant instructional approaches (g‾s = 0.49–1.02). Moderator analyses indicated that the effect sizes differed only marginally between the instructional approaches and conditions—however, collaboration in metacognition instruction, problem solving instruction outside of regular lessons, short-term interventions focusing on physicality, and interventions focusing on visualization through Scratch were especially effective. Our meta-analysis synthesizes the existing research evidence on the effectiveness of computer programming instruction and, ultimately, provides references with which the effects of future studies could be compared.}
}
@incollection{GARDNER20243,
title = {Chapter 1 - Scaling the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {3-25},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318452900001X},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, IoT, Scalar, Scale, Scaling, Smart city, Techno-urban imaginary, Urban design, Urban technology},
abstract = {This chapter explores the smart city through the conceptual lens of scale, as a scale-making project and as a project that is subject to scaling processes. It explores how scalar notions figure in smart city discourses, and how the drive to scale shapes the prevailing approach to digital technology and urban space integration. It argues that deprioritizing the smart city's scalability logic can bring into view different ways of designing the integration of digital technologies and urban space that can better connect with the contextual and material specificities of local contexts and less attended to dimensions of urban livability. Rescaling the smart city to the local urban precinct scale and paying close attention to life-technology relations is further reasoned as a way to productively re-orient and extend thinking on the ethical significance of the smart city.}
}
@article{JI20074338,
title = {A fuzzy logic-based computational recognition-primed decision model},
journal = {Information Sciences},
volume = {177},
number = {20},
pages = {4338-4353},
year = {2007},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2007.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025507001193},
author = {Yanqing Ji and R. Michael Massanari and Joel Ager and John Yen and Richard E. Miller and Hao Ying},
keywords = {Medical decision-making, Naturalistic decision-making, Recognition-primed decision model, Computational recognition-primed decision model, Experience-based reasoning, Adverse drug reactions, Fuzzy logic, Similarity measure},
abstract = {The recognition-primed decision (RPD) model is a primary naturalistic decision-making approach which seeks to explicitly recognize how human decision makers handle complex tasks and environment based on their experience. Motivated by the need for quantitative computer modeling and simulation of human decision processes in various application domains, including medicine, we have developed a general-purpose computational fuzzy RPD model that utilizes fuzzy sets, fuzzy rules, and fuzzy reasoning to represent, interpret, and compute imprecise and subjective information in every aspect of the model. Experiences acquired by solicitation with experts are stored in experience knowledge bases. New local and global similarity measures have been developed to identify the experience that is most applicable to the current situation in a specific decision-making context. Furthermore, an action evaluation strategy has been developed to select the workable course of action. The proposed fuzzy RPD model has been preliminarily validated by using it to calculate the extent of causality between a drug (Cisapride, withdrawn by the FDA from the market in 2000) and some of its adverse effects for 100 hypothetical patients. The simulated patients were created based on the profiles of over 1000 actual patients treated with the drug at our medical center before its withdrawal. The model validity was demonstrated by comparing the decisions made by the proposed model and those by two independent internists. The levels of agreement were established by the weighted Kappa statistic and the results suggested good to excellent agreement.}
}
@article{DORKO2023101036,
title = {Is it a function? Generalizing from single- to multivariable settings},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101036},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101036},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000068},
author = {Allison Dorko},
keywords = {Actor-oriented transfer, Generalization, Multivariable calculus, Functions of two variables, Multivariable functions},
abstract = {Generalizing is a hallmark of mathematical thinking. The term ‘generalization’ is used to mean both the process of generalizing and the product of that process. This paper reports on five calculus students’ generalizing activity and what they generalized about multivariable functions. The study makes two contributions. The first is a fine-grained, actor-oriented characterization of the ways undergraduates generalized. This adds to knowledge in two areas: the use of the actor-oriented perspective and generalization in advanced mathematics. The second contribution is the products of students’ generalizing: what they generalized about what it means for a multivariable relation to represent a function). This adds to the literature about student reasoning regarding multivariable topics by characterizing the powerful ways of reasoning students possess pre-instruction.}
}
@article{SHAHZAD2022102190,
title = {Thermal cooling process by nanofluid flowing near stagnating point of expanding surface under induced magnetism force: A computational case study},
journal = {Case Studies in Thermal Engineering},
volume = {36},
pages = {102190},
year = {2022},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2022.102190},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X22004361},
author = {Faisal Shahzad and Wasim Jamshed and Amjad Ali Pasha and Rabia Safdar and Md. Mottahir Alam and Misbah Arshad and Syed M. Hussain and Muhammad Bilal Hafeez and Marek Krawczuk},
keywords = {, , , , },
abstract = {This paper is dedicated to the exam of entropy age and research of the effect of mixing nanosolid additives over an extending sheet. In this review, Newtonian nanofluid version turned into researched at the actuated appealing field, heat radiation and variable heat conductivity results. With becoming modifications, the proven PDEs are moved into popular differential situations and paintings mathematically making use of a specific mathematical plan called the Keller box method (KBM). The ranges of different dimensionless parameters used in our study are volume fraction of nanoparticles 0.01≤φ≤0.04, magnetic parameter 0.5≤Λ≤2, thermal radiation 0.1≤Nr≤0.3, heat source/sink parameter 0.5≤Q0≤2, Prandtl number 5.7≤Pr≤6.2, variable thermal conductivity 0.1≤ε≤0.3, reciprocal magnetic Prandtl number 0.6≤λ∗≤1, Brinkman number 5≤Br≤15, Reynolds number 5≤Re≤15, which shows up during mathematical arrangement are shown as tables and charts.Positive modifications in heat radiation and heat conductivity affects increment the hotness pass coefficient of solar primarily based totally plane wings. Titanium alloy primarily based totally water (H2O) are taken into consideration for our research. We will likewise alternate the grouping of nanoparticles to pay attention on their impact on numerous dynamic barriers of the framework. We can see that because the Reynolds range and Brinkman range increment, the entropy increments. The thermodynamic exhibition of Titanium alloy-water (Ti6Al4V–H2O) nanofluid has been portrayed higher that of base nanofluid with comparable situations. Recorded hypothetical reproductions may be greater beneficial to similarly increase daylight primarily based totally nuclear strength frameworks.}
}
@article{KAWITI2025100213,
title = {Indigenous knowledge, architecture, and nature in the context of Oceania},
journal = {Nature-Based Solutions},
volume = {7},
pages = {100213},
year = {2025},
issn = {2772-4115},
doi = {https://doi.org/10.1016/j.nbsj.2025.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2772411525000035},
author = {Derek Kawiti and Albert Refiti and Amanda Yates and Elisapeta Heta and Sibyl Bloomfield and Victoria Chanse and Maibritt Pedersen Zari},
keywords = {Pacific, Indigenous, Architecture, Ecology, Climate change adaptation, Māori, Samoan},
abstract = {This perspective article is derived from conversations between leading Indigenous academics and practitioners in the fields of architecture and urban design recorded at a keynote panel at the 2023 NUWAO International Symposium on Nature-based Urban Climate Adaptation for Wellbeing, held at Te Herenga Waka Victoria University of Wellington, Aotearoa New Zealand. The focus of the discussion was Indigenous design for adaptation to climate change in Moananui Oceania with an emphasis on relationships to nature. Given the diversity of Moananui Oceania in terms of languages, cultures, histories, and worldviews, this discussion represented a unique convergence of Indigenous leadership and thought in the field. It highlighted key themes related to Indigenous design for climate change adaptation and offered a novel, distinctive perspective aimed at advancing thinking around nature-based solutions (NbS). It is important to recognise and integrate Indigenous values and approaches to knowledge generation, particularly within academic settings. In the context of Moananui Oceania this can require adapting oral traditions and formats, such as talanoa, and hui or kōrero, into conventional Western-based research formats such as the journal article. This paper is an attempt to capture important Indigenous knowledge and discussion in a western format to enable further dissemination and sharing. This means the format and methodologies described in the paper do not align exactly with traditional scientific journal article formats, however the discussions and findings help to meet the motivation of the authors, which is to transform traditional Indigenous ways of sharing information into a perspective article format and share insights with a wider audience. This methodology aligns well with the special issue call that this paper resides in (Just, Socio-ecological Urban Transformation: Nature-based Solutions and Traditional Ecological Knowledge), underpinning the relevance and potential contribution to the field. Two key themes were explored within the context of the importance of working with nature; relationships between ecologies and tikanga (customary practices), and looking backwards to generate innovation and resilience.}
}
@article{BAWDEN1984205,
title = {Systems thinking and practices in the education of agriculturalists},
journal = {Agricultural Systems},
volume = {13},
number = {4},
pages = {205-225},
year = {1984},
issn = {0308-521X},
doi = {https://doi.org/10.1016/0308-521X(84)90074-X},
url = {https://www.sciencedirect.com/science/article/pii/0308521X8490074X},
author = {Richard J. Bawden and Robert D. Macadam and Roger J. Packham and Ian Valentine},
abstract = {A systems approach has been taken to a review of agricultural education programmes and as the essential theme of resultant curricula at Hawkesbury Agricultural College in Australia. The systems thinking and practices which have guided, and been shaped by, the innovations are outlined, and the rationale and framework of the major programme are described. The subsequent emphasis has been placed on effective learning for agricultural managers and their technologist advisors. It is argued that problem solving and learning are essentially the same psychological processes and that taking a systems approach to investigating problem situations provides a more useful paradigm for learning about agriculture than reductionist, discipline-based approaches. Experiential learning and autonomy in learning are seen as consistent with this and are basic features of the programmes. A conceptual framework for problem solving that incorporates soft and hard systems and scientific reductionist methodologies has been developed. A contingency approach to situation improving is emerging as a less restrictive and more realistic alternative to a normative approach to problem solving.}
}
@article{AGGARWAL2023110458,
title = {Quantum healthcare computing using precision based granular approach},
journal = {Applied Soft Computing},
volume = {144},
pages = {110458},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110458},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623004763},
author = {Lakshita Aggarwal and Shelly Sachdeva and Puneet Goswami},
keywords = {Quantum computing, Qubits, Healthcare, Diagnosis, Classical computing, Precision},
abstract = {Previously, doctors interpreted diseases and their outcomes according to their experience in diagnosis. However, with the rapid increase in technology and population, the task of examining the patient becomes cumbersome and sometimes human efforts produce inconsistent results. Several research is being done for healthcare in terms of improving visualization and accuracy by using machine learning models. The current research targets to explore quantum computing as a different way of processing information compared to classical computer systems such as the use of quantum bits (qubits) along with superposition and entanglement for extending the computation capabilities at an unprecedented level of thinking in the healthcare domain. Quantum computing systems provide exponential benefits in terms of high-speed processing, faster and easier diagnostic assistance, unimaginable reduction in processing throughput, and many more. An extensive comparative analysis of existing approaches has been made which benchmarks the need for quantum healthcare computing. The objective of this work is to interpret whether Quantum computers prove to be more trusted when it comes to patient diagnosis, and faster analysis leading to cost optimization. In order to accelerate patient diagnosis, different approaches have been presented. The authors have proposed a precision-based granular approach for patient diagnosis that incorporates diagnosing the disease with enhanced precision and granularity. It involves reporting symptoms by the patient, encountering by healthcare expert on multiple factors, precise examination, granular health status (understanding past and present medical history), followed by a precise intervention by understanding biomolecular simulations. The algorithm has been presented to describe the flow process for patient diagnosis modeling using quantum computing. It involves qubits initialization, pairing the values, assigning probabilistic values, cross-validation, and quantum circuit formation. Precision-based granular approach has been implemented for a scenario (consisting of medical parameters such as oxygen and heart rate level, with the functionality of diagnosing oxygen level and heart range which lies as either normal or not normal (high/low)). Precision-based granular approach deals specifically with the individual ‘biomolecular simulation by understanding variations in the individual body whereas the umbrella-based approach does not deal with specifically to individual mechanisms. Granular level of encounter is not possible in umbrella-based treatment. Python Jupyter notebook and IBM Composer tool is used for the implementation of results. Bloch sphere and computational state graph are obtained as an output for better visualization and understanding. Falcon r5.11H processor is used with the version of 1.0.24 of IBM Composer to simulate the experiment. The methodology using precision based granular approach provides timely encounter of disease along with umbrella diagnosis and precise treatment. The time is taken and frequency of qubits have been presented with promising results. The diagnosis process and optimizing cost efficiency can aid in an early detection of the disease.}
}
@article{GANUTHULA2016216,
title = {Rationality and the reflective mind: A case for typical performance measure of cognitive ability},
journal = {Learning and Individual Differences},
volume = {49},
pages = {216-223},
year = {2016},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2016.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S1041608016301029},
author = {Venkat Ram Reddy Ganuthula and Lata Dyaram},
keywords = {Typical performance measure of cognitive ability, Thinking dispositions, Rationality, Tripartite model of mind},
abstract = {Intelligence and cognitive abilities often denoted good thinking. However, critics of intelligence tests have long pointed out that the failures of rational judgments and decision-making imperfectly correlate with intelligence. Reviewing the work of Keith Stanovich and his colleagues, paper highlights the role of individual differences in judgment and decision-making. Paper presents a case for typical performance measure of cognitive ability besides thinking dispositions to explain variations in rational thought. Specifically, we examine and model the relationship between need for cognition (a measure of thinking dispositions), absorptive capacity (typical performance measure of intelligence) and normative decision-making tasks.}
}
@article{CARROLL1999111,
title = {Invented Computational Procedures of Students in a Standards-Based Curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {111-121},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00024-3},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000243},
author = {William M. Carroll},
abstract = {Fourth graders who had been in a standards-based elementary curriculum since kindergarten were individually interviewed and administered a whole-class test that probed their knowledge of facts and multidigit computation. Standard algorithms are not taught as part of the curriculum, which instead emphasizes student-invented procedures and discussions of solution methods. Of interest were the types of student-invented procedures that were used as well as their computational accuracy. Students used several procedures that involved sophisticated mental calculation strategies, such as decomposing numbers or adding from left to right. Many students also used the standard written algorithms. Both invented and standard algorithms used by the students were highly accurate, although invented procedures often indicated better mental flexibility and awareness of place value. On the written test, students' computational abilities were above national normative levels.}
}
@article{BOTTEGONI201223,
title = {The role of fragment-based and computational methods in polypharmacology},
journal = {Drug Discovery Today},
volume = {17},
number = {1},
pages = {23-34},
year = {2012},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2011.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359644611002534},
author = {Giovanni Bottegoni and Angelo D. Favia and Maurizio Recanatini and Andrea Cavalli},
abstract = {Polypharmacology-based strategies are gaining increased attention as a novel approach to obtaining potentially innovative medicines for multifactorial diseases. However, some within the pharmaceutical community have resisted these strategies because they can be resource-hungry in the early stages of the drug discovery process. Here, we report on fragment-based and computational methods that might accelerate and optimize the discovery of multitarget drugs. In particular, we illustrate that fragment-based approaches can be particularly suited for polypharmacology, owing to the inherent promiscuous nature of fragments. In parallel, we explain how computer-assisted protocols can provide invaluable insights into how to unveil compounds theoretically able to bind to more than one protein. Furthermore, several pragmatic aspects related to the use of these approaches are covered, thus offering the reader practical insights on multitarget-oriented drug discovery projects.}
}
@incollection{SLEPIAN2024516,
title = {4.01 - Synergistic Approaches of Cross-Fertilization and Feedback Together Driving and Advancing Health for All},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {516-523},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00081-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000812},
author = {Marvin J. Slepian},
keywords = {Digital Health, Multiscale, Nested pots, Personalized medicine, Pharmacogenomics, Point-of-care, Precision medicine, System synergies, Systems biology, Wearable technologies},
abstract = {We are at a point in time with rapid advances occurring in digital technologies, developing a range of new quantifiable markers termed “digital biomarkers,” which are increasingly utilized for diagnostics, as well as defining new operative mechanisms of health and disease. In parallel, significant advances have occurred in precision medicine, utilizing breakthroughs in “omics biology,” coupled with our understanding of their impact across systems in “systems biology.” Contemporaneously, a new approach to thinking of how health and disease evolve and impact an individual has emerged—that of considering mechanisms and impact across scales, i.e. on a “multi-scale” level, extending from the patient down to the molecule, and similarly from the patient up to society. In this chapter details of each of these approaches, their evolution and key current concepts are outlined. Moreover, the main theme and postulate developed in this chapter outlines the interconnectedness and the way in which each approach informs each other. In essence a cyclic, reinforcing, feedback loop exists, connecting digital technologies with precision and personalization approaches, across systems and scales, leading to enhanced diagnostics, the potential for new therapeutics and increasing insight into mechanisms. This cyclic flow of information will lead to new, more exacting technologies, with the ultimate outcome of enhanced efficacy, safety and improved health outcomes for patients and society.}
}
@article{BAR202135,
title = {Wanted: Architecture for changing minds: A comment on “The growth of cognition: Free energy minimization and the embryogenesis of cortical computation”},
journal = {Physics of Life Reviews},
volume = {36},
pages = {35-36},
year = {2021},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2020.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1571064520300683},
author = {Moshe Bar}
}
@article{CRAWFORD202180,
title = {Efficient mechanisms for level-k bilateral trading},
journal = {Games and Economic Behavior},
volume = {127},
pages = {80-101},
year = {2021},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2021.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825621000282},
author = {Vincent P. Crawford},
keywords = {Mechanism design, Bilateral trading, Level- thinking, Behavioral game theory},
abstract = {This paper revisits Myerson and Satterthwaite's (1983) classic analysis of mechanism design for bilateral trading, replacing equilibrium with a level-k model of strategic thinking and focusing on direct mechanisms. The revelation principle fails for level-k models, so restricting attention to direct mechanisms and imposing incentive-compatibility are not without loss of generality. If, however, only direct, level-k-incentive-compatible mechanisms are feasible and traders' levels are observable, Myerson and Satterthwaite's characterization of mechanisms that maximize traders' total surplus subject to incentive constraints generalizes qualitatively to level-k models. If only direct, level-k-incentive-compatible mechanisms are feasible but traders' levels are not observable, generically a particular posted-price mechanism maximizes traders' total expected surplus subject to incentive constraints. If direct, non-level-k-incentive-compatible mechanisms are feasible and traders best respond to them, total expected surplus-maximizing mechanisms may take completely different forms.}
}
@article{BOND200481,
title = {A computational model for the primate neocortex based on its functional architecture},
journal = {Journal of Theoretical Biology},
volume = {227},
number = {1},
pages = {81-102},
year = {2004},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2003.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0022519303003825},
author = {Alan H Bond},
keywords = {Brain architecture, Perception-action hierarchy, Computational model, Logic programming, Primate social behavior},
abstract = {Experimental evidence has shown that the primate neocortex consists in the main of a set of cortical regions which form a perception hierarchy, an action hierarchy and connections between them. By using a computer science analysis, we develop a computational architecture for the brain in which each cortical region is represented by a computational module with processing and storage abilities. Modules are interconnected according to the connectivity of the corresponding cortical regions. We develop computational principles for designing such a hierarchical and parallel computing system. We demonstrate this approach by proposing a causal functioning model of the brain. We report on results obtained with an implementation of this model. We conclude with a brief discussion of some consequences and predictions of our work.}
}
@article{WEBB2010903,
title = {Troubleshooting assessment: an authentic problem solving activity for it education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {903-907},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.256},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002361X},
author = {David C. Webb},
keywords = {authentic assessment, computational thinking, computer programming, game design, problem solving, STEM education, technologybased assessment},
abstract = {To evaluate the effectiveness of an instructional unit for game design and computer programming, we designed an authentic assessment with five troubleshooting scenarios. This assessment was completed by 24 middle grades students (age 12 – 14 years) after 10hours of instruction using a visual programming environment. Students successfully completed most of the tasks in 45minutes. Results from the Troubleshooting Assessment demonstrated that students developed sufficient fluency with programming to be able to apply their knowledge to new problems. These results suggest that troubleshooting scenarios can be used to assess student fluency in computer programming and computer-based problem solving.}
}
@article{SALVATORE2024143,
title = {The affective grounds of the mind. The Affective Pertinentization (APER) model},
journal = {Physics of Life Reviews},
volume = {50},
pages = {143-165},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524000903},
author = {Sergio Salvatore and Arianna Palmieri and Raffaele {De Luca Picione} and Vincenzo Bochicchio and Matteo Reho and Maria Rita Serio and Giampaolo Salvatore},
keywords = {Affective Pertinentization model, Affective Landscape, Phase Space of Meaning, Meaning dimensionality},
abstract = {The paper presents the Affective Pertinentization model (APER), a theory of the affect and its role it plays in meaning-making. APER views the affect as the basic form of making sense of reality. It consists of a global, bipolar pattern of neurophysiological activity through which the organism maps the instant-by-instant variation of its environment. Such a pattern of neuropsychological activity is constituted by a plurality of bipolar affective dimensions, each of which maps a component of the environmental variability. The affect has a pluri-componential structure defining a multidimensional affective landscape that foregrounds (i.e., makes pertinent) a certain pattern of facets of the environment (e.g., its pleasantness/unpleasantness) relevant to survival, while backgrounding the others. Doing so, the affect grounds the following cognitive processes. Accordingly, meaning-making can be modeled as a function of the dimensionality of the affective landscape. The greater the dimensionality of the affective landscape, the more differentiated the system of meaning is. Following a brief review of current theories pertaining to the affect, the paper proceeds discussing the APER's core tenets – the multidimensional view of the affect, its semiotic function, and the concepts of Affective Landscape and Phase Space of Meaning. The paper then proceeds deepening the relationship between the APER model and other theories, highlighting how the APER succeeds in framing original conceptualizations of several challenging issues – the intertwinement between affect and sensory modalities, the manner in which the mind constitutes the content of the experience, the determinants of psychopathology, the intertwinement of mind and culture, and the spreading of affective forms of thinking and behaving in society. Finally, the unsolved issues and future developments of the model are briefly envisaged.}
}
@article{LITTLE20031285,
title = {The computational science major at SUNY Brockport},
journal = {Future Generation Computer Systems},
volume = {19},
number = {8},
pages = {1285-1292},
year = {2003},
note = {Selected papers from the Workshop on Education in Computational Sciences held at the International Conference on Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(03)00086-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03000864},
author = {Leigh J. Little},
keywords = {Computational science, Education, Undergraduate, Graduate},
abstract = {The field of computational science is a recent addition to academic study. While the content of such an education is generally agreed upon, effective methods for imparting this knowledge are still being investigated. This paper describes the current state of the computational science degree programs at SUNY Brockport and the successes that have been obtained. Issues relating to the implementation of such programs in the context of a small, liberal arts college are also discussed.}
}
@article{OLTETEANU201615,
title = {Object replacement and object composition in a creative cognitive system. Towards a computational solver of the Alternative Uses Test},
journal = {Cognitive Systems Research},
volume = {39},
pages = {15-32},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2015.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716000073},
author = {Ana-Maria Olteţeanu and Zoe Falomir},
keywords = {Cognitive systems, Computational creativity, Creative object replacement, Creative object composition, Alternative uses test},
abstract = {In creative problem solving, humans perform object replacement and object composition to improvise tools in order to carry out tasks in everyday situations. In this paper, an approach to perform Object Replacement and Object Composition (OROC) inside a Creative Cognitive framework (CreaCogs) is proposed. Multi-feature correspondence is used to define similarity between objects in an everyday object domain. This enables the cognitive system OROC to perform creative replacement of objects and creative object composition. The generative properties of OROC are analysed and proof-of-concept experiments with OROC are reported. An evaluation of the results is carried out by human judges and compared to human performance in the Alternative Uses Test.}
}
@article{DEOLIVEIRA2023133,
title = {Transdisciplinary competency-based development in the process engineering subjects: A case study in Brazil},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {133-154},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000246},
author = {Roger Assis {de Oliveira} and Giovanna Milena Borges Hipólito and Ricardo de Freitas Fernandes Pontes and Paulo Henrique Nascimento Ferreira and Ricardo Sanz Moreira and José Plácido and Carlos Alexandre Moreira da Silva and Laura Plazas Tovar},
keywords = {Chemical engineering education, Competency, Learning outcome, Lifelong learning, Process systems engineering, Sustainability},
abstract = {Recently, the Brazilian Ministry of Education issued New Curriculum Guidelines for engineering programs. This paper encompasses a pedagogical intervention reflecting our efforts to incorporate these new guidelines into our engineering program. Specifically, this work has led to the competency-based rework of the following subjects offered in the Chemical Engineering Undergraduate Program at the Federal University of São Paulo (Unifesp): I) Modeling and Systems Analysis; II) Synthesis and Optimization of Chemical Processes; III) Chemical Process Simulation; IV) Process Analysis and Control; V) Chemical Process Design; and VI) Chemical Installations Design. Thirteen transdisciplinary competencies are integrated throughout the six subjects. Students highlighted design thinking, lifelong knowledge/learning, openness to act autonomously, teamwork, communication, and cooperation as essential qualities. Moreover, the greater focus on the process systems engineering approach involving the analysis, synthesis, design, and control of sustainable processes helps chemical engineers to face new challenges using renewable resources.}
}
@article{JAGER2014117,
title = {Thinking outside the channel: Timing pulse flows to benefit salmon via indirect pathways},
journal = {Ecological Modelling},
volume = {273},
pages = {117-127},
year = {2014},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2013.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380013005437},
author = {Henriette I. Jager},
keywords = {Reservoir releases, Environmental flows, Natural flow paradigm, Optimization, Quantile model, Pulse flows},
abstract = {Using models to represent relationships between flow and fishes has important practical applications for managing reservoir releases. Attempts to model such relationships often neglect indirect mechanisms by which flow influences fish. For example, growth of salmon juveniles is measurably faster when flows inundate floodplain and promote higher production of invertebrate prey, but out-of-channel flows have not yet been incorporated into models. The QUANTUS model developed here represents indirect linkages between flow and freshwater survival, mediated by temperature and prey availability, for fall Chinook salmon (Oncorhynchus tshawytscha). Quantiles of spawning time and place were used to define cohorts of salmon in a regulated Central Valley, California river. Survival of these quantile-cohorts was simulated through incubation, juvenile growth, and eventual downstream migration. A genetic algorithm was used to optimize the seasonal timing of pulse flows. Simulated survival was highest for flow regimes that provided a modest, temperature-moderating pulse flow in early summer and, for wetter years, a second, larger pulse of over-bank flow in late winter. For many rivers of the Pacific coast that support fall Chinook salmon, the thermal window of opportunity for spawning and rearing is narrow. Optimized flows made the most of this window by providing access to accelerated juvenile growth and early survival in floodplain habitat, a result that should be verified with field experiments. Timing of optimized pulse flows differed in some respects from the region's natural hydrograph, dominated by spring runoff. This suggests that understanding the mechanisms by which flow influences fishes can be important when shaping flows in the changed context of a regulated river.}
}
@incollection{VASSILOPOULOS2020349,
title = {10 - Computational intelligence methods for the fatigue life modeling of composite materials},
editor = {Anastasios P. Vassilopoulos},
booktitle = {Fatigue Life Prediction of Composites and Composite Structures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {349-383},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102575-8},
doi = {https://doi.org/10.1016/B978-0-08-102575-8.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081025758000103},
author = {Anastasios P. Vassilopoulos and Efstratios F. Georgopoulos},
keywords = {Fatigue, Composites, Artificial neural network, Genetic programming, ANFIS, S-N curves},
abstract = {Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.}
}
@article{DELORME2019133,
title = {When the meditating mind wanders},
journal = {Current Opinion in Psychology},
volume = {28},
pages = {133-137},
year = {2019},
note = {Mindfulness},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X1830157X},
author = {Arnaud Delorme and Tracy Brandmeyer},
abstract = {The capacity for thought and the ability to assemble and manipulate concepts are cognitive features unique to humans. Spontaneous thoughts often occur when we are engaged in attention-demanding tasks, with an increased frequency predicting negative affect. Meditation does not require thinking; however, thinking occurs naturally during meditation. We develop the hypothesis that chronic thinking associated with strong emotional arousal during meditation practice might be detrimental to meditation practice and well-being. One goal of meditation is to identify the arousal of emotions and thoughts, and remain equanimous with them. Over time, meditation may help dampen the attention-grabbing power of these thoughts both during practice and in daily life, which may consequently help deepen meditation practice. However, when meditators fail to remain equanimous, the effects of these thoughts may be deleterious. We discuss how this hypothesis may help guide future research on meditation.}
}
@article{BRIMKOV2005233,
title = {Exact Image Reconstruction from a Single Projection through Real Computation},
journal = {Electronic Notes in Discrete Mathematics},
volume = {20},
pages = {233-246},
year = {2005},
note = {Proceedings of the Workshop on Discrete Tomography and its Applications},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2005.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S1571065305050705},
author = {Valentin E. Brimkov and Reneta P. Barneva},
keywords = {Discrete tomography, Computed tomography, Algebraic computation model, Algebraic complexity, Linear Diophantine equation},
abstract = {In Discrete Tomography one aims to reconstruct a function (image) with a known discrete range from its projection along certain directions. By modern electron-microscopy techniques, one can count the number of atoms laying on a line representing, e.g., an X-ray. The so obtained data is used in the integer programming formulation. However, in real applications the size of the problem, that is well-known to be NP-hard, is so large that no method seems to be applicable to it. Other natural restrictions can make the problem even harder. In an attempt to avoid such kind of difficulties, we present an alternative approach to the problem. With this, we also aim to shed more light on the theoretical limitations for efficient computation in Discrete Tomography. Our approach is based on image reconstruction from a single projection, under the hypothesis that all computations take place in an algebraic computation model. In terms of computational efficiency, the proposed algorithm is significantly superior to the known algorithms for the problem. We also discuss on the possibilities for practical implementation of our method.}
}
@article{GOLDBERG2011171,
title = {Computational physiology of the neural networks of the primate globus pallidus: function and dysfunction},
journal = {Neuroscience},
volume = {198},
pages = {171-192},
year = {2011},
note = {Function and Dysfunction of the Basal Ganglia},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2011.08.068},
url = {https://www.sciencedirect.com/science/article/pii/S0306452211010268},
author = {J.A. Goldberg and H. Bergman},
keywords = {basal ganglia, primate, neurons, correlations, oscillations, Parkinson's disease},
abstract = {The dorsal pallidal complex is made up of the external and internal segments of the globus pallidus (GPe and GPi respectively). It is part of the main axis of the basal ganglia (BG) that connects the thalamo-cortical networks to the BG input stages (striatum and subthalamic nucleus) and continues directly, and indirectly through the GPe, to the BG output stages (GPi and substantia nigra reticulata). Here we review the unique anatomical and physiological features of the pallidal complex and argue that they support the main computational goal of the BG main axis (actor); namely, a behavioral policy that maximizes future cumulative gains and minimizes costs. The three mono-layer competitive networks of the BG main axis flexibly extract relevant features from the current state of the thalamo-cortical activity to control current (ongoing) and future actions. We hypothesize that the striatal and the subthalamic projections neurons act as mono-stable integrators (class I excitability) and the in-vivo pallidal neurons act as bi-stable resonators (class II excitability). GPe neurons exhibit pausing behavior because their membrane potential lingers in the vicinity of an unstable equilibrium point and bi-stability, and these pauses enable a less-greedy exploratory behavioral policy. Finally, degeneration of midbrain dopaminergic neurons and striatal dopamine depletion (as in Parkinson's disease) lead to augmentation of striatal excitability and competitive dynamics. As a consequence the pallidal network, whose elements tend to synchronize as a result of their bi-stable resonance behavior, shifts from a Poissonian-like non-correlated to synchronous oscillatory discharge mode. This article is part of a Special Issue entitled: Function and Dysfunction of the Basal Ganglia.}
}
@article{EKINS201165,
title = {Computational databases, pathway and cheminformatics tools for tuberculosis drug discovery},
journal = {Trends in Microbiology},
volume = {19},
number = {2},
pages = {65-74},
year = {2011},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2010.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X10001939},
author = {Sean Ekins and Joel S. Freundlich and Inhee Choi and Malabika Sarker and Carolyn Talcott},
abstract = {We are witnessing the growing menace of both increasing cases of drug-sensitive and drug-resistant Mycobacterium tuberculosis strains and the challenge to produce the first new tuberculosis (TB) drug in well over 40 years. The TB community, having invested in extensive high-throughput screening efforts, is faced with the question of how to optimally leverage these data to move from a hit to a lead to a clinical candidate and potentially, a new drug. Complementing this approach, yet conducted on a much smaller scale, cheminformatic techniques have been leveraged and are examined in this review. We suggest that these computational approaches should be optimally integrated within a workflow with experimental approaches to accelerate TB drug discovery.}
}
@article{HAMED2018112,
title = {Quantitative modeling of gene networks of biological systems using fuzzy Petri nets and fuzzy sets},
journal = {Journal of King Saud University - Science},
volume = {30},
number = {1},
pages = {112-119},
year = {2018},
issn = {1018-3647},
doi = {https://doi.org/10.1016/j.jksus.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1018364716307819},
author = {Raed I. Hamed},
keywords = {FPNs, Fuzzy sets, Uncertain data, GRNs, Quantitative modeling},
abstract = {Quantitative demonstrating of organic frameworks has turned into an essential computational methodology in the configuration of novel and investigation of existing natural frameworks. Be that as it may, active information that portrays the framework's elements should be known keeping in mind the end goal to get pertinent results with the routine displaying strategies. This information is frequently robust or even difficult to get. Here, we exhibit a model of quantitative fuzzy rational demonstrating approach that can adapt to obscure motor information and hence deliver applicable results despite the fact that dynamic information is fragmented or just dubiously characterized. Besides, the methodology can be utilized as a part of the blend with the current cutting edge quantitative demonstrating strategies just in specific parts of the framework, i.e., where the data are absent. The contextual analysis of the methodology suggested in this paper is performed on the model of nine-quality genes. We propose a kind of FPN model in light of fuzzy sets to manage the quantitative modeling of biological systems. The tests of our model appear that the model is practical and entirely powerful for information impersonation and thinking of fuzzy expert frameworks.}
}
@article{BLOOM2001453,
title = {Novel thinking},
journal = {Trends in Cognitive Sciences},
volume = {5},
number = {10},
pages = {453-454},
year = {2001},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(00)01758-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661300017587},
author = {Paul Bloom}
}
@incollection{ZOHURI202225,
title = {Chapter 2 - A general approach to business resilience system (BRS)},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {25-57},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000039},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Artificial intelligence, Data analysis and information, Market and market share, Predictive analytics, Super artificial intelligence},
abstract = {The business resilience system (BRS) with its risk atom and processing data point is based on fuzzy logic and cloud computation in real time. Its purpose and objectives define a clear set of expectations for organizations and enterprises, so their network system and supply chain are totally resilient and protected against cyberattacks, man-made threats, and natural disasters. These enterprises include financial, organizational, homeland security, and supply chain operations with multipoint manufacturing across the world. Market share and marketing advantages are expected to result from the implementation of the system. The collected information and defined objectives provide the basis to monitor and analyze the data through cloud computation and will guarantee the success of their survivability against any unexpected threats. Putting this kind of operation in place allows the executive and stakeholders within those organizations and enterprises to make the right decision when encountering threats that interrupt their normal day-to-day operations, as well as, in cases such as defense and homeland security, to predict the next move of an adversary. Given the fact that the BRS, as part of its functionality, processes the incoming data and information if not real time, then near real time with the help of superartificial intelligence in place, this gives the stakeholder an edge against and threats as well as predicting issues with operational intelligence. Artificial intelligence (AI) is one of those technologies that seem to be expanding in every direction. This technology will take center stage at Think 2018. Resilience thinking is inevitably systems thinking, at least as much as sustainable development is. In fact, “when considering systems of humans and nature (social-ecological systems), it is important to consider the system as a whole.” The term “resilience” originated in the 1970s in the field of ecology from the research of C.S. Holling, who defined resilience as “a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables.” In short, resilience is best defined as “the ability of a system to absorb disturbances and still retain its basic function and structure.” In this chapter, we explain the BRS and how it works. Please note that the with minor editing and manipulation, the materials presented in this chapter have been borrowed from the book published from Zohuri and Moghaddam10 with permission from both authors and publisher as well.}
}
@article{IWENDI20225016,
title = {Combined power generation and electricity storage device using deep learning and internet of things technologies},
journal = {Energy Reports},
volume = {8},
pages = {5016-5025},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.304},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722005510},
author = {Celestine Iwendi and Gai-Ge Wang},
keywords = {Energy storage, Machine learning, Internet of things, Fuzzy logic, Electricity storage device, Power generation},
abstract = {In microgrids, residential customers play a significant part in the operation. An alternative to client administration should be to utilize smart houses to deal with demand and implement demand responsiveness measures. A power generation and electricity storage device (PGESD) for next-generation technologies is proposed in this article. The current research provides an intelligent home load control system that promotes reaction to demand thinking about this circumstance. The technology is adapted to scenarios where users can charge fluctuating electric power and transmit microgeneration devices. The suggested system utilizes deep learning technology and a fuzzy logic model for better computation and lesser complexity. The choice process involves monitoring environmental information, power production, and battery storage. This article proposes a next-generation power generation and electricity storage device (PGESD). To create Smart Buildings and Microgrids, the proposed system employs technologies and techniques that have become increasingly important. With a precision and accuracy ratio of 89% and 92%, respectively, the proposed PGESD method yields precise numerical results.}
}
@incollection{HOUSE2018335,
title = {Chapter 14 - Comments on Computational Methods},
editor = {J.E. House},
booktitle = {Fundamentals of Quantum Mechanics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {335-347},
year = {2018},
isbn = {978-0-12-809242-2},
doi = {https://doi.org/10.1016/B978-0-12-809242-2.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092422000140},
author = {J.E. House},
keywords = {Basis set, Slater-type orbitals, Gaussian orbitals, Extended Hückel, Wolfsberg-Helmoltz approximation, Ballhausen-Gray approximation, Cusachs' approximation, Self-consistent field, Density functional theory},
abstract = {There are numerous types of molecular orbital calculations that are routinely performed. One of the early versions is the extended Hückel method that begins with the approach used in the Hückel method, but with the overlap and exchange integrals (approximated by the Wolfsberg-Helmholtz, Ballhausen-Gray, or Cusachs' method) included. A more robust type of calculation is that in which an electron is presumed to move in a field generated by the nucleus and other electrons. A trial wave function with some adjustable parameter(s) is taken, and the energy calculated. The wave function improves and the calculations continue until there is no additional improvement (i.e., a “self-consistent field” has been obtained). There are numerous variations of this approach that differ in the trial wave function chosen, extent of electron-electron interaction included, etc. Density functional theory is a newer approach that uses less computational capacity. These types of molecular orbital calculations are surveyed briefly in this chapter.}
}
@article{ZHANG2022116187,
title = {Tri-level attribute reduction in rough set theory},
journal = {Expert Systems with Applications},
volume = {190},
pages = {116187},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116187},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015050},
author = {Xianyong Zhang and Yiyu Yao},
keywords = {Attribute reduction, Three-way decision, Tri-level analysis, Object-specific attribute reducts, Tri-level attribute reducts, Granular computing},
abstract = {Attribute reduction serves as a pivotal topic of rough set theory for data analysis. The ideas of tri-level thinking from three-way decision can shed new light on three-level attribute reduction. Existing classification-specific and class-specific attribute reducts consider only macro-top and meso-middle levels. This paper introduces a micro-bottom level of object-specific reducts. The existing two types of reducts apply to the global classification with all objects and a local class with partial objects, respectively. The new type applies to an individual object. These three types of reducts constitute tri-level attribute reducts. Their development and hierarchy are worthy of systematical explorations. Firstly, object-specific reducts are defined by object consistency from dependency, and they improve both classification-specific and class-specific reducts. Secondly, tri-level reducts are unified by tri-level consistency. Hierarchical relationships between object-specific reducts and class-specific, classification-specific reducts are analyzed, and relevant connections of three-way classifications of attributes are given. Finally, tri-level reducts are systematically analyzed, and two approaches, i.e., the direct calculation and hierarchical transition, are suggested for constructing a specific reduct. We build a framework of tri-level thinking and analysis of attribute reduction to enrich three-way granular computing. Tri-level reducts lead to the sequential development and hierarchical deepening of attribute reduction, and their results profit intelligence processing and system reasoning.}
}
@article{BELLEMAREPEPIN2022105103,
title = {Processing visual ambiguity in fractal patterns: Pareidolia as a sign of creativity},
journal = {iScience},
volume = {25},
number = {10},
pages = {105103},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105103},
url = {https://www.sciencedirect.com/science/article/pii/S258900422201375X},
author = {Antoine Bellemare-Pepin and Yann Harel and Jordan O’Byrne and Geneviève Mageau and Arne Dietrich and Karim Jerbi},
keywords = {Cognitive neuroscience, Social sciences, Psychology},
abstract = {Summary
Creativity is a highly valued and beneficial skill that empirical research typically probes using “divergent thinking” (DT) tasks such as problem solving and novel idea generation. Here, in contrast, we examine the perceptual aspect of creativity by asking whether creative individuals are more likely to perceive recognizable forms in ambiguous stimuli –a phenomenon known as pareidolia. To this end, we designed a visual task in which participants were asked to identify as many recognizable forms as possible in cloud-like fractal images. We found that pareidolic perceptions arise more often and more rapidly in highly creative individuals. Furthermore, high-creatives report pareidolia across a broader range of image contrasts and fractal dimensions than do low creatives. These results extend the established body of work on DT by introducing divergent perception as a complementary manifestation of the creative mind, thus clarifying the perception-creation link while opening new paths for studying creative behavior in humans.}
}
@article{BROCAS2021105366,
title = {Value computation and modulation: A neuroeconomic theory of self-control as constrained optimization},
journal = {Journal of Economic Theory},
volume = {198},
pages = {105366},
year = {2021},
issn = {0022-0531},
doi = {https://doi.org/10.1016/j.jet.2021.105366},
url = {https://www.sciencedirect.com/science/article/pii/S0022053121001836},
author = {Isabelle Brocas and Juan D. Carrillo},
keywords = {Neuroeconomic theory, Multiple brain systems, Self-control, Cue-triggered behavior, Self-regulation},
abstract = {We develop a theory based on the evidence reported in Hare et al. (2009) to explain consumption of goods that feature a low-order attribute (e.g., taste) and a high-order attribute (e.g., health). One brain system with access to the low-order attribute computes the goal value of consumption while another brain system can modulate this value, at a cost, by transmitting information regarding the high-order attribute. We determine the optimal modulation and consumption strategy as a function of the cost of information transmission and the environment. We show that in healthy environments, modulation is used to signal surprisingly unhealthy goods so as to trigger abstinence when consumption would ordinarily occur. Conversely, in unhealthy environments, modulation is used to signal surprisingly healthy choices so as to trigger consumption when abstinence would ordinarily occur. From an outside perspective, individuals may appear to under-regulate their choices (self-indulgence) but also to over-regulate them (self-restraint). Both modulation and decisions are affected by factors orthogonal to the decision problem. In particular, taxing executive functions results in less modulation and more inefficient behavior. Finally, the model can shed light on issues related to eating disorders, present-biased preferences, habit formation and compulsive behavior.}
}
@article{PEZZANO2024100078,
title = {Are we done with (Wordy) manifestos? Towards an introverted digital humanism},
journal = {Journal of Responsible Technology},
volume = {17},
pages = {100078},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000040},
author = {Giacomo Pezzano},
keywords = {Mediatic turn, Philosophy of technology, Learning, Book, Video game},
abstract = {Beginning with a reconstruction of the anthropological paradigms underlying The Vienna Manifesto and The Onlife Manifesto (§ 1.1), this paper distinguishes between two possible approaches to digital humanism: an extroverted one, principally engaged in finding a way to humanize digital technologies, and an introverted one, pointing instead attention to how digital technologies can re-humanize us, particularly our “mindframe” (§ 1.2). On this basis, I stress that if we take seriously the consequences of the “mediatic turn”, according to which human reason is finally recognized as mediatically contingent (§ 2.1), then we should accept that just as the book created the poietic context for the development of traditional humanism and its “bookish” idea of private and public reason, so too digital psycho-technologies today provide the conditions for the rise of a new humanism (§ 2.2). I then discuss the possible humanizing potential of digital simulated worlds: I compare the symbolic-reconstructive mindset to the sensorimotor mindset (§ 3.1), and I highlight their respective mediological association with the book and the video game, advocating for the peculiar thinking and reasoning affordances now offered by the new digital psycho-technologies (§ 3.2).}
}
@article{CHIN20201054,
title = {Rethinking Cancer Immunotherapy by Embracing and Engineering Complexity},
journal = {Trends in Biotechnology},
volume = {38},
number = {10},
pages = {1054-1065},
year = {2020},
note = {Special Issue: Therapeutic Biomanufacturing},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2020.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167779920301244},
author = {Matthew H.W. Chin and Eileen Gentleman and Marc-Olivier Coppens and Richard M. Day},
keywords = {bioengineering, complex systems, holism, immunotherapy, process intensification},
abstract = {The meteoric rise of cancer immunotherapy in the past decade has led to promising treatments for a number of hard-to-treat malignancies. In particular, adoptive T cell therapy has recently reached a major milestone with two products approved by the US FDA. However, the inherent complexity of cell-based immunotherapies means that their manufacturing time, cost, and controllability limit their effectiveness and geographic reach. One way to address these issues may lie in complementing the dominant, reductionistic mentality in modern medicine with complex systems thinking. In this opinion article, we identify key concepts from complexity theory to address manufacturing challenges in cell-based immunotherapies and raise the possibility of a unifying framework upon which future bioprocessing strategies may be designed.}
}
@article{JANES200673,
title = {A biological approach to computational models of proteomic networks},
journal = {Current Opinion in Chemical Biology},
volume = {10},
number = {1},
pages = {73-80},
year = {2006},
note = {Proteomics and genomics},
issn = {1367-5931},
doi = {https://doi.org/10.1016/j.cbpa.2005.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S1367593105001687},
author = {Kevin A Janes and Douglas A Lauffenburger},
abstract = {Computational modeling is useful as a means to assemble and test what we know about proteins and networks. Models can help address key questions about the measurement, definition and function of proteomic networks. Here, we place these biological questions at the forefront in reviewing the computational strategies that are available to analyze proteomic networks. Recent examples illustrate how models can extract more information from proteomic data, test possible interactions between network proteins and link networks to cellular behavior. No single model can achieve all these goals, however, which is why it is critical to prioritize biological questions before specifying a particular modeling approach.}
}
@article{YANG2023106838,
title = {Neuromorphic electronics for robotic perception, navigation and control: A survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106838},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106838},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010229},
author = {Yi Yang and Chiara Bartolozzi and Haiyan H. Zhang and Robert A. Nawrocki},
keywords = {Neuromorphic electronics, Organic and flexible electronic materials, Neuromorphic robot, Perception, Navigation, Control, SLAM, Path planning},
abstract = {Neuromorphic electronics have great potential in the emulation of the sensory, cognitive, self-learning, and actuating functions of robots. While typically implemented in rigid silicon, emerging technologies in organic and flexible electronic materials have also led to tremendous advances in the development of neuromorphic perception systems. However, a comprehensive review of the contribution/role of organic neuromorphic electronics for robotic applications is still missing. This review presents advancements in silicon-based and organic neuromorphic electronics for intelligent robot development, focusing on perception, navigation, and learning-based control. Organic synaptic devices, along with dynamic vision sensors, enable diverse forms of sensory-enabled computational perception, offering tunability, stability, low power consumption, and conformal substrates. Integration of simultaneous localization and mapping techniques and path planning algorithms empowers robots to efficiently navigate, build accurate maps, and make informed decisions. Different learning algorithms and their hardware implementations in neuromorphic robotic control are explored, enabling robots to learn and adapt to dynamic environments. The review highlights the potential of neuromorphic electronics for sensing, thinking, and acting in advanced robotic systems. Organic, inorganic, and hybrid materials are discussed for implementing perception, navigation, and control in robots. Future research directions in the field are outlined. Leveraging various neuromorphic electronics unlocks the full potential of intelligent robotic systems for diverse applications.}
}
@article{CHEN2005121,
title = {Computational intelligence in economics and finance: Carrying on the legacy of Herbert Simon},
journal = {Information Sciences},
volume = {170},
number = {1},
pages = {121-131},
year = {2005},
note = {Computational Intelligence in Economics and Finance},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2003.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0020025503004444},
author = {Shu-Heng Chen},
keywords = {Computational intelligence, Artificial intelligence, Agent-based computational economics, Autonomous agents, Stock price-volume relation, Micro-macro relation},
abstract = {This is an editorial guide for the special issue on computational intelligence (CI) in economics and finance. A historical introduction to the background is given. This research paradigm is traced back to Herbert Simon, who, as a founder of artificial intelligence, pioneered the applications of AI to economics. The move from the classical AI to CI indicates a continuation of the legacy of Herbert Simon. Computational intelligence has proved to be a constructive foundation for economics. In responding to what Herbert Simon referred as procedural rationality, our study of bounded rationality has been enriched by bringing autonomous agents into the economic analysis.}
}
@article{GOODSELL2020472,
title = {Art and Science of the Cellular Mesoscale},
journal = {Trends in Biochemical Sciences},
volume = {45},
number = {6},
pages = {472-483},
year = {2020},
issn = {0968-0004},
doi = {https://doi.org/10.1016/j.tibs.2020.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968000420300566},
author = {David S. Goodsell and Arthur J. Olson and Stefano Forli},
keywords = {mesoscale modeling, integrative structural biology, drug discovery, drug design, cell structure, cell function, molecular structure, molecular function},
abstract = {Experimental information from microscopy, structural biology, and bioinformatics may be integrated to build structural models of entire cells with molecular detail. This integrative modeling is challenging in several ways: the intrinsic complexity of biology results in models with many closely packed and heterogeneous components; the wealth of available experimental data is scattered among multiple resources and must be gathered, reconciled, and curated; and computational infrastructure is only now gaining the capability of modeling and visualizing systems of this complexity. We present recent efforts to address these challenges, both with artistic approaches to depicting the cellular mesoscale, and development and application of methods to build quantitative models.}
}
@article{AMADORHIDALGO2021103694,
title = {Cognitive abilities and risk-taking: Errors, not preferences},
journal = {European Economic Review},
volume = {134},
pages = {103694},
year = {2021},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2021.103694},
url = {https://www.sciencedirect.com/science/article/pii/S0014292121000477},
author = {Luis Amador-Hidalgo and Pablo Brañas-Garza and Antonio M. Espín and Teresa García-Muñoz and Ana Hernández-Román},
keywords = {Decision making under uncertainty, Cognitive abilities, Online experiment, Risk and loss aversion, Factor analysis},
abstract = {There is an intense debate whether risk-taking behavior is partially driven by cognitive abilities. The critical issue is whether choices arising from subjects with lower cognitive abilities are more likely driven by errors or lack of understanding than pure preferences for risk. The latter implies that the often-argued link between risk preferences and cognitive abilities (a common finding is that abilities relate negatively to risk aversion and positively to loss aversion) might be a spurious correlation. This experiment reports evidence from a sample of 556 participants who made choices in two risk-related tasks and completed three cognitive tasks, all with real monetary incentives: number-additions (including incentive-compatible expected number of correct additions), the Cognitive Reflection Test (to measure analytical/reflective thinking) and the Remote Associates Test (for convergent thinking). Results are unambiguous: none of our cognition measures plays any systematic role on risky decision making. Using structural equation modeling and factor analysis, we show that cognitive abilities are negatively associated with noisy, inconsistent choices and this effect may make higher ability individuals appear to be less risk averse and more loss averse. Yet we show that errors are more likely to appear when the two payoffs in a given decision exhibit similar probability. Therefore, our results suggest that failing to account for noisy decision making might have led to erroneously inferring a correlation between cognitive abilities and risk preferences in previous studies.}
}