volume = {11},
number = {2},
pages = {e41099},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41099},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024171307},
author = {Yolanda Guerra-Macías and Sergio Tobón},
keywords = {Generic competencies, 21st-century skills, Virtual education, Higher education, Socioformation, Generic skills, Socioformative rubrics},
abstract = {This study investigates the development of transversal skills and their association with academic performance in university students enrolled in on-campus programs with online activities. A cross-sectional, descriptive, and quantitative research was conducted with 252 students from a public university in Mexico. Transversal skills, socioformative project-based practices, learning strategies, and the relevance of online activities were assessed using validated rubrics. The results indicated a low level of development in three transversal skills: research, entrepreneurship, and English, with the latter being the poorest rated. Critical and creative thinking exhibited the highest level of development. In the didactic component, socioformative project-based pedagogical practices and learning strategies showed acceptable levels. Students expressed satisfaction with complementary online activities, showing a preference for interactive videos and short videos under 4 min. Regression analysis and structural equations were used to examine the relationships between various factors. Results demonstrated that socioformative project-based pedagogical practices, learning strategies, and online education positively correlated with the development of transversal skills. Furthermore, a higher level of transversal skills was associated with better academic averages among students. Socioformative project-based pedagogical practices also correlated with academic performance through transversal skills. The study concludes that integrating online activities into on-campus programs, based on the socioformative pedagogical model, can enhance the development of transversal skills and improve academic performance. Further research into the implementation of this educational model and its long-term impact on university education and professional success is recommended.}
}
@article{YOUNG201719,
title = {Technology-enhanced mathematics instruction: A second-order meta-analysis of 30 years of research},
journal = {Educational Research Review},
volume = {22},
pages = {19-33},
year = {2017},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X1730026X},
author = {Jamaal Young},
keywords = {Meta-analysis, Mathematics achievement, Technology, Calculators, Computer-assisted instruction},
abstract = {It is important to assess the cumulative effects of technology on student achievement captured in the last 30 years of technologyenhanced mathematics instruction. Synthesizing the thousands of articles and gray literature on this subject is necessary but would require a considerable commitment of academic resources. A second-order metaanalysis or meta-analysis of meta-analyses is an alternative that is reasonable and effective. Thus, a second-order meta-analysis of 19 prior meta-analyses with minimum overlap between primary studies was conducted. The results represent 663 primary studies (approximately 141,733 participants) and 1,263 effect sizes. The random effects' mean effect size of .38 was statistically significantly different from zero. The results provide a historical and contextualized summary of 30 years of meta-analytic research, which supports meta-analytic thinking and better interpretation of future effect sizes. Results indicate that technology function and study quality are major contributors to effect size variation. Specifically, computation enhancement technologies were most effective, while studies that examine combinations of enhancements were least effective. Implications for technology-enhanced mathematics instruction and meta-analytic research are provided.}
}
@article{ZHOU2025103462,
title = {Co-design of analogical and embodied representations with children for child-centered AI learning experiences},
journal = {International Journal of Human-Computer Studies},
pages = {103462},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103462},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925000199},
author = {Xiaofei Zhou and Yunfan Gong and Yushan Zhou and Yufei Jiang and Zhen Bai},
keywords = {Embodied learning, Analogical learning, AI recommendation systems, The filter bubble, Augmented reality, Children},
abstract = {AI recommendations shape our daily decisions and our young generation is no exception. The convenience of navigating personalized content comes with the notorious “filter bubble” effect, which can reduce exposure to diverse options and opinions. Children are particularly vulnerable to this due to their limited AI literacy and critical thinking skills. In this study, we explore how to engage children as co-designers to create child-centered experiences for learning AI concepts related to the filter bubble. Leveraging embodied and analogical learning theories, we co-designed an Augmented Reality (AR) application, BeeTrap, with children from underrepresented backgrounds in STEM. BeeTrap not only raises awareness of filter bubbles but also empowers children to understand recommendation system mechanisms. Our contributions include (1) insights into child-centered AI learning using embodied metaphors and analogies as educational representations of AI concepts; and (2) implications for enhancing children’s understanding of AI concepts through co-design processes.}
}
@article{CUMMINGS2003369,
title = {Computational challenges in high angle of attack flow prediction},
journal = {Progress in Aerospace Sciences},
volume = {39},
number = {5},
pages = {369-384},
year = {2003},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(03)00041-1},
url = {https://www.sciencedirect.com/science/article/pii/S0376042103000411},
author = {Russell M. Cummings and James R. Forsythe and Scott A. Morton and Kyle D. Squires},
abstract = {Aircraft aerodynamics have been predicted using computational fluid dynamics for a number of years. While viscous flow computations for cruise conditions have become commonplace, the non-linear effects that take place at high angles of attack are much more difficult to predict. A variety of difficulties arise when performing these computations, including challenges in properly modeling turbulence and transition for vortical and massively separated flows, the need to use appropriate numerical algorithms if flow asymmetry is possible, and the difficulties in creating grids that allow for accurate simulation of the flowfield. These issues are addressed and recommendations are made for further improvements in high angle of attack flow prediction. Current predictive capabilities for high angle of attack flows are reviewed, and solutions based on hybrid turbulence models are presented.}
}
@article{SPEISER2011271,
title = {Models for products},
journal = {The Journal of Mathematical Behavior},
volume = {30},
number = {4},
pages = {271-290},
year = {2011},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000307},
author = {Bob Speiser and Chuck Walter},
keywords = {Model, Representation, Presentation, Operator product, Controlled variable, Frame, Core knowledge, Analog magnitude, Parallel individuation, Shared intentionality},
abstract = {This paper explores how models can support productive thinking. For us a model is a thing, a tool to help make sense of something. We restrict attention to specific models for whole-number multiplication, hence the wording of the title. They support evolving thinking in large measure through the ways their users redesign them. They assume new forms, come to be seen and understood in different ways. We show how work that learners do with models can help them to transform, not simply their understanding of key concepts, but also how they come to view themselves as thinkers and learners, as collaborators in a social process that their work and thinking help to constitute. We draw on recent research on core knowledge, especially by Carey, Spelke, and Tomasello, to clarify how models, as we view them here, can underpin specific actions that support emerging understanding.}
}
@article{MARINIER200948,
title = {A computational unification of cognitive behavior and emotion},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {48-69},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000302},
author = {Robert P. Marinier and John E. Laird and Richard L. Lewis},
abstract = {Existing models that integrate emotion and cognition generally do not fully specify why cognition needs emotion and conversely why emotion needs cognition. In this paper, we present a unified computational model that combines an abstract cognitive theory of behavior control (PEACTIDM) and a detailed theory of emotion (based on an appraisal theory), integrated in a theory of cognitive architecture (Soar). The theory of cognitive control specifies a set of required computational functions and their abstract inputs and outputs, while the appraisal theory specifies in more detail the nature of these inputs and outputs and an ontology for their representation. We argue that there is a surprising functional symbiosis between these two independently motivated theories that leads to a deeper theoretical integration than has been previously obtained in other computational treatments of cognition and emotion. We use an implemented model in Soar to test the feasibility of the resulting integrated theory, and explore its implications and predictive power in several task domains.}
}
@article{SCHMALZL20031021,
title = {Using standard image compression algorithms to store data from computational fluid dynamics},
journal = {Computers & Geosciences},
volume = {29},
number = {8},
pages = {1021-1031},
year = {2003},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(03)00098-0},
url = {https://www.sciencedirect.com/science/article/pii/S0098300403000980},
author = {Jörg Schmalzl},
keywords = {Computational fluid dynamics, Data compression, Visualization, Post-processing},
abstract = {Three-dimensional numerical modeling of fluid flows is an important research tool to understand many fluid dynamical effects observed in nature. With the strong growth of available computational resources the use of such models has greatly increased over the last years. Because the available mass storage has not increased in the same order as the CPU speed many researchers nowadays face the problem of how to store and transfer the large data sets produced by the model calculations for post-processing. The use of lossy wavelet-based compression techniques on this data has been investigated in several publications. These techniques are often specialized to one problem and are not easy to implement. In the area of digital media, however, advances have been made for still image (JPEG, JPEG-2000) and motion image (MPEG) compression. In this paper we investigate the usefulness of these image compression algorithms for the storage of data from computational fluid dynamics on regular cartesian grids. We analyze both the compression ratios achieved and the error introduced by these lossy compression schemes. We found that, for our purposes, the JPEG compression scheme allows an easy-to-use, portable, robust, and computationally efficient lossy compression. For the easy use of these compression algorithms we present a simple wrapper library.}
}
@article{KARAGIANNIS2022103631,
title = {The OMiLAB Digital Innovation environment: Agile conceptual models to bridge business value with Digital and Physical Twins for Product-Service Systems development},
journal = {Computers in Industry},
volume = {138},
pages = {103631},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000264},
author = {Dimitris Karagiannis and Robert Andrei Buchmann and Wilfrid Utz},
keywords = {Digital twin, Physical twin, Smart Product-service Systems, Agile modeling method engineering, OMiLAB, Domain-specific conceptual modeling},
abstract = {OMiLAB is a community of practice which offers a digital ecosystem bringing together open technologies to investigate and apply conceptual modeling methods for varying purposes and domains. One of the core value propositions is a dedicated Digital Innovation environment comprising several toolkits and workspaces, designed to support Product-Service Systems (PSS) prototyping – a key ingredient for PSS lifecycle management. At the core of this environment is a notion of Agile Digital Twin – a conceptual representation that can be tailored with knowledge engineering means to bridge the semantic and functional gap between a business perspective (focusing on value creation) and an engineering perspective (focusing on cyber-physical proofs-of-concept). To facilitate this bridging, the hereby proposed environment orchestrates, across three abstraction layers, methods such as Design Thinking, Agile Modeling Method Engineering and Model-driven Engineering to turn Ideation into smart Product-Service Systems experiments, in a laboratory setting. The proposed environment was built following Design Science principles. It addresses the problem of historically-disconnected skills required for Digital Innovation projects and it provides a testbed for feasibility experimentation. For design-oriented, artifact building research, a higher Technology Readiness Level can thus be achieved (compared to the level that idea development methods typically attain).}
}
@article{BYTYQIDAMONI2024137516,
title = {Synthesis, characterization, and computational study of novel carvacrol-based 2-aminothiol and sulfonic acid derivatives as metabolic enzyme inhibitors},
journal = {Journal of Molecular Structure},
volume = {1303},
pages = {137516},
year = {2024},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2024.137516},
url = {https://www.sciencedirect.com/science/article/pii/S0022286024000395},
author = {Arlinda Bytyqi-Damoni and Eda Mehtap Uc and Rıfat Emin Bora and Hayriye Genc Bilgicli and Mehmet Abdullah Alagöz and Mustafa Zengin and İlhami Gülçin},
keywords = {Carvacrol, Carbonic anhydrase, α-glucosidase, Acetylcholinesterase, Butyrylcholinesterase},
abstract = {Eight new 2-aminothiols (69–96%) and three new sulfonic acids (51–76%) were synthesized and characterized by NMR and HRMS spectra. This study presents the inhibitory effects of a series of novel carvacrol-based 2-aminothiol and sulfonic acid derivatives (3a-f,4a-c) against human carbonic anhydrase I and II isozymes (hCA I and II) acetylcholinesterase (AChE), butyrylcholinesterase (BChE) and α-glycosidase. Ki values were calculated as 12.52±3.61–335.65±60.56 nM for hCA I, 12.20±3.59–389.69±119.41 nM for hCA II, 1.79±0.56–84.86±23.34 nM for AChE, 6.57±2.54–88.05±21.05 nM for BChE and 14.63±4.76–116.39±33.70 nM α-glucosidase enzymes. Also, the inhibition effects of novel carvacrol-based 2-aminothiol (3a-h) and sulfonic acid derivatives (4a-c) were compared to standard and clinically used inhibitors of acetazolamide, Tacrine and acarbose, respectively. Molecular modeling studies of novel compounds, docking scores, and free binding energies were calculated. The activity results of the compounds were found to be compatible with the docking scores. Molecular dynamics studies were conducted with the best activity against CA I and CA II compounds, 4b (IC50: 4.76 nM) and 4a (IC50: 4.36 nM), respectively. In Dynamic Simulation studies, it was observed that the compounds remained stable at the active sites of the proteins.}
}
@article{STRASS201534,
title = {Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory},
journal = {Artificial Intelligence},
volume = {226},
pages = {34-74},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000776},
author = {Hannes Strass and Johannes Peter Wallner},
keywords = {Abstract dialectical frameworks, Computational complexity, Approximation fixpoint theory},
abstract = {Abstract dialectical frameworks (ADFs) have recently been proposed as a versatile generalization of Dung's abstract argumentation frameworks (AFs). In this paper, we present a comprehensive analysis of the computational complexity of ADFs. Our results show that while ADFs are one level up in the polynomial hierarchy compared to AFs, there is a useful subclass of ADFs which is as complex as AFs while arguably offering more modeling capacities. As a technical vehicle, we employ the approximation fixpoint theory of Denecker, Marek and Truszczyński, thus showing that it is also a useful tool for complexity analysis of operator-based semantics.}
}
@article{PARK20151,
title = {A Neuro-educational Study of the Development of the Creativity-based Teaching Program and its Effect},
journal = {Procedia - Social and Behavioral Sciences},
volume = {186},
pages = {1-8},
year = {2015},
note = {The Proceedings of 5th World Conference on Learning, Teaching and Educational Leadership},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.200},
url = {https://www.sciencedirect.com/science/article/pii/S187704281502460X},
author = {Sun-Hyung Park and Kwang-Ki Kim and Kyung-Hwa Lee},
keywords = {Creativity-based teaching programs, Divergent thinking, The TTCT, FMRI},
abstract = {This study aims at exploring a possibility of developing a creativity-based teaching program needed for enhancing prospective teachers’ creative potentials based on the theories of Sawyer and Renzulli. Neuroimaging tools such as fMRI were used to identify effects of the program on pre-service teachers’ neural activations on divergent thinking measured primarily by the Torrance Tests of Creative Thinking (TTCT). Since the research is still in progress, we present a theoretical model for the teaching program, and preliminary test results of comparing changes of neural recruitments in students’ brain participated in fMRI with the TTCT.}
}
@article{GARCIASANCHO201216,
title = {From the genetic to the computer program: the historicity of ‘data’ and ‘computation’ in the investigations on the nematode worm C. elegans (1963–1998)},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {43},
number = {1},
pages = {16-28},
year = {2012},
note = {Data-Driven Research in the Biological and Biomedical Sciences On Nature and Normativity: Normativity, Teleology, and Mechanism in Biological Explanation},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2011.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1369848611000781},
author = {Miguel García-Sancho},
keywords = {, Genetics, Computer, Program, Software, Data, Genomics, Model organism},
abstract = {This paper argues that the history of the computer, of the practice of computation and of the notions of ‘data’ and ‘programme’ are essential for a critical account of the emergence and implications of data-driven research. In order to show this, I focus on the transition that the investigations on the worm C. elegans experienced in the Laboratory of Molecular Biology of Cambridge (UK). Throughout the 1980s, this research programme evolved from a study of the genetic basis of the worm’s development and behaviour to a DNA mapping and sequencing initiative. By examining the changing computing technologies which were used at the Laboratory, I demonstrate that by the time of this transition researchers shifted from modelling the worm’s genetic programme on a mainframe apparatus to writing minicomputer programs aimed at providing map and sequence data which was then circulated to other groups working on the genetics of C. elegans. The shift in the worm research should thus not be simply explained in the application of computers which transformed the project from hypothesis-driven to a data-intensive endeavour. The key factor was rather a historically specific technology—in-house and easy programmable minicomputers—which redefined the way of achieving the project’s long-standing goal, leading the genetic programme to co-evolve with the practices of data production and distribution.}
}
@article{ACAR2006993,
title = {Endowing cognitive mapping with computational properties for strategic analysis},
journal = {Futures},
volume = {38},
number = {8},
pages = {993-1009},
year = {2006},
note = {Organisational Foresight},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2005.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0016328705002302},
author = {William Acar and Douglas Druckenmiller},
abstract = {A number of cognitive, causal mapping and simulation techniques exist for dealing with the growing importance of environmental uncertainty. After briefly commenting on some of the more salient extant approaches, this paper offers a new one for consideration by the scenario planning community. Comprehensive Situation Mapping (CSM) is a powerful analytical tool combined with a process for framing and debating strategic situations. The CSM approach combines the problem framing features of causal mapping with a dialectical inquiry process patterned after Churchman's. Like the better approaches to planning through cognitive mapping, it facilitates the “backward analysis” of the underlying strategic assumptions. Its novelty is that it also allows the “forward analysis” of a situation by computing the potential change scenarios. Initially developed for manual application, the principles of CSM were originally tested in appropriate case studies. The contribution of the present paper is to present its theory and point out that its future potential is even greater: in concluding we indicate that, by using recent distributed artificial intelligence (DAI) technology, a fully computerized and interactive prototype is now being set up for commercial applications.}
}
@article{EMILYESTHERRANI2025107032,
title = {Alzheimer disease classification using optimal clustering based pre-trained SqueezeNet model},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {107032},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107032},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424010905},
author = {K. {Emily Esther Rani} and S. Baulkani},
keywords = {Alzheimer disease, Optimal center, Clustering, Adaptive reptile search algorithm, Pre-trained squeezenet},
abstract = {Alzheimer’s disease (AD) is an irreversible neurological illness identified by deficits in thinking, behavior, and memory. Early detection and prevention of AD is a crucial and difficult task. DL (Deep Learning) has gained significant attention recently as a potential tool for early AD detection. However, traditional diagnostic methods such as cognitive tests and manual analysis of brain imaging are time consuming and prone to error. Hence, there is a need for an automated model which shows better classification performance. To tackle these issues, this study presents a system to improve AD recognition performance. Initially, the pre-processing and skull stripping processes are performed. Then, for segmenting the grey, whiter matters and Cerebrospinal Fluid (CSF), the optimal clustering process is carried out. Here, the optimal center of clusters is selected by the metaheuristic optimization Adaptive Reptile Search Algorithm-Clustering Approach (ARSA-CA) is utilized. The proposed ARSA is the integration of the optimization RSA and simulated annealing (SA). Finally, for classifying the different classes of AD, the DL approach pre-trained SqueezeNet is utilized. The experimentation is carried out on the ADNI and OASIS datasets and achieved better accuracies of 98.3% and 98.2% respectively. Thus, it is proved that the proposed model is suitable for identifying AD.}
}
@article{MORTON2019,
title = {Computer Programming: Should Medical Students Be Learning It?},
journal = {JMIR Medical Education},
volume = {5},
number = {1},
year = {2019},
issn = {2369-3762},
doi = {https://doi.org/10.2196/11940},
url = {https://www.sciencedirect.com/science/article/pii/S2369376219000011},
author = {Caroline E Morton and Susan F Smith and Tommy Lwin and Michael George and Matt Williams},
keywords = {coding, medical education, undergraduate curriculum},
abstract = {Background
The ability to construct simple computer programs (coding) is being progressively recognized as a life skill. Coding is now being taught to primary-school children worldwide, but current medical students usually lack coding skills, and current measures of computer literacy for medical students focus on the use of software and internet safety. There is a need to train a cohort of doctors who can both practice medicine and engage in the development of useful, innovative technologies to increase efficiency and adapt to the modern medical world.
Objective
The aim of the study was to address the following questions: (1) is it possible to teach undergraduate medical students the basics of computer coding in a 2-day course? (2) how do students perceive the value of learning computer coding at medical school? and (3) do students see computer coding as an important skill for future doctors?
Methods
We developed a short coding course to teach self-selected cohorts of medical students basic coding. The course included a 2-day introduction on writing software, discussion of computational thinking, and how to discuss projects with mainstream computer scientists, and it was followed on by a 3-week period of self-study during which students completed a project. We explored in focus groups (FGs) whether students thought that coding has a place in the undergraduate medical curriculum.
Results
Our results demonstrate that medical students who were complete novices at coding could be taught enough to be able to create simple, usable clinical programs with 2 days of intensive teaching. In addition, 6 major themes emerged from the FGs: (1) making sense of coding, (2) developing the students’ skill set, (3) the value of coding in medicine, research, and business, (4) role of teaching coding in medical schools, (5) the concept of an enjoyable challenge, and (6) comments on the course design.
Conclusions
Medical students can acquire usable coding skills in a weekend course. They valued the teaching and identified that, as well as gaining coding skills, they had acquired an understanding of its potential both for their own projects and in health care delivery and research. They considered that coding skills teaching should be offered as an optional part of the medical curriculum.}
}
@incollection{LANDAUER200243,
title = {On the computational basis of learning and cognition: Arguments from LSA},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {41},
pages = {43-84},
year = {2002},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(02)80004-4},
url = {https://www.sciencedirect.com/science/article/pii/S0079742102800044},
author = {Thomas K Landauer},
abstract = {Publisher Summary
This chapter discusses the computational basis of learning and cognition. To deal with a continuously changing environment, living things have three choices: (1) evolve unvarying processes that usually succeed, (2) evolve genetically fixed effector, perceptual, and computational functions that are contingent on the environment, and (3) learn adaptive functions during their lifetimes. The theme of this chapter is the relation between (2) and (3): the nature of evolutionarily determined computational processes that support learning. The principal goal of this chapter has been to suggest that high-dimensional vector space computations based on empirical associations among very large numbers of components could be a close model of a fundamental computational basis of most learning in both verbal and perceptual domains. More powerful representational effects can be brought about by linear inductive combinations of the elements of very large vocabularies than has often been realized. Success of one such model to demonstrate many natural properties of language commonly assumed to be essentially more complex, nonlinear, and/or unlearned, along with evidence and argument that similar computations may serve similar roles in object recognition, are taken to reaffirm the possibility that a single underlying associational mechanism lies behind many more special and complex appearing cognitive phenomena.}
}
@incollection{DEAN2014157,
title = {Chapter 7 - Decorrelation Learning in the Cerebellum: Computational Analysis and Experimental Questions},
editor = {Narender Ramnani},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {210},
pages = {157-192},
year = {2014},
booktitle = {Cerebellar Learning},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63356-9.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444633569000078},
author = {Paul Dean and John Porrill},
keywords = {Cerebellum, eye blink conditioning, vestibulo-ocular reflex, spike-timing dependent plasticity, avoidance learning, long-term depression, long-term potentiation, supervised learning, reafference, least mean squares},
abstract = {Many cerebellar models use a form of synaptic plasticity that implements decorrelation learning. Parallel fibers carrying signals positively correlated with climbing-fiber input have their synapses weakened (long-term depression), whereas those carrying signals negatively correlated with climbing input have their synapses strengthened (long-term potentiation). Learning therefore ceases when all parallel-fiber signals have been decorrelated from climbing-fiber input. This is a computationally powerful rule for supervised learning and can be cast in a spike-timing dependent plasticity form for comparison with experimental evidence. Decorrelation learning is particularly well suited to sensory prediction, for example, in the reafference problem where external sensory signals are interfered with by reafferent signals from the organism’s own movements, and the required circuit appears similar to the one found to mediate classical eye blink conditioning. However, for certain stimuli, avoidance is a much better option than simple prediction, and decorrelation learning can also be used to acquire appropriate avoidance movements. One example of a stimulus to be avoided is retinal slip that degrades visual processing, and decorrelation learning appears to play a role in the vestibulo-ocular reflex that stabilizes gaze in the face of unpredicted head movements. Decorrelation learning is thus suitable for both sensory prediction and motor control. It may also be well suited for generic spatial and temporal coordination, because of its ability to remove the unwanted side effects of movement. Finally, because it can be used with any kind of time-varying signal, the cerebellum could play a role in cognitive processing.}
}
@article{GOMEZTALAL2024108109,
title = {Understanding the disparities in Mathematics performance: An interpretability-based examination},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108109},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108109},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624002677},
author = {Ismael Gómez-Talal and Luis Bote-Curiel and José Luis Rojo-Álvarez},
keywords = {Programme for International Student Assessment, Interpretable machine learning, Shapley additive explanations, Explainable black-box models},
abstract = {Problem:
Educational disparities in Mathematics performance are a persistent challenge. This study aims to unravel the complex factors contributing to these disparities among students internationally, with a focus on the interpretability of the contributing factors.
Methodology:
Utilizing data from the Programme for International Student Assessment (PISA), we conducted rigorous preprocessing and variable selection to prepare for applying binary classification interpretability models. These models were trained using the Stratified K-Fold technique to ensure balanced representation and assessed using six key metrics.
Solution:
By applying interpretability models such as Shapley Additive Explanations (SHAP) analysis, we identified critical factors impacting student performance, including reading accessibility, critical thinking skills, gender, and geographical location.
Results:
Our findings reveal significant disparities linked to resource availability, with students from lower socioeconomic backgrounds possessing fewer books and demonstrating lower performance in Mathematics. The geographical analysis highlighted regional educational disparities, with certain areas consistently underperforming in PISA assessments. Gender also emerged as a determinant, with females contributing differently to performance levels across the spectrum.
Conclusion:
The study provides insights into the multifaceted determinants of student Mathematics performance and suggests potential avenues for future research to explore global interpretability models and further investigate the socioeconomic, cultural, and educational factors at play.}
}
@article{HEYLIGHEN2014113,
title = {Designing in the absence of sight: Design cognition re-articulated},
journal = {Design Studies},
volume = {35},
number = {2},
pages = {113-132},
year = {2014},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000926},
author = {Ann Heylighen and Greg Nijs},
keywords = {design cognition, design research, epistemology},
abstract = {Starting from the study of an architect who designs in the absence of sight, we question to what extent prevailing notions of design may be complemented with alternative articulations. In doing so, we point to the cognitivist understanding of human cognition underlying design researchers' strong attention to ‘visual thinking’, and contrast this with more situated understandings of human cognition. The ontological and epistemological differences between both raise questions about how design research is produced, and consequently what design can also be. By accounting for how a blind architect re-articulates prevailing notions of design, we invite researchers to keep the discussion open and call for an ontological and epistemological re-articulation in design research.}
}
@incollection{KRETZER202153,
title = {Chapter 4 - Digital crafting: a new frontier for material design},
editor = {Owain Pedgley and Valentina Rognoli and Elvin Karana},
booktitle = {Materials Experience 2},
publisher = {Butterworth-Heinemann},
pages = {53-66},
year = {2021},
isbn = {978-0-12-819244-3},
doi = {https://doi.org/10.1016/B978-0-12-819244-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012819244300003X},
author = {Manuel Kretzer},
keywords = {Digital, crafting, design, architecture, production, fabrication, parametric, generative, Industry 4.0},
abstract = {This chapter provides an overview of the potentials of employing computational design methods and digital fabrication tools for the creation of novel, material-based design. Just as in the early days of architecture, when the master builder was responsible for all areas of building, these new technologies allow a return to the exploration of experimental design methods and the direct exchange with different materials. Designing for and through digital production techniques thus shifts the focus from formal design representations toward the physically realized. As such, material and tectonic thinking are reintroduced as the very base of the design approach. Due to this a new type of design becomes possible with a formerly unknown degree of complexity—both on a formal and on a functional level. This chapter gives an overview of the history of design, speaks about the so-called “digital continuum,” highlights the benefits of customization and individual production, stresses the nuisance of new esthetic formalizations and the importance of education to mediate such understanding to students of design and architecture.}
}
@article{TRINDADE2025101104,
title = {Teaching mathematical concepts in management with generative artificial intelligence: The power of human oversight in AI-driven learning},
journal = {The International Journal of Management Education},
volume = {23},
number = {2},
pages = {101104},
year = {2025},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101104},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001757},
author = {Maria A.M. Trindade and Gihan S. Edirisinghe and Lan Luo},
keywords = {Generative artificial intelligence in education, Generative AI-Driven learning, Mathematics in management education, Operations management, Economic order quantity, Generative AI in management education},
abstract = {This study demonstrates a successful use of Generative Artificial Intelligence (AI) in teaching mathematical material to management students. We herein introduce the EOQ World Tour game, which substantially improves understanding of inventory-related concepts and long-term knowledge retention compared with traditional methods. Generative AI is revolutionizing management education, by offering innovative methods for teaching and learning. The integration of AI into quantitative business disciplines through novel learning mechanisms provides significant benefits, including enhanced data analysis, improved decision-making models, and sophisticated simulations for hands-on experience. This study introduces the EOQ World Tour game, specifically designed to teach the Economic Order Quantity concept in Operations Management. The game addresses challenges in integrating Generative AI into mathematics in management education by combining human oversight and instructor control through three innovative features: (1) a Generative AI-based simulation, (2) a macropowered Excel worksheet for validating the calculations of an AI chatbot, and (3) a Google Sheets dashboard for centralizing team-generated AI data for postgame analysis. Our study included 41 students divided into experimental and control groups. Pretest results indicated no significant differences in baseline knowledge. However, the post-test results showed that the experimental group achieved a better understanding of inventory-related concepts and practical applications, along with higher engagement, excitement, confidence, and long-term knowledge retention.}
}
@article{GISIGER2000250,
title = {Computational models of association cortex},
journal = {Current Opinion in Neurobiology},
volume = {10},
number = {2},
pages = {250-259},
year = {2000},
issn = {0959-4388},
doi = {https://doi.org/10.1016/S0959-4388(00)00075-1},
url = {https://www.sciencedirect.com/science/article/pii/S0959438800000751},
author = {Thomas Gisiger and Stanislas Dehaene and Jean-Pierre Changeux},
abstract = {Recent computational models, or mathematical realizations of neurobiological theories, are providing insights into the organization and workings of the association cortex. Such models concern the construction of cortical maps, the neural basis of cognitive functions such as visual perception, reward-motivated learning and some aspects of consciousness.}
}
@article{COSTA20221810,
title = {Multicriteria analysis by PROMETHEE-SAPEVO-M1 method: choice of Brazilian sugar and ethanol plants for biomethane production},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1810-1815},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.661},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019796},
author = {Wallace L.T. Costa and Igor P.A. Costa and Adilson V. Terra and Miguel Â.L. Moreira and Carlos, F.S. Gomes and Marcos Santos},
keywords = {Multicriteria, PROMETHEE, SAPEVO, Energy, Biomethane},
abstract = {Given the need for cleaner energy sources associated with the ESG (Environmental, social and corporate governance) policy, the work in question is a case study referring to the project to expand biomethane production in national territory, especially for industrial commercialization. By applying the Value Focused Thinking (VFT) methodology, the study initially seeks the approach based on the values of decision-makers, these being three professionals in the energy sector. After the central objective of supporting decision-making, the hybrid method PROMETHEE-SAPEVO-M1 was used, characterized by the possibility of evaluating quantitative data and qualitative variables. To this end, the modeling occurred through the software of the PROMETHEE-SAPEVO-M1 method to clarify the best plants, because of the range of possibilities in the national territory, for project implementation and subsequent production of biomethane for industrial use. As a result, we verified that São Paulo is the best alternative for applying the investment in biomethane production.}
}
@article{2004263,
title = {Computational Statistics and Data Analysis},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {73},
number = {2},
pages = {263},
year = {2004},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2004.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169743904001820}
}
@article{FELLOWS2013541,
title = {Towards fully multivariate algorithmics: Parameter ecology and the deconstruction of computational complexity},
journal = {European Journal of Combinatorics},
volume = {34},
number = {3},
pages = {541-566},
year = {2013},
note = {Combinatorial Algorithms and Complexity},
issn = {0195-6698},
doi = {https://doi.org/10.1016/j.ejc.2012.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0195669812001400},
author = {Michael R. Fellows and Bart M.P. Jansen and Frances Rosamond},
abstract = {The aim of this article is to motivate and describe the parameter ecology program, which studies how different parameters contribute to the difficulty of classical problems. We call for a new type of race in parameterized analysis, with the purpose of uncovering the boundaries of tractability by finding the smallest possible parameterizations which admit FPT-algorithms or polynomial kernels. An extensive overview of recent advances on this front is presented for the Vertex Cover problem. Moving even beyond the parameter ecology program we advocate the principle of model enrichment, which raises the challenge of generalizing positive results to problem definitions with greater modeling power. The computational intractability which inevitably emerges can be deconstructed by introducing additional parameters, leading towards a theory of fully multivariate algorithmics.}
}

@article{JOKONYA20141533,
title = {Towards a Big Data Framework for the Prevention and Control of HIV/AIDS, TB and Silicosis in the Mining Industry},
journal = {Procedia Technology},
volume = {16},
pages = {1533-1541},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.175},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314004022},
author = {Osden Jokonya},
keywords = {Tuberculosis, Big Data, HIV/AIDS, Silicosis, Systems Approach, Viable Systems Model, Organizational Cybernetics, Hard Systems Thinking, Soft Systems Thinking, Emancipatory Systems Thinking, Critical Systems Thinking, Epidemiology},
abstract = {This paper proposes a big data integrated framework to assist with prevention and control of HIV/AIDS, TB and silicosis (HATS) in the mining industry. The linkage between HATS presents a major challenge to the mining industry globally. When the immune system is compromised by HIV/AIDS and silicosis, it makes it easier for tuberculosis to infect the body. In addition, the silica dust which affects the lungs may also cause silicosis and tuberculosis. The objective of this paper is to posit a big data integrated framework to assist in the prevention and control of HATS in the mining industry. Literature was reviewed in order to build a conceptual framework. Although this study is not the first to apply big data in healthcare, to the researcher's knowledge, it is the first to apply big data in understanding the linkage between HATS in the mining industry. The literature review indicates only a few studies using big data in healthcare with no research found on big data and HATS. It therefore makes a contribution to existing body of literature on the control of HATS. The proposed big data framework has the potential of addressing the needs of predictive epidemiology which is important in forecasting and disease control in the mining industry. The paper therefore lays a foundation for the use of viable systems model and big data to address the challenges of HATS in the mining industry. As part of future work, the framework will be validated using sequential explanatory mixed methods case study approach in mining organizations.}
}
@article{JOHNSON20013201,
title = {Methods for 3D computation of fluid–object interactions in spatially periodic flows},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {24},
pages = {3201-3221},
year = {2001},
note = {Advances in Computational Methods for Fluid-Structure Interaction},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(00)00389-3},
url = {https://www.sciencedirect.com/science/article/pii/S0045782500003893},
author = {Andrew Johnson and Tayfun Tezduyar},
abstract = {We present computational methods for 3D simulation of fluid–object interactions in spatially periodic flows. These methods include a stabilized space-time finite element formulation for incompressible flows with spatial periodicity, automatic mesh generation and update techniques for fluid–object mixtures with spatial periodicity, and parallel implementations. The methods can be applied to uni-periodic (i.e., periodic in one direction), bi-periodic, or tri-periodic flows. The methods are described here in the context of tri-periodic flows with fluid–object interactions, and are applied to the simulation of sedimentation of particles in a fluid. We present several case studies where the results obtained provide notable insight into the behavior of fluid–particle mixtures during sedimentation.}
}
@article{GROSS199653,
title = {The Electronic Cocktail Napkin—a computational environment for working with design diagrams},
journal = {Design Studies},
volume = {17},
number = {1},
pages = {53-69},
year = {1996},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(95)00006-D},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9500006D},
author = {Mark D. Gross},
keywords = {conceptual design, computer-based environment, diagrams, sketching},
abstract = {The Electronic Cocktail Napkin is an experimental computer-based environment for sketching and diagramming in conceptual design. The project's goal is to develop a computational drawing environment to support conceptual designing in a way that leads smoothly from diagrams to more formal and structured representations of schematic design. With computational representations for conceptual designs, computer-supported editing, critquing, analysis, and simulation can be employed earlier in the design process, where it can have a greater impact on outcomes. The paper describes the Electronic Cocktail Napkin program-its recognition and parsing of diagrams and management of spatial constraints, its drawing environment, and two experimental query-by-diagram schemes for retrieving information from architectural databases.}
}
@article{IMJAI2025100308,
title = {Fraud detection skills of Thai Gen Z accountants: The roles of digital competency, data science literacy and diagnostic skills},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {1},
pages = {100308},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000971},
author = {Narinthon Imjai and Watcharawat Promma and Nimnual Visedsun and Berto Usman and Somnuk Aujirapongpan},
keywords = {Fraud detection skills, Digital competency, Data science literacy, Diagnostic skills, Thai Gen Z accountants},
abstract = {The issue of accounting fraud presents a significant challenge within the business sector, prompting an increase in scholarly investigations across various contexts. Despite this growing interest, research specifically addressing the Thai context has remained scarce. Thus, this quantitative study aimed to bridge this gap by assessing the proficiency of Thai Gen Z accountants in detecting accounting fraud, with a particular emphasis on their digital, data science, and diagnostic skills. The study collected data from 150 participants using a structured survey questionnaire distributed to licensed accountants affiliated with the Thailand accounting program. It adopted a theoretical framework inspired by social learning theory and information processing theory to examine both direct and mediated relationships among the key variables under investigation. The results were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM) to examine these relationships. The results showed that digital competency have significant direct effects on the fraud detection skills, with diagnostic skills playing a key role in the process. The study revealed that digital competency not only furnishes accountants with necessary technological expertise but also bolsters their analytical skills, which are vital for identifying fraudulent activities. Likewise, data science literacy—encompassing skills in predictive analytics, big data management, and data insight communication—significantly enhances accountants' capacity to identify and understand fraudulent patterns. The emergent role of diagnostic skills as a key intermediary emphasizes the importance of comprehensive training programs that foster both technical prowess and critical analytical thinking.}
}
@article{EMER20252196,
title = {Examples of Potential Applications of Bio-intelligent Manufacturing},
journal = {Procedia Computer Science},
volume = {253},
pages = {2196-2205},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.280},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002881},
author = {Asja Emer and Matteo {De Marchi} and Angelika Hofer and Benedikt G. Mark and Walburga Kerschbaumer and Erwin Rauch and Dominik T. Matt},
keywords = {Sustainable Manufacturing, Industry 4.0, Industry 5.0, Biological Transformation, Bio-Intelligent Manufacturing, SME},
abstract = {Bio-intelligence in manufacturing integrates biological principles and advanced computational techniques to enhance industrial processes. This interdisciplinary approach is based on already known principles like biomimicry, bio-sensing, and bio-based materials with the aim to further innovate and optimize industrial production by combining manufacturing and biology / biotechnology. Although there is a raising interest in research, it is not yet clear where and how bio-intelligence will have practical implications for manufacturing enterprises. In this work we use systematic literature review methodology to identify and analyze the current status quo of scientific literature related to biointelligent manufacturing. A main result of this work was to deduce potential applications, their suitability for small and medium sized enterprises (SME) and to highlight still existing challenges such as scalability, integration with existing systems, and economic viability.}
}
@incollection{KRUSHINSKY1981171,
title = {STRUCTURAL ANALYSIS OF NON-VERBAL THINKING IN MAN},
editor = {N.P. BECHTEREVA},
booktitle = {Psychophysiology},
publisher = {Pergamon},
pages = {171-178},
year = {1981},
isbn = {978-0-08-025930-7},
doi = {https://doi.org/10.1016/B978-0-08-025930-7.50018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080259307500181},
author = {L.V. Krushinsky and N.P. Popova},
abstract = {Publisher Summary
This chapter discusses the characteristics of nonverbal thinking in man and elementary reasoning activity in animals and children. Verbal thinking is associated with the formation of a structural–functional organization of a pattern-code of verbal signals. Nonverbal thinking is associated with comprehension of the principles that make up the structural organization of environment. This type of thinking occurs in humans and other vertebrates. The chapter also discusses physiological and phenotypic aspects of nonverbal thinking. Both types of thinking have their own characteristics, which are related to the chemistry of the brain, and both reflect the nature of man. The main characteristic of nonverbal thinking is the success in performing the task at the first attempt. Repeated presentations of the task often lead to refusals to resolve it, the appearance of fear of the experimental conditions, and numerous signs of an overexcited state. A study described in the chapter revealed age to be a leading factor in determining the accurate understanding of object movement within the spatial–temporal coordinate system when the exact whereabouts of the object is unknown.}
}
@article{KOLACHEV2023101756,
title = {General intelligence in middle school students from different Russian regions: Results of PISA-like tests},
journal = {Intelligence},
volume = {98},
pages = {101756},
year = {2023},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2023.101756},
url = {https://www.sciencedirect.com/science/article/pii/S0160289623000375},
author = {Nikita Kolachev and Galina Kovaleva},
keywords = {Intelligence, G-factor, General cognitive ability, Bifactor model, PISA},
abstract = {This study is aimed at investigating the contribution of the general intelligence factor if six PISA domains (reading, mathematical, scientific, financial literacies, global competence, and creative thinking) are combined in one measurement instrument. For achieving our goal, items based on the PISA frameworks are developed, students in grades 5–8 from three different Russian regions are assessed, and three IRT models (unidimensional, multidimensional, and bifactor) are applied to process the data. In addition, the correlations from the multidimensional model are estimated to examine the degree of cognitive specificity and mixture modeling is implemented to investigate ability differentiation across grades. Statistical analysis reveals that the bifactor model comprising one general and six specific factors, has a better fit in each grade. Based on this model, we compute the variance explained by the general factor, with the estimates varying between 60% and 70%. In general, the pure variance explained by specific factors does not exceed 10%. The correlations are above 0.40 in each grade and the averaged associations tend to increase from 6th to 8th grade, although they are smaller in years 6 and 7 compared to year 5. The general ability differentiation effect is observed in grades 6 to 8 and is not present in grade 5. Specific ability differentiation is more pronounced in reading literacy, especially in grade 5 to 7. The results obtained are discussed from the perspective of the ability and developmental differentiation/dedifferentiation problem.}
}
@article{CLERJUSTE2024105922,
title = {Unpacking the challenges and predictors of elementary–middle school students’ use of the distributive property},
journal = {Journal of Experimental Child Psychology},
volume = {244},
pages = {105922},
year = {2024},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2024.105922},
url = {https://www.sciencedirect.com/science/article/pii/S0022096524000626},
author = {Sarah N. Clerjuste and Claire Guang and Dana Miller-Cotto and Nicole M. McNeil},
keywords = {Distributive property, Cognitive reflection, Multiplication, Worked examples},
abstract = {The distributive property plays a pivotal role in advancing students’ understanding of multiplication, enabling the decomposition of problems and the acquisition of new facts. However, this property of multiplication is difficult for students to understand. We used two unique data sets to explore middle school students’ use of the distributive property. Study 1 involved data from 1:1 structured interviews of students (N = 24) discussing worked examples and solving associated practice problems. We examined whether or not students used the distributive property to solve the problems and whether or not interviewers followed the recommended distributive property prompts or defaulted to more conventional methods. Despite exposure to worked examples using the distributive property and a protocol calling for attention to it, students and interviewers favored methods like PEMDAS (parentheses, exponents, multiplication, division, addition, subtraction) or long multiplication. Study 2 used a data set with middle school students’ (N = 131) item-level responses on Kirkland’s (2022; doctoral dissertation, University of Notre Dame) Brief Assessment of Mature Number Sense along with several related measures of domain-general and domain-specific skills. We extracted problems involving the distributive property for analysis. Surprisingly, there was no evidence that students’ use of the distributive property improved from sixth grade to eighth grade. However, both grade-level mathematics achievement and cognitive reflection uniquely predicted the correct use of the distributive property. Results suggest that middle school students who exhibit stronger reflective thinking tend to perform better on distributive property problems. Findings highlight cognitive reflection as a potentially important construct involved in the understanding and use of the distributive property.}
}
@incollection{GARDNER2024129,
title = {Chapter 6 - Smart design for sustainable behaviors},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {129-151},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000070},
author = {Nicole Gardner},
keywords = {Behavior change, Cyber-physical system, Interaction, Physical computing, Persuasive technology, Nudge theory, Responsibilization, Smart city, Sustainability, Transition design, Urban design, Urban technology},
abstract = {While smart city initiatives have netted energy savings and carbon reductions, many cities and nations are still falling short of their sustainability targets and climate goals. This chapter draws on transitions concepts and behavior change theory to explore how smaller-scale and localized examples of existing and speculative urban technology projects that combine spatial design thinking and physical computing can help to anchor sustainability culture within everyday citizen's lives. It discusses the ethical significance of designing interactive urban technology projects that aim to nudge behavior change in relation to sustainability awareness.}
}
@article{ALBUS20101519,
title = {A model of computation and representation in the brain},
journal = {Information Sciences},
volume = {180},
number = {9},
pages = {1519-1554},
year = {2010},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2009.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0020025510000095},
author = {James S. Albus},
keywords = {Brain modeling, Cognitive modeling, Human neocortex, Image processing, Knowledge representation, Perception, Reverse engineering the brain, Segmentation, Signals to symbols},
abstract = {The brain is first and foremost a control system that is capable of building an internal representation of the external world, and using this representation to make decisions, set goals and priorities, formulate plans, and control behavior with intent to achieve its goals. The internal representation is distributed throughout the brain in two forms: (1) firmware embedded in synaptic connections and axon-dendrite circuitry, and (2) dynamic state-variables encoded in the firing rates of neurons in computational loops in the spinal cord, midbrain, subcortical nuclei, and arrays of cortical columns. It assumes that clusters and arrays of neurons are capable of computing logical predicates, smooth arithmetic functions, and matrix transformations over a space defined by large input vectors and arrays. Feedback from output to input of these neural computational units enable them to function as finite-state-automata (fsa), Markov decision processes (MDP), or delay lines in processing signals and generating strings and grammars. Thus, clusters of neurons are capable of parsing and generating language, decomposing tasks, generating plans, and executing scripts. In the cortex, neurons are arranged in arrays of cortical columns that interact in tight loops with their underlying subcortical nuclei. It is hypothesized that these circuits compute sophisticated mathematical and logical functions that maintain and use complex abstract data structures. It is proposed that cortical hypercolumns together with their underlying thalamic nuclei can be modeled as a cortical computational unit (CCU) consisting of a frame-like data structure (containing attributes and pointers) plus the computational processes and mechanisms required to maintain it and use it for perception cognition, and sensory-motor behavior. In sensory processing areas of the brain, CCU processes enable focus of attention, segmentation, grouping, and classification. Pointers stored in CCU frames define relationships that link pixels and signals to objects and events in situations and episodes. CCU frame pointers also link objects and events to class prototypes and overlay them with meaning and emotional values. In behavior generating areas of the brain, CCU processes make decisions, set goals and priorities, generate plans, and control behavior. In general, CCU pointers are used to define rules, grammars, procedures, plans, and behaviors. CCU pointers also define abstract data structures analogous to lists, frames, objects, classes, rules, plans, and semantic nets. It is suggested that it may be possible to reverse engineer the human brain at the CCU level of fidelity using next-generation massively parallel computer hardware and software.}
}
@incollection{PRATT198497,
title = {A Theoretical Framework for Thinking About Depiction},
editor = {W. Ray Crozier and Antlony J. Chapman},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {19},
pages = {97-109},
year = {1984},
booktitle = {Cognitive Processes in the Perception of Art},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)62347-X},
url = {https://www.sciencedirect.com/science/article/pii/S016641150862347X},
author = {Francis Pratt},
abstract = {Publisher Summary
This chapter provides a chronological account of the steps which describes the present theoretical framework for thinking about depiction. The experimental results provides good evidence for the following assertions: (1) knowledge is a necessary part of all acts of depiction done by people of all ages and of all levels of skill, (2) knowledge is a main determinant of looking strategies, (3) the role of knowledge in the organization of looking strategies is one of determining the level of description to be used as the basis of analytic processes, and (4) "good" copying performance (i.e., "accurate" in terms of scene-specific and view-specific relations) can be equated with level of description accessed. The chapter emphasizes on: (1) each descending level of description implies an increasing disintegration of the analytic task. (2) Analysis for depiction is concerned with variance. It is concerned with relations that change according to viewing circumstances. In effect, they can be considered as novel relations. There is much evidence that people's ability to maintain "novel" relations in memory is severely limited. (3) The model consisting of a group of straight lines is only capable of being analyzed at the lowest levels of description.}
}
@article{NEWMAN2024101778,
title = {Misinformed by images: How images influence perceptions of truth and what can be done about it},
journal = {Current Opinion in Psychology},
volume = {56},
pages = {101778},
year = {2024},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2023.101778},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X23002233},
author = {Eryn J. Newman and Norbert Schwarz},
keywords = {Visual misinformation, Truthiness, Cognitive fluency, Artificial intelligence (AI), Memory, Truth assessment},
abstract = {We organize image types by their substantive relationship with textual claims and discuss their impact on attention, comprehension, memory, and judgment. Photos do not need to be false (altered or generated) to mislead; real photos can create a slanted representation or be repurposed from different events. Even semantically related non-probative photos, merely inserted to attract eyeballs, can increase message acceptance through increased fluency. Messages with images receive more attention and reach a wider audience. Text-congruent images can scaffold the comprehension of true and false claims and support the formation of correct and false memories. Standard laboratory procedures may underestimate the impact of images in natural media contexts: by drawing all participants' attention to a message that may be ignored without an image, they inflate message effects in the control condition. Misleading images are difficult to identify and their influence often remains outside of awareness, making it hard to curb their influence through critical-thinking interventions. Current concerns about deep fakes may reduce trust in all images, potentially limiting their power to mislead as well as inform. More research is needed to understand how knowing that an image is misleading influences inferences, impressions, and judgments beyond immediate assessments of the image's credibility.}
}
@article{LI2018359,
title = {Experimental and computational Fluid Dynamics study of separation gap effect on gas explosion mitigation for methane storage tanks},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {359-380},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018304224},
author = {Jingde Li and Hong Hao and Yanchao Shi and Qin Fang and Zhan Li and Li Chen},
keywords = {Separation gap, Safety gap, External pressure, Vented gas explosion, CFD, FLACS},
abstract = {This paper presented both experimental and numerical assessments of separation gap effect on vented explosion pressure in and around the area of a tank group. A series of vented gas explosion layouts with different separation gaps between tanks were experimentally investigated. In order to qualitatively determine the relationship between the separation gap distance and explosion pressure, intensive computational Fluid Dynamics (CFD) simulations, verified with testing data, were conducted. Good agreement between CFD simulation results and experimental data was achieved. By using CFD simulation, more gas explosion cases were included to consider different gas cloud coverage scenarios. Separation gap effects on internal and external pressures at various locations were investigated.}
}
@article{GREIF2024126,
title = {Selection, growth and form. Turing’s two biological paths towards intelligent machinery},
journal = {Studies in History and Philosophy of Science},
volume = {106},
pages = {126-135},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0039368124000657},
author = {Hajo Greif and Adam P. Kubiak and Paweł Stacewicz},
keywords = {Universal computing machines, Mechanism, Connectionism, Morphogenesis, D’Arcy Thompson, Darwinian evolution, Cellular automata},
abstract = {We inquire into the role of Turing’s biological thought in the development of his concept of intelligent machinery. We trace the possible relations between his proto-connectionist notion of ‘organising’ machines in Turing (1948) on the one hand and his mathematical theory of morphogenesis in developmental biology (1952) on the other. These works were concerned with distinct fields of inquiry and followed distinct paradigms of biological theory, respectively postulating analogues of Darwinian selection in learning and mathematical laws of form in organic pattern formation. Still, these strands of Turing’s work are related, first, in terms of being amenable in principle to his (1936) computational method of modelling. Second, they are connected by Turing’s scattered speculations about the possible bearing of learning processes on the anatomy of the brain. We argue that these two theories form an unequal couple that, from different angles and in partial fashion, point towards cognition as a biological and embodied phenomenon while, for reasons inherent to Turing’s computational approach to modelling, not being capable of directly addressing it as such. We explore ways in which these two distinct-but-related theories could be more explicitly and systematically connected, using von Neumann’s contemporaneous and related work on Cellular Automata and more recent biomimetic approaches as a foil. We conclude that the nature of ‘initiative’ and the mode of material realisation are the key issues that decide on the possibility of intelligent machinery in Turing.}
}
@article{BAILEY2006793,
title = {Clover: Connecting technology and character education using personally-constructed animated vignettes},
journal = {Interacting with Computers},
volume = {18},
number = {4},
pages = {793-819},
year = {2006},
note = {Special Theme Papers from Special Editorial Board Members (contains Regular Papers)},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2005.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0953543805001153},
author = {Brian P. Bailey and Sharon Y. Tettegah and Terry J. Bradley},
keywords = {Animation, Character education, Multimedia, Narratives, Vignettes},
abstract = {Schools are increasingly integrating character education to facilitate improved moral thinking and pro social behavior among students. An effective method for delivering character education is problem solving moral and social situations represented visually as animated vignettes. However, schools are rarely able to use animated vignettes since existing tools do not allow them to be easily created and having them created externally is overly expensive. In this paper, we describe the design, use, and evaluation of a computational tool that enables students to construct their own animated vignettes. By building, sharing, and responding to vignettes, students become engaged in problem solving moral and social situations. Evaluations showed that users are able to build meaningful vignettes, our tool is easy to learn and fun to use, and our tool's multimedia features are often used and well-liked. Educators can download and use our tool while researchers can draw upon our design rationale and lessons learned when building similar tools.}
}
@article{GANO201556,
title = {Starting with Universe: Buckminster Fuller's Design Science Now},
journal = {Futures},
volume = {70},
pages = {56-64},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002055},
author = {Gretchen Gano},
keywords = {Comprehensiveness, Big data, Design science, Buckminster Fuller, Worldviews Network},
abstract = {Increasingly, decision makers seek to harness “big data” to guide choices in management and policy settings as well as in professions that manufacture, build, and innovate. Scholars examining this trend tend to diagnose it at once as techno positivist in its insistence on design yoked to quantifiable variables and computational modeling and, alternatively, as an imperative integral to realizing ecologically sustainable innovation. This article investigates this tension. It reflects on the role of futurists, designers, architects, urban planners, social scientists, and artists in interpreting and utilizing comprehensiveness as a design frame. Among nine experimental foresight workshops at the inaugural Emerge conference at Arizona State University, many focused on producing physical objects or media, one modeled and expanded upon a method pioneered by architect and polymath R. Buckminster Fuller. At a time when many of the capabilities to realize Fuller's specifications for big data have matured, I investigate whether comprehensive design as framed by Fuller's method shows promise as a trend enabling ecologically sustainable innovations. A historical look at Fuller's Design Science and the reflection on it in the Emerge workshop marks an opportunity to highlight and interpret the resurgence of comprehensive thinking in design while navigating the contradictions this orientation engenders.}
}
@article{GRANDE2022105470,
title = {Nursing competency inventory and professional competence of graduating students in six Asian countries: A cross-sectional study},
journal = {Nurse Education Today},
volume = {116},
pages = {105470},
year = {2022},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2022.105470},
url = {https://www.sciencedirect.com/science/article/pii/S0260691722002064},
author = {Rizal Angelo N. Grande and Daniel Joseph E. Berdida and Tantut Susanto and Anwar Khan and Wanpen Waelveerakup and Zahrah Saad},
keywords = {Asian countries, Competency, Graduating nursing students, Nursing competency inventory, Professional competence},
abstract = {Aims
To investigate graduating nursing students' nursing and professional competencies and the predictors of their competencies.
Background
Across Asian countries, there is a paucity of literature that explores graduating nursing students' competency and professional competence during the ongoing COVID-19 pandemic.
Design
Descriptive, cross-sectional, and predictive approaches.
Method
Convenience sampling was used among graduating nursing students from the six Asian countries (n = 375). The STROBE guidelines for cross-sectional studies were used. Two self-report instruments were utilized to collect data. We conducted multiple linear regression analyses to assess the predictors of nursing competency and professional competence domains.
Results
Country of residence and general point average (GPA) showed statistically significant multivariate effects. Value-based nursing care and critical thinking and reasoning domains recorded the highest in professional competence and competency inventory for nursing students, respectively. Country of residence, GPA, and preferred nursing major were significant predictors of graduating nursing students' nursing competency and professional competence domains.
Conclusion
Our study's findings revealed a high level of diversity among nursing students regarding ethical care obligations, caring pedagogies, and lifelong learning, all of which may be ascribed to their distinct culture, background, and belief systems.}
}
@article{WOODRUFF1994463,
title = {Some computational challenges of developing efficient parallel algorithms for data-dependent computations in thermal-hydraulics supercomputer applications},
journal = {Nuclear Engineering and Design},
volume = {146},
number = {1},
pages = {463-471},
year = {1994},
issn = {0029-5493},
doi = {https://doi.org/10.1016/0029-5493(94)90351-4},
url = {https://www.sciencedirect.com/science/article/pii/0029549394903514},
author = {S.B. Woodruff},
abstract = {The Transient Reactor Analysis Code (TRAC), which features a two-fluid treatment of thermal-hydraulics, is designed to model transients in water reactors and related facilities. One of the major computational costs associated with TRAC and similar codes is calculating constitutive coefficients. Although the formulations for these coefficients are local, the costs are flow-regime- or data-dependent; i.e., the computations needed for a given spatial node often vary widely as a function of time. Consequently, a fixed, uniform assignment of nodes to parallel processors will result in degraded computational efficiency due to the poor load balancing. A standard method for treating data-dependent models on vector architectures has been to use gather operations (or indirect addressing) to sort the nodes into subsets that (temporarily) share a common computational model. However, this method is not effective on distributed memory data parallel architectures, where indirect addressing involves expensive communication overhead. Another serious problem with this method involves software engineering challenges in the areas of maintainability and extensibility. For example, an implementation that was hand-tuned to achieve good computational efficiency would have to be rewritten whenever the decision tree governing the sorting was modified. Using an example based on the calculation of the wall-to-liquid and wall-to-vapor heat-transfer coefficients for three nonboiling flow regimes, we describe how the use of the Fortran 90 WHERE construct and automatic inlining of functions can be used to ameliorate this problem while improving both efficiency and software engineering. Unfortunately, a general automatic solution to the load-balancing problem associated with data-dependent computations is not yet available for massively parallel architectures. We discuss why developers should either wait for such solutions or consider alternative numerical algorithms, such as a neural network representation, that do not exhibit load-balancing problems.}
}
@article{MEHMOOD2023100122,
title = {A multi-stage optimisation-based decision-making framework for sustainable hybrid energy system in the residential sector},
journal = {Sustainable Futures},
volume = {6},
pages = {100122},
year = {2023},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2023.100122},
url = {https://www.sciencedirect.com/science/article/pii/S2666188823000187},
author = {Aamir Mehmood and Long Zhang and Jingzheng Ren},
keywords = {Hybrid energy system, System thinking approach, Genetic algorithm, Multi-criteria decision-making, Energy sustainability},
abstract = {Integrating renewables into existing energy infrastructure to construct hybrid energy systems (HES) plays a vital role for advancing energy sustainability. While various approaches, such as energy systems analysis and linear or non-linear optimisation, have been employed to achieve energy sustainability mainly at the national or city level, there has been a lack of focus on achieving energy sustainability in the residential sector through a holistic optimal decision-making approach for efficient HES design. This study focuses on developing a multi-stage optimisation-based decision-making framework that models, quantifies, and optimises the performance indicators of HES, allowing for an assessment of the trade-off between benefits and systems costs under various design scenarios. The initial step involves designing the HES model and constructing scenarios that cater to the electrification requirements of water, energy, and food elements in the residential sector by using a systematic thinking approach. Then, a preliminary evaluation of the modelled scenarios is conducted to assess energy sustainability in terms of technical and economic aspects. Afterwards, an optimal decision-making setup is established by integrating a multi-objective HES model into the NSGA-II algorithm, which approximates the Pareto optimal solutions. These solutions are then ranked by using a multi-criteria decision-making method. According to the findings, the Quetta region in Pakistan contains the best optimal solution. The results underscore the utility of the developed framework in facilitating the optimal design of renewables-integrated HES for the residential sector. Furthermore, intergovernmental organizations can leverage this framework to formulate effective policies aimed at encouraging residents to invest in HES installation.}
}
@article{KOBSIRIPAT2015227,
title = {Effects of the Media to Promote the Scratch Programming Capabilities Creativity of Elementary School Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {174},
pages = {227-232},
year = {2015},
note = {International Conference on New Horizons in Education, INTE 2014, 25-27 June 2014, Paris, France},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.651},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815007028},
author = {Worarit Kobsiripat},
keywords = {Scratch programming, Higher-order Thinking, Creative Thinking, Computer Multimedia;},
abstract = {Developing creative Promote higher-order thinking processes Give learners specific ability to think on their wide variety and innovative of the original. It led to the discovery and creation of new inventions or form new ideas. Consistent with the educational goals of the program.This research aim to study a guild line of using Scratch Computer Program that leading to creativity. And study the effects of media on the Scratch programming capabilities creativity. The sample consisted of 60 students who were studying in semester 1. 2013 academic year, using purposive sampling (Purposive Sampling) tool used in this research is a lesson plan. Scratch and computer media test innovative ideas. Statistics used Data analysis were percentage, mean, standard deviation and Dependent t-test. The findings indicated that First, Mediums Scratch program can be used as a medium for learning activities. The adoption includes a multimedia interactive media as a tool to support learning. Second, Scratch media performance of computer programs is equal according to the criteria set 82.46/82.25 E1/E2 is 80/80. Creativity of students. Received instruction from the learning activities through the medium of a computer program Scratch by elements of creativity is an idea ingenious ideas flexibility. Initiatives and ideas census. Higher posttest than pretest statistically significant at the .05 level of performance, computer media Scratch equals 82.46/82.25 according to defined criteria E1/E2 is 80 /80. In conclusion the computer program Scratch media can lead creative development of students through the learning activities that promote innovative education that cause the learners’ desirable.}
}
@article{BRENNAN2023100070,
title = {Generalised Kuramoto models with time-delayed phase-resetting for k-dimensional clocks},
journal = {Brain Multiphysics},
volume = {4},
pages = {100070},
year = {2023},
issn = {2666-5220},
doi = {https://doi.org/10.1016/j.brain.2023.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2666522023000084},
author = {Martin Brennan and Peter Grindrod CBE},
keywords = {Kuramoto models, Range-dependent networks, High dimensional clocks, Phase-resetting maps, The human cortex, Consciousness},
abstract = {We consider a class of Kuramoto models, with an array of N individual k-dimensional clocks (k>1), coupled within a directed, range dependent, network. For each directed connection, a signal triggered at the sending clock incurs a (real valued) time delay before arriving at the receiving clock, where it induces an instantaneous phase reset affecting all k-phases. Instantaneous phase resetting maps (PRMs) for k-dimensional clocks have received little attention. The system may be treated as open and subject to periodic, or other types of, PRM forcing at any individual clock, as a result of external forcing stimuli. We show how the full system, with Nk phase variables, responds to such stimuli, as the impact spreads across the entire network. Beyond simulations, we employ methods to reverse engineer the dynamical behaviour of the whole: estimating the intrinsic dimensions of the responses to different experiments; and by analysing pairwise comparisons between those responses. This shows that the system’s responses are governed by a hierarchy of internal dynamical modes, existing across both the Nk phases and over time. We argue that this Kuramoto system is a model for the human cortex, where each k-dimensional clock models the dynamics of a single neural column, which contains 10,000 densely inter-connected neurons. The Kuramoto model exploits the natural network of networks architecture of the human cortex. An array of N=1M such columns/clocks is at the 10B neuron scale of the human cortex. However its simulation is far more accessible than very large scale (VLS) simulations of neuron-to-neuron systems on supercomputers. The latent modes may have important implications for cognition (information processing) and for consciousness (the existence of internal phenomenological experiences). We argue that the existence of the latter plays a key role in preconditioning the former, reducing the decision sets and the cognitive load, and thus enabling a fast-thinking evolutionary advantage. This is the first time that systems of k-dimensional clocks (k> 1), coupled via time-lagged PRMs, within range dependent networks, have been deployed to demonstrate the basic internal phenomenological elements (of consciousness) and their potential role within immediate cognition. Statement of Significance: We argue that this Kuramoto system is a model for the human cortex, where each of 1M k-dimensional clocks models the dynamics of a single neural column (containing 10,000 densely inter-connected neurons). This Kuramoto model exploits the natural network of networks architecture of the human cortex. Large scale human cortex simulations, with 10B neutrons, usually require a super computer. We show that similar results, using this model, can be obtained on a laptop. In particular we show that such dynamical can support internal phenomenological elements (of conscious experience) and we discuss their potential role in preconditioning immediate cognition, furnishing a “fast thinking” evolutionary advantage to the human brain.}
}
@article{RUPESH20213320,
title = {Computational investigation of heat transfer on the surface of engine cylinder with fins of different shapes and materials},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {3320-3326},
year = {2021},
note = {International Conference on Materials, Manufacturing and Mechanical Engineering for Sustainable Developments-2020 (ICMSD 2020)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.471},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391355},
author = {P.L. Rupesh and K. Raja and N.V. {Sai Deepak Raj} and M. {Pruthviraj Bharmal} and Pandey {Aditya Ramjatan}},
keywords = {Two stroke engine, Fin, Circular fin, Tapered fin, Silumin, Thermal Conductivity},
abstract = {In everyday life the use of vehicles has expanded immensely for some ventures and house hold applications, likewise the running time of engine cycle is exceptionally long. Thus because of the consistent running enormous measure of heat is produced. At the point when this heat isn't appropriately disseminated, the engine gets more fragile very soon and life of the engine declines because of the heat development. To build the life of the engine, heat dispersal is expanded by giving fins at external of engine chamber. The shape of the fins and the material used for the fin increases its heat dissipation capacity and in turn increases the cooling of the engine for proper functioning. The present work focuses on the design of fins of circular and tapered shapes for a 2-stroke engine. The temperature distribution and the heat dissipation along the fin surface of two shapes has been observed by a steady state thermal analysis. Alusil and Silumin has been selected as the fin materials and a computational evaluation has also been done using FEM. A better shape of the fin along with a suitable material has been selected based on the results observed by FEM and on comparison with the existing shape and material of the fin.}
}
@article{TEEPLE2023102847,
title = {Level-k predatory trading},
journal = {Journal of Mathematical Economics},
volume = {106},
pages = {102847},
year = {2023},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2023.102847},
url = {https://www.sciencedirect.com/science/article/pii/S030440682300040X},
author = {Keisuke Teeple},
keywords = {Behavioral finance, Level- models, Front running, Price overshooting},
abstract = {I incorporate the level-k thinking solution concept into a simplified (Brummermeier and Pedersen, 2005) predatory trading model to investigate the possibility of arbitraging arbitrageurs. While naive financial predators prey upon a single distressed investor, higher-level thinkers best respond to this and prey upon fellow predators. For some parameter values, sophisticated predators are able to reason their way to the Nash equilibrium strategy, and prices do not oscillate. As parameter values are perturbed, the system undergoes a bifurcation and predators select strategies from a mean-preserving spread of the Nash equilibrium strategy. In these settings, prices display excess volatility and a single shock can send predators into an oscillatory trading frenzy.}
}
@article{HEGER2018177,
title = {We should totally open a restaurant: How optimism and overconfidence affect beliefs},
journal = {Journal of Economic Psychology},
volume = {67},
pages = {177-190},
year = {2018},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167487017305585},
author = {Stephanie A. Heger and Nicholas W. Papageorge},
keywords = {Subjective beliefs, Overconfidence, Optimism, Information, Experiments},
abstract = {Wishful thinking, defined as the tendency to over-estimate the probability of high-payoff outcomes, is a widely-documented phenomenon that can affect decision-making across numerous domains, including finance, management, and entrepreneurship. We design an experiment to distinguish and test the relationship between two easily-confounded biases, optimism and overconfidence, both of which can contribute to wishful thinking. We find that optimism and overconfidence are positively correlated at the individual level and that both help to explain wishful thinking. These findings suggest that ignoring optimism results in an upwardly biased estimate of the role of overconfidence in explaining wishful thinking. To illustrate this point, we show that 30% of our observations are misclassified as under- or overconfident if optimism is omitted from the analysis. Our findings have potential implications for the design of information interventions since how agents incorporate information depends on whether the bias is ego-related.}
}
@article{BOWMAN201834,
title = {Big questions, informative data, excellent science},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {34-36},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300622},
author = {Adrian W. Bowman},
keywords = {Big data, Statistical models},
abstract = {The expression big data is often used in a manner which implies that immediate insight is readily available. Unfortunately, this raises unrealistic expectations. A model which encapsulates the powerful concepts of statistical thinking remains an invaluable component of good analysis.}
}
@article{TEZDUYAR199997,
title = {CFD methods for three-dimensional computation of complex flow problems},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {81},
number = {1},
pages = {97-116},
year = {1999},
issn = {0167-6105},
doi = {https://doi.org/10.1016/S0167-6105(99)00011-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167610599000112},
author = {Tayfun E. Tezduyar},
keywords = {CFD methods, T*AFSM, Three-dimensional flow simulations},
abstract = {This paper provides an overview of some of the CFD methods developed by the Team for Advanced Flow Simulation and Modeling (T*AFSM) [http://www.mems.rice.edu/TAFSM/]. The paper also provides many examples of three-dimensional flow simulations carried out with these CFD methods and advanced parallel supercomputers. The methods and tools described in this paper include: stabilized finite element formulations; formulations for flows with moving boundaries and interfaces; mesh update methods; iterative solution techniques for large nonlinear equation systems; and parallel implementation of these methods. Our target is to be able to address effectively certain classes of flow simulation problems. These include: unsteady flows with interfaces; fluid–object interactions; fluid–structure interactions; airdrop systems; aerodynamics of complex shapes; and contaminant dispersion.}
}
@article{BAKER2022942,
title = {Three aspects of representation in neuroscience},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {11},
pages = {942-958},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322002108},
author = {Ben Baker and Benjamin Lansdell and Konrad P. Kording},
keywords = {representation, information, coding, explanation, function, teleology, philosophy},
abstract = {Neuroscientists often describe neural activity as a representation of something, or claim to have found evidence for a neural representation, but there is considerable ambiguity about what such claims entail. Here we develop a thorough account of what ‘representation’ does and should do for neuroscientists in terms of three key aspects of representation. (i) Correlation: a neural representation correlates to its represented content; (ii) causal role: the representation has a characteristic effect on behavior; and (iii) teleology: a goal or purpose served by the behavior and thus the representation. We draw broadly on literature in both neuroscience and philosophy to show how these three aspects are rooted in common approaches to understanding the brain and mind. We first describe different contexts that ‘representation’ has been closely linked to in neuroscience, then discuss each of the three aspects in detail.}
}
@incollection{KATZ2016123,
title = {Chapter 6 - Development of Counting Ability: An Evolutionary Computation Point of View},
editor = {Avishai Henik},
booktitle = {Continuous Issues in Numerical Cognition},
publisher = {Academic Press},
address = {San Diego},
pages = {123-145},
year = {2016},
isbn = {978-0-12-801637-4},
doi = {https://doi.org/10.1016/B978-0-12-801637-4.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128016374000068},
author = {Gali Barabash Katz and Amit Benbassat and Moshe Sipper},
keywords = {numerical cognition, size perception, counting, evolutionary algorithms, genetic algorithms, artificial neural networks},
abstract = {Examination of numerical cognition encompasses multiple facets (eg, discrete vs. continuous properties, subitizing, estimation, counting, etc.). Many models have been suggested to explain these features. By looking into the basic ability to perceive size, against the complex one of counting, we hypothesize that counting system evolved on the basis of a primitive size perception system rather than the two systems evolved separately. In this chapter, we present a novel way of using evolutionary computation techniques to evolve artificial neural networks (ANNs) first to perceive size and then to count, and compare their counting skills to a different group of ANNs who evolved to count from scratch. The results revealed better counting skills when evolving first to perceive size (or other classification task) and then to count over those who evolved just to count. In addition, ANNs who evolved with continuous stimuli presented better counting skills than those evolved with discrete stimuli.}
}
@article{SUPPES201295,
title = {Phase-oscillator computations as neural models of stimulus–response conditioning and response selection},
journal = {Journal of Mathematical Psychology},
volume = {56},
number = {2},
pages = {95-117},
year = {2012},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2012.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S002224961200003X},
author = {P. Suppes and J. Acacio {de Barros} and G. Oas},
keywords = {Learning, Neural oscillators, Three-oscillator Kuramoto model, Stability points of the Kuramoto model, Stimulus–response theory, Phase representation, Continuum of responses},
abstract = {The activity of collections of synchronizing neurons can be represented by weakly coupled nonlinear phase oscillators satisfying Kuramoto’s equations. In this article, we build such neural-oscillator models, partly based on neurophysiological evidence, to represent approximately the learning behavior predicted and confirmed in three experiments by well-known stochastic learning models of behavioral stimulus–response theory. We use three Kuramoto oscillators to model a continuum of responses, and we provide detailed numerical simulations and analysis of the three-oscillator Kuramoto problem, including an analysis of the stability points for different coupling conditions. We show that the oscillator simulation data are well-matched to the behavioral data of the three experiments.}
}
@article{LINDELL2024103428,
title = {The dialectics of digitalisation: A critique of the modernistic imperative for the development of digital technology},
journal = {Futures},
volume = {162},
pages = {103428},
year = {2024},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103428},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724001113},
author = {Rikard Lindell},
keywords = {Dialectics, Digital transformation, Digitalisation, Postdigital},
abstract = {This text discusses today’s digital transformation through the lens of Horkheimer and Adornos’ study of the enlightenment. Policy and public discourse around digitalisation embrace and adhere to the narrow tenets enlightenment thinking; the idea that rationality, individual freedom, and a society free from superstition are necessary and attainable goals. The costs of what has come to be called ‘Modernity’ are many. Through the application of rationality to all spheres of life, married with disruptive technological advancement, humanity has diminished its’ imagination – its ability to seek new directions. To paraphrase Horkheimer and Adorno, Modernism fights against nature, of which we are a part, and thus, paradoxically, sets us in a fight against ourselves. Environmental degradation, the price of progress, being just one example of this – deadening work, consumerism and severed social connections being amongst others. In this framing, digitalisation itself comes to be understood itself as akin to a force of nature – one that we can do little about, other than adjust and adapt or be swept away. But this by no means a foregone conclusion, there is light at the end of the optical fibre. Albeit that recent technical developments around artificial intelligence appears to be pushing policy makers into hasty decisions, the pace of the technical development is not as fast as we believe, and in comparison with the Reformation – we have time. If we can restrain ourselves from the resist, adapt or die responses promoted in popular discourse in face of the shock of large language models and rising threat of automation, then we create room to consider economic, social, and ecological alignment and accord, in the decision making and design of future interactive artefacts and digital services. The article argues that through postdigital aesthetics, technology makers can embrace materiality and the inherent qualities of digital technology to formulate a critique of existing trajectories in digital transformation, with consequences for a more sustainable future.}
}
@article{HONG201611,
title = {Ontology-based conceptual design for ultra-precision hydrostatic guideways with human–machine interaction},
journal = {Journal of Industrial Information Integration},
volume = {2},
pages = {11-18},
year = {2016},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X16300188},
author = {Haibo Hong and Yuehong Yin},
keywords = {Human machine integrated conceptual design, Information integration, High dimensional information integration, Ontology},
abstract = {This paper proposed a human–machine integrated conceptual design method based on ontology, aiming at eliminating the uncertainties and blindness during the design process of ultra-precision grinding machine, especially for its key component–the ultra-precision hydrostatic guideways. Both the required knowledge and the database of hydrostatic guideways are modelled using ontologies to provide a consensual understanding among collaborators. Moreover, a formalized knowledge searching interface is developed to obtain similar instances as references according to the design principles and rules. Based on the imaginal thinking theory, the search process and the results are attempted to be presented in the form of image in order to fit human's customary intuitive thinking frame, facilitating the decision making process. Finally, our design of hydrostatic guideways for an ultra-precision grinding machine is used to validate the effectiveness of the method.}
}
@article{JOHAN1992113,
title = {A data parallel finite element method for computational fluid dynamics on the Connection Machine system},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {99},
number = {1},
pages = {113-134},
year = {1992},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(92)90124-3},
url = {https://www.sciencedirect.com/science/article/pii/0045782592901243},
author = {Zdeněk Johan and Thomas J.R. Hughes and Kapil K. Mathur and S.Lennart Johnsson},
abstract = {A finite element method for computational fluid dynamics has been implemented on the Connection Machine systems CM-2 and CM-200. An implicit iterative solution strategy, based on the preconditioned matrix-free GMRES algorithm, is employed. Parallel data structures built on both nodal and elemental sets are used to achieve maximum parallelization. Communication primitives provided through the Connection Machine Scientific Software Library substantially improved the overall performance of the program. Computations of three-dimensional compressible flows using unstructured meshes having close to one million elements, such as a complete airplane, demonstrate that the Connection Machine systems are suitable for these applications. Performance comparisons are also carried out with the vector computers Cray Y-MP and Convex C-1.}
}
@article{SOTELOMONGE2021869,
title = {Conceptualization and cases of study on cyber operations against the sustainability of the tactical edge},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {869-890},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002788},
author = {Marco Antonio {Sotelo Monge} and Jorge {Maestre Vidal}},
keywords = {Cyber defense, Economical Denial of Sustainability, Military operations, Situational Awareness, Tactical Denial of Sustainability},
abstract = {The last decade consolidated the cyberspace as fifth domain of military operations, which extends its preliminarily intelligence and information exchange purposes towards enabling complex offensive and defensive operations supported/supportively of parallel kinetic domain actuations. Although there is a plethora of well documented cases on strategic and operational interventions of cyber commands, the cyber tactical military edge is still a challenge, where cyber fires barely integrate to the traditional joint targeting cycle due to, among others, long planning/development times, asymmetric effects, strict target reachability requirements, or the fast propagation of collateral damage; the latter rapidly deriving on hybrid impacts (political, economic, social, etc.) and evidencing significant socio-technical gaps. In this context, it is expected that Tactical Clouds disruptively facilitate cyber operations at the edge while exposing the rest of the digital assets of the operation to them. On these grounds, the main purpose of the conducted research is to review and in depth analyze the risks and opportunities of jeopardizing the sustainability of the military Tactical Clouds at their cyber edge. Along with a 1) comprehensively formulation of the researched problematic, the study 2) formalizes the Tactical Denial of Sustainability (TDoS) concept; 3) introduces the phasing, potential attack surfaces, terrains and impact of TDoS attacks; 4) emphasizes the related human and socio-technical aspects; 5) analyzes the threats/opportunities inherent to their impact on the cloud energy efficiency; 6) reviews their implications at the military cyber thinking for tactical operations; 7) illustrates five extensive CONOPS that facilitate the understanding of the TDoS concept; and given the high novelty of the discussed topics, this paper 8) paves the way for further research and development actions.}
}
@incollection{BRACHMAN20041,
title = {Chapter 1 - Introduction},
editor = {Ronald J. Brachman and Hector J. Levesque},
booktitle = {Knowledge Representation and Reasoning},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-14},
year = {2004},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-932-7},
doi = {https://doi.org/10.1016/B978-155860932-7/50086-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781558609327500868},
author = {Ronald J. Brachman and Hector J. Levesque},
abstract = {Publisher Summary
This introductory chapter discusses the main issues associated with Artificial Intelligence (AI) and the prospect of a machine that could think. AI is the study of intelligent behavior that is achieved through computational means. One striking aspect of intelligent behavior is that it is conditioned by knowledge. Knowledge representation and reasoning are the parts of AI that are concerned with how an agent uses what it knows in deciding what to do. It is the study of thinking as a computational process. The book introduces the symbolic structures invented for representing knowledge and the computational processes devised for reasoning with those symbolic structures. The reason why logic is relevant to knowledge representation and reasoning is that logic is the study of entailment relations—languages, truth conditions, and rules of inference… Despite the centrality of knowledge representation and reasoning to AI, there are alternate views. Some authors have claimed that human-level reasoning is not achievable via purely computational means. Others suggest that intelligence derives from computational mechanisms.}
}
@article{BALLESTERRIPOLL2025109368,
title = {Global sensitivity analysis of uncertain parameters in Bayesian networks},
journal = {International Journal of Approximate Reasoning},
volume = {180},
pages = {109368},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2025.109368},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2500009X},
author = {Rafael Ballester-Ripoll and Manuele Leonelli},
keywords = {Bayesian networks, Sensitivity analysis, Sobol indices, Tensor networks, Uncertainty quantification},
abstract = {Traditionally, the sensitivity analysis of a Bayesian network studies the impact of individually modifying the entries of its conditional probability tables in a one-at-a-time (OAT) fashion. However, this approach fails to give a comprehensive account of each inputs' relevance, since simultaneous perturbations in two or more parameters often entail higher-order effects that cannot be captured by an OAT analysis. We propose to conduct global variance-based sensitivity analysis instead, whereby n parameters are viewed as uncertain at once and their importance is assessed jointly. Our method works by encoding the uncertainties as n additional variables of the network. To prevent the curse of dimensionality while adding these dimensions, we use low-rank tensor decomposition to break down the new potentials into smaller factors. Last, we apply the method of Sobol to the resulting network to obtain n global sensitivity indices, one for each parameter of interest. Using a benchmark array of both expert-elicited and learned Bayesian networks, we demonstrate that the Sobol indices can significantly differ from the OAT indices, thus revealing the true influence of uncertain parameters and their interactions.}
}
@article{MANIKANTAN2009639,
title = {Challenges for the future modifications of the TNM staging system for head and neck cancer: Case for a new computational model?},
journal = {Cancer Treatment Reviews},
volume = {35},
number = {7},
pages = {639-644},
year = {2009},
issn = {0305-7372},
doi = {https://doi.org/10.1016/j.ctrv.2009.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0305737209000632},
author = {Kapila Manikantan and Suhail I. Sayed and Konstantinos N. Syrigos and Peter Rhys-Evans and Chris M. Nutting and Kevin J. Harrington and Rehan Kazi},
keywords = {TNM stage, Head and neck cancer, Co-morbidity},
abstract = {Summary
The TNM system of staging cancers is a simple and effective method to map the extent of tumours. It had traditionally strived to maintain a balance between being simple and user-friendly on one hand and comprehensive on the other. A number of revisions have taken place over the years with the goal of improving utility. However, numerous controversies surround the TNM system. There is a school of thought that contends that patient co-morbidity and specific tumour-related factors should be incorporated to add further prognostic capabilities in the TNM system, but this raises concerns that such an approach may unnecessarily complicate the system. This review highlights some controversies that surround the TNM system and suggests prognostic indicators that may be added to make it more useful in guiding treatment decisions and predicting outcomes.}
}
@incollection{NUNES2010457,
title = {Learning Outside of School},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {457-463},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00525-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894700525X},
author = {T. Nunes},
keywords = {Guided participation, Informal learning, Informal mathematics, Learning outside school, Nonformal learning, Oral arithmetic, Situated learning, Street mathematics, Thinking in action, Work-based learning},
abstract = {Learning can take place everywhere: in the home, the community, or at work. Learning outside school is often invisible because it is taken for granted, as common sense or cultural knowledge. It happens in the course of activities not designed for learning, so it can be described as thinking in action. The representational tools (number systems, graphs) and objects (crates, coins, bills) used outside school become part of our thinking as we act and think with them. A major process in learning outside school is guided participation, where learners take responsibility for accomplishing tasks guided by a more experienced person.}
}
@incollection{PROCHAZKOVA2020121,
title = {Chapter 6 - Altered states of consciousness and creativity},
editor = {David D. Preiss and Diego Cosmelli and James C. Kaufman},
booktitle = {Creativity and the Wandering Mind},
publisher = {Academic Press},
pages = {121-158},
year = {2020},
series = {Explorations in Creativity Research},
isbn = {978-0-12-816400-6},
doi = {https://doi.org/10.1016/B978-0-12-816400-6.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128164006000067},
author = {Luisa Prochazkova and Bernhard Hommel},
keywords = {Altered states of consciousness (ASC), Cannabis, Convergent thinking, Creativity, Divergent thinking, Hallucinations, Meditation, Metactontrol, Psychedelics},
abstract = {Increasing evidence suggests that altered states of consciousness (ASC) are associated with both positive and negative effects on components of creative performance, and convergent and divergent thinking in particular. We provide a metacontrol framework that allows characterizing factors that induce ASC in terms of their general impact on the information processing style of problem solvers. We discuss behavioral and neuronal findings from three areas that reflect strong connections between ASC and the underlying effects on metacontrol on the one hand and components of creativity on the other hand: drug-induced ASC, meditation-induced ASC, and hallucinations. While more, and especially more systematic research is needed, we identify a general trend, suggesting that factors that induce ASC are likely to alter the metacontrol state by biasing it toward either persistence, which is beneficial for convergent thinking and other persistence-heavy operations, or flexibility, which is beneficial for divergent thinking and other flexibility-heavy operations.}
}
@article{SNOW201462,
title = {Emergent behaviors in computer-based learning environments: Computational signals of catching up},
journal = {Computers in Human Behavior},
volume = {41},
pages = {62-70},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214004518},
author = {Erica L. Snow and G. Tanner Jackson and Danielle S. McNamara},
keywords = {Intelligent tutoring systems, Individual differences, Self-regulated learning, Agency, Log data, Dynamic analyses},
abstract = {Self-regulative behaviors are dynamic and evolve as a function of time and context. However, dynamical fluctuations in behaviors are often difficult to measure and therefore may not be fully captured by traditional measures alone. Utilizing system log data and two novel statistical methodologies, this study examined emergent patterns of controlled and regulated behaviors and assessed how variations in these patterns related to individual differences in prior literacy ability and target skill acquisition. Conditional probabilities and Entropy analyses were used to examine nuanced patterns manifested in students’ interaction choices within a computer-based learning environment. Forty high school students interacted with the game-based intelligent tutoring system iSTART-ME, for a total of 11 sessions (pretest, 8 training sessions, posttest, and a delayed retention test). Results revealed that high and low reading ability students differed in their patterns of interactions and the amount of control they exhibited within the game-based system. However, these differences converged overtime along with differences in students’ performance within iSTART-ME. The findings from this study indicate that individual differences in students’ prior reading ability relate to the emergence of controlled and regulated behaviors during learning tasks.}
}
@article{GARDNER201854,
title = {SMLXL: Scaling the smart city, from metropolis to individual},
journal = {City, Culture and Society},
volume = {12},
pages = {54-61},
year = {2018},
note = {Innovation and identity in next generation smart cities},
issn = {1877-9166},
doi = {https://doi.org/10.1016/j.ccs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877916617301315},
author = {Nicole Gardner and Luke Hespanhol},
keywords = {Smart cities, Architecture, Design, Physical computing, Proxemics, Computational design},
abstract = {The ‘smart city’ is an oft-cited techno-urban imaginary promoted by businesses and governments alike. It thinks big, and is chiefly imagined in terms of large-scale information communications systems that hinge on the collection of real-time and so-called ‘big data’. Less talked about are the human-scale implications and user-experience of the smart city. Much of the current academic scholarship on smart cities offers synoptic and technical perspectives, leaving the users of smart systems curiously unaccounted for. While they purport to empower citizens, smart cities initiatives are rarely focused at the citizen-scale, nor do they necessarily attend to the ways initiatives can be user-led or co-designed. Drawing on the outcomes of a university studio, this article rethinks the smart city as a series of urban scales—metropolis, community, individual, and personal—and proposes an analytical model for classifying smart city initiatives in terms of engagement. Informed by the theory of proxemics, the model proposed analyses smart city initiatives in terms of the scope of their features and audience size; the actors accountable for their deployment and maintenance; their spatial reach; and the ability of design solutions to re-shape and adapt to different urban scenarios and precincts. We argue that the significance of this model lies in its potential to facilitate modes of thinking across and between scales in ways that can gauge the levels of involvement in the design of digitally mediated urban environments, and productively re-situate citizens as central to the design of smart city initiatives.}
}
@article{MONAT2024103429,
title = {The self-awareness of the forest},
journal = {Futures},
volume = {163},
pages = {103429},
year = {2024},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103429},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724001125},
author = {Jamie P. Monat},
keywords = {Forest, Emergence, Systems thinking, Self-awareness, Neural network},
abstract = {Systems Thinking theorist J. P. Monat has hypothesized that human-level organismal self-awareness will emerge spontaneously in a well-connected neural network as the number of interconnected nodes exceeds ∼70 billion; he speculates that computer networks may achieve self-awareness as the number of nodes approaches this figure. Forests have historically not been perceived as interconnected networks of trees; recently however, researchers have described the “wood-wide web” in which underground fungi interconnect large numbers of trees and plants via chemical and electrical signals. Some of earth’s forests number many billions of trees, and some of the world’s prairies and seagrass meadows also contain billions of individual plants. These plant ecosystems may thus be self-aware, and in fact there may be a multitude of self-aware plant-based ecosystems on earth already. The speed of signal transmission via fungi within each ecosystem is much slower than that in humans, and therefore their organismal self-awareness may be of a different nature than the self-awareness that we associate with humans and upper primates. However, the possibility that our plant systems may be aware of the environmental insults that are being wrought upon them should make us reconsider our anthropocentric activities, as well as the possibility that humanity may need to collaborate with other intelligent non-human earth-based life forms to ensure mutual survival.}
}
@article{CHEN2025126364,
title = {Strain-driven anisotropic enhancement in the thermal conductivity of KCaBi: the role of optical phonons},
journal = {International Journal of Heat and Mass Transfer},
volume = {236},
pages = {126364},
year = {2025},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2024.126364},
url = {https://www.sciencedirect.com/science/article/pii/S0017931024011931},
author = {Xue-Kun Chen and Yue Zhang and Qing-Qing Luo and Pin-Zhen Jia and Wu-Xing Zhou},
keywords = {anisotropic thermal conductivity, optical phonons, four-phonon scattering, strain engineering, machine learning potential},
abstract = {Acoustic phonons have long been believed to dominate the lattice thermal conductivity (κl) and the contribution of optical phonons can be neglected in crystal structures. KCaBi, as a high-throughput screening semiconductor with ultralow κl [J. Am. Chem. Soc. 144, 4448 (2022)], has been demonstrated that the contribution of optical phonons plays an important role in thermal transport. In this work, by solving the Boltzmann transport equation, it is found that the κl of KCaBi is 2.2 at 300K, with acoustic phonons dominating the z-direction κl and optical phonons contributing around 50% to the x-direction κl under the four-phonon picture. The uncommon contribution of optical phonons also manifests the possibility of tuning the κl anisotropy based on optical phonons. Following this line of thinking, it is found that applying tensile strain can cause a more pronounced decrease of acoustic phonon contribution than that of optical counterpart due to the highly dispersive optical branches, thus enhancing the anisotropic ratio of κl. Moreover, the microscopic mechanism is elucidated by analyzing the phonon dispersion relation, phonon mode-wise contribution and phonon scattering rates. Our study could provide appealing alternatives for the regulation of phonon transport from the viewpoint of optical phonons.}
}
@article{LAING2011306,
title = {Computational approaches to RNA structure prediction, analysis, and design},
journal = {Current Opinion in Structural Biology},
volume = {21},
number = {3},
pages = {306-318},
year = {2011},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2011.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X11000674},
author = {Christian Laing and Tamar Schlick},
abstract = {RNA molecules are important cellular components involved in many fundamental biological processes. Understanding the mechanisms behind their functions requires RNA tertiary structure knowledge. Although modeling approaches for the study of RNA structures and dynamics lag behind efforts in protein folding, much progress has been achieved in the past two years. Here, we review recent advances in RNA folding algorithms, RNA tertiary motif discovery, applications of graph theory approaches to RNA structure and function, and in silico generation of RNA sequence pools for aptamer design. Advances within each area can be combined to impact many problems in RNA structure and function.}
}
@article{SHARIF2022104090,
title = {Robotic sheet metal folding: Tool vs. material programming},
journal = {Automation in Construction},
volume = {134},
pages = {104090},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104090},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005410},
author = {Shani Sharif and Russell Gentry},
keywords = {Robotic fabrication, Mass-customization, Dieless sheet metal folding},
abstract = {This research explores how deductive engineering thinking, as opposed to an abductive design rationale, can influence how robotic methods of fabricating building components are developed. The goal of this research is to demonstrate how creative thinking can introduce alternative robotic fabrication techniques targeted for the architectural mass-customization process. For this purpose, we chose robotic dieless sheet metal folding as the main fabrication technique, due to its wide range of applications in both the architectural construction and manufacturing industries. Two robotic sheet metal folding projects were developed. The first, an example of tool programming, took advantage of an engineering approach and was focused on the affordances of the tool (an industrial robotic arm). The second project, one of material programming, employed a design methodology and was directed towards the affordances of the material (i.e., stainless steel sheet metal). By discussing the advantages and disadvantages of each approach, this research argues that both engineering and design should be considered required and complementary processes in the development of new creative fabrication solutions, allowing them to and make the overall production process more efficient.}
}
@article{FU2024107632,
title = {Dissecting behavioral inertia in shaping different resident participation behaviors in neighborhood regeneration: A quantitative behavioral experiment},
journal = {Environmental Impact Assessment Review},
volume = {109},
pages = {107632},
year = {2024},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2024.107632},
url = {https://www.sciencedirect.com/science/article/pii/S0195925524002191},
author = {Xinyue Fu and Guiwen Liu and Hongjuan Wu and Taozhi Zhuang and Ruopeng Huang and Fanning Yuan and Yuhang Zhang},
keywords = {Neighborhood regeneration, Resident participation, Behavioral inertia, Behavioral experiment},
abstract = {Research on resident participation in neighborhood regeneration provides valuable insights for urban policymakers in environmental governance. While previous studies have extensively examined various influencing factors, they often neglect the impact of behavioral inertia. To address this gap, this study conducts a behavioral experiment to quantitatively assess the presence and impact of behavioral inertia on residents' governance and financial participation behaviors. A total of 576 valid survey questionnaires were collected, and conditional logit model and ordered logit model were utilized for analysis. The study reveals that behavioral inertia is indeed observable in residents' governance participation and financial participation behaviors. Furthermore, the findings underscore distinct drivers of behavioral inertia for these two types of participation behaviors, with emotional reactions predominantly influencing governance participation, while short-term thinking largely shapes financial participation. Theoretically, this study uses the innovative concept of “behavioral inertia” to offer a new explanatory framework for aspects of behavior that cannot be solely explained by the attributes of regeneration plans. Furthermore, the behavioral experiments utilized in this study exemplify how the research framework of behavioral science can be applied to the study of urban governance in a broad context internationally. Practically, the research findings provide valuable insights for urban policymakers to tailor measures aimed at promoting resident participation and fostering sustainable urban development.}
}
@incollection{GLENBERG199977,
title = {4 Why mental models must be embodied},
editor = {Gert Rickheit and Christopher Habel},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {128},
pages = {77-90},
year = {1999},
booktitle = {Mental Models in Discourse Processing and Reasoning},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(99)80048-X},
url = {https://www.sciencedirect.com/science/article/pii/S016641159980048X},
author = {Arthur Glenberg},
abstract = {Publisher Summary
Mental models are related to the concept of meaning and language comprehension; in other words, comprehending a linguistic message means that an appropriate mental model has been formed. The manipulation of mental models corresponds to thinking, and it is the manipulation that generates emergent ideas. The chapter discusses the importance of considering the ways ideas combine and presents the data from two experiments that illustrate the combination of ideas. The chapter illustrates the major implications for the theories of mental models. The first implication is that the computational theories cannot account for the data. The second implication is that something like embodiment is needed, and the chapter outlines one account of embodied mental models. The third implication is the most important and most controversial. It is that the human cognition is not a computational phenomenon.}
}
@incollection{VARGAS201945,
title = {Cell Adhesion: Basic Principles and Computational Modeling},
editor = {Roger Narayan},
booktitle = {Encyclopedia of Biomedical Engineering},
publisher = {Elsevier},
address = {Oxford},
pages = {45-58},
year = {2019},
isbn = {978-0-12-805144-3},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.99930-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383999306},
author = {Diego A. Vargas and Hans {Van Oosterwyck}},
keywords = {Adherens junction, Adhesion dynamics, Bond lifetime, Cell adhesion, Cellularized material, Focal adhesion, Force spectroscopy, Mathematical modeling, Mechanotransduction, Multiscale modeling, Rate constant, Vertex model},
abstract = {A cell interacts with its environment through adhesion complexes. These are protein complexes that form through noncovalent interactions between adhesion receptors in the cell membrane and similar receptors in neighboring cells or ligand molecules in the surrounding extracellular matrix. Cell adhesions are crucial to maintain tissue integrity and cellular communication. Communication and sensing occur through the transmittal of forces through adhesions. This relevant role motivated researchers to develop theoretical models of adhesion. Initial models were based on studies of association kinetics of proteins, which later were expanded to explicitly include the role of force in determining bond strength. The introduction of techniques that allowed measurements of force in the range of a single adhesion produced models that describe the inner workings of the adhesion molecules themselves. Despite the relative simplicity of these models, they are still relevant. Not only were these studies novel and creative, they have been integrated into models describing larger cellular aggregates, unraveling the role of mechanics in biology. These models have been used in the study of cell migration, developmental biology, and cancer biology.}
}
@article{DUCH1996136,
title = {Computational physics of the mind},
journal = {Computer Physics Communications},
volume = {97},
number = {1},
pages = {136-153},
year = {1996},
note = {High-Performance Computing in Science},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(96)00027-6},
url = {https://www.sciencedirect.com/science/article/pii/0010465596000276},
author = {Włodzisław Duch},
abstract = {In the XIX century and earlier physicists such as Newton, Mayer, Hooke, Helmholtz and Mach were actively engaged in the research on psychophysics, trying to relate psychological sensations to intensities of physical stimuli. Computational physics allows to simulate complex neural processes giving a chance to answer not only the original psychophysical questions but also to create models of the mind. In this paper several approaches relevant to modeling of the mind are outlined. Since direct modeling of the brain functions is rather limited due to the complexity of such models a number of approximations is introduced. The path from the brain, or computational neurosciences, to the mind, or cognitive sciences, is sketched, with emphasis on higher cognitive functions such as memory and consciousness. No fundamental problems in understanding of the mind seem to arise. From a computational point of view realistic models require massively parallel architectures.}
}
@article{CAMERON2019102,
title = {Education in Process Systems Engineering: Why it matters more than ever and how it can be structured},
journal = {Computers & Chemical Engineering},
volume = {126},
pages = {102-112},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418311773},
author = {Ian T. Cameron and Sebastian Engell and Christos Georgakis and Norbert Asprion and Dominique Bonvin and Furong Gao and Dimitrios I. Gerogiorgis and Ignacio E. Grossmann and Sandro Macchietto and Heinz A. Preisig and Brent R. Young},
abstract = {This position paper is an outcome of discussions that took place at the third FIPSE Symposium in Rhodes, Greece, between June 20–22, 2016 (http://fi-in-pse.org). The FIPSE objective is to discuss open research challenges in topics of Process Systems Engineering (PSE). Here, we discuss the societal and industrial context in which systems thinking and Process Systems Engineering provide indispensable skills and tools for generating innovative solutions to complex problems. We further highlight the present and future challenges that require systems approaches and tools to address not only ‘grand’ challenges but any complex socio-technical challenge. The current state of Process Systems Engineering (PSE) education in the area of chemical and biochemical engineering is considered. We discuss approaches and content at both the unit learning level and at the curriculum level that will enhance the graduates’ capabilities to meet the future challenges they will be facing. PSE principles are important in their own right, but importantly they provide significant opportunities to aid the integration of learning in the basic and engineering sciences across the whole curriculum. This fact is crucial in curriculum design and implementation, such that our graduates benefit to the maximum extent from their learning.}
}
@article{MATTHEWS201973,
title = {Introducing a computational method to estimate and prioritize systemic body exposure of organic chemicals in humans using their physicochemical properties},
journal = {Computational Toxicology},
volume = {9},
pages = {73-99},
year = {2019},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468111318300276},
author = {Edwin John Matthews},
keywords = {Absorption, Bioavailability, Chemical disposition, Data-gaps, Distribution, Food ingredient, GRAS, Hazard identification, , OCS (optimal chemical space), Pharmacokinetics, Physicochemical property, Preservative, Prioritization, QSAR, QSPR, Read-across, Risk-ranking, Sequestration, Signal-detection, Toxicokinetics},
abstract = {This report describes a computational method developed to predict systemic exposure (s-exposure), chemical disposition {(CD) intestinal absorption, transport, membrane permeability, distribution, sequestration, phospholipidosis and toxicokinetics} of organic chemicals in humans. The method qualitatively and quantitatively estimates a chemical's CD activity profile based upon computed molecular descriptor properties (descriptors), and it facilitates in silico signal-detection of data-gaps, prioritization, risk-ranking, read-across, and re-assessments (if mandated) of large sets of chemicals in a safety evaluation setting. The investigation used a reference set of 2372 marketed human pharmaceuticals to define decision rules for an optimal chemical space (OCS) in which chemicals have high s-exposure, good CD, and a potential for chemical toxicity (CT); conversely, chemicals outside the OCS have low s-exposure, poor CD into the body, and low potential for CT. The method requires computation of 29 descriptors, identification of OCS molecular descriptor property violations (descriptor_violations), and alignment of descriptor_violations with specific decision rules for individual CD endpoint activities. The investigation predicted the CD activities of food and cosmetic preservatives, ingredients in GRAS (generally recognized as safe). Notices submitted to the FDA, reference pharmaceuticals, and it provides prioritization metrics and indices that facilitate prioritization of chemical in silico computed CD activities.}
}
@article{SINGH2024483,
title = {How has the AI boom impacted algorithmic biology?},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {483-487},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001522},
author = {Mona Singh and Cenk Sahinalp and Jianyang Zeng and Wei Vivian Li and Carl Kingsford and Qiangfeng Zhang and Teresa Przytycka and Joshua Welch and Jian Ma and Bonnie Berger},
abstract = {This Voices piece will highlight the impact of artificial intelligence on algorithm development among computational biologists. How has worldwide focus on AI changed the path of research in computational biology? What is the impact on the algorithmic biology research community?}
}
@article{RIVEST19931,
title = {On Choosing between Experimenting and Thinking when Learning},
journal = {Information and Computation},
volume = {106},
number = {1},
pages = {1-25},
year = {1993},
issn = {0890-5401},
doi = {https://doi.org/10.1006/inco.1993.1047},
url = {https://www.sciencedirect.com/science/article/pii/S0890540183710473},
author = {R.L. Rivest and R.H. Sloan},
abstract = {We introduce a model of inductive inference, or learning, that extends the conventional Bayesian approach by explicitly considering the computational cost of formulating predictions to be tested. We view the learner as a scientist who must divide her time between doing experiments and deducing predictions from promising theories, and we wish to know how she can do so most effectively. We explore several approaches based on the cost of making a prediction relative to the cost of performing an experiment. The resulting strategies share many qualitative characteristics with "real" science. This model is significant for the following reasons: •It allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) both performing experiments and making predictions from theories require time and effort.•It lays the foundation for a rigorous machine-implementable notion of "subjective probability." Good (1959, , 443-447) argues persuasively that subjective probability is at the heart of probability theory. Previous treatments of subjective probability do not handle the complication that the learner′s subjective probabilities may change as the result of pure thinking; our model captures this and other effects in a realistic manner. In addition, we begin to answer the question of how to trade off versus -a question that is fundamental for computers that must exist in the world and learn from their experience.}
}
@article{OSINGA2022103298,
title = {Big data in agriculture: Between opportunity and solution},
journal = {Agricultural Systems},
volume = {195},
pages = {103298},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X21002511},
author = {Sjoukje A. Osinga and Dilli Paudel and Spiros A. Mouzakitis and Ioannis N. Athanasiadis},
keywords = {Big data solutions, Precision Agriculture, Case study, Stakeholders, Technological maturity level, Mixed-method approach},
abstract = {CONTEXT
Big data applications in agriculture evolve fast, as more experience, applications, good practices and computational power become available. Actual solutions to real-life problems are scarce. What characterizes the adoption of big data problems to solutions and to what extent is there a match between them?
OBJECTIVE
We aim to assess the conditions of the adoption of big data technologies in agricultural applications, based on the investigation of twelve real-life practical use cases in the precision agriculture and livestock domain.
METHODS
We use a mixed method approach: a case study research around the twelve use cases of Horizon 2020 project CYBELE, varying from precision arable and livestock farming to fishing and food security, and a stakeholder survey (n = 56). Our analysis focuses on four perspectives: (1) the drivers of change that initiated the use cases; (2) the big data characteristics of the problem; (3) the technological maturity level of the solution both at start and end of the project; (4) the stakeholder perspective.
RESULTS AND CONCLUSIONS
Results show that the use cases’ drivers of change are a combination of data-, technology, research- and commercial interests; most have at least a research drive. The big data characteristics (volume, velocity, variety, veracity) are well-represented, with most emphasis on velocity and variety. Technology readiness levels show that the majority of use cases started at experimental or lab environment stage and aims at a technical maturity of real-world small-scale deployment. Stakeholders’ main concern is cost, user friendliness and to embed the solution within their current work practice. The adoption of better-matching big data solutions is modest. Big data solutions do not work out-of-the-box when changing application domains. Additional technology development is needed for addressing the idiosyncrasies of agricultural applications.
SIGNIFICANCE
We add a practical, empirical assessment of the current status of big data problems and solutions to the existing body of mainly theoretical knowledge. We considered the CYBELE research project as our laboratory for this. Our strength is that we interviewed the use case representatives in person, and that we included the stakeholders’ perspective in our results. Large-scale deployments need effective interdisciplinary approaches and long-term project horizons to address issues emerging from big data characteristics, and to avoid compartmentalization of agricultural sciences. We need both an engineering perspective – to make things work in practice – and a systems thinking perspective – to offer holistic, integrated solutions.}
}
@article{CHENG2013267,
title = {Shape-anisotropic particles at curved fluid interfaces and role of Laplace pressure: A computational study},
journal = {Journal of Colloid and Interface Science},
volume = {402},
pages = {267-278},
year = {2013},
issn = {0021-9797},
doi = {https://doi.org/10.1016/j.jcis.2013.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0021979713003056},
author = {Tian-Le Cheng and Yu U. Wang},
keywords = {Capillary forces, Surface tension, Laplace pressure, Diffuse interface field approach, Gibbs–Duhem relation, Shape anisotropy, Pickering emulsions},
abstract = {The self-assembly behavior of shape-anisotropic particles at curved fluid interfaces is computationally investigated by diffuse interface field approach (DIFA). A Gibbs–Duhem-type thermodynamic formalism is introduced to treat heterogeneous pressure within the phenomenological model, in agreement with Young–Laplace equation. Computer simulations are performed to study the effects of capillary forces (interfacial tension and Laplace pressure) on particle self-assembly at fluid interfaces in various two-dimensional cases. For isolated particles, it is found that the equilibrium liquid interface remains circular and particles of different shapes do not disturb the homogeneous curvature of liquid interface, while the equilibrium position, orientation and stability of a particle at the liquid interface depend on its shape and initial location with respect to the liquid interface. For interacting particles, the curvature of local liquid interfaces is different from the apparent curvature of the particle shell; nevertheless, irrespective of the particle shapes, a particle-coated droplet always tends to deform into a circular morphology under positive Laplace pressure, loses mechanical stability and collapses under negative Laplace pressure, while adapts to any morphology and stays in neutral equilibrium under zero Laplace pressure. Finally, the collective behaviors of particles and Laplace pressure evolution in bicontinuous interfacially jammed emulsion gels (bijels) are investigated.}
}
@article{VAHLDICK2020100037,
title = {A blocks-based serious game to support introductory computer programming in undergraduate education},
journal = {Computers in Human Behavior Reports},
volume = {2},
pages = {100037},
year = {2020},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2020.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2451958820300373},
author = {Adilson Vahldick and Paulo Roberto Farah and Maria José Marcelino and António José Mendes},
keywords = {Computer programming learning, Blocks-based approach, Serious games},
abstract = {Blocks-based environments have been used to promote computational thinking (CT) and programming learning mostly in elementary and middle schools. In many countries, like Brazil and Portugal, isolated initiatives have been launched to promote CT learning, but until now there is no evidence of a widespread use of this type of environments. Consequently, it is not common that students that reach higher education nowadays are familiar with CT and programming. This paper presents the development of a serious game to support the learning of basic computer programming. It is a blocks-based environment including also resources that allow the teacher to follow the student’s progress and customize in-game tasks. Four cycles of experiments were conducted, improving both the game and how it was used. Based on the results of these experiences, the key contribution of this paper is a set of fourteen findings and recommendations to the creation and use of a game-based approach to support introductory computer programming learning for novices.}
}
@article{AIZENBERG199987,
title = {One computational approach in support of the Riemann hypothesis},
journal = {Computers & Mathematics with Applications},
volume = {37},
number = {1},
pages = {87-94},
year = {1999},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(98)00244-2},
url = {https://www.sciencedirect.com/science/article/pii/S0898122198002442},
author = {L. Aizenberg and V. Adamchik and V.E. Levit},
keywords = {ς-function, Riemann Hypothesis, Analytic continuation of a function given on a part of its boundary, Holomorphic functions, Conformal mappings, Unit disk, Computational experiments},
abstract = {Some of the results on the criteria for the existence of an analytic continuation into a domain of a function given on a part of its boundary obtained by one of the authors are applied to the Riemann Hypothesis on the zeta-function zeroes. We include all of the basic structural information needed on the previous results on analytic continuation. Some comprehensive numerical experiments have been performed. We have found two important trends in the associated numerical results. The first one is that these findings favor the view that the Riemann Hypothesis is valid. The second one corresponds to a new conjecture on monotonic behavior of some sequences of integrals. The computational experiments have been performed with the Mathematica V3.0.}
}
@article{COURTNEY2025168819,
title = {memerna: Sparse RNA folding including coaxial stacking},
journal = {Journal of Molecular Biology},
volume = {437},
number = {3},
pages = {168819},
year = {2025},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2024.168819},
url = {https://www.sciencedirect.com/science/article/pii/S0022283624004418},
author = {Eliot Courtney and Amitava Datta and David H. Mathews and Max Ward},
keywords = {RNA secondary structure, sparsification, dynamic programming, nearest neighbor, energy model},
abstract = {Determining RNA secondary structure is a core problem in computational biology. Fast algorithms for predicting secondary structure are fundamental to this task. We describe a modified formulation of the Zuker-Stiegler algorithm with coaxial stacking, a stabilising interaction in which the ends of helices in multi-loops are stacked. In particular, optimal coaxial stacking is computed as part of the dynamic programming state, rather than in an inner loop. We introduce a new notion of sparsity, which we call replaceability. Replaceability is a more general condition and applicable in more places than the triangle inequality that is used by previous sparse folding methods. We also introduce non-monotonic candidate lists as an additional sparsification tool. Existing usages of the triangle inequality for sparsification can be thought of as an application of both replaceability and monotonicity together. The modified recurrences along with replaceability allows sparsification to be applied to coaxial stacking as well, which increases the speed of the algorithm. We implemented this algorithm in software we call memerna, which we show to have the fastest exact (non–heuristic) implementation of RNA folding under the complete Turner 2004 model with coaxial stacking, out of several popular RNA folding tools supporting coaxial stacking. We also introduce a new notation for secondary structure which includes coaxial stacking, terminal mismatches, and dangles (CTDs) information. The memerna package 0.1 release is available at https://github.com/Edgeworth/memerna/tree/release/0.1.}
}
@article{CRUJEIRAS2013208,
title = {Challenges in the implementation of a competency-based curriculum in Spain},
journal = {Thinking Skills and Creativity},
volume = {10},
pages = {208-220},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2013.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S187118711300045X},
author = {Beatriz Crujeiras and María Pilar Jiménez-Aleixandre},
keywords = {Scientific competency, Epistemic practices, Higher-order thinking, Policy},
abstract = {This paper addresses some of the challenges involved in implementing the new approach established in the Spanish National Curriculum in 2006, which brought as a major change a focus on the development of key competencies. The paper focuses on scientific competency and the challenges involved in the itinerary from policy documents to classrooms are addressed in three sections: (i) an analysis is made of the changes in the science curriculum as a consequence of the emphasis on scientific competency, comparing the assessment criteria in the previous and current steering documents; (ii) trends in teacher education are discussed; (iii) the findings of the diagnostic evaluation are analyzed. The paper is framed in a theoretical approach, viewing students’ participation in scientific practices, and the development of higher-order thinking as necessary goals of science education. We argue that the focus on competencies, characterized as the ability to apply knowledge and skills in new contexts, involves a major change towards knowledge transfer and higher-order thinking skills. Some issues emerging from the analysis relate to the implications of assessment criteria and the challenges involved in its implementation, to the trends in teacher professional development and the difficulties related to the current economic crisis and to the results of the diagnostic evaluation and time frame needed for reforms to have an impact. It is argued that the development of both competencies and higher-order thinking requires students’ prolonged engagement.}
}
@article{MEDINAOLIVA201338,
title = {PRM-based patterns for knowledge formalisation of industrial systems to support maintenance strategies assessment},
journal = {Reliability Engineering & System Safety},
volume = {116},
pages = {38-56},
year = {2013},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2013.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0951832013000616},
author = {G. Medina-Oliva and P. Weber and B. Iung},
keywords = {Maintenance strategies, Performances analysis, Decision-making, Bayesian Networks (BN), Probabilistic Relational Model (PRM)},
abstract = {The production system and its maintenance system must be now developed on “system thinking” paradigm in order to guarantee that Key Performance Indicators (KPI) will be optimized all along the production system (operation) life. In a recursive way, maintenance system engineering has to integrate also KPI considerations with regards to its own enabling systems. Thus this paper develops a system-based methodology wherein a set of KPIs is computed in order to verify if the objectives of the production and maintenance systems are satisfied. In order to help the decision-making process for maintenance managers, a “unified” generic model have been developed. This model integrates (a) the interactions of the maintenance system with its enabling systems, (b) the impact of the maintenance strategies through the computation of some key performance indicators, and (c) different kinds of knowledge regarding the maintenance system and the system of interest, including quantitative and qualitative knowledge. This methodology is based on an executable unified model built with Probabilistic Relational Model (PRM). PRM allows a modular representation and inferences computation of large size models. The methodology added-value is shown on a test-bench.}
}
@article{DOWNES1993229,
title = {Modeling scientific practice: Paul Thagard's computational approach},
journal = {New Ideas in Psychology},
volume = {11},
number = {2},
pages = {229-243},
year = {1993},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(93)90036-D},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9390036D},
author = {Stephen Downes},
abstract = {In this paper I examine Paul Thagard's computational approach to studying science, which is a contribution to the cognitive science of science. I present several criticisms of Thagard's approach and use them to motivate some suggestions for alternative approaches in cognitive science of science. I first argue that Thagard does not clearly establish the units of analysis of his study. Second, I argue that Thagard mistakenly applies the same model to both individual and group decision making. Finally, I argue that in attempting to account for psychological and social processes as well as providing a philosophical model of successful reasoning Thagard attempts to explain too much with one model, thus straining the plausibility of his model.}
}
@article{SONG2023100812,
title = {Robert Mare’s legacy: Multi-generational processes},
journal = {Research in Social Stratification and Mobility},
volume = {88},
pages = {100812},
year = {2023},
note = {Robert D. Mare’s Legacy},
issn = {0276-5624},
doi = {https://doi.org/10.1016/j.rssm.2023.100812},
url = {https://www.sciencedirect.com/science/article/pii/S0276562423000562},
author = {Xi Song},
abstract = {This paper summarizes some of Robert Mare’s major contributions as a sociologist, demographer, and social statistician; as a pioneer who advanced the multi-generational perspective in social science research; as a leader who introduced demographic thinking to social mobility studies; and as a trailblazer who developed new approaches to studying multi-generational processes}
}
@article{MUNSON2019100736,
title = {After eliciting: Variation in elementary mathematics teachers’ discursive pathways during collaborative problem solving},
journal = {The Journal of Mathematical Behavior},
volume = {56},
pages = {100736},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.100736},
url = {https://www.sciencedirect.com/science/article/pii/S073231231930046X},
author = {Jen Munson},
keywords = {Classroom discourse, Eliciting, Responsiveness, Student understanding},
abstract = {Mathematics teachers are called on to craft instruction that centers students’ mathematical ideas and creates consistent, pervasive opportunities for meaning-making through discourse. In the context of collaborative problem solving, teachers can use eliciting and probing to uncover student thinking while students work together to develop mathematical ideas and strategies. After eliciting and probing, teachers can further respond to the student thinking that has been revealed. This study explored the discursive pathways two fourth grade mathematics teachers used after eliciting student thinking, when their aim was to be responsive to and advance student thinking. Drawing on interactions (n = 97) from nine lessons, qualitative analysis identified five distinct discursive pathways after eliciting, two of which, praise and funneling, were associated with the nature of student understanding uncovered during eliciting. Implications for future research and professional development on teacher-student discourse are discussed.}
}
@incollection{PENN2012143,
title = {Computational Linguistics},
editor = {Ruth Kempson and Tim Fernando and Nicholas Asher},
booktitle = {Philosophy of Linguistics},
publisher = {North-Holland},
address = {Amsterdam},
pages = {143-173},
year = {2012},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51747-0.50005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517470500056},
author = {Gerald Penn}
}
@incollection{DAVIES2025414,
title = {Software Tools for Green and Sustainable Chemistry},
editor = {Béla Török},
booktitle = {Encyclopedia of Green Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {414-425},
year = {2025},
isbn = {978-0-443-28923-1},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00049-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000491},
author = {Joseph C. Davies and Jonathan D. Hirst},
keywords = {Computer aided synthesis planning., Digitalization, Electronic laboratory notebook, Green metrics, Solvent selection},
abstract = {We present the concepts of green and sustainable chemistry and related software tools. Making chemistry greener and more sustainable is a growing priority for researchers and software tools have been developed to aid in this pursuit. Software tools for green and sustainable chemistry have been developed to assess existing methods, propose new ones, and replace some experimental methods altogether with in silico approaches. We discuss the digitalization of chemistry and the computational advances that enable software tools to play a growing role in all aspects of chemical research. Barriers and limitations of current tools are discussed along with future trajectories.}
}
@article{SANCHEZTORRUBIA201212177,
title = {An approach to automatic learning assessment based on the computational theory of perceptions},
journal = {Expert Systems with Applications},
volume = {39},
number = {15},
pages = {12177-12191},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.04.069},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412006665},
author = {M. Gloria Sánchez-Torrubia and Carmen Torres-Blanc and Gracian Trivino},
keywords = {Automatic learning assessment, Computing with words and perceptions, Granular linguistic model of a phenomenon},
abstract = {E-learning systems output a huge quantity of data on a learning process. However, it takes a lot of specialist human resources to manually process these data and generate an assessment report. Additionally, for formative assessment, the report should state the attainment level of the learning goals defined by the instructor. This paper describes the use of the granular linguistic model of a phenomenon (GLMP) to model the assessment of the learning process and implement the automated generation of an assessment report. GLMP is based on fuzzy logic and the computational theory of perceptions. This technique is useful for implementing complex assessment criteria using inference systems based on linguistic rules. Apart from the grade, the model also generates a detailed natural language progress report on the achieved proficiency level, based exclusively on the objective data gathered from correct and incorrect responses. This is illustrated by applying the model to the assessment of Dijkstra’s algorithm learning using a visual simulation-based graph algorithm learning environment, called GRAPHs.}
}
@article{NIKNAM20112805,
title = {Non-smooth economic dispatch computation by fuzzy and self adaptive particle swarm optimization},
journal = {Applied Soft Computing},
volume = {11},
number = {2},
pages = {2805-2817},
year = {2011},
note = {The Impact of Soft Computing for the Progress of Artificial Intelligence},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2010.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S1568494610002875},
author = {Taher Niknam and Hasan Doagou Mojarrad and Hamed Zeinoddini Meymand},
keywords = {Economic dispatch, New adaptive particle swarm optimization (NAPSO), Mutation operator, Multi-fuel effects, Self-adaptive parameter control},
abstract = {Economic dispatch (ED) problem is a nonlinear and non-smooth optimization problem when valve-point effects, multi-fuel effects and prohibited operating zones (POZs) have been considered. This paper presents an efficient evolutionary method for a constrained ED problem using the new adaptive particle swarm optimization (NAPSO) algorithm. The original PSO has difficulties in premature convergence, performance and the diversity loss in optimization process as well as appropriate tuning of its parameters. In the proposed algorithm, to improve the global searching capability and prevent the convergence to local minima, a new mutation is integrated with adaptive particle swarm optimization (APSO). In APSO, the inertia weight is tuned by using fuzzy IF/THEN rules and the cognitive and the social parameters are self-adaptively adjusted. The proposed NAPSO algorithm is validated on test systems consisting of 6, 10, 15, 40 and 80 generators with the objective functions possessing prohibited zones, multi-fuel effects and valve-point loading effects. The research results reveal the effectiveness and applicability of the proposed algorithm to the practical ED problem.}
}
@article{CASH2023101219,
title = {Method in their madness: Explaining how designers think and act through the cognitive co-evolution model},
journal = {Design Studies},
volume = {88},
pages = {101219},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101219},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000601},
author = {Philip Cash and Milene Gonçalves and Kees Dorst},
keywords = {co-evolution, design process(es), design cognition, design thinking, creativity},
abstract = {Designers often face situations where the only way forward is through the exploration of possibilities. However, there is a critical disconnect between understanding of how designer’s think and act in such situations. We address this disconnect by proposing and testing (via protocol analysis) the cognitive co-evolution model. Our model comprises a new approach to co-evolutionary design theory by explaining both the progression of the process itself and the creation of design outputs via an interplay between metacognitive perceived uncertainty, cognition, and the external world. We thus connect explanations of how designers think with descriptions of how they act. We provide a foundation for connecting to other theories, models, and questions in design research via common links to cognition and metacognition.}
}
@article{LASAPONARA202460,
title = {Temperament and probabilistic predictive coding in visual-spatial attention},
journal = {Cortex},
volume = {171},
pages = {60-74},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223002599},
author = {Stefano Lasaponara and Gabriele Scozia and Silvana Lozito and Mario Pinto and David Conversi and Marco Costanzi and Tim Vriens and Massimo Silvetti and Fabrizio Doricchi},
keywords = {Attention, Temperament, Personality, Posner task, Neurotransmitters},
abstract = {Cholinergic (Ach), Noradrenergic (NE), and Dopaminergic (DA) pathways play an important role in the regulation of spatial attention. The same neurotransmitters are also responsible for inter-individual differences in temperamental traits. Here we explored whether biologically defined temperamental traits determine differences in the ability to orient spatial attention as a function of the probabilistic association between cues and targets. To this aim, we administered the Structure of Temperament Questionnaire (STQ-77) to a sample of 151 participants who also performed a Posner task with central endogenous predictive (80 % valid/20 % invalid) or non-predictive cues (50 % valid/50 % invalid). We found that only participants with high scores in Plasticity and Intellectual Endurance showed a selective abatement of attentional costs with non-predictive cues. In addition, stepwise regression showed that costs in the non-predictive condition were negatively predicted by scores in Plasticity and positively predicted by scores in Probabilistic Thinking. These results show that stable temperamental characteristics play an important role in defining the inter-individual differences in attentional behaviour, especially in the presence of different probabilistic organisations of the sensory environment. These findings emphasize the importance of considering temperamental and personality traits in social and professional environments where the ability to control one's attention is a crucial functional skill.}
}
@article{VELDHUIS2025100708,
title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100708},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000771},
author = {Annemiek Veldhuis and Priscilla Y. Lo and Sadhbh Kenny and Alissa N. Antle},
keywords = {Artificial intelligence, Critical literacy, AI ethics, AI literacy, Computational empowerment, Literature review},
abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.}
}
@article{CIRAOLO201378,
title = {A computational method for the Helmholtz equation in unbounded domains based on the minimization of an integral functional},
journal = {Journal of Computational Physics},
volume = {246},
pages = {78-95},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113002258},
author = {Giulio Ciraolo and Francesco Gargano and Vincenzo Sciacca},
keywords = {Helmholtz equation, Transparent boundary conditions, Minimization of integral functionals},
abstract = {We study a new approach to the problem of transparent boundary conditions for the Helmholtz equation in unbounded domains. Our approach is based on the minimization of an integral functional arising from a volume integral formulation of the radiation condition. The index of refraction does not need to be constant at infinity and may have some angular dependency as well as perturbations. We prove analytical results on the convergence of the approximate solution. Numerical examples for different shapes of the artificial boundary and for non-constant indexes of refraction will be presented.}
}
@article{GANGWAL2025,
title = {Artificial Intelligence in Natural Product Drug Discovery: Current Applications and Future Perspectives},
journal = {Journal of Medicinal Chemistry},
year = {2025},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.4c01257},
url = {https://www.sciencedirect.com/science/article/pii/S1520480425001826},
author = {Amit Gangwal and Antonio Lavecchia},
abstract = {Drug discovery, a multifaceted process from compound identification to regulatory approval, historically plagued by inefficiencies and time lags due to limited data utilization, now faces urgent demands for accelerated lead compound identification. Innovations in biological data and computational chemistry have spurred a shift from trial-and-error methods to holistic approaches to medicinal chemistry. Computational techniques, particularly artificial intelligence (AI), notably machine learning (ML) and deep learning (DL), have revolutionized drug development, enhancing data analysis and predictive modeling. Natural products (NPs) have long served as rich sources of biologically active compounds, with many successful drugs originating from them. Advances in information science expanded NP-related databases, enabling deeper exploration with AI. Integrating AI into NP drug discovery promises accelerated discoveries, leveraging AI’s analytical prowess, including generative AI for data synthesis. This perspective illuminates AI’s current landscape in NP drug discovery, addressing strengths, limitations, and future trajectories to advance this vital research domain.
}
}
@article{ZENASNI201149,
title = {Pleasantness of creative tasks and creative performance},
journal = {Thinking Skills and Creativity},
volume = {6},
number = {1},
pages = {49-56},
year = {2011},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2010.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871187110000404},
author = {Franck Zenasni and Todd Lubart},
keywords = {Creativity, Perceived pleasantness, Emotion, Story writing, Divergent thinking},
abstract = {To examine the impact of emotion on creative potential, experimental studies have typically focused on the impact of induced or spontaneous mood states on creative performance. In this report the relationship between the perceived pleasantness of tasks (using divergent thinking and story writing tasks) and creative performance was examined. Overall perceived pleasantness did not differ between tasks. However, results indicate that the perceived pleasantness of the story writing task increased during task completion whereas the perceived pleasantness of divergent thinking tasks remained stable during task performance. The number of generated ideas in a divergent thinking task (fluency) was significantly related to overall perceived pleasantness of the task.}
}
@article{AIMONE2009187,
title = {Computational Influence of Adult Neurogenesis on Memory Encoding},
journal = {Neuron},
volume = {61},
number = {2},
pages = {187-202},
year = {2009},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2008.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0896627308010192},
author = {James B. Aimone and Janet Wiles and Fred H. Gage},
keywords = {SYSNEURO, MOLNEURO, STEMCELL},
abstract = {Summary
Adult neurogenesis in the hippocampus leads to the incorporation of thousands of new granule cells into the dentate gyrus every month, but its function remains unclear. Here, we present computational evidence that indicates that adult neurogenesis may make three separate but related contributions to memory formation. First, immature neurons introduce a degree of similarity to memories learned at the same time, a process we refer to as pattern integration. Second, the extended maturation and change in excitability of these neurons make this added similarity a time-dependent effect, supporting the possibility that temporal information is included in new hippocampal memories. Finally, our model suggests that the experience-dependent addition of neurons results in a dentate gyrus network well suited for encoding new memories in familiar contexts while treating novel contexts differently. Taken together, these results indicate that new granule cells may affect hippocampal function in several unique and previously unpredicted ways.}
}
@article{DONG2022134394,
title = {Understanding robustness in multiscale nutrient abatement: Probabilistic simulation-optimization using Bayesian network emulators},
journal = {Journal of Cleaner Production},
volume = {378},
pages = {134394},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134394},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203966X},
author = {Feifei Dong and Jincheng Li and Chao Dai and Jie Niu and Yan Chen and Jiacong Huang and Yong Liu},
keywords = {Diffuse nutrient, BMPs, Machine learning, Uncertainty, Simulation-optimization, Bayesian network},
abstract = {Ecosystem management in the face of uncertain disturbances has triggered increasing practices of resilience thinking. A multiscale probabilistic simulation-optimization framework is developed based on the nested nature of watersheds to inform decision robustness for Best Management Practices (BMPs). We presented a novel approach using hybrid Bayesian Networks (BNs) as interpretable and probabilistic emulators of process-based models. The hybrid BNs established at the scale of Hydrologic Response Units (HRUs) are embedded into simulation-optimization, whereby we analyze the cost-effectiveness-robustness of candidate BMP strategies at the subbasin scale. The optimal strategy is identified in compliance with water quality standards using watershed-scale integer programming. We apply the approaches in a typical intensively cultivated plateau watershed adjacent to Lake Dianchi, one of the three most eutrophic lakes in China. Our findings suggest that the hybrid BNs, incorporating both quantitative and qualitative information, are reliable emulators of the Soil and Water Assessment Tool (SWAT) in capturing critical pathways of diffuse phosphorus. Tradeoffs among cost, effectiveness, and robustness follow the law of diminishing marginal benefits. The optimum BMP strategies vary with policymakers’ preference toward robustness levels. Our findings indicate that robustness should be accounted for as an additional decision attribute besides costs and pollution mitigation. The benefits of the modeling framework are to (i) reduce over 99% computation complexity and support efficient decision-making under multifaceted uncertainties; (ii) improve interpretability and reliability of machine learning emulators; and (iii) inform policymakers of robustness with the probability of water quality restoration success.}
}
@article{STONE2004781,
title = {Intention, interpretation and the computational structure of language},
journal = {Cognitive Science},
volume = {28},
number = {5},
pages = {781-809},
year = {2004},
note = {2003 Rumelhart Prize Special Issue Honoring Aravind K. Joshi},
issn = {0364-0213},
doi = {https://doi.org/10.1016/j.cogsci.2004.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S036402130400062X},
author = {Matthew Stone},
keywords = {Dialogue, Pragmatics, Tree adjoining grammar},
abstract = {I show how a conversational process that takes simple, intuitively meaningful steps may be understood as a sophisticated computation that derives the richly detailed, complex representations implicit in our knowledge of language. To develop the account, I argue that natural language is structured in a way that lets us formalize grammatical knowledge precisely in terms of rich primitives of interpretation. Primitives of interpretation can be correctly viewed intentionally, as explanations of our choices of linguistic actions; the model therefore fits our intuitions about meaning in conversation. Nevertheless, interpretations for complex utterances can be built from these primitives by simple operations of grammatical derivation. In bridging analyses of meaning at semantic and symbol-processing levels, this account underscores the fundamental place for computation in the cognitive science of language use.}
}
@article{FIORE2006S248,
title = {Multi-scale computational analysis of fluid dynamics in the Toraymyxin adsorption cartridge},
journal = {Journal of Biomechanics},
volume = {39},
pages = {S248},
year = {2006},
note = {Abstracts of the 5th World Congress of Biomechanics},
issn = {0021-9290},
doi = {https://doi.org/10.1016/S0021-9290(06)83940-0},
url = {https://www.sciencedirect.com/science/article/pii/S0021929006839400},
author = {G.B. Fiore and G. Guadagni and M. Soncini and S. Vesentini and A. Redaelli}
}
@article{HOWARD2006464,
title = {Cumulative semantic inhibition in picture naming: experimental and computational studies},
journal = {Cognition},
volume = {100},
number = {3},
pages = {464-482},
year = {2006},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2005.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027705001393},
author = {David Howard and Lyndsey Nickels and Max Coltheart and Jennifer Cole-Virtue},
keywords = {Semantic inhibition, Spoken word production, Picture naming, Competition, Word retrieval, Computational modelling, Priming},
abstract = {We report an experiment in which subjects named 120 pictures, consisting of series of five pictures drawn from each of 24 semantic categories (and intermixed with 45 fillers). The number of intervening trials (lag) between successive presentations of members of the same category varied from two to eight. Subjects' naming latencies were slowed by 30ms for each preceding member of the category. This effect was both cumulative and linear, and unrelated to the lag elapsing since the previous presentation of a category member. These results definitively demonstrate the occurrence of cumulative interference for word retrieval by prior retrieval of other exemplars of the same semantic category—cumulative semantic inhibition. We claim that this inhibition effect could only occur if the spoken word production system possesses three specific properties (competition, priming, and sharing of semantic activation). We provide computational-modelling evidence in support of this claim. We show that no current theory of spoken word production has all of these properties. In their current form, all these theories are falsified by these results. We briefly discuss the obstacles that may be encountered by current models were they modified to account for our findings.}
}
@article{HSU2011380,
title = {The probabilistic analysis of language acquisition: Theoretical, computational, and experimental analysis},
journal = {Cognition},
volume = {120},
number = {3},
pages = {380-390},
year = {2011},
note = {Probabilistic models of cognitive development},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711000734},
author = {Anne S. Hsu and Nick Chater and Paul M.B. Vitányi},
keywords = {Child language acquisition, Poverty of the stimulus, No negative evidence, Bayesian models, Minimum description length, Simplicity principle, Natural language, Probabilistic models, Identification in the limit},
abstract = {There is much debate over the degree to which language learning is governed by innate language-specific biases, or acquired through cognition-general principles. Here we examine the probabilistic language acquisition hypothesis on three levels: We outline a novel theoretical result showing that it is possible to learn the exact generative model underlying a wide class of languages, purely from observing samples of the language. We then describe a recently proposed practical framework, which quantifies natural language learnability, allowing specific learnability predictions to be made for the first time. In previous work, this framework was used to make learnability predictions for a wide variety of linguistic constructions, for which learnability has been much debated. Here, we present a new experiment which tests these learnability predictions. We find that our experimental results support the possibility that these linguistic constructions are acquired probabilistically from cognition-general principles.}
}
@incollection{1991344,
title = {Appendix A - Scientific chaos: a new way of thinking about dynamics},
editor = {Ralph D. Stacey},
booktitle = {The Chaos Frontier},
publisher = {Butterworth-Heinemann},
pages = {344-365},
year = {1991},
isbn = {978-0-7506-0139-9},
doi = {https://doi.org/10.1016/B978-0-7506-0139-9.50021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780750601399500212}
}
@article{GRETREGAMEY2024104978,
title = {Key factors to enhance efficacy of 3D digital environments for transformative landscape and urban planning},
journal = {Landscape and Urban Planning},
volume = {244},
pages = {104978},
year = {2024},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2023.104978},
url = {https://www.sciencedirect.com/science/article/pii/S0169204623002979},
author = {Adrienne Grêt-Regamey and Nora Fagerholm},
abstract = {The unprecedented expansion of digital technologies has led to a rapid increase in the development and application of 3D digital environments for landscape and urban planning in the past two decades. Considering the significant challenges in guiding human societies towards sustainability, these technologies must not only assist decision-makers in adapting to changes but promote fast, transformative shifts in the relationship between human societies and nature. Based on a set of global exemplars, this Perspective Essay outlines six key factors that can enhance efficacy of 3D digital environments to guide knowledge-informed landscape and urban planning. We call for (1) explicitly representing dynamic interplay between the social, ecological, and technical systems, (2) exploring the integration of design with simulation models to address cross-scale dynamics, (3) developing features to foster imagination, (4) employing multisensory stimuli to encourage profound changes in environmentally and socially sustainable behavior, (5) tailoring the incorporation of active sensing by and with non-experts into 3D digital environments to better acknowledge indigenous and local knowledge systems, and finally, (6) carrying out a usability evaluation to facilitate participation and collaboration in an efficient co-creation process. We conclude by recommending the establishment of a collaborative knowledge platform that unites researchers, developers, and stakeholders for stimulating social-ecological-technological system thinking in the development of 3D digital environments and harnessing the technological advancements to accelerate and drive the needed transformative change within urban and landscape planning.}
}
@article{SAHA2024,
title = {Predicting depression level based on human activities and feelings: A fuzzy logic-based analysis},
journal = {Data Science and Management},
year = {2024},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2024.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S266676492400064X},
author = {Urmi Saha and Syed Mohammod {Minhaz Hossain} and Iqbal H. Sarker},
keywords = {Depression Level Prediction, Human Activities, Human Feelings, Pearson Correlation Method, R-Squared Method, Uncertainty, Fuzzy Rules},
abstract = {Millions of individuals die by suicide each year, and many more suffer from severe depression. Furthermore, these deaths harm education, the economy, and healthcare worldwide. An individual’s persistent feelings and activities, which include sleep disorders such as insomnia, sleeping overtime, or spending the majority of time lying down; pessimistic thinking about the future; and thoughts of committing suicide, help uncover the cause of depression. Several attempts have been made to prevent these losses and deaths. Our study proposes a simple fuzzy inference model that accurately predicts depression levels based on emotions and activities—even with incomplete data and uncertainties—and enhances mental health prediction, bridging theory and practice effectively. Psychologists and professors collaborated to create a survey to collect data for this study. The experiment was conducted using a Google survey form. This method effectively captures the ambiguity and imprecision in depression evaluation by combining linguistic elements of psychological traits. Using the Pearson correlation and R-squared methods, 15 features were chosen from 30 features, followed by five membership functions (poor, mediocre, average, decent, and good) and fuzzy rules to evaluate and create accurate forecasts of depression severity. Our proposed architecture can correctly classify depression levels based on human activities and feelings with 94% accuracy using a less sophisticated rules dictionary than previous pre-trained or hybrid models. Fuzzy logic performs better here by accurately categorizing ambiguous human emotional inputs into distinct degrees.}
}
@article{TAGHIKHAH2022101854,
title = {Machine-assisted agent-based modeling: Opening the black box},
journal = {Journal of Computational Science},
volume = {64},
pages = {101854},
year = {2022},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2022.101854},
url = {https://www.sciencedirect.com/science/article/pii/S1877750322002137},
author = {Firouzeh Taghikhah and Alexey Voinov and Tatiana Filatova and J. Gareth Polhill},
keywords = {Behavioral analytics, Social communications, Interpretable artificial intelligence, Conceptual modeling, Systems thinking},
abstract = {While agent-based modeling (ABM) has become one of the most powerful tools in quantitative social sciences, it remains difficult to explain their structure and performance. We propose to use artificial intelligence both to build the models from data, and to improve the way we communicate models to stakeholders. Although machine learning is actively employed for pre-processing data, here for the first time, we used it to facilitate model development of a simulation model directly from data. Our suggested framework, ML-ABM accounts for causality and feedback loops in a complex nonlinear system and at the same time keeps it transparent for stakeholders. As a result, beside the development of a behavioral ABM, we open the ‘blackbox’ of purely empirical models. With our approach, artificial intelligence in the simulation field can open a new stream in modeling practices and provide insights for future applications.}
}
@incollection{FEIGENSON201113,
title = {Chapter 2 - Objects, Sets, and Ensembles},
editor = {Stanislas Dehaene and Elizabeth M. Brannon},
booktitle = {Space, Time and Number in the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {13-22},
year = {2011},
isbn = {978-0-12-385948-8},
doi = {https://doi.org/10.1016/B978-0-12-385948-8.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859488000025},
author = {Lisa Feigenson},
abstract = {Publisher Summary
This chapter discusses how different types of entities such as individual objects, sets of objects, and ensembles can function as items to be maintained in working memory (WM). All of these types of representations can be relevant to thinking about quantities, and each supports different kinds of quantity-relevant computations. One way to overcome the three- to four-item limit of WM is to bind together representations of individual objects into representations of sets of objects. Binding multiple individuals into a single higher-order group can increase the number of individual items that can be remembered, as in the well-known demonstrations of chunking by adults. Set representations play a critical role in many numerical processes, including representing the nested relationships between as well as representing the cardinality of an array. Many real-world scenes contain stimuli that do not lend themselves to representation qua individual objects or sets of objects. One can imagine enumerating a flock containing dozens of individual birds.}
}
@article{KADUWELA2024105337,
title = {Application of a human-centered design for embedded machine learning model to develop data labeling software with nurses: Human-to-Artificial Intelligence (H2AI)},
journal = {International Journal of Medical Informatics},
volume = {183},
pages = {105337},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105337},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623003556},
author = {Naomi A. Kaduwela and Susan Horner and Priyansh Dadar and Renee C.B. Manworren},
keywords = {Clinical decision support software, Data labeling, Human-centered Design for Embedded Machine Learning Solutions Machine Learning, Machine learning models},
abstract = {Background
Nurses are essential for assessing and managing acute pain in hospitalized patients, especially those who are unable to self-report pain. Given their role and subject matter expertise (SME), nurses are also essential for the design and development of a supervised machine learning (ML) model for pain detection and clinical decision support software (CDSS) in a pain recognition automated monitoring system (PRAMS). Our first step for developing PRAMS with nurses was to create SME-friendly data labeling software.
Purpose
To develop an intuitive and efficient data labeling software solution, Human-to-Artificial Intelligence (H2AI).
Method
The Human-centered Design for Embedded Machine Learning Solutions (HCDe-MLS) model was used to engage nurses. In this paper, HCDe-MLS will be explained using H2AI and PRAMS as illustrative cases.
Findings
Using HCDe-MLS, H2AI was developed and facilitated labeling of 139 videos (mean = 29.83 min) with 3189 images labeled (mean = 75 s) by 6 nurses. OpenCV was used for video-to-image pre-processing; and MobileFaceNet was used for default landmark placement on images. H2AI randomly assigned videos to nurses for data labeling, tracked labelers’ inter-rater reliability, and stored labeled data to train ML models.
Conclusions
Nurses’ engagement in CDSS development was critical for ensuring the end-product addressed nurses’ priorities, reflected nurses’ cognitive and decision-making processes, and garnered nurses’ trust for technology adoption.}
}
@article{GOLDBACH2016249,
title = {Computational Cutting Pattern Generation Using Isogeometric B-Rep Analysis},
journal = {Procedia Engineering},
volume = {155},
pages = {249-255},
year = {2016},
note = {TENSINET – COST TU1303 International Symposium 2016 "Novel structural skins - Improving sustainability and efficiency through new structural textile materials and designs"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816321671},
author = {Ann-Kathrin Goldbach and Michael Breitenberger and Armin Widhammer and Kai-Uwe Bletzinger},
keywords = {Cutting pattern generation, Variation of Reference Strategy, Isogeometric Analysis, Isogeometric B-Rep Analysis, Design cycle of structural membranes},
abstract = {The cutting pattern plays a major role for the design of structural membranes, since it influences both their aesthetical appearance and structural behavior. A novel approach towards cutting pattern generation is the so-called Variation of Reference Strategy (VaReS) [1], which minimizes the total potential energy arising from the motion of a planar cutting pattern to its corresponding three-dimensional shape. With non-uniform rational B-Splines (NURBS) being the standard tool for geometry description in CAD, it is only consequent to use these for analysis as well. Isogeometric B-Rep Analysis (IBRA) [2] follows up on this idea and enriches the original Isogeometric Analysis (IGA), which was introduced by Hughes et al. [3], by the possibility of analysing trimmed NURBS geometries. This paper presents cutting pattern generation with the Variation of Reference Strategy in the context of IGA/IBRA. With this approach, the whole design of a membrane structure can be represented by NURBS geometries – including blueprint plans. To use the benefits of IBRA for cutting pattern generation, a NURBS-based membrane-element was developed for the VaReS routine. A developable surface serves as a benchmark example, since its analytical cutting pattern is known. Examples of double-curved geometries show the applicability and benefits of the proposed procedure for real structures.}
}
@article{SILVEIRA1980165,
title = {Generic masculine words and thinking},
journal = {Women's Studies International Quarterly},
volume = {3},
number = {2},
pages = {165-178},
year = {1980},
note = {The voices and words of women and men},
issn = {0148-0685},
doi = {https://doi.org/10.1016/S0148-0685(80)92113-2},
url = {https://www.sciencedirect.com/science/article/pii/S0148068580921132},
author = {Jeanette Silveira},
abstract = {Synopsis
It has been alleged that, in appropriate verbal contexts, man and he are generic, i.e. that the words include women as well as men, as for example in, Man is mortal, or One must watch his language. Many feminists argue for the elimination of this generic use of man and he and the substitution of such non-male words as people and they. Others argue on various grounds that these changes are unnecessary. This paper isolates the issues involved in such arguments and provisionally concludes that a reduction in the generic use of man and he would result in a long term reduction in sexist thinking. Recent feminist research on man and he is carefully reviewed. In its final section, the paper develops the implication that women experience more alienation than men in the presence of the generic man and he.}
}
@article{PILA201952,
title = {Learning to code via tablet applications: An evaluation of Daisy the Dinosaur and Kodable as learning tools for young children},
journal = {Computers & Education},
volume = {128},
pages = {52-62},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518302422},
author = {Sarah Pila and Fashina Aladé and Kelly J. Sheehan and Alexis R. Lauricella and Ellen A. Wartella},
keywords = {Apps, Coding, Computational thinking, Digital games, Educational technology, STEM},
abstract = {Despite the growing number of digital apps designed to teach coding skills to young children, we know little about their effectiveness. To formally explore this question, we conducted a naturalistic observation of a one-week program designed to teach foundational coding skills (i.e., sequencing, conditions, loops) to young children (N = 28, Mage = 5.15 years) using two tablet applications: Daisy the Dinosaur and Kodable. Pre- and post-assessments measured familiarity with technology, appeal of coding apps, knowledge of Daisy commands, ability to play Kodable, and conceptual understanding of coding. Participants improved in their knowledge of Daisy commands (i.e., move, grow, jump) and Kodable gameplay (i.e., placing arrows in the correct sequence to move a character through a maze), but did not improve in their ability to verbally explain what coding is. Appeal of the games was significantly related to children's learning of Daisy commands, but child gender was not related to either Daisy or Kodable learning outcomes. Results suggest that young children can learn foundational coding skills via apps, especially when the apps are appealing to children.}
}
@article{DEBARROS2012171,
title = {Quantum-like model of behavioral response computation using neural oscillators},
journal = {Biosystems},
volume = {110},
number = {3},
pages = {171-182},
year = {2012},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2012.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264712001736},
author = {J. Acacio {de Barros}},
keywords = {Disjunction effect, Quantum cognition, Quantum-like model, Neural oscillators, Stimulus-response theory},
abstract = {In this paper we propose the use of neural interference as the origin of quantum-like effects in the brain. We do so by using a neural oscillator model consistent with neurophysiological data. The model used was shown elsewhere to reproduce well the predictions of behavioral stimulus-response theory. The quantum-like effects are brought about by the spreading activation of incompatible oscillators, leading to an interference-like effect mediated by inhibitory and excitatory synapses.}
}
@article{REICHLE20062,
title = {Computational models of eye-movement control during reading: Theories of the “eye–mind” link},
journal = {Cognitive Systems Research},
volume = {7},
number = {1},
pages = {2-3},
year = {2006},
note = {Models of Eye-Movement Control in Reading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2005.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041705000227},
author = {Erik D. Reichle}
}
@article{RIAUX2023130189,
title = {Riding the waves of discomforts: Reflecting on the dialogue of hydrologists with society},
journal = {Journal of Hydrology},
volume = {626},
pages = {130189},
year = {2023},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2023.130189},
url = {https://www.sciencedirect.com/science/article/pii/S0022169423011319},
author = {Jeanne Riaux and Marcel Kuper and Sylvain Massuel and Insaf Mekki},
keywords = {Socio-hydrology, Society, Hydrology, Interdisciplinarity, Reflexivity, Tunisia},
abstract = {Although there is a deep historical relationship between hydrology and society, the relationship has considerably evolved in the last three decades. Hydrologists, in particular those involved in designing of decision-support tools, are experiencing a widening gap between an academic discipline which has progressively moved away from field-based applied natural science to computational hydrology, and the multiplication of stakeholders involved in the water-related issues addressed by research. The challenge for hydrology is now to negotiate this shift and to rethink its engagement in society. This paper provides a description of a planned process designed to improve hydrology-society interactions and to foster reflexivity in socio-hydrology. Based on an interdisciplinary reflexive process undertaken in Tunisia from 2016 to 2020, we identified three types of discomforts in the dialogue with society, inviting scientists to lucidly engage with these discomforts. We formulated four key reflexive propositions to achieve a better alignment of scientific stance, research practices and discourse. The first proposition concerns the need to explain more clearly the value systems scientists engage in and with society. The second concerns the need to position hydrology in society and not outside it, by reconsidering the functions that research fulfils in society. The third is an invitation to redefine the perimeter of the research interlocutors and the way to reach them. The fourth is to revisit scientific practices to build on the strengths of the dialogue between field-based natural science and computational hydrology. The paper concludes that adopting a reflexive posture towards these four dimensions of the dialogue between hydrology and society is an effective way to overcome discomforts and to refocus research stance, practices and discourse. It is a way to renew hydrology's place in society and to contribute to the current thinking in socio-hydrology initiated by hydrologists.}
}
@article{KOOLSCHIJN2024101453,
title = {Resources, costs and long-term value: an integrative perspective on serotonin and meta-decision making},
journal = {Current Opinion in Behavioral Sciences},
volume = {60},
pages = {101453},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101453},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001049},
author = {Renée S Koolschijn and Bertalan Polner and Julie M Hoomans and Roshan Cools and Eliana Vassena and Hanneke EM {den Ouden}},
abstract = {Serotonin has been associated with a wide range of neural computations and behaviours, yet an overarching function of this neurotransmitter has been hard to pinpoint. Here, we combine recent theories and findings on serotonin and propose a framework where serotonin integrates information on resource availability and state value to represent a cost–benefit trade-off at the neural level. Critically, this framework supports meta-decision making, that is, the flexible allocation of resources to decision-making. We highlight a computational and neural implementation of this framework, and through this novel, lens interpret empirical findings in the domains of controllability and persistence.}
}
@incollection{MYUNG20012453,
title = {Computational Approaches to Model Evaluation},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2453-2457},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00589-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005891},
author = {I.J. Myung},
abstract = {The induction problem of inferring a predictive function (i.e., model) from finite data is a central component of the scientific enterprise in cognitive science, computer science and statistics, and yet the problem is fundamentally ill posed. Many models can often provide equally good fits to a given observed data set but they may differ considerably in their ability to generalize to new, as yet unseen, data sets generated from the same underlying process. To make this inductive inference problem well posed one needs to define a justifiable measure of generalizability and then use the measure to choose among a set of competing models. Many such measures have been proposed in the past, notably by scientists in the fields of machine learning and algorithmic coding theory. A representative list of such approaches includes the structural risk minimization method and Vapnik-Chervonenkis dimension, the regularization theory, and the minimum description length principle. This article presents a review of these computational approaches to model evaluation. Also discussed are the interesting connections between the computational approaches and some of the statistical approaches to model evaluation such as the Akaike information criterion, the Bayesian information criterion and Bayesian model selection.}
}
@article{ANANTHASWAMY201742,
title = {That's a termite colony between your ears},
journal = {New Scientist},
volume = {233},
number = {3112},
pages = {42-43},
year = {2017},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(17)30275-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407917302750},
author = {Anil Ananthaswamy},
abstract = {After wrestling with the nature of the mind for over half a century, Daniel Dennett uploads his latest thinking on consciousness, word-based “mind viruses” and why we must doubt the power of artificial intelligence}
}
@article{JONES20171,
title = {Diversity not quantity in caregiver speech: Using computational modeling to isolate the effects of the quantity and the diversity of the input on vocabulary growth},
journal = {Cognitive Psychology},
volume = {98},
pages = {1-21},
year = {2017},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2017.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028516302274},
author = {Gary Jones and Caroline F. Rowland},
keywords = {Input quantity, Lexical diversity, Vocabulary acquisition, CLASSIC, Language acquisition},
abstract = {Children who hear large amounts of diverse speech learn language more quickly than children who do not. However, high correlations between the amount and the diversity of the input in speech samples makes it difficult to isolate the influence of each. We overcame this problem by controlling the input to a computational model so that amount of exposure to linguistic input (quantity) and the quality of that input (lexical diversity) were independently manipulated. Sublexical, lexical, and multi-word knowledge were charted across development (Study 1), showing that while input quantity may be important early in learning, lexical diversity is ultimately more crucial, a prediction confirmed against children’s data (Study 2). The model trained on a lexically diverse input also performed better on nonword repetition and sentence recall tests (Study 3) and was quicker to learn new words over time (Study 4). A language input that is rich in lexical diversity outperforms equivalent richness in quantity for learned sublexical and lexical knowledge, for well-established language tests, and for acquiring words that have never been encountered before.}
}
@article{RIETMAN2003249,
title = {Analog computation with rings of quasiperiodic oscillators: the microdynamics of cognition in living machines},
journal = {Robotics and Autonomous Systems},
volume = {45},
number = {3},
pages = {249-263},
year = {2003},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2003.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0921889003001520},
author = {Edward A. Rietman and Mark W. Tilden and Manor Askenazi},
keywords = {Quasiperiodic oscillators, Microdynamics, Schmitt trigger},
abstract = {We describe experimental results to demonstrate the wide-ranging computational ability of quasiperiodic oscillators built from rings of differentiating Schmitt triggers. We describe a theoretical model based on necklace functions to compute the number of states supportable by a ring circuit of a given size. Experimental results are presented to demonstrate that probabilistic state machines can be built from these ring circuits. Other experimental results are given to demonstrate that the rings can model spiking neural network circuits.}
}
@incollection{BUCHANAN2024,
title = {Semantic Feature Production Norms},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00045-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041000454},
author = {Erin M. Buchanan},
keywords = {Datasets, Features, Knowledge, Memory, Norms, Production task, Semantics},
abstract = {Feature production norms are collected by asking participants to name the defining features of a concept, such as tail, fur, and meow for the concept cat. Collection and analysis of features should include special considerations for methodology and data processing including the task instructions, segmentation, word removal, spell checking and more. The processed data can be used to define concept similarity, control stimuli for new experimental studies, and in computational models of memory and knowledge representation.}
}
@incollection{2016295,
title = {10 - Computational fluid dynamics in aerospace field and CFD-based multidisciplinary simulations},
editor = {Qun Zhang and Song Cen},
booktitle = {Multiphysics Modeling},
publisher = {Academic Press},
address = {Oxford},
pages = {295-328},
year = {2016},
series = {Elsevier and Tsinghua University Press Computational Mechanics Series},
isbn = {978-0-12-407709-6},
doi = {https://doi.org/10.1016/B978-0-12-407709-6.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077096000109},
keywords = {aerospace engineering, finite volume method, ALE formulation, discrete geometric conservation law, mesh deformation, remeshing, flapping wing, store separation, wing flutter},
abstract = {In this chapter, the ALE formulation of the finite volume method is proposed for the simulation of compressible fluid flow. Two major topics of the discrete geometric conservation lawgeometric conservation law and mesh deformationmesh deformation algorithm in this chapter are to handle the moving boundary problem accurately and efficiently. Three examples are given to verify the effectiveness of the presented methods in multiphysics simulation for aerospace engineering problems.}
}
@article{CORREABAENA20181410,
title = {Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing},
journal = {Joule},
volume = {2},
number = {8},
pages = {1410-1420},
year = {2018},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2542435118302289},
author = {Juan-Pablo Correa-Baena and Kedar Hippalgaonkar and Jeroen {van Duren} and Shaffiq Jaffer and Vijay R. Chandrasekhar and Vladan Stevanovic and Cyrus Wadia and Supratik Guha and Tonio Buonassisi},
keywords = {accelerated materials development, machine learning, artificial intelligence, energy materials},
abstract = {Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by ten times or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return on investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-performance computing concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research, with considerable first-mover advantages at stake.}
}
@article{PEZZULO2011275,
title = {Computational explorations of perceptual symbol systems theory},
journal = {New Ideas in Psychology},
volume = {29},
number = {3},
pages = {275-297},
year = {2011},
note = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2009.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X09000336},
author = {Giovanni Pezzulo and Gianguglielmo Calvi},
keywords = {Perceptual symbol systems, Schemas, Embodiment, Anticipation, Simulation},
abstract = {The aim of this paper is twofold. First, we provide a methodological pathway from theories of situated, embodied cognition to simulations with an eye to empirical evidence, and suggest a possible cross-fertilization between cognitive robotics and psychology. Psychological theories, in particular those formulated at an abstract level, include models which are often severely underspecified at the level of mechanisms. This is true in the synchronic, constructive perspective (how can the effects observed in experiments be concretely generated by the model's mechanisms?) and in the diachronic, developmental perspective (how can such mechanisms be learned and developed?). The synthetic method of artificial cognitive systems research, and in particular of cognitive robotics, can complement research in psychology (and neurosciences) by exploring the constructive and developmental aspects of theories. Our second aim is to provide an example of such a methodology by describing simulations aiming at developing a perceptual symbol system (PSS) (Barsalou, 1999). We then describe the two main theoretical constructs of the PSS, perceptual symbols and simulators, illustrate their development in an artificial system, and test the system in prediction, categorization, and abstraction tasks.}
}
@article{JIANG2023119343,
title = {Film cooling comparison of shaped holes among the pressure surface, the suction surface and the leading edge of turbine vane},
journal = {Applied Thermal Engineering},
volume = {219},
pages = {119343},
year = {2023},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2022.119343},
url = {https://www.sciencedirect.com/science/article/pii/S135943112201273X},
author = {Yan Jiang and Haiwang Li and Runzhou Liu and Zhi Tao and Zhiyu Zhou},
keywords = {Film cooling, NHFR, Shaped holes, Turbine guide vane},
abstract = {The present study employed commercial computational fluid dynamics software ANSYS 2019R3 to explore the adiabatic film cooling effectiveness and the net heat flux reduction (NHFR) for the comparison of the five selected shaped holes and conventional cylindrical holes between the pressure surface, the suction surface and the leading edge. Amo4ng the shape parameters of shaped holes, the lateral divergence angle (β) and the forward divergence angle (δ) were fixed as 12° and 7° in all shaped holes structures, respectively. The others varied with different regions of the turbine vane. Results showed that different holes fit different positions of vanes. On the suction surface, laidback holes performed the worst net heat flux reduction in most blowing ratios conditions, which indicated the forward divergence angle was not conducive to the flow field and heat transfer characteristics on the suction surface. Whereas, the lateral divergence angle was beneficial to the film cooling and heat transfer characteristics. Laidback fan-shaped holes performed the best adiabatic film cooling effectiveness, but once simultaneously thinking about the heat transfer, fan-shaped holes performed better in net heat flux reduction due to less vortices at holes exit. On the leading edge, the divergence angle towards the upper wall (the lateral divergence angle of spanwise expansion holes) of vanes was not conducive to steady flow. And conical holes performed best, which indicated that coolant from holes with axial divergence angles (the lateral divergence angle in axial direction of conical holes) under the influence of mainstream impact could perform higher film cooling effectiveness and more stable flow fields. On the pressure surface, holes had a lateral divergence angle in the direction of vane height, which was conducive to increasing the coolant coverage area and improving the ability to attach to the pressure surface. Additionally, the laidback hole case was observed the lowest net heat flux reduction when the blowing ratio was less than 2, which revealed that holes that expanded only in flow direction was not conducive to film cooling and heat transfer characteristics.}
}

@article{CAMARENA2020122574,
title = {Artificial intelligence in the design of the transitions to sustainable food systems},
journal = {Journal of Cleaner Production},
volume = {271},
pages = {122574},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122574},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620326214},
author = {Stéphanie Camaréna},
keywords = {Artificial intelligence, Design ethics, Transdisciplinary research, Design for sustainability, Sustainable food systems, Systems thinking},
abstract = {Food systems and our ability to secure food and nutrition for current and future generations is challenged by population growth, climate change, resource depletion and pollution. The current agricultural and supply chain systems are one of the main contributors to the issues. Transformational, not incremental change is needed to transition to sustainable food systems capable of feeding close to 10 billion people in less than 30 years. Artificial intelligence (AI) is pervading all parts of food systems in ways that indicate transformative system changes are possible. Designers, as mediators between people, technology and the environment have a responsibility to recognise and reflect on ways AI could bring the change needed to move to sustainable food systems. This literature review is situated at the intersection of Food systems, Design, Artificial Intelligence and Sustainability. The transdisciplinary approach reveals what exists across the disciplines, what can be done with AI to transition to sustainable food systems, how Design proposes to approach the change, and which ethical or philosophical considerations start to emerge. The discussion reflects on AI as a potential leverage point to bring changes in the system and on the designer's role in establishing the human-technology-environmental relationships. Further research and recommendations are provided.}
}
@article{SOMMER2013e201302014,
title = {MEMBRANE PACKING PROBLEMS: A SHORT REVIEW ON COMPUTATIONAL MEMBRANE MODELING METHODS AND TOOLS},
journal = {Computational and Structural Biotechnology Journal},
volume = {5},
number = {6},
pages = {e201302014},
year = {2013},
issn = {2001-0370},
doi = {https://doi.org/10.5936/csbj.201302014},
url = {https://www.sciencedirect.com/science/article/pii/S2001037014600428},
author = {Björn Sommer},
abstract = {The use of model membranes is currently part of the daily workflow for many biochemical and biophysical disciplines. These membranes are used to analyze the behavior of small substances, to simulate transport processes, to study the structure of macromolecules or for illustrative purposes. But, how can these membrane structures be generated? This mini review discusses a number of ways to obtain these structures. First, the problem will be formulated as the Membrane Packing Problem. It will be shown that the theoretical problem of placing proteins and lipids onto a membrane area differ significantly. Thus, two sub-problems will be defined and discussed. Then, different – partly historical – membrane modeling methods will be introduced. And finally, membrane modeling tools will be evaluated which are able to semi-automatically generate these model membranes and thus, drastically accelerate and simplify the membrane generation process. The mini review concludes with advice about which tool is appropriate for which application case.}
}
@article{CHEN20071153,
title = {Computationally intelligent agents in economics and finance},
journal = {Information Sciences},
volume = {177},
number = {5},
pages = {1153-1168},
year = {2007},
note = {Including: The 3rd International Workshop on Computational Intelligence in Economics and Finance (CIEF’2003)},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2006.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020025506002301},
author = {Shu-Heng Chen},
keywords = {Computational intelligence, Agent-based computational economics},
abstract = {This paper is an editorial guide for the second special issue on Computational Intelligence in economics and finance, which is a continuation of the special issue of Information Sciences, Vol. 170, No. 1. This second issue appears as a part of the outcome from the 3rd International Workshop on Computational Intelligence in Economics and Finance, which was held in Cary, North Carolina, September 26–30, 2003. This paper offers some main highlights of this event, with a particular emphasis on some of the observed progress made in this research field, and a brief introduction to the papers included in this special issue.}
}
@article{YUNG2021101566,
title = {A visual approach to interpreting the career of the network metaphor},
journal = {Poetics},
volume = {88},
pages = {101566},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101566},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000498},
author = {Vincent Yung},
keywords = {Metaphor, Network, Formal model, Computational hermeneutics, Text analysis},
abstract = {Metaphor has had a prolific presence in sociology: as a subject of empirical study, as a tool for theorizing, and as ideas that live public lives beyond the ivory tower. Despite all this work, tracing the core dynamics of metaphors over time remains a persistent challenge. To address this problem, I propose a systematic analysis of metaphor anchored around four core questions. What do metaphors describe? How do metaphors change over time? What do metaphors do? And how do conventional metaphors emerge? I address these questions in the context of one persistent and prolific metaphor: the network metaphor. To do so, I offer a formal model of the career of metaphor by drawing on cognitive linguistics and sociological theories of public ideas and professional careers. Specifically, I integrate the Google Books Ngram corpus with computational techniques that leverage co-occurrence to visualize and interpret the career of the network metaphor with respect to four areas: its jurisdiction, temporality, ecology, and conventionality. I leverage the network metaphor as both a revelatory case for evaluating my model and a critical case for deepening our historical understanding of the network. My analysis lends validation to the argument that the network metaphor has achieved the status of a broad category for thinking about contemporary life. However, it also demonstrates that the network metaphor had a lively history prior to the twentieth century. This includes significant changes in how it was used in the first half of the nineteenth century, a career standing in for anatomical structures of the body in the mid-nineteenth century, and a predominant expression in simile form prior to the start of the twentieth century.}
}
@incollection{KALBFLEISCH2010641,
title = {2.33 - Genomics, Bioinformatics, and Computational Biology},
editor = {Charlene A. McQueen},
booktitle = {Comprehensive Toxicology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {641-661},
year = {2010},
isbn = {978-0-08-046884-6},
doi = {https://doi.org/10.1016/B978-0-08-046884-6.00236-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080468846002360},
author = {T.S. Kalbfleisch and G.A. Rempala and K.S. Ramos},
abstract = {In this chapter, we describe how computational biology can be aided by informatics infrastructure to provide the basis for in silico studies that no longer require the generation of data, and instead facilitate the collection, organization, and analysis of existing datasets that can drive discovery. A new reality is that we are awash in data and tools to analyze these data and one of the most significant challenges is that of enabling the researcher to discover datasets relevant to their work, collect these data, assess its quality, and analyze it. The development of an adequate infrastructure within a researcher’s institution greatly facilitates progress on this front, both in terms of the development of tools and the development of local informatics expertise necessary to complement the domain-specific expertise of the researcher. As an informatics community we often ponder the heterogeneity of tools and resources on a global scale at the expense of the more immediate local problems encountered on a routine basis. Here, we suggest that getting our own houses in order by first employing interoperable solutions that support and facilitate collaboration amongst the complementary disciplines within our own institutions places the informatics community in a better position to address global informatics challenges. This approach can ensure that the solutions implemented employ an architecture and standards that support interoperability. Indeed, this is an organizational and cultural challenge rather than a technological one. Organizational structure and practices are described that provide a comprehensive base of talent capable of creating an environment that supports a sustainable informatics infrastructure, and that can quickly grow as needed to support the specific and rapidly evolving needs unique to that institution.}
}
@article{CARNEY2004135,
title = {Denis Noble discusses his career in computational biology},
journal = {Drug Discovery Today: BIOSILICO},
volume = {2},
number = {4},
pages = {135-137},
year = {2004},
issn = {1741-8364},
doi = {https://doi.org/10.1016/S1741-8364(04)02414-X},
url = {https://www.sciencedirect.com/science/article/pii/S174183640402414X},
author = {Stephen Carney},
keywords = {Interview, computer modeling, grid computing, cardiac electrophysiology, whole organ models, arrhythmia},
abstract = {Denis Noble was born in 1936 and obtained a BSc and PhD from University College London. He is one of the pioneers of computational biology related to cardiac cell electrophysiology and its incorporation into the first detailed biophysical models of the whole organ. He has made many major contributions to this work spanning from his groundbreaking work in 1960, showing that in heart, contrary to the situation in nerve, the first effect of membrane depolarisation is to greatly reduce potassium conductance, which in turn is greatly dependent on plasma potassium levels. His work over the last forty years has culminated in a highly successful virtual model of the heart, which has allowed theoretical interpretation of cardiac arrhythmias and the development of antiarrhythmic drugs. Professor Noble was made a fellow of the Royal Society in 1979, one of the highlights of his many awards. He is in great demand as a presenter of plenary lectures at many august meetings. In addition to his abilities as a computational biologist, Professor Noble is an accomplished linguist and has given lectures in French and Italian, and has significant abilities in Japanese, Korean and even Maori.}
}
@article{LAZARO2021111384,
title = {Policy and governance dynamics in the water-energy-food-land nexus of biofuels: Proposing a qualitative analysis model},
journal = {Renewable and Sustainable Energy Reviews},
volume = {149},
pages = {111384},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111384},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121006699},
author = {Lira Luz Benites Lazaro and Leandro Luiz Giatti and Celio Bermann and Angelica Giarolla and Jean Ometto},
keywords = {Water–energy–food nexus, Nexus thinking, Governance, Policy, Biofuels, Nexus methodology, Nexus method, Innovation},
abstract = {The production of biofuels is inextricably linked with the water-energy-food-land (WEFL) nexus. Understanding these linkages is necessary to formulate effective policies that can influence positive outcomes and contribute to the realization of long-term economic, environmental, and social goals. The use of biofuels can help achieve the United Nation's Sustainable Development Goals (SDGs) and implement the Paris Agreement on climate change. However, the biofuels sector must account for its interdependencies and trade-offs with other sectors. In this study, we formulate a qualitative analytical model that goes beyond the three water-energy-food nexus components by incorporating other elements, such as policy, innovation, governance, and labor to examine their effect as influencing factors and to understand how synergies, trade-offs, and long-overlooked interlinkages between sectors and among existing policies and institutions can become visible. This qualitative model was applied to the case of ethanol in Brazil, for which a large corpus was constructed from the scientific literature, documents and sustainability reports from sugarcane ethanol companies. We used a supervised latent Dirichlet allocation (sLDA) algorithm along with co-occurrence and network analyses. The results demonstrate this approach can be used to evaluate the interfaces between science, policy, and businesses within the WEFL-biofuels nexus. This is done by identifying how best to integrate the development of policies, governance, and stakeholder actions to support cost-effective decisions for optimal resource management and regulatory processes while enabling better integration of scientific insight and policy-making. We also identified how these four influencing factors are of vital importance within the nexus and, if properly addressed, can contribute to more holistic nexus thinking management.}
}
@article{LI2024141569,
title = {Programming experiment course for innovative and sustainable education: A case study of Java for Millikan Oil-Drop experiment},
journal = {Journal of Cleaner Production},
volume = {447},
pages = {141569},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.141569},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624010175},
author = {Yizheng Li and Guandong Su and Haocheng Pan and Chengwei Tan and Gongsheng Li},
keywords = {Laboratory instruction, Java object-oriented programming, Innovative and sustainable education, Student-centered learning},
abstract = {A teaching approach of programming experiment courses with a new education process is proposed in this study to improve the sustainability of engineering education and extend the accessibility of lab experiments across experiment courses, with the goal of encouraging innovative and scientific thinking among students. The Millikan Oil-Drop experiment combined with Java object-oriented programming is demonstrated as a case study to validate the feasibility and advantages of this teaching approach. The new education process of the experiment course is designed based on Jean Piaget's cognitive development theory, which has high practical potential for popularization among tertiary institutions without additional cost. Additionally, this work discussed the relevance of the general criterion of the Accreditation Board for Engineering and Technology on student outcomes to educational accreditation, further indicating that introducing programming into experiment education can improve students' all-round ability and strengthen the triangular relationship among the three main subjects in tertiary education, namely, students, faculty, and higher educational institutions. This study may serve as an educational guide for teachers and tertiary institutions to pursue innovative and sustainable education.}
}
@article{DREANY20181,
title = {Safety engineering of computational cognitive architectures within safety-critical systems},
journal = {Safety Science},
volume = {103},
pages = {1-11},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517301947},
author = {Harry H. Dreany and Robert Roncace and Paul Young},
keywords = {Safety engineering, Artificial intelligence, Cognitive architecture, Decision support model, Intelligent technologies},
abstract = {This paper presents the integration of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety critical system. The IDSM will integrate multi-criteria decision making via intelligent technologies like expert systems, fuzzy logic, machine learning and genetic algorithms. Cognitive technology is currently simulated in safety–critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves a system’s cognitive performance. In this study, the IDSM is being applied to an actual safety–critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real world nautical based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety–critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms and methodologies. The uniqueness of this research will be on bounding the decision making associated with the cognitive architecture’s key safety parameters (KSP). Other real-time applications that could benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers a reference for safety engineering artificially intelligent safety–critical systems.}
}
@article{KENDON2008187,
title = {Optimal computation with non-unitary quantum walks},
journal = {Theoretical Computer Science},
volume = {394},
number = {3},
pages = {187-196},
year = {2008},
note = {From Gödel to Einstein: Computability between Logic and Physics},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507008791},
author = {Viv Kendon and Olivier Maloyer},
keywords = {Quantum computing, Quantum walks, Quantum algorithms},
abstract = {Quantum versions of random walks on the line and the cycle show a quadratic improvement over classical random walks in their spreading rates and mixing times, respectively. Non-unitary quantum walks can provide a useful optimisation of these properties, producing a more uniform distribution on the line, and faster mixing times on the cycle. We investigate the interplay between quantum and random dynamics by comparing the resources required, and examining numerically how the level of quantum correlations varies during the walk. We show numerically that the optimal non-unitary quantum walk proceeds such that the quantum correlations are nearly all removed at the point of the final measurement. This requires only O(logT) random bits for a quantum walk of T steps.}
}
@article{SANTOS2023102749,
title = {Policy entrepreneurs in the global education complex: The case of Finnish education experts working in international organisations},
journal = {International Journal of Educational Development},
volume = {98},
pages = {102749},
year = {2023},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2023.102749},
url = {https://www.sciencedirect.com/science/article/pii/S0738059323000263},
author = {Íris Santos and Elias Pekkola},
keywords = {Development cooperation for education, Influence, Finnish education experts, Complexity, Multiple streams approach},
abstract = {This article analyses the perceived role of Finnish education experts working in development cooperation for education. We interviewed 31 education experts working in international organisations representing Finland. A theoretically pluralist approach is utilised combining complexity thinking with a multiple streams approach. The analysis demonstrates that the context of educational development cooperation is ambiguous and complex. Influencing policymaking is a strategic, non-linear task which takes time, resources, and personal skills. Policy entrepreneurs need to understand the dynamics of development cooperation, identify actors that trust them, and recognise when policy windows are likely to open.}
}
@incollection{MAURYA2010175,
title = {Chapter 8 - Computational Challenges in Systems Biology},
editor = {Edison T. Liu and Douglas A. Lauffenburger},
booktitle = {Systems Biomedicine},
publisher = {Academic Press},
address = {San Diego},
pages = {175-223},
year = {2010},
isbn = {978-0-12-372550-9},
doi = {https://doi.org/10.1016/B978-0-12-372550-9.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123725509000080},
author = {Mano Ram Maurya and Shankar Subramaniam},
abstract = {Publisher Summary
This chapter examines the challenges and some of the recent advances in computational systems biology. Research in computational systems biology has moved beyond interaction networks based simply on clustering and correlation. There are two paradigms in computational systems biology: the iterative cycle of biochemical model—mathematical model—computational model, and integration of novel data and legacy knowledge to develop context-specific biochemical, mathematical, and computational models. Challenges in building biochemical models include the complexity of proteomic states and interactions, integration of diverse data to infer biochemical interactions, and the temporal state of biochemical models. Challenges in building mathematical models include incorporating statistical/probabilistic information into analytical models, using qualitative constraints in mathematical models, and incomplete knowledge and coarse-graining. Challenges in computational modeling include the absence of knowledge about model parameters such as rate constants, local versus global concentrations of species and multiple scales of distance and time, and variation among different cell types and subpopulation variability, or variability among biological repeats. Advanced research in coarse graining will pave the way for progress in the development of multiscale multidomain modeling that can connect fundamental research in network biology to clinical research.}
}
@article{ANDROUTSOPOULOS2024100817,
title = {Scaling as method: A three-stage, mixed-methods approach to digital discourse analysis},
journal = {Discourse, Context & Media},
volume = {62},
pages = {100817},
year = {2024},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S2211695824000631},
author = {Jannis Androutsopoulos},
keywords = {Scaling, Mixed-methods, Digital discourse, Abduction, Indignation mark, Reddit},
abstract = {Drawing on research on graphic contextualization cues in punctuation and typography, this paper describes a three-stage, mixed-methods approach to digital discourse analysis. It introduces the terms ‘scale’ and ‘scaling’ as methodological metaphors for a researcher’s planned, yet contingent movement through formations of digital textual data that differ in terms of volume, method of collection, processing, and analysis. ‘Scaling-as-method’ aims to replace static binaries (such as ‘micro’ and ‘macro’, ‘small’ and ‘big’ data, ‘manual’ and ‘automated’ processing) by the vision of a researcher who shifts their degree of abstraction, or ‘distance’, towards digital data, while moving from close to distant reading and back again. The paper exemplifies this three-stage process on the example of the indignation mark, aka <!!1>, a twist on the iterated exclamation mark that is attested in digital discourse in various languages as a cue of double-voicing. The explorative examination of a small dataset (Stage 1) leads to the computational collection and distributional analysis of a much larger dataset (‘scaling up’, Stage 2), followed by the manual annotation of a selected subset of this data (‘scaling down’, Stage 3). Each stage draws on a different amount of data, which enables different techniques of processing and analysis, and relies on a specific combination of abductive, deductive, and inductive reasoning. Yet all three stages complement one another in a kaleidoscopic way towards understanding connections between punctuation practices and participatory political discourse online. Scaling as method is not a closed recipe, but an adaptable procedure that can be applied to a variety of discrete digital features. It does not aim to replace established methods of computational social media analysis, but to boost research that is predominantly based on the manual collection and annotation of social media data, and to enables a dialogue between multiple understandings of context.}
}
@incollection{MACWHINNEY2008229,
title = {CHAPTER 22 - Neurolinguistic Computational Models},
editor = {BRIGITTE STEMMER and HARRY A. WHITAKER},
booktitle = {Handbook of the Neuroscience of Language},
publisher = {Elsevier},
address = {San Diego},
pages = {229-236},
year = {2008},
isbn = {978-0-08-045352-1},
doi = {https://doi.org/10.1016/B978-0-08-045352-1.00022-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080453521000227},
author = {BRIAN MACWHINNEY and PING LI}
}
@article{ZHENG2025192,
title = {The unbearable slowness of being: Why do we live at 10 bits/s?},
journal = {Neuron},
volume = {113},
number = {2},
pages = {192-204},
year = {2025},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324008080},
author = {Jieyu Zheng and Markus Meister},
keywords = {human behavior, speed of cognition, neural computation, bottleneck, attention, neural efficiency, information rate, memory sports},
abstract = {Summary
This article is about the neural conundrum behind the slowness of human behavior. The information throughput of a human being is about 10 bits/s. In comparison, our sensory systems gather data at ∼109 bits/s. The stark contrast between these numbers remains unexplained and touches on fundamental aspects of brain function: what neural substrate sets this speed limit on the pace of our existence? Why does the brain need billions of neurons to process 10 bits/s? Why can we only think about one thing at a time? The brain seems to operate in two distinct modes: the “outer” brain handles fast high-dimensional sensory and motor signals, whereas the “inner” brain processes the reduced few bits needed to control behavior. Plausible explanations exist for the large neuron numbers in the outer brain, but not for the inner brain, and we propose new research directions to remedy this.}
}
@article{OZER20114514,
title = {An application of fuzzy information granulation in the emerging area of online sports},
journal = {Expert Systems with Applications},
volume = {38},
number = {4},
pages = {4514-4521},
year = {2011},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2010.09.125},
url = {https://www.sciencedirect.com/science/article/pii/S0957417410010821},
author = {Muammer Ozer},
keywords = {Information granulation, Fuzzy granulation, Application, Online business},
abstract = {Abstract
One of the major computational challenges that online businesses face today is to make sense of huge amount of information. Granular computing has emerged as an important conceptual and computational paradigm of information processing. As an emerging field of study, it has been suggested that granular computing at philosophical level concerns with structural thinking and at the application level concerns with structured problem solving. It has been further suggested that fuzzy information granulation, as a special case of granular computing, is likely to play an important role in the evolution of fuzzy logic and may eventually have a far-reaching impact on its applications. Responding to the calls for future studies dealing with the applications of fuzzy information granulation, this paper presents an application of the theory of fuzzy information granulation in an emerging and important area where there has not yet been any application, showing how online sports services can make sense of huge amount of data in a structured way and how they can structure their decisions. The empirical results show that despite the huge amount of data that needs to be processed, fuzzy information granulation can help online sports services make sense of it and identify meaningful granules for easier decision making.}
}
@article{CHATURVEDI2005694,
title = {Agent-based simulation for computational experimentation: Developing an artificial labor market},
journal = {European Journal of Operational Research},
volume = {166},
number = {3},
pages = {694-716},
year = {2005},
note = {Advances in Complex Systems Modeling},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.03.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704004102},
author = {Alok Chaturvedi and Shailendra Mehta and Daniel Dolk and Rick Ayer},
keywords = {Artificial intelligence, Decision support systems, Simulation, Modelling systems and languages, Economics},
abstract = {This paper discusses the creation of an artificial labor market (ALM) as an agent-based simulation model. We trace the development of the ALM by adapting the traditional simulation life cycle into two main parts: the model phase and the simulation phase. In the modeling phase of the life cycle, we focus upon agent representation and specification within the virtual world. In the simulation phase, we discuss the use of scenario planning as the experimentation vehicle. Throughout, we use military recruit market as an example to illustrate the methodology. The benefits of the ALM are (1) it provides a virtual world for continuous computational experimentation, (2) it supports market segmentation by allowing “drilldowns” to finer and finer levels of granularity, and (3) when connected via a common OLAP interface to a “real world” counterpart, it facilitates a tightly integrated, persistent, “sense and respond” decision support functionality.}
}
@article{ALLEN2006428,
title = {Innovations in computational type theory using Nuprl},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {428-469},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000704},
author = {S.F. Allen and M. Bickford and R.L. Constable and R. Eaton and C. Kreitz and L. Lorigo and E. Moran},
keywords = {Martin-Löf type theory, Dependent intersection types, Union types, Polymorphic subtyping, Logic of events, Formal digital libraries, Computational type theory, Proofs as programs, Program extraction, Tactics},
abstract = {For twenty years the Nuprl (“new pearl”) system has been used to develop software systems and formal theories of computational mathematics. It has also been used to explore and implement computational type theory (CTT)—a formal theory of computation closely related to Martin-Löf's intuitionistic type theory (ITT) and to the calculus of inductive constructions (CIC) implemented in the Coq prover. This article focuses on the theory and practice underpinning our use of Nuprl for much of the last decade. We discuss innovative elements of type theory, including new type constructors such as unions and dependent intersections, our theory of classes, and our theory of event structures. We also discuss the innovative architecture of Nuprl as a distributed system and as a transactional database of formal mathematics using the notion of abstract object identifiers. The database has led to an independent project called the Formal Digital Library, FDL, now used as a repository for Nuprl results as well as selected results from HOL, MetaPRL, and PVS. We discuss Howe's set theoretic semantics that is used to relate such disparate theories and systems as those represented by these provers.}
}
@article{DUAN2024100234,
title = {Making waves: Knowledge and data fusion in urban water modelling},
journal = {Water Research X},
volume = {24},
pages = {100234},
year = {2024},
issn = {2589-9147},
doi = {https://doi.org/10.1016/j.wroa.2024.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2589914724000240},
author = {Haoran Duan and Jiuling Li and Zhiguo Yuan},
keywords = {Modelling, Data-driven, Machine learning, Hybrid model, Urban water systems},
abstract = {Mathematical modeling plays a crucial role in understanding and managing urban water systems (UWS), with mechanistic models often serving as the foundation for their design and operations. Despite the wide adoptions, mechanistic models are challenged by the complexity of dynamic processes and high computational demands. Data-driven models bring opportunities to capture system complexities and reduce computational cost, by leveraging the abundant data made available by recent advance in sensor technologies. However, the interpretability and data availability hinder their wider adoption. This paper advocates for a paradigm shift in the application of data-driven models within the context of UWS. Integrating existing mechanistic knowledge into data-driven modeling offers a unique solution that reduces data requirements and enhances model interpretability. The knowledge-informed approach balances model complexity with dataset size, enabling more efficient and interpretable modeling in UWS. Furthermore, the integration of mechanistic and data-driven models offers a more accurate representation of UWS dynamics, addressing lingering uncertainties and advancing modelling capabilities. This paper presents perspectives and conceptual framework on developing and implementing knowledge-informed data-driven modeling, highlighting their potential to improve UWS management in the digital era.}
}
@article{BERKOVICHOHANA2020116626,
title = {Inter-participant consistency of language-processing networks during abstract thoughts},
journal = {NeuroImage},
volume = {211},
pages = {116626},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116626},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301130},
author = {Aviva Berkovich-Ohana and Niv Noy and Michal Harel and Edna Furman-Haran and Amos Arieli and Rafael Malach},
keywords = {Abstract-thoughts, Visual imagery, Default mode network, Language, fMRI},
abstract = {Human brain imaging typically employs structured and controlled tasks to avoid variable and inconsistent activation patterns. Here we expand this assumption by showing that an extremely open-ended, high-level cognitive task of thinking about an abstract content, loosely defined as “abstract thinking” - leads to highly consistent activation maps. Specifically, we show that activation maps generated during such cognitive process were precisely located relative to borders of well-known networks such as internal speech, visual and motor imagery. The activation patterns allowed decoding the thought condition at >95%. Surprisingly, the activated networks remained the same regardless of changes in thought content. Finally, we found remarkably consistent activation maps across individuals engaged in abstract thinking. This activation bordered, but strictly avoided visual and motor networks. On the other hand, it overlapped with left lateralized language networks. Activation of the default mode network (DMN) during abstract thought was similar to DMN activation during rest. These observations were supported by a quantitative neuronal distance metric analysis. Our results reveal that despite its high level, and varied content nature - abstract thinking activates surprisingly precise and consistent networks in participants’ brains.}
}
@article{SVOZIL2005845,
title = {Computational universes},
journal = {Chaos, Solitons & Fractals},
volume = {25},
number = {4},
pages = {845-859},
year = {2005},
note = {TRANSFINITE PHYSICS Treading the Path of Cantor and Einstein A collection of papers in honour of the Egyptian Engineering Scientist},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2004.11.055},
url = {https://www.sciencedirect.com/science/article/pii/S0960077904007830},
author = {Karl Svozil},
abstract = {Suspicions that the world might be some sort of a machine or algorithm existing “in the mind” of some symbolic number cruncher have lingered from antiquity. Although popular at times, the most radical forms of this idea never reached mainstream. Modern developments in physics and computer science have lent support to the thesis, but empirical evidence is needed before it can begin to replace our contemporary world view.}
}
@article{DOHERTYSNEDDON2013616,
title = {Gaze aversion during social style interactions in autism spectrum disorder and Williams syndrome},
journal = {Research in Developmental Disabilities},
volume = {34},
number = {1},
pages = {616-626},
year = {2013},
issn = {0891-4222},
doi = {https://doi.org/10.1016/j.ridd.2012.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0891422212002557},
author = {Gwyneth Doherty-Sneddon and Lisa Whittle and Deborah M. Riby},
keywords = {Eye contact, Gaze, Williams syndrome, Gaze aversion, Autism spectrum disorder},
abstract = {During face-to-face interactions typically developing individuals use gaze aversion (GA), away from their questioner, when thinking. GA is also used when individuals with autism (ASD) and Williams syndrome (WS) are thinking during question-answer interactions. We investigated GA strategies during face-to-face social style interactions with familiar and unfamiliar interlocutors. Participants with WS and ASD used overall typical amounts/patterns of GA with all participants looking away most while thinking and remembering (in contrast to listening and speaking). However there were a couple of specific disorder related differences: participants with WS looked away less when thinking and interacting with unfamiliar interlocutors; in typical development and WS familiarity was associated with reduced gaze aversion, however no such difference was evident in ASD. Results inform typical/atypical social and cognitive phenotypes. We conclude that gaze aversion serves some common functions in typical and atypical development in terms of managing the cognitive and social load of interactions. There are some specific idiosyncracies associated with managing familiarity in ASD and WS with elevated sociability with unfamiliar others in WS and a lack of differentiation to interlocutor familiarity in ASD. Regardless of the familiarity of the interlocutor, GA is associated with thinking for typically developing as well as atypically developing groups. Social skills training must take this into account.}
}
@article{CHENG2024104948,
title = {Exploring differences in self-regulated learning strategy use between high- and low-performing students in introductory programming: An analysis of eye-tracking and retrospective think-aloud data from program comprehension},
journal = {Computers & Education},
volume = {208},
pages = {104948},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104948},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002257},
author = {Gary Cheng and Di Zou and Haoran Xie and Fu Lee Wang},
keywords = {Introductory programming, Self-regulated learning strategies, Eye tracking, Retrospective think aloud, Higher education},
abstract = {Previous studies have reported mixed results regarding the relationship between students’ use of self-regulated learning (SRL) strategies and their performance in introductory programming courses. These studies were constrained by their reliance on self-report questionnaires as a means of collecting and analysing data. To address this limitation, this study aimed to employ eye-tracking and retrospective think-aloud techniques to identify differences in SRL strategy use for program comprehension tasks between high-performing students (N = 31) and low-performing students (N = 31) in an undergraduate programming course. All participants attended individual eye-tracking sessions to comprehend two Python program codes with different constructs. Their eye-tracking data and video-recalled retrospective think-aloud data were captured and recorded for analysis. The findings reveal that higher-order cognitive skills, such as elaboration and critical thinking, were mostly adopted by high-performing students, while basic cognitive and resource management strategy, such as rehearsal and help-seeking, were mostly employed by low-performing students when comprehending the program codes. This study not only demonstrates the design of combining eye-tracking and retrospective think-aloud data to explore students’ use of SRL strategies but also provides evidence to support the notion that program comprehension is a complex process that cannot be effectively addressed by employing merely rudimentary strategies, such as repetitively reading the same code segment. In the future, researchers could explore the possibility of using a webcam to monitor and assess students’ online programming processes and provide feedback based on their eye movements. They could also examine the effects of SRL strategies training on students’ motivation, engagement, and performance in various types of programming activities.}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{JOHNSON200337,
title = {Parallel processing in computational stochastic dynamics},
journal = {Probabilistic Engineering Mechanics},
volume = {18},
number = {1},
pages = {37-60},
year = {2003},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(02)00041-3},
url = {https://www.sciencedirect.com/science/article/pii/S0266892002000413},
author = {E.A. Johnson and C. Proppe and B.F. Spencer and L.A. Bergman and G.S. Székely and G.I. Schuëller},
keywords = {Computational stochastic dynamics, Parallel computing, Monte Carlo simulation, Random eigenvalue, Fokker–Planck equation},
abstract = {Studying large complex problems that often arise in computational stochastic dynamics (CSD) demands significant computer power and data storage. Parallel processing can help meet these requirements by exploiting the computational and storage capabilities of multiprocessing computational environments. The challenge is to develop parallel algorithms and computational strategies that can take full advantage of parallel machines. This paper reviews some of the characteristics of parallel computing and the techniques used to parallelize computational algorithms in CSD. The characteristics of parallel processor environments are discussed, including parallelization through the use of message passing and parallelizing compilers. Several applications of parallel processing in CSD are then developed: solutions of the Fokker–Planck equation, Monte Carlo simulation of dynamical systems, and random eigenvector problems. In these examples, parallel processing is seen to be a promising approach through which to resolve some of the computational issues pertinent to CSD.}
}
@article{BLACK2008723,
title = {Deriving an approximation algorithm for automatic computation of ripple effect measures},
journal = {Information and Software Technology},
volume = {50},
number = {7},
pages = {723-736},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907000791},
author = {Sue Black},
keywords = {Software measurement, Ripple effect, Matrix algebra},
abstract = {The ripple effect measures impact, or how likely it is that a change to a particular module may cause problems in the rest of a program. It can also be used as an indicator of the complexity of a particular module or program. Central to this paper is a reformulation in terms of matrix arithmetic of the original ripple effect algorithm produced by Yau and Collofello in 1978. The main aim of the reformulation is to clarify the component parts of the algorithm making the calculation more explicit. The reformulated algorithm has been used to implement REST (Ripple Effect and Stability Tool) which produces ripple effect measures for C programs. This paper describes the reformulation of Yau and Collofello’s ripple effect algorithm focusing on the computation of matrix Zm which holds intramodule change propagation information. The reformulation of the ripple effect algorithm is validated using fifteen programs which have been grouped by type. Due to the approximation spurious 1s are contained within matrix Zm. It is discussed whether this has an impact on the accuracy of the reformulated algorithm. The conclusion of this research is that the approximated algorithm is valid and as such can replace Yau and Collofello’s original algorithm.}
}
@article{CHISCI1995487,
title = {Fast Computation of Stabilizing Predictive Control Laws},
journal = {IFAC Proceedings Volumes},
volume = {28},
number = {5},
pages = {487-493},
year = {1995},
note = {3rd IFAC/IFIP Workshop on Algorithms and Architectures for Real-Time Control 1995 (AARTC'95), Ostend, Belgium, 31 May-2 June 1995},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)47270-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017472703},
author = {L. Chisci and A. Garulli and G. Zappa},
keywords = {Predictive control, linear quadratic regulators, control algorithms, fast parallel algorithms, fast Kalman algorithms, computational methods, adaptive control},
abstract = {A fast algorithm for Linear Quadratic(LQ) control with linear equality constraints is derived and exploited for stabilizing predictive control synthesis. The algorithm requires only O(Nn) computations for an nth order plant and N-steps prediction horizon, and possesses a remarkable numerical accuracy.}
}
@article{SINGER200948,
title = {The dynamic infrastructure of mind—A hypothesis and some of its applications},
journal = {New Ideas in Psychology},
volume = {27},
number = {1},
pages = {48-74},
year = {2009},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2008.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X08000032},
author = {Florence Mihaela Singer},
keywords = {Cognitive architecture, Dynamic infrastructure of mind, Learning, Operational categories},
abstract = {A mechanism underlying the computational properties of the cognitive architecture is construed based on a minimal list of operational clusters. This general processing mechanism constitutes the dynamic infrastructure of mind (DIM). DIM consists in categories of mental operations foundational for learning that contain inborn components called inner operations, which are self-developing in the interaction mind-environment. Within the DIM paradigm, the input cognitive systems are not domain specific or core-knowledge specific, they are operational specific and capable of further developments that become domain specific while experiencing the environment. Arguments for this construal come from three sources: literature review, data collected through classroom observations, and a four-year experimental study of teaching and learning mathematics in primary grades. The outcomes of that experiment led to a methodology of learning based on activating the operational infrastructure of mind, which enhances students' flexibility of thinking and predicts the capacity to solve creatively a variety of problems.}
}
@article{PIAW20114019,
title = {Establishing a Brain Styles Test: The YBRAINS Test},
journal = {Procedia - Social and Behavioral Sciences},
volume = {15},
pages = {4019-4027},
year = {2011},
note = {3rd World Conference on Educational Sciences - 2011},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.04.407},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811009530},
author = {Chua Yan Piaw},
keywords = {Brain style, thinking and learning, YBRAINS, validity and reliability},
abstract = {Teaching with knowledge of students’ thinking and learning styles increases its effectiveness. The YBRAINS test is developed to help school teachers to understand the thinking and learning readiness levels of their students in the process of providing effective teaching and learning activities. The test was established based on theories and brain experiment research evidences. This article reports the rationale of establishing the test and its validity and reliability.}
}
@article{JOHNSON200033,
title = {Thinking ahead: the case for motor imagery in prospective judgements of prehension},
journal = {Cognition},
volume = {74},
number = {1},
pages = {33-70},
year = {2000},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(99)00063-3},
url = {https://www.sciencedirect.com/science/article/pii/S0010027799000633},
author = {Scott H Johnson},
keywords = {Motor imagery, Prospective judgements, Prehension},
abstract = {How similar are judgements concerning how we expect to perform an action, to how we actually behave? The veracity of such prospective action judgements, and the mechanisms by which they are computed, was explored in a series of tasks that involved either grasping (MC conditions) or thinking about grasping (PJ conditions) a dowel presented in various orientations. PJs concerning limits of comfortable hand supination and pronation when turning a dowel in the picture plane were highly consistent with values obtained during actual hand rotation (Exp. 1). The same was true for judgements regarding the level of awkwardness involved in adopting a prescribed grip (e.g. overhand with right hand) for dowels in various picture plane orientations (Exp. 2). When allowed to select the most natural grip (overhand versus underhand) or hand (left versus right) for engaging dowels in these orientations, subjects preferred virtually identical responses in both PJ and MC conditions. In both instances, they consistently chose the least awkward response options. As would be expected for actual movements, PJs involving awkward hand postures had longer response times (RTs), and were less accurate. Likewise, latencies for both grip and hand judgements tended to increase as a function of the angular distance between the current positions of subjects’ hands, and the orientation of the chosen posture. Together, these findings are consistent with a the hypothesis that PJs involve mentally simulated actions, or motor imagery. These results suggest that motor imagery does not depend on the existence of a completed premotor plan (Jeannerod, 1994), but may instead be involved in the planning process itself. A provisional model for the involvement of imagery in motor planning is outlined, as are a set of criteria for evaluating claims of the involvement of motor imagery in problem solving.}
}
@article{MILLER1993205,
title = {Educational tools for computational modelling},
journal = {Computers & Education},
volume = {21},
number = {3},
pages = {205-261},
year = {1993},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(93)90019-F},
url = {https://www.sciencedirect.com/science/article/pii/036013159390019F},
author = {Rob Miller and Jon Ogborn and Jonathan Briggs and Derek Brough and Joan Bliss and Richard Boohan and Tim Brosnan and Harvey Mellar and Babis Sakonidis},
abstract = {The paper reports both a theoretical analysis and a comparison of educational tools for computational modelling, and describes three prototype tools developed in the Programme for use in empirical studies of children reasoning with the aid of computational tools, together with an outline of the result obtained by using the tools with children.}
}
@incollection{VALLERO2024613,
title = {Chapter 20 - Future},
editor = {Daniel A. Vallero and Trevor M. Letcher},
booktitle = {Unraveling Environmental Disasters (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {613-634},
year = {2024},
isbn = {978-0-443-18651-6},
doi = {https://doi.org/10.1016/B978-0-443-18651-6.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443186516000044},
author = {Daniel A. Vallero and Trevor M. Letcher},
keywords = {Deconstructing disasters, Failure, Integrated pest management (IPM), Land use, Maslow's hierarchy of needs, Information technology, Interoperability, Systems thinking, Spills, Life-cycle analysis, Design for the environment (DfE), Design for disassembly (DfD), Aesop's fables, Tragedy of the Commons, Triple bottom line, Categorical imperative},
abstract = {This chapter revisits the concept of disasters as failures introduced in Chapter 2. This entails approaches to consider the many factors and causes. System thinking is introduced as a means of preventing or reducing the damage of disasters. Systematic approaches must make use of various tools, including regulatory measures; economic incentives; property rights; infrastructure installment; public education; for international projects, international inspections; and cooperation. Disaster prevention and mitigation must integrate planning and engineering approaches, especially thoughtful land use. This also requires an understanding of how people expect to meet basic and advanced needs, as exemplified by Maslow's hierarchy. Other tools include optimizing information technologies and interoperability. The good news is that with education and consideration of past disasters, the implementation of rules and regulations there can be a reduction of number and severity of disasters; e.g., decrease in oil tanker spills over the past half century.}
}
@article{ZHANG2024,
title = {Crowdsourcing Adverse Events Associated With Monoclonal Antibodies Targeting Calcitonin Gene–Related Peptide Signaling for Migraine Prevention: Natural Language Processing Analysis of Social Media},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/58176},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24006255},
author = {Pengfei Zhang and Brad K Kamitaki and Thien Phu Do},
keywords = {internet, patient reported outcome, headache, health information, Reddit, registry, monoclonal antibody, crowdsourcing, postmarketing, safety, surveillance, migraine, preventives, prevention, self-reported, calcitonin gene–related peptide, calcitonin, therapeutics, social media, medication-related, posts, propranolol, topiramate, erenumab, fremanezumab, cross-sectional, surveys},
abstract = {Background
Clinical trials demonstrate the efficacy and tolerability of medications targeting calcitonin gene–related peptide (CGRP) signaling for migraine prevention. However, these trials may not accurately reflect the real-world experiences of more diverse and heterogeneous patient populations, who often have higher disease burden and more comorbidities. Therefore, postmarketing safety surveillance is warranted. Regulatory organizations encourage marketing authorization holders to screen digital media for suspected adverse reactions, applying the same requirements as for spontaneous reports. Real-world data from social media platforms constitute a potential venue to capture diverse patient experiences and help detect treatment-related adverse events. However, while social media holds promise for this purpose, its use in pharmacovigilance is still in its early stages. Computational linguistics, which involves the automatic manipulation and quantitative analysis of oral or written language, offers a potential method for exploring this content.
Objective
This study aims to characterize adverse events related to monoclonal antibodies targeting CGRP signaling on Reddit, a large online social media forum, by using computational linguistics.
Methods
We examined differences in word frequencies from medication-related posts on the Reddit subforum r/Migraine over a 10-year period (2010-2020) using computational linguistics. The study had 2 phases: a validation phase and an application phase. In the validation phase, we compared posts about propranolol and topiramate, as well as posts about each medication against randomly selected posts, to identify known and expected adverse events. In the application phase, we analyzed posts discussing 2 monoclonal antibodies targeting CGRP signaling—erenumab and fremanezumab—to identify potential adverse events for these medications.
Results
From 22,467 Reddit r/Migraine posts, we extracted 402 (2%) propranolol posts, 1423 (6.33%) topiramate posts, 468 (2.08%) erenumab posts, and 73 (0.32%) fremanezumab posts. Comparing topiramate against propranolol identified several expected adverse events, for example, “appetite,” “weight,” “taste,” “foggy,” “forgetful,” and “dizziness.” Comparing erenumab against a random selection of terms identified “constipation” as a recurring keyword. Comparing erenumab against fremanezumab identified “constipation,” “depression,” “vomiting,” and “muscle” as keywords. No adverse events were identified for fremanezumab.
Conclusions
The validation phase of our study accurately identified common adverse events for oral migraine preventive medications. For example, typical adverse events such as “appetite” and “dizziness” were mentioned in posts about topiramate. When we applied this methodology to monoclonal antibodies targeting CGRP or its receptor—fremanezumab and erenumab, respectively—we found no definite adverse events for fremanezumab. However, notable flagged words for erenumab included “constipation,” “depression,” and “vomiting.” In conclusion, computational linguistics applied to social media may help identify potential adverse events for novel therapeutics. While social media data show promise for pharmacovigilance, further work is needed to improve its reliability and usability.}
}
@article{ELLIOT201878,
title = {A Proposal to Integrate System Dynamics and Carbon Metabolism for Urban Planning},
journal = {Procedia CIRP},
volume = {69},
pages = {78-82},
year = {2018},
note = {25th CIRP Life Cycle Engineering (LCE) Conference, 30 April – 2 May 2018, Copenhagen, Denmark},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117307618},
author = {Thomas Elliot and Benedetto Rugani and Javier Babí Almenar and Samuel Niza},
keywords = {Life-cycle thinking, system dynamics, urban metabolism, urban planning},
abstract = {Coupling of life-cycle thinking with urban metabolism (UM) has the potential to improve sustainable urban planning. Current urban metabolism models are largely ‘black-box’ methods which do not reveal the non-linearity of feedback loops and complex internal dynamics of urban systems. The integration of system dynamics (SD) with UM based on a life-cycle thinking approach can provide built environment professionals (e.g. town planners, civil engineers, architects) with a ‘transparent-box’ solution for assessing the potential of urban projects, plans, and their implementation. This paper describes the development of a method that integrates input-output (IO) table flows with SD modelling to improve the completeness of UM assessments. This modelling framework can also allow for a ‘nested’ multi-region assessment which takes into account sustainability burdens consequent to urban system changes occurring elsewhere in the national and/or global economy. Pros and cons of this proposal are showcased by the illustration of a model for Lisbon.}
}
@article{MAHAL2007491,
journal = {Economics & Human Biology},
volume = {5},
number = {3},
pages = {491-493},
year = {2007},
note = {Special Issue on Obesity in Eastern Europe},
issn = {1570-677X},
doi = {https://doi.org/10.1016/j.ehb.2007.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570677X07000573},
author = {Ajay Mahal}
}
@incollection{MEDEIROS2023275,
title = {Chapter 19 - Promises and realities of artificial creativity},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {275-289},
year = {2023},
isbn = {978-0-323-91841-1},
doi = {https://doi.org/10.1016/B978-0-323-91841-1.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918411000105},
author = {Kelsey E. Medeiros and Rebecca L. Marrone and Srecko Joksimovic and David H. Cropley and George Siemens},
keywords = {Creativity, Artificial intelligence, Cognition},
abstract = {Proponents of artificial intelligence boast its many promises, including the potential for creativity. Whether the realities of artificial cognition align with these promises, however, remains hotly debated. In this chapter, we explore the role of artificial intelligence in creative problem-solving through the lens of cognition. Through this lens, we advance the argument that, at present, creative problem-solving remains a distinctly human capability. Specifically, we examine how artificial cognition can and cannot engage in each stage of creative problem-solving, as well as the underlying mechanisms of divergent and convergent thinking. Although we find little evidence to support the creativity of artificial cognition, we advance several ways in which artificial cognition can augment human cognition to enhance creative problem-solving.}
}
@article{HABIB2020445,
title = {Computation Analysis of Brand Experience Dimensions: Indian Online Food Delivery Platforms},
journal = {Computers, Materials and Continua},
volume = {67},
number = {1},
pages = {445-462},
year = {2020},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.014047},
url = {https://www.sciencedirect.com/science/article/pii/S1546221820001575},
author = {Sufyan Habib and Nawaf N. Hamadneh and S. Al wadi and Ra’ed Masa’deh},
keywords = {Hypothesis tests, brand experience, online food delivery platform, statistical tests, COVID-19},
abstract = {Online Food Delivery Platforms (OFDPs) has witnessed phenomenal growth in the past few years, especially this year due to the COVID-19 pandemic. This Pandemic has forced many governments across the world to give momentum to OFD services and make their presence among the customers. The Presence of several multinational and national companies in this sector has enhanced the competition and companies are trying to adapt various marketing strategies and exploring the brand experience (BEX) dimension that helps in enhancing the brand equity (BE) of OFDPs. BEXs are critical for building brand loyalty (BL) and making companies profitable. Customers can experience different kinds of brand experiences through feeling, emotions, affection, behavior, and intellect. The present research work is taken up to analyze the factors affecting BEX and its impact on BL and BE of the OFDPs and analyze the mediating role of BL in the relationship between BEX and BE of the OFDPs in the Indian context. A survey of 457 Indian customers was carried out. A questionnaire was used for data collection and a mediation study was used to test hypothesized relationships. Our computational analysis reveals that BEX influences the BL and BE of OFDPs. The study further indicates that BL mediates the relationship between BEX and BE of OFDPs. The effective marketing and relationship management practices will help company to enhance BEX that will enable in enhancing BL and raising BE of their product. It therefore provides a more thorough analysis of BEX constructs and their consequences than previous research. Some of the managerial implication, limitations, and scope of future research are also presented in the study.}
}
@article{WANG2022102240,
title = {What is the competence boundary of Algorithms? An institutional perspective on AI-based video generation},
journal = {Displays},
volume = {73},
pages = {102240},
year = {2022},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2022.102240},
url = {https://www.sciencedirect.com/science/article/pii/S0141938222000671},
author = {Jun Wang and Sichen Li and Ke Xue and Li Chen},
keywords = {Artificial intelligence, Video, Information fluency, Perception},
abstract = {As automated journalism based on AI came into being, it is important to understand the algorithm competence possibilities and limitations for the institutional facilitating the human–machine collaboration. Meanwhile, videos become mainstream in the advertisement realm. To expand the scope of research from journalism to advertisement, from text news to video, a comparative study was conducted to examine how the users perceive the video created by AI and humans. There is no significant difference explicitly, but the implicit appraisals were in favor of human-generated video. The key discussion is the boundary thinking of AI in both the academic and industrial spheres.}
}
@article{GALLISTEL201266,
title = {Extinction from a rationalist perspective},
journal = {Behavioural Processes},
volume = {90},
number = {1},
pages = {66-80},
year = {2012},
note = {Society for the Quantitative Analyses of Behavior: Extinction},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2012.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0376635712000447},
author = {C.R. Gallistel},
keywords = {Acquisition, Extinction, Partial reinforcement, Spontaneous recovery, Renewal, Reinstatement, Resurgence, Information theory, Bayesian inference},
abstract = {The merging of the computational theory of mind and evolutionary thinking leads to a kind of rationalism, in which enduring truths about the world have become implicit in the computations that enable the brain to cope with the experienced world. The dead reckoning computation, for example, is implemented within the brains of animals as one of the mechanisms that enables them to learn where they are (Gallistel, 1990, Gallistel, 1995). It integrates a velocity signal with respect to a time signal. Thus, the manner in which position and velocity relate to one another in the world is reflected in the manner in which signals representing those variables are processed in the brain. I use principles of information theory and Bayesian inference to derive from other simple principles explanations for: (1) the failure of partial reinforcement to increase reinforcements to acquisition; (2) the partial reinforcement extinction effect; (3) spontaneous recovery; (4) renewal; (5) reinstatement; (6) resurgence (aka facilitated reacquisition). Like the principle underlying dead-reckoning, these principles are grounded in analytic considerations. They are the kind of enduring truths about the world that are likely to have shaped the brain's computations.}
}
@article{NAJJAR19991907,
title = {Advances in the dataflow computational model},
journal = {Parallel Computing},
volume = {25},
number = {13},
pages = {1907-1929},
year = {1999},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(99)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819199000708},
author = {Walid A Najjar and Edward A Lee and Guang R Gao},
keywords = {Computational models, Dataflow, Multithreaded computer architecture, von Neumann computer, Dataflow history, Memory models},
abstract = {The dataflow program graph execution model, or dataflow for short, is an alternative to the stored-program (von Neumann) execution model. Because it relies on a graph representation of programs, the strengths of the dataflow model are very much the complements of those of the stored-program one. In the last thirty or so years since it was proposed, the dataflow model of computation has been used and developed in very many areas of computing research: from programming languages to processor design, and from signal processing to reconfigurable computing. This paper is a review of the current state-of-the-art in the applications of the dataflow model of computation. It focuses on three areas: multithreaded computing, signal processing and reconfigurable computing.}
}
@article{GREGOR19981481,
title = {A computational study of the focus-of-attention EM-ML algorithm for PET reconstruction1Research supported in part by the National Science Foundation under grant CDA-95-29459.1},
journal = {Parallel Computing},
volume = {24},
number = {9},
pages = {1481-1497},
year = {1998},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(98)00067-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167819198000672},
author = {Jens Gregor and Dean A. Huff},
keywords = {Distributed computing, Expectation-maximization, Image reconstruction, Positron emission tomography},
abstract = {The expectation-maximization maximum-likelihood (EM-ML) algorithm for image reconstruction in positron emission tomography (PET) essentially solves a large linear system of equations. In this paper, we study computational aspects of a recently developed preprocessing scheme for focusing the attention, and thus the computational resources, on a subset of the equations and unknowns in order to reduce the storage, computation, and communication requirements of the EM-ML algorithm. The approach is completely data-driven and uses no prior anatomic knowledge. The experimental results are obtained from runs on a small network of workstations using simulated phantom data as well as data obtained from a clinical ECAT 921 PET scanner.}
}
@article{SNODGRASS20161,
title = {Instructional supports for students with disabilities in K-5 computing: Findings from a cross-case analysis},
journal = {Computers & Education},
volume = {100},
pages = {1-17},
year = {2016},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2016.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0360131516300999},
author = {Melinda R. Snodgrass and Maya Israel and George C. Reese},
keywords = {Universal design for learning, Students with disabilities, Pedagogy, Supports},
abstract = {As computer programming and computational thinking (CT) become more integrated into K-12 instruction, content teachers and special educators need to understand how to provide instructional supports to a wide range of learners, including students with disabilities. This cross-case analysis study examined the supports that two students with disabilities, who were initially disengaged during computing activities, received during computing instruction. Data revealed that students' support needs during computing activities were not CT-specific. Rather, supports specific to these students' needs that were successful in other educational areas were also successful and sufficient in CT. Although additional studies would need to be conducted to ascertain the transferability of these findings to other contexts and students, our results contribute evidence that students with disabilities can and should participate in CT and be provided with the supports they need, just as in all other areas of the curriculum. We present a framework for evaluating student engagement to identify student-specific supports and, when needed, refine the emerging K-12 CT pedagogy to facilitate full participation of all students. We then offer a list of four implications for practice based on the findings.}
}
@incollection{CHORAFAS2004141,
title = {7 - Five models by the Basel Committee for computation of operational risk},
editor = {Dimitris N. Chorafas},
booktitle = {Operational Risk Control with Basel II},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {141-162},
year = {2004},
isbn = {978-0-7506-5909-3},
doi = {https://doi.org/10.1016/B978-075065909-3.50009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780750659093500098},
author = {Dimitris N. Chorafas},
abstract = {Publisher Summary
This chapter analyzes the four methods of capital allocation for operational risk by the Basel Committee on Banking Supervision: (1) the basic indicator approach, (2) standard approach, (3) internal measurement approach, and (4) loss distribution approach. The chapter also explains why the internal measurement approach and loss distribution approach require leadership in databasing and datamining. The chapter depicts these four methods for computation of operational risk charges on a double scale: expected amount of capital allocation and complexity. Valid solutions take account of different perspectives and definitions of operational risk. The way operational risk is managed is affected by the manner in it is viewed, added with how the board and CEO come to grips with operational risk, the skills and tools at the regulators' disposal, and the resolve to put the risk under lock and key. This is the reason why Basel II wants banks to put aside capital for operational risk control.}
}
@article{ZORARPACI2024108162,
title = {A fast intrusion detection system based on swift wrapper feature selection and speedy ensemble classifier},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108162},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108162},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003208},
author = {Ezgi Zorarpaci},
keywords = {Data mining, Ensemble classifier, Feature selection, Intrusion detection system},
abstract = {Due to the widespread use of the internet, computer network systems may be exposed to different types of attacks. For this reason, the intrusion detection systems (IDSs) are often used to protect the network systems. Network traffic data (i.e., network packets) includes many features. However, most of them are irrelevant and can lead to a decrease in the runtime and/or the detection performance of the IDS. Although various data mining methods have been applied to improve the effectiveness of IDS, research regarding IDSs having high detection rates and better runtime performance (i.e., lower computational cost) is ongoing. On the other hand, the dimensionality reduction techniques help to eliminate unnecessary features and reduce the computation time of a classification algorithm. In the literature, the feature selection methods (i.e., filter and wrapper) have been widely used for the dimensionality reduction in IDSs. Although the wrapper feature selection techniques outperform the filters, they are time-consuming. Again, the ensemble classifiers can achieve higher detection rates for IDSs compared to the stand-alone classifiers, but they require more computation time to build the model. In order to improve the runtime performance and the detection rate of IDS, a swift wrapper feature selection and a speedy ensemble classifier are proposed in this study. For the dimensionality reduction, the swift wrapper feature selection (i.e., DBDE-QDA) is used, which consists of dichotomous binary differential evolution (DBDE) and quadratic discriminant analysis (QDA). For attack detection, the speedy ensemble classifier is used, which combines Holte's 1R, random tree, and reduced error pruning tree. In the experiments, the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets are used. According to the experimental results, the proposed IDS reaches 95%–97.4%, 82.7%, and 99.5%–99.9% detection rates for the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets. In this way, the proposed IDS competes with the state-of-the-art methods in terms of detection rate and false alarm rate. In addition, the proposed IDS has a lower computational cost than the state-of-the-art methods. Moreover, DBDE-QDA reduces the dimension by 60.97%–82.92%, 73.46%, and 96.55%–98.85% for the NSL-KDD, UNSW-NB15, and CICDoS2019 datasets.}
}
@article{WATKINS200867,
title = {Specifying Properties of Concurrent Computations in CLF},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {199},
pages = {67-87},
year = {2008},
note = {Proceedings of the Fourth International Workshop on Logical Frameworks and Meta-Languages (LFM 2004)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066108000790},
author = {Kevin Watkins and Iliano Cervesato and Frank Pfenning and David Walker},
keywords = {logical frameworks, type theory, linear logic, concurrency},
abstract = {CLF (the Concurrent Logical Framework) is a language for specifying and reasoning about concurrent systems. Its most significant feature is the first-class representation of concurrent executions as monadic expressions. We illustrate the representation techniques available within CLF by applying them to an asynchronous pi-calculus with correspondence assertions, including its dynamic semantics, safety criterion, and a type system with latent effects due to Gordon and Jeffrey.}
}
@article{ERVIN201612,
title = {Technology in geodesign},
journal = {Landscape and Urban Planning},
volume = {156},
pages = {12-16},
year = {2016},
note = {Geodesign—Changing the world, changing design},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2016.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169204616301888},
author = {Stephen M. Ervin},
keywords = {Geodesign, Algorithmic processes, Collaboration, Dynamic modeling, Simulation, Systems thinking},
abstract = {Based on an idealized model with six distinguishing criteria of geodesign projects -- large areas, complex issues, and multi-person teams; digital computing, algorithmic processes, and communications technologies; collaborative, information-based projects; timely feedback about impacts and implications of proposals; dynamic modeling and simulation; and systems thinking -- the technological supports required for each of these criteria are described.}
}
@article{INAL2024101338,
title = {Current clinical status of IC/BPS and what the future holds in basic & translational science},
journal = {Continence},
volume = {11},
pages = {101338},
year = {2024},
issn = {2772-9737},
doi = {https://doi.org/10.1016/j.cont.2024.101338},
url = {https://www.sciencedirect.com/science/article/pii/S2772973724002716},
author = {Guldal Inal and Dick Janssen and Naside Mangir and Francisco Cruz and Ana Charrua},
keywords = {IC/BPS, Biomarkers, Bioinformatics, Artificial intelligence, Animal models},
abstract = {The present review summarizes the scientific content in the workshop “Current clinical status of IC/BPS and what the future holds in basic & translational science” at the International Continence Society (ICS) 2023, Toronto. In the workshop, clinicians and scientists from different disciplines and nationalities discussed the current clinical status of IC/BPS diagnostic and treatment. They defined the available trends in biomarker search and translational medicine. The recent contribution of computational science and bioinformatics, together with artificial intelligence and the recent improvements in the use of animal models were also explored. The search for diagnostic and predictive biomarkers for IC/BPS patients is important. The use of well-refined and characterized animal models and the use of bioinformatics and artificial intelligence will be useful in this search.}
}
@article{LIM2025103550,
title = {Exploring trends and topics in hybrid intelligence using keyword co-occurrence networks and topic modelling},
journal = {Futures},
volume = {167},
pages = {103550},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103550},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725000138},
author = {Jihye Lim and Junseok Hwang},
keywords = {Hybrid Intelligence, Artificial Intelligence, Human Intelligence, Keywords Co-occurrence Network, Topic Modeling, Knowledge classification},
abstract = {Hybrid Intelligence (HI) represents the ability to solve problems by using human and artificial intelligence (AI) together, pursuing the strengths of both the former (e.g., ethical, creative, and common-sense thinking) and the latter (e.g., the fast and efficient ability). This study conducted a keyword network analysis and topic modelling to extract topics, examine research trends, and explore future development directions. HI-related research has increased rapidly since 2017, and the applied research fields have increased in diversity. The most active research fields are computer science, engineering, and mathematics. Based on the topic and keyword analysis, we found four trending topics (decision making, medical science, social factors, and automation) and three emerging topics (crowdsourcing, data science, and teamwork). Among the four types of knowledge (factual, conceptual, expectational, and methodological), previous papers have been lacking focus on methodological research in terms of basic research. Since HI can include both human and artificial intelligences, it offers endless possibilities for methodological knowledge in terms of the research process. We therefore propose the development of research methodologies leveraging HI as a promising future research topic.}
}
@article{BOTTINI2020606,
title = {Knowledge Across Reference Frames: Cognitive Maps and Image Spaces},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {8},
pages = {606-619},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301327},
author = {Roberto Bottini and Christian F. Doeller},
keywords = {conceptual knowledge, space, hippocampus, parietal lobe, conceptual metaphors, analogy},
abstract = {In human and non-human animals, conceptual knowledge is partially organized according to low-dimensional geometries that rely on brain structures and computations involved in spatial representations. Recently, two separate lines of research have investigated cognitive maps, that are associated with the hippocampal formation and are similar to world-centered representations of the environment, and image spaces, that are associated with the parietal cortex and are similar to self-centered spatial relationships. We review evidence supporting cognitive maps and image spaces, and we propose a hippocampal–parietal network that can account for the organization and retrieval of knowledge across multiple reference frames. We also suggest that cognitive maps and image spaces may be two manifestations of a more general propensity of the mind to create low-dimensional internal models.}
}
@article{CASTRO2006811,
title = {Patient-Specific Computational Modeling of Cerebral Aneurysms With Multiple Avenues of Flow From 3D Rotational Angiography Images},
journal = {Academic Radiology},
volume = {13},
number = {7},
pages = {811-821},
year = {2006},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2006.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1076633206001826},
author = {Marcelo A. Castro and Christopher M. Putman and Juan R. Cebral},
abstract = {Rationale and Objectives
Previous studies of aneurysm flow dynamics based on three-dimensional (3D) rotational angiography (RA) images were limited to aneurysms with a single route of blood inflow. However, aneurysms of the circle of Willis frequently involve locations with more than one source of inflow, such as aneurysms of the anterior communicating artery. The highest resolution images of cerebral vessels are from RA images, but this technique is limited to visualizing only one route of inflow at a time, leaving a significant limitation in the application of 3DRA image sets for clinical studies of patient-specific computational fluid dynamics (CFD) simulations. In this report, subject-specific models of cerebral aneurysms with multiple avenues of flow are constructed from RA images by using a novel combination of image coregistration and surface merging techniques.
Materials and Methods
RA images are obtained by means of contrast injection in each vessel that provides inflow to the aneurysm. Anatomic models are constructed independently of each of these vascular trees and fused together into a single model. The model is used to construct a finite element grid for CFD simulations of hemodynamics.
Results
Three examples of patient-specific models are presented: an anterior communicating artery aneurysm, a basilar tip aneurysm, and a model of an entire circle of Willis with five coincident aneurysms. The method is evaluated with a numeric phantom of an aneurysm in the anterior communicating artery.
Conclusion
These examples show that this new technique can be used to create merged network numeric models for CFD modeling. Furthermore, intra-aneurysmal flow patterns are influenced strongly by merging of the two inflow streams. This effect decreases as distance from the merging streams increases.}
}
@incollection{GARDNER202477,
title = {Chapter 4 - Smart design for urban activation and placemaking},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {77-102},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000082},
author = {Nicole Gardner},
keywords = {Cyber-physical systems, Design, Digital placemaking, Interaction, Placemaking, Smart design, Urban activation, Urban amenitization},
abstract = {This chapter charts out the distinction between the concepts of digital placemaking and smart placemaking. It examines existing and speculative urban technology projects that combine spatial design thinking and physical computing to address placemaking design goals such as urban activation, amenitization, and safety and security. In ways different from conventional smart city initiatives that surveil urban dynamics en masse to imperceptibly calibrate large-scale urban services and infrastructural systems, the projects discussed in this chapter engage sensor-based technologies to create localized, responsive, and interactive urban environments to address the social, cultural, and aesthetic dimensions of urban livability.}
}
@article{SINGH2012185,
title = {Towards an integrated generative design framework},
journal = {Design Studies},
volume = {33},
number = {2},
pages = {185-207},
year = {2012},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X11000391},
author = {Vishal Singh and Ning Gu},
keywords = {generative design, architectural design, digital design, design cognition, reflective practise},
abstract = {Design creativity techniques encourage divergent thinking. But how well do the existing generative design techniques support this requirement? How can these general techniques be augmented for supporting design exploration and creativity? This paper investigates these questions through a review of five different generative design techniques used in architectural design that includes cellular automata, genetic algorithms, L-systems, shape grammars, and swarm intelligence. Based on the literature on design cognition and the recent theoretical works on digital design thinking, this paper proposes the need for an integrated generative design framework to enhance design exploration support for human designers. Potential challenges and strategies towards developing such an integrated framework are discussed.}
}
@article{KUHN202428,
title = {A landscape of consciousness: Toward a taxonomy of explanations and implications},
journal = {Progress in Biophysics and Molecular Biology},
volume = {190},
pages = {28-169},
year = {2024},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2023.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610723001128},
author = {Robert Lawrence Kuhn},
keywords = {Consciousness, Mind-body problem, Materialism, Monism, Dualism, Idealism},
abstract = {Diverse explanations or theories of consciousness are arrayed on a roughly physicalist-to-nonphysicalist landscape of essences and mechanisms. Categories: Materialism Theories (philosophical, neurobiological, electromagnetic field, computational and informational, homeostatic and affective, embodied and enactive, relational, representational, language, phylogenetic evolution); Non-Reductive Physicalism; Quantum Theories; Integrated Information Theory; Panpsychisms; Monisms; Dualisms; Idealisms; Anomalous and Altered States Theories; Challenge Theories. There are many subcategories, especially for Materialism Theories. Each explanation is self-described by its adherents, critique is minimal and only for clarification, and there is no attempt to adjudicate among theories. The implications of consciousness explanations or theories are assessed with respect to four questions: meaning/purpose/value (if any); AI consciousness; virtual immortality; and survival beyond death. A Landscape of Consciousness, I suggest, offers perspective.}
}
@article{YANG20242009,
title = {Maximum Power Point Tracking Technology for PV Systems: Current Status and Perspectives},
journal = {Energy Engineering},
volume = {121},
number = {8},
pages = {2009-2022},
year = {2024},
issn = {0199-8595},
doi = {https://doi.org/10.32604/ee.2024.049423},
url = {https://www.sciencedirect.com/science/article/pii/S0199859524000708},
author = {Bo Yang and Rui Xie and Zhengxun Guo},
keywords = {PV systems, MPPT, partial shading condition, DC-DC converter},
abstract = {Maximum power point tracking (MPPT) technology plays a key role in improving the energy conversion efficiency of photovoltaic (PV) systems, especially when multiple local maximum power points (LMPPs) occur under partial shading conditions (PSC). It is necessary to modify the operating point efficiently and accurately with the help of MPPT technology to maximize the collected power. Even though a lot of research has been carried out and impressive progress achieved for MPPT technology, it still faces some challenges and dilemmas. Firstly, the mathematical model established for PV cells is not precise enough. Second, the existing algorithms are often optimized for specific conditions and lack comprehensive adaptability to the actual operating environment. Besides, a single algorithm may not be able to give full play to its advantages. In the end, the selection criteria for choosing the suitable MPPT algorithm/converter combination to achieve better performance in a given scenario is very limited. Therefore, this paper systematically discusses the current research status and challenges faced by PV MPPT technology around the three aspects of MPPT models, algorithms, and hardware implementation. Through in-depth thinking and discussion, it also puts forward positive perspectives on future development, and five forward-looking solutions to improve the performance of PV systems MPPT are suggested.}
}
@article{MENGOV2022101944,
title = {Virtual social networking increases the individual's economic predictability},
journal = {Journal of Behavioral and Experimental Economics},
volume = {101},
pages = {101944},
year = {2022},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2022.101944},
url = {https://www.sciencedirect.com/science/article/pii/S221480432200115X},
author = {George Mengov and Nikolay Georgiev and Irina Zinovieva and Anton Gerunov},
keywords = {Decision making, Virtual social network, Emotional economic choice, Neural model},
abstract = {Forecasting economic choice is hard because today we still do not know enough about human motivation. A fundamental problem is the lack of knowledge about how the neural networks in the brain give rise to thinking and decision making. One way to address the issue has been to develop simplified economic experiments, in which participants need skills of little complexity and their minds employ cognitive mechanisms, already well understood by mathematical psychology and neuroscience. Here we take a neural model for rudimentary emotion generation and memorizing and use it as a guiding theory to understand decision making in an experimental oligopoly market. For the first time in that line of research, participants are put in a lab virtual social network serving to exchange opinions about deals with companies. On average, choices become significantly more predictable when people participate in the network, in contrast to working alone with expert information. Calibrating the model for each person, we find that some people are predicted with startling precision.}
}
@article{ZHONG2016423,
title = {The impact of social factors on pair programming in a primary school},
journal = {Computers in Human Behavior},
volume = {64},
pages = {423-431},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216305064},
author = {Baichang Zhong and Qiyun Wang and Jie Chen},
keywords = {Primary education, Pair programming, Social factors, Gender, Partnership},
abstract = {Pair programming (PP) is a usefulness approach to fostering computational thinking (CT) for young students. However, there are many factors to impact the effectiveness of PP. Among all factors, the social factors are often ignored by researchers. Therefore, this study aimed to explore the impact of two social factors (gender and partnership) on PP in a primary school setting. To that end, we conducted PP experiments in four classes from the sixth grade in a Chinese primary school. The research results indicated: (a) there was no significant difference on compatibility among the gender pairs, but a significant difference among partnership pairs; (b) there was no significant difference on programming achievement and confidence among different pairs, and girls became more productive and confidence in PP; and (c) PP tightened up the partnership within pairs. These findings suggest that teachers should take partnership into account as an important factor in PP or other collaborative learning, and adopt PP as an effective approach to decrease the gender gap in programming courses, and make students socialize.}
}
@incollection{YANG20151,
title = {Chapter 1 - Bio-Inspired Computation and Optimization: An Overview},
editor = {Xin-She Yang and Su Fong Chien and Tiew On Ting},
booktitle = {Bio-Inspired Computation in Telecommunications},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-21},
year = {2015},
isbn = {978-0-12-801538-4},
doi = {https://doi.org/10.1016/B978-0-12-801538-4.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801538400001X},
author = {Xin-She Yang and Su Fong Chien and Tiew On Ting},
keywords = {Algorithm, Ant algorithm, Bee algorithm, Bat algorithm, Bio-inspired computation, Cuckoo search, Firefly algorithm, Harmony search, Particle swarm optimization, Metaheuristics, Swarm intelligence, Telecommunications},
abstract = {All design problems in telecommunications can be formulated as optimization problems, and thus may be tackled by some optimization techniques. However, these problems can be extremely challenging due to the stringent time requirements, complex constraints, and a high number of design parameters. Solution methods tend to use conventional methods such as Lagrangian duality and fractional programming in combination with numerical solvers, while new trends tend to use evolutionary algorithms and swarm intelligence. This chapter provides a summary review of the bio-inspired optimization algorithms and their applications in telecommunications. We also discuss key issues in optimization and some active topics for further research.}
}
@article{FARRELL20175597,
title = {N‐Aryl‐9,10‐phenanthreneimines as Scaffolds for Exploring Noncovalent Interactions: A Structural and Computational Study},
journal = {European Journal of Organic Chemistry},
volume = {2017},
number = {37},
pages = {5597-5609},
year = {2017},
issn = {1434-193X},
doi = {https://doi.org/10.1002/ejoc.201700884},
url = {https://www.sciencedirect.com/science/article/pii/S1434193X22035940},
author = {David Farrell and Samuel J. Kingston and Dmitry Tungulin and Stefano Nuzzo and Brendan Twamley and James A. Platts and Robert J. Baker},
keywords = {Stacking interactions, Noncovalent interactions, Density functional calculations},
abstract = {A series of 10‐[(4‐halo‐2,6‐diisopropylphenyl)imino]phenanthren‐9‐ones and derivatives of the phenanthrene‐9,10‐dione ligand have been synthesised and structurally characterised to explore two types of noncovalent interactions, namely the influence of the steric bulk upon the resulting C–H···π and π‐stacking interactions and halogen bonding. Selected noncovalent interactions have additionally been analysed by DFT and AIM techniques. No halogen bonding has been observed in these systems, but X lone pair···π, C–H···O=C and C–H···π interactions are the prevalent ones in the halogenated systems. Removal of the steric bulk in N‐(2,4,6‐trimethylphenyl)‐9,10‐iminophenanthrenequinone affords different noncovalent interactions, but the C–H···O=C hydrogen bonds are observed. Surprisingly, in N‐(2,6‐dimethylphenyl)‐9,10‐iminophenanthrenequinone and N‐(phenyl)‐9,10‐iminophenanthrenequinone these C–H···O=C hydrogen bonds are not observed. However, they are observed in the related 2,6‐di‐tert‐butylphenanthrene‐9,10‐dione. The π‐interactions in dimers extracted from the crystal structures have been analysed by DFT and AIM. Spectroscopic investigations are also presented and these show only small perturbations to the O=C–C=N fragment.}
}
@article{CARTER2021100065,
title = {Comparing manual and computational approaches to theme identification in online forums: A case study of a sex work special interest community},
journal = {Methods in Psychology},
volume = {5},
pages = {100065},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000229},
author = {Pelham Carter and Matt Gee and Hollie McIlhone and Harkeeret Lally and Robert Lawson},
keywords = {Sex work, Online forums, Language, gender and sexuality, Mixed methods, Corpus linguistics},
abstract = {Online forums afford individuals opportunities to take part in a community with shared interests and goals. This involves the sharing of experiences and advice (Attard and Coulson, 2012) and can lead to positive effects (Pendry and Salvatore, 2015). Online forums also afford access to rich sources of detailed data, personal experiences, and hard-to-reach or taboo communities. Such online research, though well-suited to qualitative analysis, leads to a number of practical problems in terms of range, depth, and ease of access to data. Even extensive data collection and manual analysis often only engage with a small percentage of the data available in online communities. In this article, we present a traditional manual collection and thematic analysis of data (2631 posts across 60 different threads, approximately 300,000 words) from forums where sex workers and men who pay for sex discuss matters relating to prostitution. This analysis revealed five themes of forum use: preference sharing, personal narrative sharing, practical advice, philosophical issues, and community maintenance. Further automated data collection and corpus analysis, such as keyness and topic modelling, are presented as a potential innovation within online qualitative research. This approach allowed for the analysis of a larger dataset of 255,891 posts, across 14,232 threads (16,472,006 words), revealing additional themes such as sexual hygiene, desire, legality, and ethnicity, as well as differences in the use of terms of address and slang by punters and sex workers. The automated methods presented allow for more comprehensive investigations of online communities than traditional approaches, but we also note that manual interpretation should still be incorporated into the analysis.}
}
@article{NORTON2006600,
title = {Computational fluid dynamics (CFD) – an effective and efficient design and analysis tool for the food industry: A review},
journal = {Trends in Food Science & Technology},
volume = {17},
number = {11},
pages = {600-620},
year = {2006},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2006.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924224406001981},
author = {Tomás Norton and Da-Wen Sun},
abstract = {Computational fluid dynamics (CFD) is a powerful numerical tool that is becoming widely used to simulate many processes in the food industry. Recent progression in computing efficacy coupled with reduced costs of CFD software packages has advanced CFD as a viable technique to provide effective and efficient design solutions. This paper discusses the fundamentals involved in developing a CFD solution. It also provides a state-of-the-art review on various CFD applications in the food industry such as ventilation, drying, sterilisation, refrigeration, cold display and storage, and mixing and elucidates the physical models most commonly used in these applications. The challenges faced by modellers using CFD in the food industry are also discussed.}
}
@article{ZHANG2024101587,
title = {Effects of a problem posing instructional interventions on student learning outcomes: A three-level meta-analysis},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101587},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101587},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001251},
author = {Cheng Zhang and Ying Zhou and Tommy Tanu Wijaya and Jihe Chen and Yimin Ning},
keywords = {Problem posing, Learning outcomes, Three-level meta-analysis, Instructional interventions},
abstract = {Problem posing is increasingly being considered in the field of education, with many experts exploring its positive effects on student learning outcomes. In this case, different perspectives have emerged regarding the impact of the intervention, claiming the overall effect remains uncertain. Therefore, this study aims to explore the effects of a problem posing instructional intervention on student learning outcomes at the cognitive and non-cognitive levels from 2000 to 2023, using a three-level meta-analysis. 32 studies and 4,068 participants were included to compare the classrooms with and without problem posing instructional interventions in elementary to higher education. The results showed a moderate-positive and small positive effect on students cognitive (Hedges' g = 0.681, 95 % CI [0.552, 0.810], p < 0.001) and non-cognitive (Hedges' g = 0.367, 95 % CI [0.113, 0.620], p = 0.003) levels, respectively. Based on the moderator analysis, there were differences in the learning outcomes among students across various task formats. Notably, tasks that included specific information and involved problem posing in context demonstrated significantly better performance. In conclusion, these results indicate the importance of problem posing instructional interventions in promoting student's development and their impact on cognitive and non-cognitive dimensions.}
}
@article{ARTHUR2023638,
title = {Economics in nouns and verbs},
journal = {Journal of Economic Behavior & Organization},
volume = {205},
pages = {638-647},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.10.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167268122003936},
author = {W. Brian Arthur},
keywords = {Economic theory, Mathematics in economics, Algorithms, Complexity economics, Computational economics},
abstract = {Standard economic theory uses mathematics as its main means of understanding, and this brings clarity of reasoning and logical power. But there is a drawback: algebraic mathematics restricts economic modeling to what can be expressed only in quantitative nouns, and this forces theory to leave out matters to do with process, formation, adjustment, and creation—matters to do with nonequilibrium. For these we need a different means of understanding, one that allows verbs as well as nouns. Algorithmic expression is such a means. It allows verbs—processes—as well as nouns—objects and quantities. It allows fuller description in economics, and can include heterogeneity of agents, actions as well as objects, and realistic models of behavior in ill-defined situations. The world that algorithms reveal is action-based as well as object-based, organic, possibly ever-changing, and not fully knowable. But it is strangely and wonderfully alive.}
}
@article{MOORE2021,
title = {Age-Related Differences in Experiences With Social Distancing at the Onset of the COVID-19 Pandemic: A Computational and Content Analytic Investigation of Natural Language From a Social Media Survey},
journal = {JMIR Human Factors},
volume = {8},
number = {2},
year = {2021},
issn = {2292-9495},
doi = {https://doi.org/10.2196/26043},
url = {https://www.sciencedirect.com/science/article/pii/S2292949521000274},
author = {Ryan C Moore and Angela Y Lee and Jeffrey T Hancock and Meghan C Halley and Eleni Linos},
keywords = {COVID-19, natural language processing, public health messaging, social distancing compliance, age differences, older adults, younger adults, age, NLP, public health, elderly, youth, adult, emotion, compliance, guideline},
abstract = {Background
As COVID-19 poses different levels of threat to people of different ages, health communication regarding prevention measures such as social distancing and isolation may be strengthened by understanding the unique experiences of various age groups.
Objective
The aim of this study was to examine how people of different ages (1) experienced the impact of the COVID-19 pandemic and (2) their respective rates and reasons for compliance or noncompliance with social distancing and isolation health guidance.
Methods
We fielded a survey on social media early in the pandemic to examine the emotional impact of COVID-19 and individuals’ rates and reasons for noncompliance with public health guidance, using computational and content analytic methods of linguistic analysis.
Results
A total of 17,287 participants were surveyed. The majority (n=13,183, 76.3%) were from the United States. Younger (18-31 years), middle-aged (32-44 years and 45-64 years), and older (≥65 years) individuals significantly varied in how they described the impact of COVID-19 on their lives, including their emotional experience, self-focused attention, and topical concerns. Younger individuals were more emotionally negative and self-focused, while middle-aged people were other-focused and concerned with family. The oldest and most at-risk group was most concerned with health-related terms but were lower in anxiety (use of fewer anxiety-related terms) and higher in the use of emotionally positive terms than the other less at-risk age groups. While all groups discussed topics such as acquiring essential supplies, they differentially experienced the impact of school closures and limited social interactions. We also found relatively high rates of noncompliance with COVID-19 prevention measures, such as social distancing and self-isolation, with younger people being more likely to be noncompliant than older people (P<.001). Among the 43.1% (n=7456) of respondents who did not fully comply with health orders, people differed substantially in the reasons they gave for noncompliance. The most common reason for noncompliance was not being able to afford to miss work (n=4273, 57.3%). While work obligations proved challenging for participants across ages, younger people struggled more to find adequate space to self-isolate and manage their mental and physical health; middle-aged people had more concerns regarding childcare; and older people perceived themselves as being able to take sufficient precautions.
Conclusions
Analysis of natural language can provide insight into rapidly developing public health challenges like the COVID-19 pandemic, uncovering individual differences in emotional experiences and health-related behaviors. In this case, our analyses revealed significant differences between different age groups in feelings about and responses to public health orders aimed to mitigate the spread of COVID-19. To improve public compliance with health orders as the pandemic continues, health communication strategies could be made more effective by being tailored to these age-related differences.}
}
@article{JAUK2012219,
title = {Tackling creativity at its roots: Evidence for different patterns of EEG alpha activity related to convergent and divergent modes of task processing},
journal = {International Journal of Psychophysiology},
volume = {84},
number = {2},
pages = {219-225},
year = {2012},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2012.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167876012000748},
author = {Emanuel Jauk and Mathias Benedek and Aljoscha C. Neubauer},
keywords = {EEG, Creativity, Alpha synchronization, Divergent thinking, Convergent thinking},
abstract = {The distinction between convergent and divergent cognitive processes given by Guilford (1956) had a strong influence on the empirical research on creative thinking. Neuroscientific studies typically find higher event-related synchronization in the EEG alpha rhythm for individuals engaged in creative ideation tasks compared to intelligence-related tasks. This study examined, whether these neurophysiological effects can also be found when both cognitive processing modes (convergent vs. divergent) are assessed by means of the same task employing a simple variation of instruction. A sample of 55 participants performed the alternate uses task as well as a more basic word association task while EEG was recorded. On a trial-by-trial basis, participants were either instructed to find a most common solution (convergent condition) or a most uncommon solution (divergent condition). The answers given in the divergent condition were in both tasks significantly more original than those in the convergent condition. Moreover, divergent processing was found to involve higher task-related EEG alpha power than convergent processing in both the alternate uses task and the word association task. EEG alpha synchronization can hence explicitly be associated with divergent cognitive processing rather than with general task characteristics of creative ideation tasks. Further results point to a differential involvement of frontal and parietal cortical areas by individuals of lower versus higher trait creativity.}
}
@article{KONOPKA1994V,
title = {Computational experiments in molecular biology: Searching for the ‘big picture’},
journal = {Computers & Chemistry},
volume = {18},
number = {3},
pages = {V-VIII},
year = {1994},
issn = {0097-8485},
doi = {https://doi.org/10.1016/0097-8485(94)85013-5},
url = {https://www.sciencedirect.com/science/article/pii/0097848594850135},
author = {AndrzejK. Konopka}
}
@article{BULLEN2022101933,
title = {Patterns of math and reading achievement in children and adolescents with autism spectrum disorder},
journal = {Research in Autism Spectrum Disorders},
volume = {92},
pages = {101933},
year = {2022},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2022.101933},
url = {https://www.sciencedirect.com/science/article/pii/S1750946722000204},
author = {Jennifer C. Bullen and Matthew C. Zajic and Nancy McIntyre and Emily Solari and Peter Mundy},
keywords = {Autism spectrum disorder, Academic achievement, Hierarchical cluster analysis, Math achievement, Reading fluency},
abstract = {Background
There has been an increase of autistic students without intellectual disabilities (autisticWoID) placed in general education settings (Hussar et al., 2020), but there is a lack of understanding of how to best support classroom learning for these children. Previous research has pointed to subgroups of autisticWoID children who display difficulty with mathematics and reading achievement (Chen et al., 2018; Estes et al., 2011; Jones et al., 2009; Wei et al., 2015). Research has primarily focused on symptomatology and communication factors related to learning in subgroups of autistic children. The current study sought to expand upon this research by assessing the validity of these previous studies and by investigating the specific contribution of domain-general cognitive abilities to differences in these subgroups.
Method
Seventy-eight autisticWoID individuals (M = 11.34 years, SD = 2.14) completed measures of mathematics and reading achievement, IQ, working memory, inferential thinking, and Theory of Mind (ToM). A hierarchical cluster analysis was performed on the math and reading measures.
Results
The analysis revealed two unique achievement groups: one group that performed lower than expected on math and reading achievement and a second group that performed higher than expected. Groups differed significantly on IQ and working memory and were distinguished by performance on reading fluency. Groups did not differ on ToM, inferential thinking, or symptomatology.
Conclusion
These findings describe a group of autisticWoID individuals that may be more likely to experience difficulty learning, which should be accounted for in general education settings.}
}
@article{WESTRA2023645,
title = {Accounting for systemic complexity in the assessment of climate risk},
journal = {One Earth},
volume = {6},
number = {6},
pages = {645-655},
year = {2023},
issn = {2590-3322},
doi = {https://doi.org/10.1016/j.oneear.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2590332223002063},
author = {Seth Westra and Jakob Zscheischler},
abstract = {Summary
Widespread changes to climate-sensitive systems are placing increased demands on risk assessments as a foundation for managing risk. Recent attention to compounding and cascading risks, deep uncertainty, and “bottom-up” risk assessment frameworks have foregrounded the need to account for systemic complexity in risk assessment methodology. We describe the sources of systemic complexity and highlight the role of risk assessments as a formal sense-making device that enables learning and organizing knowledge of the dynamic interplay between the climate-sensitive system and its (climatological) environment. We highlight boundary judgments as a core concern of risk assessments, helping to create islands of analytical and cognitive tractability in a complex, uncertain, and ambiguous world. We then point to three key concepts—boundary critique, multi-methodology, and second-order learning—as critical elements of contemporary risk assessment practice, and we weave these into an overarching framework to better account for systemic complexity in the assessment of climate risk.}
}
@article{BECKER2003164,
title = {A computational model of the role of dopamine and psychotropic drugs in modulating motivated action},
journal = {Schizophrenia Research},
volume = {60},
number = {1, Supplement },
pages = {164},
year = {2003},
note = {Abstracts of the IXth International Congress on Schizophrenia Research},
issn = {0920-9964},
doi = {https://doi.org/10.1016/S0920-9964(03)81019-8},
url = {https://www.sciencedirect.com/science/article/pii/S0920996403810198},
author = {S. Becker and A. Chan and P. Fletcher and A. Smith and S. Kapur}
}
@article{KIM20141,
title = {Studying children's tactile problem-solving in a digital environment},
journal = {Thinking Skills and Creativity},
volume = {12},
pages = {1-13},
year = {2014},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2013.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1871187113000734},
author = {Mi Jeong Kim and Myung Eun Cho},
keywords = {Children's problem solving, Tactile interaction, Cognitive perspectives, Empirical study, Protocol analysis},
abstract = {Given that modern children have grown up with numerous digital interactive devices it is essential to understand how the digital environment might affect children's cognitive development. As an extension of previous studies, this research investigates the cognitive effects of tactile interaction on children's problem solving. In order to explore the cognitive development of children with respect to tactile interaction, we compared furniture arrangements by elementary school students of 3D blocks and pencils. A protocol analysis was adopted for examining the ways in which children used the two different tools. The results of this study show that tactile interaction supports children's problem solving. This research implies that children in early education need to experience a wide range of digital devices utilizing rich sensorial dimensions as such devices stimulate divergent thinking, affecting cognitive developmental trajectories.}
}
@article{TABATABAEI2018133,
title = {With a little help from my friends: A computational model for the role of social support in mood regulation},
journal = {Cognitive Systems Research},
volume = {47},
pages = {133-146},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716302030},
author = {Seyed Amin Tabatabaei and Altaf Hussain Abro and Michel Klein},
keywords = {Social support, Stress buffering, Human ambient agent, Perceived support, Social network characteristics},
abstract = {The growing interest in the role of social support in mental and physical health has led to the development of several intelligent systems that aim to use social mechanisms to simulate healthy behaviour. In this paper a computational model of a human agent is presented which describes the effect of social support on mood. According to the literature, social support can either refer to the social resources that individuals perceive to be available or to the support that is actually provided in problematic situations. The proposed model distinguishes between both roles of social support. The role of social network characteristics has been taken into account, as an individual can perceive or receive social support through his/her social network. In addition, the number of connections (friends), strength of ties (relationships), social isolation and social integration have been studied. Simulation experiments have been done to analyze the effect of the different types of support in different scenarios and also to analyze the role of various social network characteristics on the mood level. It is shown that support can help to reduce the induced stress and thus can contribute to healthy mood regulation and prevention of depression. The presented model provides a basis for an intelligent support system for people with mood regulation problems that take the social network of people into account.}
}
@article{WU2019104407,
title = {A review of performance assessment methods for construction and demolition waste management},
journal = {Resources, Conservation and Recycling},
volume = {150},
pages = {104407},
year = {2019},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104407},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919303027},
author = {Huanyu Wu and Jian Zuo and Hongping Yuan and George Zillante and Jiayuan Wang},
keywords = {Construction and demolition waste, Waste management, Performance assessment methods, System thinking, Life cycle assessment},
abstract = {Significant efforts have been devoted to assessing construction and demolition waste management (CDWM). However, there is little knowledge to understand the utilisation of the developed models for assessing CDWM performance, thus limiting the comparison and generalization of recognized methods and tools. By reviewing the prior published literature, this study assesses the current research methods, in particular, data collection. It also reviews the range of critical indicators for CDWM performance assessment considered by the literature and put forwards a new framework for better assessing CDWM performance. The proposed framework summarises the system boundary, research scale and performance assessment aspects documented by previous studies, and further integrate an integrated framework with procedures for better assessing CDWM performance. The literature review found that while some studies adopt a system thinking and life cycle thinking to assess CDWM performance, other research they adopt a sustainability based model to finalize CDWM performance assessment. The results also demonstrate that compared with environmental and economic aspects, the social aspect has attracted less attention. Social factors, however are crucial in CDWM. The findings about current performance assessment practices in CDWM and the proposed procedures are possible to implement for researchers and practitioners to develop sound CDWM approaches.}
}
@article{WILLIAMS2009420,
title = {Analysis of externally loaded bolted joints: Analytical, computational and experimental study},
journal = {International Journal of Pressure Vessels and Piping},
volume = {86},
number = {7},
pages = {420-427},
year = {2009},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2009.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0308016109000179},
author = {J.G. Williams and R.E. Anley and D.H. Nash and T.G.F. Gray},
keywords = {Bolted joints, Pre-loading, Bolt stiffness, Member stiffness, Member contact},
abstract = {The behaviour of a simple single-bolted-joint under tensile separating loads is analysed using conventional analytical methods, a finite element approach and experimental techniques. The variation in bolt force with external load predicted by the finite element analysis conforms well to the experimental results. It is demonstrated that certain detailed features such as thread interaction do not need to be modelled to ensure useful results. Behaviour during the pre-loading phase of use agrees with previous long-standing studies. However, the pre-loading analysis does not carry over to the stage when external loading is applied, as is normally assumed and it is shown that the current, conventional analytical methods substantially over-predict the proportion of the external load carried by the bolt. The basic reason for this is shown to be related to the non-linear variation in contact conditions between the clamped members during the external loading stage.}
}
@article{CEKIRGE199461,
title = {Oil spill modeling using parallel computations},
journal = {Spill Science & Technology Bulletin},
volume = {1},
number = {1},
pages = {61-68},
year = {1994},
issn = {1353-2561},
doi = {https://doi.org/10.1016/1353-2561(94)90008-6},
url = {https://www.sciencedirect.com/science/article/pii/1353256194900086},
author = {H.M. Cekirge and C.P. Giammona and J. Berlin and C. Long and M. Koch and R. Jamail},
abstract = {The current status of oil spill modeling is presented. The physical and chemical processes taking place in oil spills are explained for the design of an ideal oil spill model (IOSM). The requirements of an IOSM for forecasting are rapid response, contingency planning and training. The use of parallel computation techniques in oil spill modeling is introduced and delineated.}
}
@article{SWARUP2006273,
title = {A new evolutionary computation technique for economic dispatch with security constraints},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {28},
number = {4},
pages = {273-283},
year = {2006},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2006.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142061506000020},
author = {K.S. Swarup and P. Rohit Kumar},
keywords = {Power system optimization, Economic load dispatch, Particle swarm optimization, Line-flow, Voltage constraints},
abstract = {This paper presents an efficient and reliable evolutionary based approach to solve the economic load dispatch (ELD) with security constraints. A new approach is proposed which employs attractive and repulsive particle swarm optimization (ARPSO) algorithm for ELD. Incorporation of ARPSO as a derivative-free optimization technique in solving ELD with security (voltages and line-flows) constraints significantly relieves the assumptions imposed on the optimized objective function. The proposed approach has been implemented on three representative systems, i.e. IEEE 14 bus, IEEE 30 bus and IEEE 57 bus systems, respectively. The feasibility of the proposed method is demonstrated and the results are compared with linear programming, quadratic programming and genetic algorithm, respectively. The premature convergence problem, that is common in all evolutionary computation techniques, is solved in ARPSO by including the diversity factor in the Type 1 PSO algorithm. The developed algorithms are computationally faster (in terms of the number of load flows carried out) than the other methods because only one run is required.}
}
@article{FALLOON2019138,
title = {Using simulations to teach young students science concepts: An Experiential Learning theoretical analysis},
journal = {Computers & Education},
volume = {135},
pages = {138-159},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S036013151930051X},
author = {Garry Falloon},
keywords = {Simulations, Young students, Electricity, Circuits, Experiential learning},
abstract = {Early research investigated young students' understandings of science concepts using physical equipment, but technological advances now mean there are new options to introduce these ideas, through devices such as iPads and simulations. However, research investigating the use of simulations in early years' science learning is limited. This study applied revisions of Kolb's Experiential Learning theoretical model to determine if age-indicated science simulations were effective for teaching 5 year olds simple circuit building procedures and electricity concepts, and the function of circuit components. It also explored whether their engagement with the simulations provided worthwhile opportunities to exercise higher order capabilities such as reflective thinking and abstraction – skills oftencited in literature as valuable outcomes from older student and adult use of simulations. Findings indicate students developed a solid base of procedural knowledge about constructing different circuits, and functional knowledge about circuit components they applied to different circuit designs. The emergence of tentative, generalised theories about current and the effects of different circuit designs on the performance of resistors - linked to the exercise of reflective and descriptive thinking, were also noted in many students. However, examples were found of some simulations appearing to foster common misconceptions, such as current being ‘consumed’ by resistors – indicating teachers need to be highly vigilant and work closely with students, to ensure accurate understandings are developed. Overall, with appropriate teacher support and careful selection and review, the study concludes simulations can be effective for introducing young students to simple physical science concepts, and for providing them with opportunities to engage in higher order thinking processes.}
}
@article{OLCAYSOYOKTEN2022111606,
title = {When knowledge is blinding: The dangers of being certain about the future during uncertain societal events},
journal = {Personality and Individual Differences},
volume = {195},
pages = {111606},
year = {2022},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2022.111606},
url = {https://www.sciencedirect.com/science/article/pii/S0191886922001106},
author = {Irmak {Olcaysoy Okten} and Anton Gollwitzer and Gabriele Oettingen},
keywords = {Subjective certainty, Future thinking, COVID-19, Elections, Information seeking, Uncertainty},
abstract = {Past research has independently examined the concepts of certainty and future thought. Here we combine these concepts by examining the cognitive and behavioral outcomes of certainty about the future during periods of societal uncertainty. Three studies (N = 1218) examined future certainty, defined as feeling certain about some future event or outcome, during two major societal events of uncertainty—the COVID-19 pandemic and the 2020 U.S. Presidential Election. In Study 1, certainty about positive or negative futures of COVID-19 (e.g., the pandemic will end soon; the pandemic will never end) predicted poorer information seeking—ignorance of medical experts, adherence to conspiratorial thinking, and lower objective knowledgeability about COVID-19. Building on these findings, in Study 2, future certainty predicted antisocial health behaviors, including failing to social distance. Study 3 extended these findings to the political domain—the 2020 Presidential Election. Future certainty that one's preferred candidate would win the election predicted poor information seeking and antisocial behaviors in terms of claiming that the election was rigged, endorsing violence if one's candidate lost, and, among Trump supporters, identifying with Capitol insurrectionists. These findings suggest that future certainty is linked to intellectual blindness and antisocial behaviors during important periods of societal uncertainty.}
}
@article{ZHOU2025104052,
title = {Adaptive-solver framework for dynamic strategy selection in large language model reasoning},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104052},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104052},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004114},
author = {Jianpeng Zhou and Wanjun Zhong and Yanlin Wang and Jiahai Wang},
keywords = {Large language models, Reasoning, Math word problems, Test-time computation allocation, Dynamic strategy selection},
abstract = {Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The datasets and code are available at https://github.com/john1226966735/Adaptive-Solver.}
}
@article{POHL201654,
title = {Using lag-sequential analysis for understanding interaction sequences in visualizations},
journal = {International Journal of Human-Computer Studies},
volume = {96},
pages = {54-66},
year = {2016},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581916300829},
author = {Margit Pohl and Günter Wallner and Simone Kriglstein},
keywords = {Interaction sequences, Lag-sequential analysis, Visualization, Log files, Thinking aloud},
abstract = {The investigation of how users make sense of the data provided by information systems is very important for human computer interaction. In this context, understanding the interaction processes of users plays an important role. The analysis of interaction sequences, for example, can provide a deeper understanding about how users solve problems. In this paper we present an analysis of sequences of interactions within a visualization system and compare the results to previous research. We used log file analysis and thinking aloud as methods. There is some indication based on log file analysis that there are interaction patterns which can be generalized. Thinking aloud indicates that some cognitive processes occur together with a higher probability than others.}
}
@article{RAPAKA2025100809,
title = {Revolutionizing learning − A journey into educational games with immersive and AI technologies},
journal = {Entertainment Computing},
volume = {52},
pages = {100809},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100809},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001770},
author = {Anuj Rapaka and S.C. Dharmadhikari and Kishori Kasat and Chinnem Rama Mohan and Kuldeep Chouhan and Manu Gupta},
keywords = {Artificial intelligence (AI), Educational games, Learning styles, Stochastic swing golf optimization (SSGOA)},
abstract = {Educational games rapidly integrate entertainment technology and learning, engaging individuals in dynamic educational experiences. These games incorporate multimedia content to encourage critical thinking, problem-solving and information retention. Educational games employ immersive technology such as virtual and augmented reality to transfer individuals to simulated worlds, hence improving learning. Furthermore, artificial intelligence (AI) technologies optimize educational experiences by adjusting information to individual learning styles, providing focused feedback as well as encouraging a more effective and entertaining learning technology. The integration of educational games with immersive and AI technology provides great potential for transforming how individuals acquire and apply information sharing. This research determined the creation of significant educational applications that are personalized and adaptive through the use of image, emotional recognition and speech, intelligent agents that replicate the effects of an individual opponent and control over the complexities of game levels along with information. The study evaluated the different tools that educators and learners could utilize to develop immersive and artificial intelligence-based instructional games without a requirement for programming knowledge. The study demonstrates that immersive technology and AI technology could represent beneficial resources for creating educational video games and entertainment technology. The research highlights the novel possibilities of stochastic swing golf optimization (SSGOA) immersive and AI technologies providing an innovative approach to developing effective as well as attractive learning environments.}
}
@article{WEHNER202387,
title = {On the ‘cognitive map debate’ in insect navigation},
journal = {Studies in History and Philosophy of Science},
volume = {102},
pages = {87-89},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S003936812300105X},
author = {Rüdiger Wehner and Thierry Hoinville and Holk Cruse},
keywords = {Insect navigation, Cognitive map, Neural network model, Path integration, Landmark guidance, Ants, Bees},
abstract = {In a historical account recently published in this journal Dhein argues that the current debate whether insects like bees and ants use cognitive maps (centralized map hypothesis) or other means of navigation (decentralized network hypothesis) largely reflects the classical debate between American experimental psychologists à la Tolman and German ethologists à la Lorenz, respectively. In this dichotomy we, i.e., the proponents of the network hypothesis, are inappropriately placed on the Lorenzian line. In particular, we argue that in contrast to Dhein's claim our concepts are not based on merely instinctive or peripheral modes of information processing. In general, on the one side our approaches have largely been motivated by the early biocybernetics way of thinking. On the other side they are deeply rooted in studies on the insect's behavioral ecology, i.e., in the ecological setting within which the navigational strategies have evolved and within which the animal now operates. Following such a bottom-up approach we are not “anti-cognitive map researchers” but argue that the results we have obtained in ants, and also the results of some decisive experiments in bees, can be explained and simulated without the need of invoking metric maps.}
}
@article{ARKOUDAS2008461,
title = {Computation, hypercomputation, and physical science},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {461-475},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1570868308000414},
author = {Konstantine Arkoudas},
keywords = {Hypercomputation, Church–Turing thesis, Gandy's thesis, Mechanical computability, Algorithms, Turing limit, Physical science, Physical computability},
abstract = {Copeland and others have argued that the Church–Turing thesis (CTT) has been widely misunderstood by philosophers and cognitive scientists. In particular, they have claimed that CTT is in principle compatible with the existence of machines that compute functions above the “Turing limit,” and that empirical investigation is needed to determine the “exact membership” of the set of functions that are physically computable. I argue for the following points: (a) It is highly doubtful that philosophers and cognitive scientists have widely misunderstood CTT as alleged.1 In fact, by and large, computability theorists and mathematical logicians understand CTT in the exact same way. (b) That understanding most likely coincides with what Turing and Church had in mind. Even if it does not, an accurate exegesis of Turing and Church need not dictate how today's working scientists understand the thesis. (c) Even if we grant Copeland's reading of CTT, an orthodox stronger version of it which he rejects (Gandy's thesis) follows readily if we only accept a highly plausible necessary condition for what constitutes a deterministic digital computer. Finally, (d) regardless of whether we accept this condition, the prospects for a scientific theory of hypercomputation are exceedingly poor because physical science does not have the wherewithal to investigate computability or to discover its ultimate “limit.”}
}
@article{MASON2020103898,
title = {Development and analysis of the Elementary Student Coding Attitudes Survey},
journal = {Computers & Education},
volume = {153},
pages = {103898},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103898},
url = {https://www.sciencedirect.com/science/article/pii/S036013152030097X},
author = {Stacie L. Mason and Peter J. Rich},
keywords = {Elementary education, Computational thinking, Coding, Attitude scale, Instrument validation},
abstract = {There is an increasing emphasis on teaching young learners to code; yet, there are few tools designed to measure the effect of learning to code on children. The purpose of this study was to develop and validate a tool to assess changes in young learners' attitudes toward coding: the Elementary Student Coding Attitudes Survey (ESCAS). We validated the scale using Confirmatory Factory Analysis and Structural Equation Modeling with responses from over 6000 4th-6th grade students (aged 9–12 years). Survey validation revealed a scale consisting of five constructs that comprise young learners' attitudes toward coding: social value, coding confidence, coding interest, perception of coders, and coding utility. In our analysis, students' grade level, ethnicity, gender, coding frequency, coding experience, and math interest influenced social value, which in turn influenced coding interest, perception of coders, and coding utility. Students' math confidence, coding frequency, coding experience, ethnicity, and coding interest predicted their coding confidence. Among observable variables, coding frequency and math interest had the greatest influence on social value, which substantially influenced all other factors. We discuss how this tool can help those who teach coding to young children to better measure and understand the variables that may influence young learners’ attitudes toward coding over time.}
}
@article{CROOKS2014344,
title = {Defining and measuring conceptual knowledge in mathematics},
journal = {Developmental Review},
volume = {34},
number = {4},
pages = {344-377},
year = {2014},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229714000380},
author = {Noelle M. Crooks and Martha W. Alibali},
keywords = {Mathematical thinking, Conceptual knowledge, Equivalence, Cardinality, Inversion},
abstract = {A long tradition of research on mathematical thinking has focused on procedural knowledge, or knowledge of how to solve problems and enact procedures. In recent years, however, there has been a shift toward focusing, not only on solving problems, but also on conceptual knowledge. In the current work, we reviewed (1) how conceptual knowledge is defined in the mathematical thinking literature, and (2) how conceptual knowledge is defined, operationalized, and measured in three mathematical domains: equivalence, cardinality, and inversion. We uncovered three general issues. First, few investigators provide explicit definitions of conceptual knowledge. Second, the definitions that are provided are often vague or poorly operationalized. Finally, the tasks used to measure conceptual knowledge do not always align with theoretical claims about mathematical understanding. Together, these three issues make it challenging to understand the development of conceptual knowledge, its relationship to procedural knowledge, and how it can best be taught to students. In light of these issues, we propose a general framework that divides conceptual knowledge into two facets: knowledge of general principles and knowledge of the principles underlying procedures.}
}
@article{HEYN2023111604,
title = {A compositional approach to creating architecture frameworks with an application to distributed AI systems},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111604},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111604},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002801},
author = {Hans-Martin Heyn and Eric Knauss and Patrizio Pelliccione},
keywords = {AI systems, Architectural frameworks, Compositional thinking, Requirements engineering, Systems engineering},
abstract = {Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). This poses additional architectural challenges. To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks in a use case of distributed AI systems. Then, we derive from the identified challenges four rules, and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project “Very efficient deep learning in the IoT” (VEDLIoT) in the form of a case study.}
}
@article{DEBARROS2012171,
title = {Quantum-like model of behavioral response computation using neural oscillators},
journal = {Biosystems},
volume = {110},
number = {3},
pages = {171-182},
year = {2012},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2012.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264712001736},
author = {J. Acacio {de Barros}},
keywords = {Disjunction effect, Quantum cognition, Quantum-like model, Neural oscillators, Stimulus-response theory},
abstract = {In this paper we propose the use of neural interference as the origin of quantum-like effects in the brain. We do so by using a neural oscillator model consistent with neurophysiological data. The model used was shown elsewhere to reproduce well the predictions of behavioral stimulus-response theory. The quantum-like effects are brought about by the spreading activation of incompatible oscillators, leading to an interference-like effect mediated by inhibitory and excitatory synapses.}
}
@article{BARBER200798,
title = {Interplay between computational models and cognitive electrophysiology in visual word recognition},
journal = {Brain Research Reviews},
volume = {53},
number = {1},
pages = {98-123},
year = {2007},
issn = {0165-0173},
doi = {https://doi.org/10.1016/j.brainresrev.2006.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0165017306000853},
author = {Horacio A. Barber and Marta Kutas},
keywords = {Language, Reading, Visual word recognition, Computational model, Event-related potential (ERP), EEG, MEG, Cognitive electrophysiology},
abstract = {In this article, we discuss the relevance of electrophysiological data to the enterprise of analyzing and understanding the reading process. Specifically, we detail how the event-related brain potential (ERP) technique (and its magnetic counterpart) can aid in development of models of visual word recognition. Any viable and accurate account of reading must take into account the temporal and anatomical constraints imposed by the fact that reading is a human brain function. We believe that neurophysiological (especially, although not limited to electrophysiological) data can serve an essential reference in the development of biologically realistic models of reading. We assess just how well extant electrophysiological data comport with specific predictions of existing computational models and offer some suggestions for the kinds of research that can address some of the remaining open questions.}
}
@article{MENGOV2014232,
title = {Person-by-person prediction of intuitive economic choice},
journal = {Neural Networks},
volume = {60},
pages = {232-245},
year = {2014},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002123},
author = {George Mengov},
keywords = {Decision making, Economic choice, Experimental economics, Gated dipole, Intuitive thinking, Differential equations},
abstract = {Decision making is an interdisciplinary field, which is explored with methods spanning from economic experiments to brain scanning. Its dominant paradigms such as utility theory, prospect theory, and the modern dual-process theories all resort to formal algebraic models or non-mathematical postulates, and remain purely phenomenological. An approach introduced by Grossberg deployed differential equations describing neural networks and bridged the gap between decision science and the psychology of cognitive–emotional interactions. However, the limits within which neural models can explain data from real people’s actions are virtually untested and remain unknown. Here we show that a model built around a recurrent gated dipole can successfully forecast individual economic choices in a complex laboratory experiment. Unlike classical statistical and econometric techniques or machine learning algorithms, our method calibrates the equations for each individual separately, and carries out prediction person-by-person. It predicted very well the behaviour of 15%–20% of the participants in the experiment–half of them extremely well–and was overall useful for two thirds of all 211 subjects. The model succeeded with people who were guided by gut feelings and failed with those who had sophisticated strategies. One hypothesis is that this neural network is the biological substrate of the cognitive system for primitive–intuitive thinking, and so we believe that we have a model of how people choose economic options by a simple form of intuition. We anticipate our study to be useful for further studies of human intuitive thinking as well as for analyses of economic systems populated by heterogeneous agents.}
}
@article{ZOMAYA2004551,
title = {Parallel and nature-inspired computational paradigms and applications},
journal = {Parallel Computing},
volume = {30},
number = {5},
pages = {551-552},
year = {2004},
note = {Parallel and nature-inspired computational paradigms and applications},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2004.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167819104000304},
author = {Albert Y Zomaya and Fikret Ercal and El-ghazali Talbi}
}
@article{HU2015287,
title = {A new face recognition method based on image decomposition for single sample per person problem},
journal = {Neurocomputing},
volume = {160},
pages = {287-299},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.02.032},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215001812},
author = {Changhui Hu and Mengjun Ye and Saiping Ji and Weili Zeng and Xiaobo Lu},
keywords = {Face recognition, Single sample per person problem, Lower-upper decomposition algorithm, Reverse thinking approach based on experimental analysis},
abstract = {The image decomposition based method is one of the efficient and important face recognition solutions for the single sample per person problem. The low image decomposition performance and the unconvincing reconstruction of the approximation image are the two main limitations of the previous methods. In this paper, a new single sample face recognition method based on lower-upper (LU) decomposition algorithm is proposed. The procedure of the proposed method is as following. First, the single sample and its transpose are decomposed to two sets of basis images by using the LU decomposition algorithm, which is more efficient than the image decomposition algorithms of the previous works. Two approximation images are reconstructed from the two basis image sets by the reverse thinking approach based on experimental analysis. Then, the fisher linear discriminant analysis (FLDA) algorithm is used to evaluate the optimal projection space by using the new training set consisting of the single sample and its two approximation images for each person. Finally, the nearest neighbor classifier based on Euclidean distance is adopted as the final classification. We make two main contributions: one is that we propose to decompose the single sample and its transpose using the efficient LU decomposition algorithm, and reorder each basis image set according to the basis image energy; the other is that we present a reverse thinking approach based on experimental analysis to reconstruct the approximation image. The performance of the proposed method is verified using four public face databases, namely FERET, AR, ORL and Yale B. The experimental results indicate that the proposed method is efficient and outperforms several state-of-the-art approaches which are proposed to address the single sample per person problem.}
}
@article{HUNTER2025100118,
title = {Is LIWC reliable, efficient, and effective for the analysis of large online datasets in forensic and security contexts?},
journal = {Applied Corpus Linguistics},
volume = {5},
number = {1},
pages = {100118},
year = {2025},
issn = {2666-7991},
doi = {https://doi.org/10.1016/j.acorp.2025.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2666799125000012},
author = {Madison Hunter and Tim Grant},
keywords = {LIWC, Reliability, Computerized text analysis, Forensic linguistics, Discourse analysis},
abstract = {This article evaluates the reliability, efficiency, and effectiveness of Linguistic Inquiry and Word Count (LIWC; Boyd et al., 2022) for the analysis of a white nationalist forum. This is important because LIWC has been the computational tool of choice for scores of studies generally and many examining extremist content in a forensic or security context. Our purpose, therefore, is to understand whether LIWC can be depended upon for large-scale analyses; we initially examine this here using a small sample of posts from a set of just eight users and manually checking the program's automated codings of a subset of categories. Our results show that the LIWC coding cannot be relied upon – precision falls to as low as 49.6 % and recall as low as 41.7 % for some categories. It would be possible to engage in considerable manual correction of these results, but this undermines its purported efficiency for large datasets.}
}
@incollection{HALL2006338,
title = {Computational Approaches to Fibril Structure and Formation},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {412},
pages = {338-365},
year = {2006},
booktitle = {Amyloid, Prions, and Other Protein Aggregates, Part B},
issn = {0076-6879},
doi = {https://doi.org/10.1016/S0076-6879(06)12020-0},
url = {https://www.sciencedirect.com/science/article/pii/S0076687906120200},
author = {Carol K. Hall and Victoria A. Wagoner},
abstract = {Assembly of normally soluble proteins into amyloid fibrils is a cause or associated symptom of numerous human disorders. Although some progress toward understanding the molecular‐level details of fibril structure has been made through in vitro experiments, the insoluble nature of fibrils make them difficult to study experimentally. We describe two computational approaches used to investigate fibril formation and structure: intermediate‐resolution discontinuous molecular dynamics simulations and atomistic molecular dynamics simulations. Each method has its strengths and weaknesses, but taken together the two approaches provide a useful molecular‐level picture of fibril structure and formation.}
}
@article{TUZUN202085,
title = {Introduction to systems engineering and sustainability PART I: Student-centred learning for chemical and biological engineers},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {85-93},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772820300269},
author = {U. Tuzun},
keywords = {Systems engineering & sustainability, Active learning, Formative assessment},
abstract = {Student-Centred Active Learning of Systems Engineering and Sustainability requires challenging metacognitive integration of high-level evaluation skills combined with discipline-based core knowledge. This two-part series aims to demonstrate the basic principles, methodology and specific examples of active learning with formative assessment implemented to achieve improved student academic performance. In this part I of the two-part series, firstly, a detailed description is introduced of the cognitive learning methodology which makes use of student-centered recognition, analysis and synthesis for decision-making when there is no entirely right or wrong decision. The concept of “decision situation” is described which combines several surrounding and contingency elements to arrive at a demonstration of the holistic decision-making through systems analysis. A Holistic thinking approach is further developed using a systems learning methodology that combines normative with descriptive analyses to arrive at a cognitive mode model of judgement and choice. Sustainability modelling using the three-gateway systems approach is introduced and compared with the multi-layered view of chemical and biochemical engineering education and research; see Gani et al. (2020). Holistic thinking strategy is applied most recently to integrating, backcasting and eco-design for the circular economy (CE); see Mendoza et al. (2017). A student-centred learning approach is advocated that makes use of these principles and enables the systematic embedding of sustainability modeling in industrial and economic activities whose success rely substantively on decision-making. Finally, the relative importance is evaluated using classroom data available with specific engineering topics of the didactic “rule-based” methods of knowledge transfer in contrast with the experiential accumulation of practical information amassed through social interactions in a co-operative learning environment that relies on sustained improvement through active communication and feedback between the teacher/instructor and the student/learner; see Stephan et al. (2017) and Shallcross and Alpay (2018).}
}
@article{BOKHOVE2023100497,
title = {Using Social Network Analysis to gain insight into social creativity while designing digital mathematics books},
journal = {Social Sciences & Humanities Open},
volume = {8},
number = {1},
pages = {100497},
year = {2023},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2023.100497},
url = {https://www.sciencedirect.com/science/article/pii/S259029112300102X},
author = {Christian Bokhove and Marios Xenos and Manolis Mavrikis},
keywords = {Technological environment, Co-creation, Social creativity, Social network analysis},
abstract = {Analysing the processes and products of creativity to better understand and support individuals and teams, is a difficult and elusive challenge despite years of research in creativity. In this article, we are particularly interested in social creativity in communities of interest. Building on Guilford's classic model of Divergent Thinking of fluency, flexibility, originality and elaboration, we employ Social Network Analysis to model the creative design process. The creative process in the current study takes place in a technological environment called the ‘MC-squared platform’, in which members of a community of interest collaborate in a social, co-creative process for designing digital, mathematical textbooks. Both the technological environment and the methodology are exemplified through two case examples, one on the design process of a digital book about a bioclimatic amusement park and one on the design process of a digital book about fractions. We conclude that, for these examples, both the technological tool and the data analysis approach provide insight into the social creativity process of the community of interest.}
}
@incollection{BICKHARD2025169,
title = {The mentality of Homo Sapiens},
editor = {Mark H. Bickhard},
booktitle = {The Whole Person},
publisher = {Academic Press},
pages = {169-260},
year = {2025},
isbn = {978-0-443-33050-6},
doi = {https://doi.org/10.1016/B978-0-443-33050-6.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330506000094},
author = {Mark H. Bickhard},
keywords = {interactive knowing, situation knowledge, presupposition, higher order motivation and values, themes, central nervous system as a microgenetic dynamic system, thinking and concepts, contact-content, indexicality, attention, will, learning, topologies in heuristic learning, “enactive”, “semantic”, episodic, and autobiographical memory, knowing levels, development, emotions, evolution of emotion processes, organizations of microgenesis processes, reflective consciousness, rehearsal, purpose, emergence of reflection in the brain, experiencing, experiencing of experiencing, higher order theory, Brentano, Searle, folk psychology},
abstract = {The central macro-evolutionary ratchet supports myriad further emergences as specializations, further developments, and various kinds of interactions among the differing processes. This chapter, in the section on interactive knowing, addresses, for example, situation knowledge, presupposition, higher order motivation and values, the important process characteristic of themes, central nervous system as a microgenetic dynamic system, thinking and concepts, the contact-content distinction, indexicality, attention, and will. Concerning learning, the function of topologies in heuristic learning is explored, multiple kinds of memory are addressed, such as “enactive”, “semantic”, episodic, and autobiographical, and some of the enabling and constraining developmental consequences of knowing levels are examined. In the section on emotions, basic principles of developmental differentiations of emotions are examined, and the evolution of emotion processes within organizations of microgenesis processes is framed. The discussion of reflective consciousness addresses enablings such as rehearsal and purpose, and extends the evolutionary analysis to the emergence of the possibility of reflection in the brain. Issues concerning experiencing, such as unities and boundaries, are addressed. Reflective experiencing of experiencing is presented as a resolution of a fundamental problem in the literature. Some further literature is addressed, such as higher order theory, Brentano, Searle, and folk psychology.}
}
@article{SAVARD2025109184,
title = {Toward cognitive models of misophonia},
journal = {Hearing Research},
volume = {458},
pages = {109184},
year = {2025},
issn = {0378-5955},
doi = {https://doi.org/10.1016/j.heares.2025.109184},
url = {https://www.sciencedirect.com/science/article/pii/S0378595525000036},
author = {Marie-Anick Savard and Emily B.J. Coffey},
keywords = {Misophonia, Cognitive science, Cognitive neuroscience, Sound sensitivity, Models},
abstract = {Misophonia is a disorder in which specific common sounds such as another person breathing or chewing, or the ticking of a clock, cause an atypical negative emotional response. Affected individuals may experience anger, irritability, annoyance, disgust, and anxiety, as well as physiological autonomic responses, and may find everyday environments and contexts to be unbearable in which their ‘misophonic stimuli’ (often called ‘trigger sounds’) are present. Misophonia is gradually being recognized as a genuine problem that causes significant distress and has negative consequences for individuals and their families. It has only recently come under scientific scrutiny, as researchers and clinicians are establishing its prevalence, distinguishing it from other disorders of sensory sensitivity such as hyperacusis, establishing its neurobiological bases, and evaluating the effectiveness of potential treatments. While ideas abound as to the mechanisms involved in misophonia, few have coalesced into models. The aim of the present work is to summarize and extend recent thinking on the mechanistic basis of misophonia, with a focus on moving towards neurologically-informed cognitive models that can (a) account for extant findings, and (b) generate testable predictions. We hope this work will facilitate future refinements in our understanding of misophonia, and ultimately inform treatments.}
}
@article{SCHEENSTRA2022,
title = {Digital Health Solutions to Reduce the Burden of Atherosclerotic Cardiovascular Disease Proposed by the CARRIER Consortium},
journal = {JMIR Cardio},
volume = {6},
number = {2},
year = {2022},
issn = {2561-1011},
doi = {https://doi.org/10.2196/37437},
url = {https://www.sciencedirect.com/science/article/pii/S256110112200037X},
author = {Bart Scheenstra and Anke Bruninx and Florian {van Daalen} and Nina Stahl and Elizabeth Latuapon and Maike Imkamp and Lianne Ippel and Sulaika Duijsings-Mahangi and Djura Smits and David Townend and Inigo Bermejo and Andre Dekker and Laura Hochstenbach and Marieke Spreeuwenberg and Jos Maessen and Arnoud {van 't Hof} and Bas Kietselaer},
keywords = {atherosclerotic cardiovascular disease, ASCVD, cardiovascular risk management, CVRM, eHealth, digital Health, personalized e-coach, big data, clinical prediction models, federated data infrastructure},
abstract = {Digital health is a promising tool to support people with an elevated risk for atherosclerotic cardiovascular disease (ASCVD) and patients with an established disease to improve cardiovascular outcomes. Many digital health initiatives have been developed and employed. However, barriers to their large-scale implementation have remained. This paper focuses on these barriers and presents solutions as proposed by the Dutch CARRIER (ie, Coronary ARtery disease: Risk estimations and Interventions for prevention and EaRly detection) consortium. We will focus in 4 sections on the following: (1) the development process of an eHealth solution that will include design thinking and cocreation with relevant stakeholders; (2) the modeling approach for two clinical prediction models (CPMs) to identify people at risk of developing ASCVD and to guide interventions; (3) description of a federated data infrastructure to train the CPMs and to provide the eHealth solution with relevant data; and (4) discussion of an ethical and legal framework for responsible data handling in health care. The Dutch CARRIER consortium consists of a collaboration between experts in the fields of eHealth development, ASCVD, public health, big data, as well as ethics and law. The consortium focuses on reducing the burden of ASCVD. We believe the future of health care is data driven and supported by digital health. Therefore, we hope that our research will not only facilitate CARRIER consortium but may also facilitate other future health care initiatives.}
}
@article{ZHOU2025110885,
title = {Developing a deep reinforcement learning model for safety risk prediction at subway construction sites},
journal = {Reliability Engineering & System Safety},
volume = {257},
pages = {110885},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2025.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0951832025000894},
author = {Zhipeng Zhou and Wen Zhuo and Jianqiang Cui and Haiying Luan and Yudi Chen and Dong Lin},
keywords = {Subway construction safety, Grounded theory, Deep reinforcement learning, Double deep Q-network, Permutation importance},
abstract = {Underground construction work is heavily affected by surrounding hydrogeology, adjacent pipelines, and existing subway lines, which can lead to a high degree of uncertainty and generate safety risk on site. In order to overcome rigid thinking of causal factors within a structured framework and incorporate features of different accidents, this study adopted grounded theory for the investigation on factors contributing to workplace accidents in subway construction. The deep reinforcement learning model of double deep Q-network (DDQN) was developed for predicting subway construction safety risk, which integrated the advantage of reinforcement learning in decision making with the advantage of deep learning in objection perception. The findings denoted that DDQN performed better than other machine learning models inclusive of random forest, extreme gradient boosting, k-nearest neighbor, and support vector machine. Contributing factors relevant to subway construction accidents were quantitatively analyzed using permutation importance of attributes. It was beneficial for determining how the 37 contributing factors had negative effects on subway construction safety risk. Safety measures for risk reduction and controlling could be optimized according to permutation importance of individual contributing factor, which paved a new way for the promotion of safety management performance at subway construction sites.}
}
@incollection{VASSILOPOULOS2010139,
title = {5 - Novel computational methods for fatig life modeling of composite materials},
editor = {Anastasios P. Vassilopoulos},
booktitle = {Fatigue Life Prediction of Composites and Composite Structures},
publisher = {Woodhead Publishing},
pages = {139-173},
year = {2010},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-1-84569-525-5},
doi = {https://doi.org/10.1533/9781845699796.1.139},
url = {https://www.sciencedirect.com/science/article/pii/B978184569525550005X},
author = {A.P. Vassilopoulos and E.F. Georgopoulos},
keywords = {fatigue, composites, artificial neural network, genetic programming, ANFIS, S–N curves},
abstract = {Abstract:
Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material-independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.}
}
@article{ELABY2022101780,
title = {Does design-build concept improve problem-solving skills? An analysis of first-year engineering students},
journal = {Ain Shams Engineering Journal},
volume = {13},
number = {6},
pages = {101780},
year = {2022},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.101780},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922000910},
author = {Mohammed F. Elaby and Hesham M. Elwishy and Saeed F. Moatamed and Mahmoud A. Abdelwahed and Ahmed E. Rashiedy},
keywords = {Problem-solving, Critical thinking, Design-build studio, Instructional design},
abstract = {The design-build studio (DBS) is an emergent paradigm in architecture education. Recently, researchers have addressed the success of integrating the design-build concept into the conventional studio in the advanced years of education to improve several issues related to the design process: social, environmental, technological, …etc. However, its efficiency in terms of contribution to the learning experience has not yet been addressed. This paper examines the implementation of the DBS concept as a new teaching model to improve the problem-solving skills (PSS) of first-year students in engineering education. It also discusses the effectson learning experiences and pedagogical outcomes in both quantitative and qualitative terms. The research’s results identify the significance of applying this model as a real-simulated method-based learning experience. It can help students to improve their learning experiences and to enhance students self-confidence regarding PSS.}
}
@incollection{RUFFONI201191,
title = {3.307 - Finite Element Analysis in Bone Research: A Computational Method Relating Structure to Mechanical Function},
editor = {Paul Ducheyne},
booktitle = {Comprehensive Biomaterials},
publisher = {Elsevier},
address = {Oxford},
pages = {91-111},
year = {2011},
isbn = {978-0-08-055294-1},
doi = {https://doi.org/10.1016/B978-0-08-055294-1.00093-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080552941000933},
author = {D. Ruffoni and G.H. {van Lenthe}},
keywords = {Bone imaging, Bone research, Computational modeling, Femur, Finite element analysis, Fracture, Hierarchical structure, Microcomputed tomography, Osteoporosis, Radius, Strength, Vertebra},
abstract = {Bone is probably the most frequently investigated biological material and finite element analysis (FEA) is the computational tool most commonly used for the analysis of bone biomechanical function. FEA has been used in bone research for more than 30years and has had a substantial impact on our understanding of the complex behavior of bone. Bone is structured in a hierarchical way covering many length scales and this chapter reflects this hierarchical organization. In particular, the focus is on the applications of FEA for understanding the relationship between bone structure and its mechanical function at specific hierarchical levels. Depending on the hierarchical level, different issues have been investigated with FEA ranging from more clinically oriented topics related to bone quality (e.g., predicting bone strength and fracture risk) to more fundamental problems dealing with the mechanical aspects of biological processes (e.g., stress and strain around osteocyte lacunae) as well as with the micromechanical behavior of bone at its ultrastructure. A better understanding of the relationship between structure and mechanical function is expected to be important for the current trends in (bio)materials design, where the structure of biological materials is considered as a possible source of inspiration, as well as for more successful approaches in the prevention and treatment of age- and disease-related fractures.}
}

@article{LEESON2008630,
title = {Cognitive ability, personality, and academic performance in adolescence},
journal = {Personality and Individual Differences},
volume = {45},
number = {7},
pages = {630-635},
year = {2008},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908002444},
author = {Peter Leeson and Joseph Ciarrochi and Patrick C.L. Heaven},
keywords = {Cognitive ability, Personality, Academic performance, Adolescents, Hope, Self-esteem, Attributional style, Psychometric },
abstract = {Does positive thinking predict variance in school grades over and above that predicted by cognitive ability? Six hundred and thirty nine high school students participated in a three-year longitudinal study that predicted grades using cognitive ability and three positive thinking variables – self-esteem, hope, and attributional style. Hope, positive attributional style and cognitive ability predicted higher grades, whilst self-esteem was a less consistent predictor of academic performance. Structural equation modelling revealed significant paths from cognitive ability, gender, and a second order positive thinking factor to grades. The results suggest that intelligence, gender, and positive thinking each play a unique role in predicting academic performance in youth. Some suggestions for further research are made.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{BATT2002185,
title = {Lateral thinking: 2-D interpretation of thermochronology in convergent orogenic settings},
journal = {Tectonophysics},
volume = {349},
number = {1},
pages = {185-201},
year = {2002},
note = {Low Temperature Thermochronology: From Tectonics to Landscape Evolution},
issn = {0040-1951},
doi = {https://doi.org/10.1016/S0040-1951(02)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040195102000537},
author = {Geoffrey E. Batt and Mark T. Brandon},
keywords = {Lateral motion, Thermochronology, Orogenic regions},
abstract = {Lateral motion of material relative to the regional thermal and kinematic frameworks is important in the interpretation of thermochronology in convergent orogens. Although cooling ages in denuded settings are commonly linked to exhumation, such data are not related to instantaneous behavior but rather to an integration of the exhumation rates experienced between the thermochronological ‘closure’ at depth and subsequent exposure at the surface. The short spatial wavelength variation of thermal structure and denudation rate typical of orogenic regions thus renders thermochronometers sensitive to lateral motion during exhumation. The significance of this lateral motion varies in proportion with closure temperature, which controls the depth at which isotopic closure occurs, and hence, the range of time and length scales over which such data integrate sample histories. Different chronometers thus vary in the fundamental aspects of the orogenic character to which they are sensitive. Isotopic systems with high closure temperature are more sensitive to exhumation paths and the variation in denudation and thermal structure across a region, while those of lower closure temperature constrain shorter-term behaviour and more local conditions. Discounting lateral motion through an orogenic region and interpreting cooling ages purely in terms of vertical exhumation can produce ambiguous results because variation in the cooling rate can result from either change in kinematics over time or the translation of samples through spatially varying conditions. Resolving this ambiguity requires explicit consideration of the physical and thermal framework experienced by samples during their exhumation. This can be best achieved through numerical simulations coupling kinematic deformation to thermal evolution. Such an approach allows the thermochronological implications of different kinematic scenarios to be tested, and thus provides an important means of assessing the contribution of lateral motion to orogenic evolution.}
}
@article{JACKSON201386,
title = {Airflow reversal and alternating corkscrew vortices in foredune wake zones during perpendicular and oblique offshore winds},
journal = {Geomorphology},
volume = {187},
pages = {86-93},
year = {2013},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2012.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13000081},
author = {Derek W.T. Jackson and Meiring Beyers and Irene Delgado-Fernandez and Andreas C.W. Baas and Andrew J. Cooper and Kevin Lynch},
keywords = {Computational fluid dynamics, Aeolian, Foredunes, Transport, Airflow modelling, Lee side eddies},
abstract = {On all sandy coastlines fringed by dunes, understanding localised air flow allows us to examine the potential sand transfer between the beach and dunes by wind-blown (Aeolian) action. Traditional thinking into this phenomenon had previously included only onshore winds as effective drivers of this transfer. Recent research by the authors, however, has shown that offshore air-flow too can contribute significantly, through lee-side back eddies, to the overall windblown sediment budget to coastal dunes. Under rising sea levels and increased erosion scenarios, this is an important process in any post-storm recovery of sandy beaches. Until now though, full visualisation in 3D of this newly recognised mechanism in offshore flows has not been achieved. Here, we show for the first time, this return flow eddy system using 3D computational fluid dynamics modelling, and reveal the presence of complex corkscrew vortices and other phenomena. The work highlights the importance of relatively small surface undulations in the dune crest which act to induce the spatial patterns of airflow (and transport) found on the adjacent beach.}
}
@article{ZENASNI2009353,
title = {Perception of emotion, alexithymia and creative potential},
journal = {Personality and Individual Differences},
volume = {46},
number = {3},
pages = {353-358},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004108},
author = {F. Zenasni and T.I. Lubart},
keywords = {Creativity, Ability EI, Alexythimia, Emotional creativity, Divergent thinking},
abstract = {Theoretical proposals suggest that emotional intelligence (EI) may favor creativity. In the present paper, two studies are reported with French adults to examine the degree to which the ability to identify emotion is related to creative performance. This component of ability EI was hypothesized to be positively associated with a divergent thinking task involving emotional information. Contrary to our expectations, the first study (n=95) indicated that ability to identify emotions in faces and images was negatively related to idea generation ability. The second study (n=100) including a measure of alexithymia confirmed this relation. Moreover, evaluating emotional creativity, we observed a significant negative link between the ability to identify emotions and the tendency to experience emotions differently from those of others. We discuss these results suggesting an opposition between consensual/convergent thinking concerning emotions (ability EI) and divergent thinking.}
}
@article{GADALLA2023200201,
title = {Concepts and experiments on psychoanalysis driven computing},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200201},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200201},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000261},
author = {Minas Gadalla and Sotiris Nikoletseas and José Roberto {de A. Amazonas} and José D.P. Rolim},
keywords = {Lacanian discourses, Psychoanalysis computing, GPT-3},
abstract = {This research investigates the effective incorporation of the human factor and user perception in text-based interactive media. In such contexts, the reliability of user texts is often compromised by behavioural and emotional dimensions. To this end, several attempts have been made in the state of the art, to introduce psychological approaches in such systems, including computational psycholinguistics, personality traits and cognitive psychology methods. In contrast, our method is fundamentally different since we employ a psychoanalysis-based approach; in particular, we use the notion of Lacanian discourse types, to capture and deeply understand real (possibly elusive) characteristics, qualities and contents of texts, and evaluate their reliability. As far as we know, this is the first time computational methods are systematically combined with psychoanalysis. We believe such psychoanalytic framework is fundamentally more effective than standard methods, since it addresses deeper, quite primitive elements of human personality, behaviour and expression which usually escape methods functioning at “higher”, conscious layers. In fact, this research is a first attempt to form a new paradigm of psychoanalysis-driven interactive technologies, with broader impact and diverse applications. To exemplify this generic approach, we apply it to the case-study of fake news detection; we first demonstrate certain limitations of the well-known Myers–Briggs Type Indicator (MBTI) personality type method, and then propose and evaluate our new method of analysing user texts and detecting fake news based on the Lacanian discourses psychoanalytic approach.}
}
@article{SUN2025129677,
title = {StereoSqueezeNet: With fewer parameters but higher accuracy than SqueezeNet},
journal = {Neurocomputing},
volume = {627},
pages = {129677},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129677},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225003492},
author = {Qiaoyan Sun and Jianfei Chen},
keywords = {SSNet, Stereo, SqueezeNet, Network compression, Network parameters},
abstract = {Convolutional neural networks (CNNs) have evolved from the initial LeNet to date, and network models have become increasingly deep and comprehensive. It has been proven that deeper networks have better fitting effects, but the corresponding parameter size and computational complexity increase rapidly. With the continuous development of mobile Internet technology, portable devices have been rapidly popularized, and users have put forward more and more demands. Thus, how to design efficient and high-performance lightweight convolutional neural networks (CNNs) is the key to solve this challenging problem. Recently, this type of convolutional neural networks (CNNs)--lightweight convolutional neural networks (CNNs), which adopt the design concept of compression networks and maintain high accuracy with fewer parameters, has attracted increasing attention. SqueezeNet is a lightweight CNN adapting to edge device deployment. Its number of parameters is only 1/50 of AlexNet, but it achieves the same accuracy as AlexNet. In order to make the network more lightweight, inspired by SqueezeNet, MobileNet, SENet, SKNet, AlexNet, etc., in this paper we propose StereoSqueezeNet, using much fewer parameters but achieving even better accuracy than SqueezeNet.}
}
@article{ARUN2009S1115,
title = {P03-116 Damage to object oriented programming in the brain explains many of the psychopathological features of schizophrenia},
journal = {European Psychiatry},
volume = {24},
pages = {S1115},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71348-3},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713483},
author = {C.P. Arun},
abstract = {Introduction
Modern computers often use programs that incorporate a programming technique called Object Oriented Programming (OOP), allowing users to manipulate complex ‘computational objects’ such as menus, screen windows, etc with very little effort, say the click of a mouse. OOP deals with structures called objects and allows time and computational effort saving devices such as inheritance, polymorphism and encapsulation. We examine whether the brain itself may use OOP and if representation of objects suffers a breakdown in schizophrenia.
Review of literature
Previous models fail to provide a unifying explanation with a computational basis that could explain the psychopathology in schizophrenia. METHODS Using the object oriented programming language JavaTM we designed a system of self-objects named ‘hand’, ‘action monitor’ etc interacting with non-self objects ‘scissors’, ‘hammer’, ‘wall’, etc. In computational experiments, we allow the ‘action monitor’ to fail; the features of disparate objects are allowed to merge, some features of an object are allowed to be shared with other objects, etc.
Results
By transposing only a few lines of code, it is possible to duplicate various features of the psychopathology of schizophrenia.
Discussion
Our model can demonstrate overinclusion (overabstraction), concrete thinking (underabstraction), loss of ego boundaries (conjoining of disparate objects), delusions (misattribution of object function), lack of insight (poor monitoring of object activity) and passivity (loss of monitoring and misattribution of object activity).
Conclusion
The brain must use the OOP model in its computations. Failure of object representation and manipulation must lie at the core of the psychopathology of schizophrenia.}
}
@article{MARENDA20232152,
title = {Sliding pendulum isolators without secretes},
journal = {Procedia Structural Integrity},
volume = {44},
pages = {2152-2157},
year = {2023},
note = {XIX ANIDIS Conference, Seismic Engineering in Italy},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2023.01.275},
url = {https://www.sciencedirect.com/science/article/pii/S2452321623002846},
author = {Ivan Marenda and Agostino Marioni and Marco Banfi and Roberto Dalpedri},
keywords = {friction coefficient, pendulum device, contact, isolation system},
abstract = {Over the last decades, anti-seismic devices have gained increasing interest in the civil engineering field. The introduction of the base isolation system has led to a new concept in the construction panorama in terms of human life safety, a new way of thinking on new constructions, improvement and retrofitting on existent structures. Therefore, rubber and friction isolators have been deeply investigated to hence performances and predict dynamic behaviour during an earthquake. While the response of the former is characterised by the composition of the elastomeric compound, the latter features special materials able to dissipate energy by moving on smooth surfaces. This paper focuses on friction pendulum devices and addresses its attention on the behaviour of sliding materials. It is well-known that stick-slip phenomenon occurs when friction excitation is present and, in the anti-seismic field is important to reduce it and have a well-representative mathematical law able to describe it. Therefore, Hirun International after performing several treatments of the sliding materials has set up a special processing to guarantee a stable response of the HI-M material used on pendulum devices. The paper, after a brief presentation of the special sliding material, shows a comparison between the material with and without the treatment in terms of the force-displacement law. The paper also analyses in detail the cinematic behaviour of the sliding pendulum with one or two main sliding surfaces, with and without central articulation and determines the stress distribution in the sliding surfaces for the different cases.}
}
@article{WEBB2008360,
title = {The role of teacher instructional practices in student collaboration},
journal = {Contemporary Educational Psychology},
volume = {33},
number = {3},
pages = {360-381},
year = {2008},
note = {Collaborative Discourse, Argumentation, and Learning},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2008.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X0800026X},
author = {Noreen M. Webb and Megan L. Franke and Marsha Ing and Angela Chan and Tondra De and Deanna Freund and Dan Battey},
keywords = {Instructional practices, Student collaboration},
abstract = {Prior research on collaborative learning identifies student behaviors that significantly predict student achievement, such as giving explanations of one’s thinking. Less often studied is the role of teachers’ instructional practices in collaboration among students. This article investigates the extent to which teachers engage in practices that support students’ explanations of their thinking, and how these teacher practices might be related to the nature of explanations that students give when asked by the teacher to collaborate with each other. The teachers observed here, all of whom received specific instruction in eliciting the details of student thinking, varied significantly in the extent to which they asked students to elaborate on their suggestions. This variation corresponded to variation across classrooms in the nature and extent of student explanations during collaborative conversations and to differences in student achievement.}
}
@article{AKTAYEVA2022285,
title = {Aesthetic education: the process of teaching mathematics with the open-source software},
journal = {Transportation Research Procedia},
volume = {63},
pages = {285-293},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522002708},
author = {Alena Aktayeva and Elena Zubareva and Aibek Dautov and Kymbat Saginbayeva and Rozamgul Niyazova and Sergey Khan and Aigerim Shonasheva},
keywords = {Aesthetic education, mathematical education, software, computer programs},
abstract = {In the article one of leading aims of educating mathematics is examined is aesthetic education of student facilities of mathematics. Presentation of aesthetic beauty at her decisions possibility of students is investigated, specifying them on the decision of one problem in several ways that assists the detailed consideration of idea of aesthetic education, through Open-source Software. The technical capabilities and elegant ease of use of systems Open-source Software provides a seamless, integrated and constantly expanding system that covers the breadth and depth of mathematical computing, and is available seamlessly through any web browser along with all modern systems used in the educational process. The article will describe understanding of beauty the decision of a problem; methods of decisions that are accompanied by the use make possible a uniquely flexible and convenient approach to charting and information visualization in a mathematical calculate. Such sort of activity assists aesthetic education, allowing to develop a culture and logical thinking, forming at students a different choice, grace of decision of problems.}
}
@article{IWASE20211,
title = {Towards a Noncompliant Pedagogy of the Image: Reading Negentropic Bifurcatory Potentials in Video Images},
journal = {Video Journal of Education and Pedagogy},
volume = {6},
number = {1},
pages = {1-27},
year = {2021},
issn = {2364-4583},
doi = {https://doi.org/10.1163/23644583-bja10020},
url = {https://www.sciencedirect.com/science/article/pii/S236445832300023X},
author = {Masayuki Iwase and Joff P. N. Bradley},
keywords = {urban film-making, critique, metamodelization, global mnemotechnical system, proletarianized knowledge, mnemonic control, artificial and living engines, machinic enslavement, negentropic bifurcation, Deleuze, Heidegger, Virilio, time-image, lectosign, spiritual automation, zooming-in/out, autistic milieus, diffractive becoming, radical pedagogy},
abstract = {The authors explore the noncompliant pedagogy of the image based on their video Autopoietic Veering: Schizo Socius of Tokyo and Vancouver (2021). It is not the kind of trendy modelized video abstract or kinetic presentation eagerly promoted by international publishers; it is a cross-cultural collaborative work intended to generate affirmative temporal ruptures of entropic habitual modes of seeing, memorizing, and thinking of human and nonhuman life in the cities of Tokyo (Japan) and Vancouver (Canada). The authors elucidate Stiegler’s (2015b) concept of a “global mnemotechnical system” that stores and produces human memories in vast digital archives and databases (tertiary retentions) through “mnemonic control” (Parisi & Goodman, 2011). The authors repurpose video images to interrupt and recontrol human perception and memories as “living engines” (Lazzarato, 2006). They foreground the philosophical work of Deleuze, Heidegger, and Virilio to rethink and revive the creative act of “critique” (Foucault, 1997) through “metamodelization” (Guattari, 1995; Manning, 2020); therefore, they plug these apparently incommensurable modes of thinking into their readings of the video’s images. They read the images as “time-images” and focus on their five dimensions that possibly activate “spiritual automation” (Deleuze, 1989), which they assess as “negentropic bifurcatory” potentials (Bradley & Kennedy, 2019).}
}
@article{HAWES201560,
title = {Effects of mental rotation training on children’s spatial and mathematics performance: A randomized controlled study},
journal = {Trends in Neuroscience and Education},
volume = {4},
number = {3},
pages = {60-68},
year = {2015},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949315000083},
author = {Zachary Hawes and Joan Moss and Beverly Caswell and Daniel Poliszczuk},
keywords = {Spatial thinking, Mental rotation, Spatial training, Computerized cognitive training, Mathematics education, STEM},
abstract = {The purpose of the current study was to (i) investigate the malleability of children’s spatial thinking, and (ii) the extent to which training-related gains in spatial thinking generalize to mathematics performance. Sixty-one 6- to 8-year-olds were randomly assigned to either computerized mental rotation training or literacy training. Training took place on iPad devices over a 6-week period as part of regular classroom activity. Results revealed that in comparison to the control group, children who received spatial training demonstrated significant gains on two measures of mental rotation and marginally significant improvements on an untrained mental transformation task; a finding that suggests that training may have had a general effect on children’s spatial ability. However, contrary to theoretical claims and prior empirical findings, there was no evidence that spatial training transferred to mathematics performance.}
}
@article{LOPEZORTEGA20133459,
title = {Computer-assisted creativity: Emulation of cognitive processes on a multi-agent system},
journal = {Expert Systems with Applications},
volume = {40},
number = {9},
pages = {3459-3470},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S095741741201295X},
author = {Omar López-Ortega},
keywords = {Computer-assisted creativity, Cognitive processes, Agent-oriented programming, Recursive systems},
abstract = {For creativity to be computed, it is paramount to understand the cognitive processes involved, which have been elucidated by either surveying creative people or discovering regions of the human brain that activate during creative endeavors. From this scattering, the author proposes a holistic framework to describe them and their interaction. Hence, creativity can be regarded as a meta process which coordinates autonomous cognitive processes such as planning or divergent thinking. To represent the interplay of cognitive processes around creativity, models are developed in the Agent Unified Modeling Language (AUML). Then, the execution of each process is delegated to autonomous agents and a global coordination protocol is devised. The implementation of the MAS is done on the JADE platform. Two modules of the resultant system are exemplified: opus planning and divergent exploration. The coordination protocol is also presented. The domain in which the software system is tested is the creation of musical pieces.}
}
@article{LI201985,
title = {Government accounting optimization based on computational linguistics},
journal = {Cognitive Systems Research},
volume = {57},
pages = {85-91},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718304650},
author = {Jiyou Li},
keywords = {Computational linguistics, Accounting optimization, Research},
abstract = {The level of moral development and moral intensity in cognitive psychology will not only affect the ethical behavior of accountants, but also have a direct impact on the quality and level of accounting work. Therefore, in this paper, the ethical behavior of accountants was analyzed from the perspective of cognitive psychology. Computer-aided data mining techniques were introduced, and government accounting risk assessment management of financial accountants was studied. In this paper, the principle of cognitive psychology to measure the ethical level of accountants was first described. The predicament of moral judgments was analyzed and an optimization plan to improve the ethical intention of accountants was proposed. Support Vector Machine classification technology in data mining was studied to explore how to conduct effective and reliable evaluation, so as to provide a scientific basis for decision-making in improving accounting management. After the simulation experiment, it is proved that continuously improving the ethical standards of accountants and strengthening the forecast of accounting risks can continue to optimize the accounting office management.}
}
@article{MUST20167,
title = {Predicting the Flynn Effect through word abstractness : Results from the National Intelligence Tests support Flynn's explanation},
journal = {Intelligence},
volume = {57},
pages = {7-14},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300253},
author = {Olev Must and Aasa Must and Jaan Mikk},
keywords = {Flynn Effect, National Intelligence Tests, Abstract thinking, Guessing, Tork, Estonia},
abstract = {The current study investigates the Flynn Effect (FE) and its relation to abstract thinking ability. We compare two cohorts of Estonian students (1933/36, n=888; 2006, n=912) using the Concepts (Logical Selection) subtest of the Estonian adaptation of the National Intelligence Tests (NIT). The item presentation order of the subtest correlates with the abstractness of the words used in the items (r=.609) of the subtest. The different test results (right, wrong and missing answers) were analysed in order to make an estimate of the FE magnitude. The FE for abstract thinking ability of those samples was 1.06 Hedges' g (adjusted for guessing). The magnitude of the FE is dependent upon the degree of difficulty of the items (an item's difficulty is estimated by determining its abstractness and its familiarity to students). The more difficult part of the subtest (the second half) showed a FE=1.80 whereas the easier part (the first half) of the subtest showed a FE=.72. Word abstractness was a strong predictor of all the testing results in both cohorts (Beta=.700). The familiarity of words used in the test items has no correlation with the test results if word abstractness is controlled in both cohorts. Our findings support Flynn's explanation that the FE is primarily an indicator of the rise in abstract thinking ability.}
}
@article{LIU2024100012,
title = {Investigating coacervates as drug carriers using molecular dynamics},
journal = {Precision Medicine and Engineering},
volume = {1},
number = {2},
pages = {100012},
year = {2024},
issn = {2950-4821},
doi = {https://doi.org/10.1016/j.preme.2024.100012},
url = {https://www.sciencedirect.com/science/article/pii/S2950482124000123},
author = {Yang Liu and Rongrong Zou and Yiwei Wang and Minghao Wang and Fan Fan and Yeqiang Zhou and Huixu Xie and Mingming Ding},
keywords = {Drug delivery, Coacervate, Molecular dynamics, Machine learning, Protein encapsulation},
abstract = {Coacervates show promise in drug delivery systems due to their biocompatibility, versatility, and outstanding ability to penetrate cells. With the advent of all-atom (AA) and coarse-grained (CG) models, these computational tools function as ‘computational microscopes’, providing valuable insights to complement experimental research. This review covers the latest innovations in coacervate-based drug delivery systems. It summarizes the molecular properties and phase behavior of coacervates composed of polyelectrolytes, intrinsically disordered proteins (IDPs), and other biomolecules, along with their interactions with carried drugs and cell membranes. Additionally, this review highlights current challenges and limitations in this fast-moving field and proposes potential avenues for future research.}
}
@incollection{HERLIHY201469,
title = {Chapter 4 - Colorless Wait-Free Computation},
editor = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
booktitle = {Distributed Computing Through Combinatorial Topology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {69-95},
year = {2014},
isbn = {978-0-12-404578-1},
doi = {https://doi.org/10.1016/B978-0-12-404578-1.00004-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045781000048},
author = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
keywords = {Configurations, Executions, Layered executions, Layered protocols, Processes, Protocols, Schedules, Tasks},
abstract = {We outline the basic connection between distributed computing and combinatorial topology in terms of two formal models: a conventional operational model, in which systems consist of communicating state machines whose behaviors unfold over time, and the combinatorial model, in which all possible behaviors are captured statically using topological notions. We start with one particular system model (shared memory) and focus on a restricted (but important) class of problems (so-called “colorless” tasks).}
}
@article{SHEFFIELD20221149,
title = {Belief Updating and Paranoia in Individuals With Schizophrenia},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {7},
number = {11},
pages = {1149-1157},
year = {2022},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2451902222000799},
author = {Julia M. Sheffield and Praveen Suthaharan and Pantelis Leptourgos and Philip R. Corlett},
keywords = {Belief updating, Computational psychiatry, Delusions, Paranoia, Volatility, Worry},
abstract = {Background
Persecutory delusions are among the most common delusions in schizophrenia and represent the extreme end of the paranoia continuum. Paranoia is accompanied by significant worry and distress. Identifying cognitive mechanisms underlying paranoia is critical for advancing treatment. We hypothesized that aberrant belief updating, which is related to paranoia in human and animal models, would also contribute to persecutory beliefs in individuals with schizophrenia.
Methods
Belief updating was assessed in 42 participants with schizophrenia and 44 healthy control participants using a 3-option probabilistic reversal learning task. Hierarchical Gaussian Filter was used to estimate computational parameters of belief updating. Paranoia was measured using the Positive and Negative Syndrome Scale and the revised Green et al. Paranoid Thoughts Scale. Unusual thought content was measured with the Psychosis Symptom Rating Scale and the Peters et al. Delusions Inventory. Worry was measured using the Dunn Worry Questionnaire.
Results
Paranoia was significantly associated with elevated win-switch rate and prior beliefs about volatility both in schizophrenia and across the whole sample. These relationships were specific to paranoia and did not extend to unusual thought content or measures of anxiety. We observed a significant indirect effect of paranoia on the relationship between prior beliefs about volatility and worry.
Conclusions
This work provides evidence that relationships between belief updating parameters and paranoia extend to schizophrenia, may be specific to persecutory beliefs, and contribute to theoretical models implicating worry in the maintenance of persecutory delusions.}
}
@article{RUNNELS201563,
title = {Capturing plasticity effects in overdriven shocks on the finite scale},
journal = {Mathematics and Computers in Simulation},
volume = {111},
pages = {63-79},
year = {2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475414003334},
author = {Scott R. Runnels},
keywords = {Shocks, Plasticity, Hardening, Hydrodynamics, Radial return},
abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.}
}
@article{HU2024101646,
title = {A flexible BERT model enabling width- and depth-dynamic inference},
journal = {Computer Speech & Language},
volume = {87},
pages = {101646},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101646},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000299},
author = {Ting Hu and Christoph Meinel and Haojin Yang},
keywords = {Grafting, Dynamic inference, Large Language Models, Deep learning},
abstract = {Fine-tuning and inference on Large Language Models like BERT have become increasingly expensive regarding memory cost and computation resources. The recently proposed computation-flexible BERT models facilitate their deployment in varied computational environments. Training such flexible BERT models involves jointly optimizing multiple BERT subnets, which will unavoidably interfere with one another. Besides, the performance of large subnets is limited by the performance gap between the smallest subnet and the supernet, despite efforts to enhance the smaller subnets. In this regard, we propose layer-wise Neural grafting to boost BERT subnets, especially the larger ones. The proposed method improves the average performance of the subnets on six GLUE tasks and boosts the supernets on all GLUE tasks and the SQuAD data set. Based on the boosted subnets, we further build an inference framework enabling practical width- and depth-dynamic inference regarding different inputs by combining width-dynamic gating modules and early exit off-ramps in the depth dimension. Experimental results show that the proposed framework achieves a better dynamic inference range than other methods in terms of trading off performance and computational complexity on four GLUE tasks and SQuAD. In particular, our best-tradeoff inference result outperforms other fixed-size models with similar amount of computations. Compared to BERT-Base, the proposed inference framework yields a 1.3-point improvement in the average GLUE score and a 2.2-point increase in the F1 score on SQuAD, while reducing computations by around 45%.}
}
@incollection{KENDRICK2008685,
title = {Chapter 17 The Supporting Role of Molecular Modelling and Computational Chemistry in Polymer Analysis},
editor = {John M. Chalmers and Robert J. Meier},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {53},
pages = {685-734},
year = {2008},
booktitle = {Molecular Characterization and Analysis of Polymers},
issn = {0166-526X},
doi = {https://doi.org/10.1016/S0166-526X(08)00417-0},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X08004170},
author = {John Kendrick},
abstract = {Publisher Summary
Molecular modeling covers a wide range of techniques and the calculation of an even wider range of properties. Although for polymers, the possibility of treating a polymer chain quantum mechanically is formidable, it is clear that the modeling approach allows calculations on monomers, dimmers, and oligomers to guide the interpretation of many spectroscopic observations with great success. For those systems, where longer times scales and larger size scales are important, molecular mechanics and molecular dynamics methods are available, but the issue of the force field and the approximations that it introduces remain significant. The key to the change in attitude to modeling and its role have to lie in the availability of mature algorithms with well-known and well-understood properties. The density functional theory method in quantum mechanics has introduced a new era in applications of quantum mechanical methods.}
}
@article{JIANG201814,
title = {Computational intelligence techniques for maximum power point tracking in PV systems: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {85},
pages = {14-45},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364032118300054},
author = {Lian L. Jiang and R. Srivatsan and Douglas L. Maskell},
keywords = {Maximum power point tracking, PV system, Computational intelligence algorithm, Heuristic algorithm, Global tracking, Partial shading},
abstract = {Maximum power point (MPP) tracking (MPPT) is an important technique for maximizing the power extraction from photovoltaic (PV) systems under varying climatic conditions. In an array of PV modules it is possible to observe multiple peaks in the power versus voltage (P-V) curve due to the current versus voltage (I–V) PV cell mismatch caused by differences in the received irradiance, such as occurs during partial shading. In these circumstances, the ability of the MPPT devices to track the global MPP of the PV array directly influences the system efficiency. In the literature, various MPPT techniques have been proposed. Among them, computational intelligence (CI) algorithm based MPPT methods have demonstrated the ability to find the global MPP. This paper presents a detailed and specific review of CI- based MPPT techniques. Each method type is classified into one of several subcategories according to its application strategy. The various ways of applying CI into MPPTs are analyzed in detail. The advantages and disadvantages of each method are discussed and compared. The purpose of this study is to provide a compendium on CI-based MPPT techniques for users to understand and select an appropriate method based on application requirements and system constraints.}
}
@article{THAKIRABED20232293,
title = {The computation intelligent system of role of parental leadership in organizational familiarity in Iraqi Airways employees},
journal = {Materials Today: Proceedings},
volume = {80},
pages = {2293-2301},
year = {2023},
note = {SI:5 NANO 2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.318},
url = {https://www.sciencedirect.com/science/article/pii/S221478532104709X},
author = {Marwan {Thakir Abed}},
keywords = {Parental leadership, Organizational familiarity, Empowerment, Iraqi Airways},
abstract = {The research aimed to know the effect of parental leadership represented by (benevolent leadership, moral leadership, and authoritarian leadership) found in the research sample, in the organizational familiarity (employee morale, empowerment, and objective merit), the research relied on the questionnaire as a key instrument to collect the necessary data to meet its goal. As (60) forms were distributed to find the level of availability of parental leadership and organizational harmony, while (56) forms were retrieved. A set of statistical methods were used, represented by normal distribution, stability factor (Alpha Kronbach), reliability, arithmetic mean, standard deviation, and coefficient Simple correlation Pearson, multiple regression coefficient. The results showed that there is a positive correlation and effect relationship with statistically significant between parental leadership with its dimensions (benevolent leadership, moral leadership, authoritarian leadership) and organizational affiliation with its dimensions (employee morale, empowerment, and merit's Objectivity), and the research showed a direct impact relationship between parental leadership and the organizational affiliation of the studied sample. Accordingly, the research concluded that the study sample should pay attention to the nature and type of empowering workers in order to give them freedom and independence in making decisions regarding the tasks assigned to them.}
}
@article{HOYTE2006S348,
title = {Computational model of levator ani muscle stretch during vaginal delivery},
journal = {Journal of Biomechanics},
volume = {39},
pages = {S348},
year = {2006},
note = {Abstracts of the 5th World Congress of Biomechanics},
issn = {0021-9290},
doi = {https://doi.org/10.1016/S0021-9290(06)84382-4},
url = {https://www.sciencedirect.com/science/article/pii/S0021929006843824},
author = {L. Hoyte and P. Krysl and G. Chukkapalli and A. Majumdar and D.J. Choi and A. Trivedi and S.K. Warfield and M.S. Damaser}
}
@article{CRILLY2021309,
title = {The Evolution of “Co-evolution” (Part I): Problem Solving, Problem Finding, and Their Interaction in Design and Other Creative Practices},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {7},
number = {3},
pages = {309-332},
year = {2021},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405872621000915},
author = {Nathan Crilly},
keywords = {Design process, Design thinking, Creativity, Design history, Interdisciplinarity},
abstract = {One of the most influential descriptions of design activity emphasizes how problems and solutions “co-evolve.” This concept has somehow escaped critical review and cross-disciplinary comparison, resulting in a fragmented approach to the subject. Reviewing the published literature on design co-evolution reveals that the term is used to refer to a range of distinct concepts, and the study of co-evolution has generated a number of elaborations and alternatives. Reviewing the broader literature in design and other disciplines further reveals that discussions of design co-evolution are disconnected from the history of relevant concepts in design research, and disconnected from a range of relevant concepts in other disciplines that describe creative work. Here I examine what the different concepts of design co-evolution are, how they have been modified and what they are related to. This leads to questioning the distinction between problems and solutions, defining them in relative terms, and drawing a connection between design co-evolution and design fixation.}
}
@article{GALTIER201683,
title = {Radiative transfer and spectroscopic databases: A line-sampling Monte Carlo approach},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {172},
pages = {83-97},
year = {2016},
note = {Eurotherm Conference No. 105: Computational Thermal Radiation in Participating Media V},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2015.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022407315003192},
author = {Mathieu Galtier and Stéphane Blanco and Jérémi Dauchet and Mouna {El Hafi} and Vincent Eymet and Richard Fournier and Maxime Roger and Christophe Spiesser and Guillaume Terrée},
keywords = {Radiative transfer, Monte Carlo method, Null-collision, Line sampling, Statistical approach, Spectroscopic databases},
abstract = {Dealing with molecular-state transitions for radiative transfer purposes involves two successive steps that both reach the complexity level at which physicists start thinking about statistical approaches: (1) constructing line-shaped absorption spectra as the result of very numerous state-transitions, (2) integrating over optical-path domains. For the first time, we show here how these steps can be addressed simultaneously using the null-collision concept. This opens the door to the design of Monte Carlo codes directly estimating radiative transfer observables from spectroscopic databases. The intermediate step of producing accurate high-resolution absorption spectra is no longer required. A Monte Carlo algorithm is proposed and applied to six one-dimensional test cases. It allows the computation of spectrally integrated intensities (over 25cm−1 bands or the full IR range) in a few seconds, regardless of the retained database and line model. But free parameters need to be selected and they impact the convergence. A first possible selection is provided in full detail. We observe that this selection is highly satisfactory for quite distinct atmospheric and combustion configurations, but a more systematic exploration is still in progress.}
}
@article{WANG20231225,
title = {Parameterization Design of 3D Fractal Images in Packaging Design Based on Genetic Algorithm},
journal = {Procedia Computer Science},
volume = {228},
pages = {1225-1232},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.104},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301935X},
author = {Jinxia Wang},
keywords = {Genetic Algorithm, Packaging Design, 3D Fractal Image, NSGA - II},
abstract = {Packaging design, as an important element in product appearance, can directly affect customers' sensory perception of the product. Many universities even offer packaging design majors, which mainly use natural science and aesthetic knowledge to promote product sales. However, many old brands remain complacent and their packaging design still adopts traditional thinking, which to some extent affects their sales. Therefore, this article decided to use genetic algorithms as a tool to parameterize the 3D fractal images in packaging design, aiming to create more creative and eye-catching packaging designs. At the end of this article, an experiment was conducted on two branches of a certain brand. Branch 1 tried out the new design provided in this article, while Branch 2 continued to use the original design. After Branch 1 fully adopted the design, sales skyrocketed, from the original daily sales of 50-60 units to 70-85 units. Branch 2 remained unchanged, with a sharp contrast.}
}
@incollection{TIWARI2025389,
title = {Chapter 13 - Quantitative data analysis methods for air quality prediction},
editor = {Ranjeet S. Sokhi},
booktitle = {Air Quality},
publisher = {Elsevier},
pages = {389-409},
year = {2025},
isbn = {978-0-12-822591-2},
doi = {https://doi.org/10.1016/B978-0-12-822591-2.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225912000135},
author = {Pushp Raj Tiwari and Saurabh Kumar},
keywords = {Machine learning, data science, statistics, statistical applications, artificial intelligence},
abstract = {A quantitative research study gathers numerical data, which needs to be examined to make conclusions. Critical thinking regarding data analysis is facilitated primarily by quantitative data analysis. The purpose of data analysis is to identify the underlying trends, patterns, and connections in the context of a study. In quantitative data analysis, the focus is not just on applying statistical tests to existing data, but also on leveraging those tests as a means of deriving accurate insights from the data. This chapter aims to give a thorough theoretical review of current methodologies, related computational techniques, and dataset applications. A wide range of topics is covered, focusing on various statistical tests, interpolation, extrapolation, regression, modeling uncertainty analysis, machine learning methods, GIS techniques, data visualization tools etc. Each of these topics is extensively explored and discussed in terms of relevant literature and methods for air quality data and its applications.}
}
@article{CHINTA20248181,
title = {Cascade reactions of HDDA-benzynes with tethered cyclohexadienones: strain-driven events originating from ortho-annulated benzocyclobutenes††Electronic supplementary information (ESI) available. CCDC 2302618–2302621. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc00571f},
journal = {Chemical Science},
volume = {15},
number = {21},
pages = {8181-8189},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc00571f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024006813},
author = {Bhavani Shankar Chinta and Dorian S. Sneddon and Thomas R. Hoye},
abstract = {Intramolecular net [2 + 2] cycloadditions between benzyne intermediates and an electron-deficient alkene to give benzocyclobutene intermediates are relatively rare. Benzynes are electrophilic and generally engage nucleophiles or electron-rich π-systems. We describe here reactions in which an alkene of a tethered enone traps thermally generated benzynes in a variety of interesting ways. The number of atoms that link the benzyne to C4 of a cyclohexa-2,5-dienone induces varying amounts of strain in the intermediates and products. This leads to a variety of different reaction outcomes by way of various strain-releasing events that are mechanistically intriguing. This work demonstrates an underappreciated class of strain that originates from the adjacent fusion of two rings to both C1–C2 and C2–C3 of a benzenoid ring – i.e. ‘ortho-annulation strain’. DFT computations shed considerable light on the mechanistic diversions among various reaction pathways as well as allow more fundamental evaluation of the strain in a homologous series of ortho-annulated carbocycles.}
}
@article{DENEF20071096,
title = {Computational complexity of the landscape: Part I},
journal = {Annals of Physics},
volume = {322},
number = {5},
pages = {1096-1142},
year = {2007},
issn = {0003-4916},
doi = {https://doi.org/10.1016/j.aop.2006.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003491606001382},
author = {Frederik Denef and Michael R. Douglas},
abstract = {We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso–Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly. In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum.}
}
@article{LEE2021100737,
title = {Turbulent boundary layer trailing-edge noise: Theory, computation, experiment, and application},
journal = {Progress in Aerospace Sciences},
volume = {126},
pages = {100737},
year = {2021},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2021.100737},
url = {https://www.sciencedirect.com/science/article/pii/S0376042121000427},
author = {Seongkyu Lee and Lorna Ayton and Franck Bertagnolio and Stephane Moreau and Tze Pei Chong and Phillip Joseph},
keywords = {Trailing-edge noise, Aeroacoustics, Turbulent boundary layer},
abstract = {When the pressure fluctuations caused by turbulence vorticity in the boundary layer are scattered by a sharp trailing edge, acoustic energy is generated and propagated to the far field. This trailing edge noise is emitted from aircraft wings, turbomachinery blades, wind turbine blades, helicopter blades, etc. Being dominant at high frequencies, this trailing-edge noise is a key element that annoys human hearing. This article covers virtually the entire landscape of modern research into trailing-edge noise including theoretical developments, numerical simulations, wind tunnel experiments, and applications of trailing-edge noise. The theoretical approach includes Green’s function formulations, Wiener–Hopf methods that solve the mixed boundary-value problem, Howe’s and Amiet’s models that relate the wall pressure spectrum to acoustic radiation. Recent analytical developments for poroelasticity and serrations are also included. We discuss a hierarchy of numerical approaches that range from semi-empirical schemes that estimate the wall pressure spectrum using mean-flow and turbulence statistics to high-fidelity unsteady flow simulations such as Large Eddy Simulation (LES) or Direct Numerical Simulation (DNS) that resolve the sound generation and scattering process based on the first-principles flow physics. Wind tunnel experimental research that provided benchmark data for numerical simulations and unravel flow physics is reviewed. In each theoretical, numerical, and experimental approach, noise control methods for mitigating trailing-edge noise are discussed. Finally, highlights of practical applications of trailing-edge noise prediction and reduction to wind turbine noise, fan noise, and rotorcraft noise are given. The current challenges in each approach are summarized with a look toward the future developments. The review could be useful as a primer for new researchers or as a reference point to the state of the art for experienced professionals.}
}
@article{AKCAOGLU201472,
title = {Cognitive outcomes from the Game-Design and Learning (GDL) after-school program},
journal = {Computers & Education},
volume = {75},
pages = {72-81},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131514000372},
author = {Mete Akcaoglu and Matthew J. Koehler},
keywords = {Game-design, Problem-solving, Quasi-experimental, Constructionism},
abstract = {The Game-Design and Learning (GDL) initiative engages middle school students in the process of game-design in a variety of in-school, after-school, and summer camp settings. The goal of the GDL initiative is to leverage students' interests in games and design to foster their problem-solving and critical reasoning skills. The present study examines the effectiveness of an after-school version of the GDL program using a quasi-experimental design. Students enrolled in the GDL program were guided in the process of designing games aimed at solving problems. Compared to students in a control group who did not attend the program (n = 24), the children who attended the GDL program (n = 20) showed a significant increase in their problem-solving skills. The results provide empirical support for the hypothesis that participation in the GDL program leads to measurable cognitive changes in children's problem-solving skills. This study bears important implications for educators and theory.}
}
@article{ALTARABICHI2023118528,
title = {Fast Genetic Algorithm for feature selection — A qualitative approximation approach},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118528},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118528},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016049},
author = {Mohammed Ghaith Altarabichi and Sławomir Nowaczyk and Sepideh Pashami and Peyman Sheikholharam Mashhadi},
keywords = {Feature selection, Evolutionary computation, Genetic Algorithm, Particle Swarm Intelligence, Fitness approximation, Meta-model, Optimization},
abstract = {Evolutionary Algorithms (EAs) are often challenging to apply in real-world settings since evolutionary computations involve a large number of evaluations of a typically expensive fitness function. For example, an evaluation could involve training a new machine learning model. An approximation (also known as meta-model or a surrogate) of the true function can be used in such applications to alleviate the computation cost. In this paper, we propose a two-stage surrogate-assisted evolutionary approach to address the computational issues arising from using Genetic Algorithm (GA) for feature selection in a wrapper setting for large datasets. We define “Approximation Usefulness” to capture the necessary conditions to ensure correctness of the EA computations when an approximation is used. Based on this definition, we propose a procedure to construct a lightweight qualitative meta-model by the active selection of data instances. We then use a meta-model to carry out the feature selection task. We apply this procedure to the GA-based algorithm CHC (Cross generational elitist selection, Heterogeneous recombination and Cataclysmic mutation) to create a Qualitative approXimations variant, CHCQX. We show that CHCQX converges faster to feature subset solutions of significantly higher accuracy (as compared to CHC), particularly for large datasets with over 100K instances. We also demonstrate the applicability of the thinking behind our approach more broadly to Swarm Intelligence (SI), another branch of the Evolutionary Computation (EC) paradigm with results of PSOQX, a qualitative approximation adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository with the complete implementation is available.22https://github.com/Ghaith81/Fast-Genetic-Algorithm-For-Feature-Selection.}
}
@article{CHU20181,
title = {Supporting scientific modeling through curriculum-based making in elementary school science classes},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {1-8},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300545},
author = {Sharon Lynn Chu and Elizabeth Deuermeyer and Francis Quek},
keywords = {Making, Maker movement, Children, Science, Science models, Scientific modeling, Model thinking, Electronics, Programming},
abstract = {Our work investigates how Making may be used in the context of scientific modeling in formal elementary school science classes. This paper presents an investigation of fourth- and fifth-grade students engaging in Making activities to create simulation, concept-process, and illustrative models in the science classroom. Based on video analyses of the Making-based class sessions, a generalized process model was developed for each type of science model. In addition, cross-cutting themes were found in Making-based science modeling: first, there are two loops that intersect and interact with each other (modeling for Making and modeling for Science content), and they interrelate in various ways depending on science model type; and second, showcasing Making products (sharing with peers, teachers, or helpers) is a primary factor that determines students’ overall engagement with science in the activity. We suggest that Making-based science kit and lesson design needs to support students to showcase their Making output, on top of science-related reflections, and to consider the balance between Making and science activity. We conclude that Making has the potential to support the development of scientific model thinking in the elementary science classroom, but much further research is needed in this area.}
}
@article{ZILLES20101072,
title = {The computational complexity of avoiding spurious states in state space abstraction},
journal = {Artificial Intelligence},
volume = {174},
number = {14},
pages = {1072-1092},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000950},
author = {Sandra Zilles and Robert C. Holte},
keywords = {Abstraction, Heuristic search, Planning},
abstract = {Abstraction is a powerful technique for speeding up planning and search. A problem that can arise in using abstraction is the generation of abstract states, called spurious states, from which the goal state is reachable in the abstract space but for which there is no corresponding state in the original space from which the goal state can be reached. Spurious states can be harmful, in practice, because they can create artificial shortcuts in the abstract space that slow down planning and search, and they can greatly increase the memory needed to store heuristic information derived from the abstract space (e.g., pattern databases). This paper analyzes the computational complexity of creating abstractions that do not contain spurious states. We define a property—the downward path preserving property (DPP)—that formally captures the notion that an abstraction does not result in spurious states. We then analyze the computational complexity of (i) testing the downward path preserving property for a given state space and abstraction and of (ii) determining whether this property is achievable at all for a given state space. The strong hardness results shown carry over to typical description languages for planning problems, including sas+ and propositional strips. On the positive side, we identify and illustrate formal conditions under which finding downward path preserving abstractions is provably tractable.}
}
@article{SELBY20001491,
title = {Computational Aspects of Complex Securities},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {11},
pages = {1491-1497},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00084-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000846},
author = {Michaël J.P Selby}
}
@article{CREELY2025101727,
title = {Creative partnerships with generative AI. Possibilities for education and beyond},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101727},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101727},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002682},
author = {Edwin Creely and Jo Blannin},
keywords = {Generative AI, Creative production, Posthumanism, Education, Autoethnography, Alterity relations},
abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.}
}
@article{ATALLAH2022B2,
title = {Society for Maternal-Fetal Medicine Special Statement: Cognitive bias and medical error in obstetrics—challenges and opportunities},
journal = {American Journal of Obstetrics and Gynecology},
volume = {227},
number = {2},
pages = {B2-B10},
year = {2022},
issn = {0002-9378},
doi = {https://doi.org/10.1016/j.ajog.2022.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0002937822003143},
author = {Fouad Atallah and Rebecca F. Hamm and Christina M. Davidson and C. Andrew Combs},
keywords = {decision-making, diagnostic error, disparities, implicit bias, inequity, medical error, racism},
abstract = {The processes of diagnosis and management involve clinical decision-making. However, decision-making is often affected by cognitive biases that can lead to medical errors. This statement presents a framework of clinical thinking and decision-making and shows how these processes can be bias-prone. We review examples of cognitive bias in obstetrics and introduce debiasing tools and strategies. When an adverse event or near miss is reviewed, the concept of a cognitive autopsy—a root cause analysis of medical decision-making and the potential influence of cognitive biases—is promoted as part of the review process. Finally, areas for future research on cognitive bias in obstetrics are suggested.}
}
@article{KUMAR20101805,
title = {A generalized computational approach to stability of static equilibria of nonlinearly elastic rods in the presence of constraints},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {199},
number = {25},
pages = {1805-1815},
year = {2010},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782510000654},
author = {Ajeet Kumar and Timothy J. Healey},
keywords = {Stability, Elasticity, Rods, Constraints},
abstract = {We present a generalized approach to stability of static equilibria of nonlinearly elastic rods, subjected to general loading, boundary conditions and constraints (of both point-wise and integral type), based upon the linearized dynamics stability criterion. Discretization of the governing equations leads to a non-standard (singular) generalized eigenvalue problem. A new efficient sparse-matrix-friendly algorithm is presented to determine its few left-most eigenvalues, which, in turn, yield stability/instability information. For conservative problems, the eigenvalue problem arising from the linearized dynamics stability criterion is also shown to be equivalent to that arising in the determination of constrained local minima of the potential energy. We illustrate the method with several examples. A novel variational formulation for extensible and unshearable rods is also proposed within the context of one of the example problems.}
}
@article{BENSASSI20233123,
title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3123-3132},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014643},
author = {Manel BenSassi and Henda Ben Ghezala},
keywords = {Learning analytics, educational recommendations, fuzzy competencies assessment, knowledge representation, fuzzy ontology, Scientific Reasoning competencies},
abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.}
}
@article{NOROOZI2019295,
title = {Multidisciplinary innovations and technologies for facilitation of self-regulated learning},
journal = {Computers in Human Behavior},
volume = {100},
pages = {295-297},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302638},
author = {Omid Noroozi and Sanna Järvelä and Paul A. Kirschner},
abstract = {Technology-enhanced learning environments provide ample opportunities for learners to self-regulate their learning processes and activities for achieving the intended learning outcomes in various disciplines from soft to hard sciences and from humanities to the natural and social sciences. This special issue discusses the emerging technological advancements and cutting-edge research on self-regulated learning dealing with different cognitive, motivational, emotional, and social processes of learning both at the individual and group levels. Specifically, it discusses how to optimally use advanced technologies to facilitate learners’ self-regulated learning for achieving their own individual learning needs and goals. In this special issue, seven researchers/research teams from the fields of collaborative learning, computational thinking, educational psychology, and learning analytics presented contributions to self-regulated learning with the goal of stimulating cross-border discussion in the field.}
}
@article{BAKEEVA2022676,
title = {Increasing the student talking time parameter under the digitalization in transport engineering learning},
journal = {Transportation Research Procedia},
volume = {63},
pages = {676-685},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522003179},
author = {L Bakeeva and L Brylevskaya and L Gonchar and E Pastukhova and Y Romanova and O Skepko},
keywords = {Digitalization of education, transport engineering, student talking time, study-train-explain methodology},
abstract = {The most recent information technologies have become an integral part of modern life. As the study shows, along with the obvious benefits, their use can lead to negative consequences, namely the loss of communication and soft skills, changes in the ability to absorb information, decreased motivation to acquire new knowledge among the younger age group. The authors propose a new methodology for organizing the educational process Study-Train-Explain. The aim of the method is to increase the Student Talking Time parameter to develop the skills of mathematical data analysis, systematic and analytical thinking to master the methods of description and construction of mathematical model of the phenomenon or process. These competencies are extremely in demand in the professional field related to the organization of transportation and operation of transport-technological machines and complexes under the conditions of digitalization of global processes. The article presents an algorithm and the results of the experimental training process carried out by the authors according to the specified methodology.}
}
@article{AGRE19951,
title = {Computational research on interaction and agency},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {1-52},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000545},
author = {Philip E. Agre},
abstract = {Recent research in artificial intelligence has developed computational theories of agents' involvements in their environments. Although inspired by a great diversity of formalisms and architectures, these research projects are unified by a common concern: using principled characterizations of agents' interactions with their environments to guide analysis of living agents and design of artificial ones. This article offers a conceptual framework for such theories, surveys several other fields of research that hold the potential for dialogue with these new computational projects, and summarizes the principal contributions of the articles in this special double volume. It also briefly describes a case study in these ideas—a computer program called Toast that acts as a short-order breakfast cook. Because its designers have discovered useful structures in the world it inhabits, Toast can employ an extremely simple mechanism to decide what to do next.}
}
@article{ONGUR2025220,
title = {Embracing complexity in psychiatry—from reductionistic to systems approaches},
journal = {The Lancet Psychiatry},
volume = {12},
number = {3},
pages = {220-227},
year = {2025},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(24)00334-1},
url = {https://www.sciencedirect.com/science/article/pii/S2215036624003341},
author = {Dost Öngür and Martin P Paulus},
abstract = {Summary
The understanding and treatment of psychiatric disorders present unique challenges due to these conditions' multifaceted nature, comprising dynamic interactions between biological, psychological, social, and environmental factors. Traditional reductionistic approaches often simplify these conditions into linear cause-and-effect relationships, overlooking the complexity and interconnectedness inherent in psychiatric disorders. Advances in complex systems approaches provide a comprehensive framework to capture and quantify the non-linear and emergent properties of psychiatric disorders. This Personal View emphasises the importance of identifying rules for generative models that govern brain and behaviour over time, which might contribute to personalised assessments and interventions for psychiatric disorders. For instance, mood fluctuations in bipolar disorder can be understood through dynamical systems modelling, which identifies modifiable parameters, such as circadian disruption, that can be addressed through targeted therapies such as light therapy. Similarly, recognition of depression as an emergent property arising from complex interactions highlights the need for integrated treatment strategies that enhance adaptive reactions in the individual. A framework for quantifying multilevel interactions and network dynamics can help researchers and clinicians to understand the interplay between neural circuits, behaviours, and social contexts. Probabilistic models and self-organisation concepts contribute to building concrete dynamical systems models of mental disorders, facilitating early identification of risk states and promoting resilience through adaptive interventions delivered with optimal timing. Embracing these complex systems approaches in psychiatry could capture the true nature of psychiatric disorders as properties of a dynamic complex system and not the manifestation of any lesion or insult. This line of thinking might improve diagnosis and treatment, offering new hope for individuals affected by psychiatric conditions and paving the way for more effective, personalised mental health care.}
}
@incollection{BALAKRISHNAN202131,
title = {Chapter 2 - Computational intelligence in healthcare and biosignal processing},
editor = {Janmenjoy Nayak and Bighnaraj Naik and Danilo Pelusi and Asit Kumar Das},
booktitle = {Handbook of Computational Intelligence in Biomedical Engineering and Healthcare},
publisher = {Academic Press},
pages = {31-64},
year = {2021},
isbn = {978-0-12-822260-7},
doi = {https://doi.org/10.1016/B978-0-12-822260-7.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222607000157},
author = {Nagaraj Balakrishnan and Valentina E. Balas and Arunkumar Rajendran},
keywords = {ANN, Clustering, Data classification, Data mining, Deep clustering networks, Deep learning},
abstract = {In this new era, technological advancement toward the mission of a better tomorrow is reaching its limit because the exploration of the advanced possibilities of Artificial Intelligence is bounded with certain limitations. The application of analyzing various features of biosignal processing is key in the fields of medicine and healthcare. Biosignals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Electromyography (EMG), Electrooculography (EOG), Galvanic Skin Response(GSR), and Magnetoencephalography (MEG) is already giving deep insight into the human body toward the identification of diverse nature and disorders. In recent years, the research toward analyzing biosignal gained interest among many researchers. The primary limitation for the algorithms to analyze these signals for more possibilities of insight is its uncertainty. Even though the algorithms of Artificial Intelligence have the capabilities to unravel the mysteries, it is bounded with specific difficulties. The machine learning algorithms designed to manage uncertain data but lacks accuracy due to many factors. Also, complete supervision is needed in a training process that involves the extraction and selection of adequate features for the training. The deep learning method (a subset of machine learning) comes into the picture due to one of these facts. This, indeed, as a supervised learning method, needs a massive volume of data to train to reach the accuracy goal. The deep learning algorithm plays a significant role in today's Artificial Intelligence–based applications. However, this platform needs many requirements, such as (a) high computational power like graphical processing units (GPU); (b) similar to machine learning methods, a massive labeled dataset for supervised learning; (c) adequate parameter selection to avoid overfitting or underfitting. To overcome the problems highlighted, the strategy of adopting the behaviors of unsupervised learning (performed by the clustering algorithm) in the deep learning methodology is needed. To achieve the goal, two-phase operations were processed, such as (1) transformation of the data elements into a latent feature space (Z) is processed through a nonlinear mapping of deep learning networks; (2) clustering the latent feature space to k-clusters, and simultaneously, the clustering loss is fed to the deep learning network for the next iteration of operation concerning the objective function convergence analyzed by the Kullback–Leibler divergence. Various strategies of enhancing the nature of deep learning methods and clustering methodologies for an unsupervised learning process are addressed in this chapter.}
}
@article{BERGER2016337,
title = {Cognitive hierarchies in the minimizer game},
journal = {Journal of Economic Behavior & Organization},
volume = {130},
pages = {337-348},
year = {2016},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167268116301639},
author = {Ulrich Berger and Hannelore {De Silva} and Gerlinde Fellner-Röhling},
keywords = {Behavioral game theory, Experimental games, Poisson cognitive hierarchy, Level- model, Minimizer game},
abstract = {Experimental tests of choice predictions in one-shot games show only little support for Nash equilibrium (NE). Poisson Cognitive Hierarchy (PCH) and level-k (LK) are behavioral models of the thinking-steps variety where subjects differ in the number of levels of iterated reasoning they perform. Camerer et al. (2004) claim that substituting the Poisson parameter τ=1.5 yields a parameter-free PCH model (pfPCH) which predicts experimental data considerably better than NE. We design a new multi-person game, the Minimizer Game, as a testbed to compare initial choice predictions of NE, pfPCH and LK. Data obtained from two large-scale online experiments strongly reject NE and LK, but are well in line with the point-prediction of pfPCH.}
}
@article{FOLTZ2023127,
title = {Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function},
journal = {Schizophrenia Research},
volume = {259},
pages = {127-139},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002833},
author = {Peter W. Foltz and Chelsea Chandler and Catherine Diaz-Asper and Alex S. Cohen and Zachary Rodriguez and Terje B. Holmlund and Brita Elvevåg},
keywords = {Natural language processing, Speech technologies, Artificial intelligence},
abstract = {Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.}
}
@article{VEITAS201716,
title = {Living Cognitive Society: A ‘digital’ World of Views},
journal = {Technological Forecasting and Social Change},
volume = {114},
pages = {16-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300610},
author = {Viktoras Veitas and David Weinbaum},
keywords = {Cognitive system, Living society, Information and communication technologies, Future social governance, Individuation, Cognitive development},
abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.}
}
@article{ZHU2024115675,
title = {Identifying influential nodes in social networks via improved Laplacian centrality},
journal = {Chaos, Solitons & Fractals},
volume = {189},
pages = {115675},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.115675},
url = {https://www.sciencedirect.com/science/article/pii/S096007792401227X},
author = {Xiaoyu Zhu and Rongxia Hao},
keywords = {Social network, Influential nodes, Centrality measure, Improved Laplacian centrality},
abstract = {Identifying influential nodes in social networks has significant applications in terms of social analysis and information dissemination. How to capture the crucial features of influential nodes without increasing the computational complexity is an urgent issue to be solved in the context of big data. Laplacian centrality (LC) measures nodal influence by computing nodes' degree, making it extremely low complexity. However, there is still significant room for improvement. Consequently, we propose the improved Laplacian centrality (ILC) to identify influential nodes based on the concept of self-consistent. Identifying results on 9 real networks prove that ILC is superior to LC and other 6 classical measures in terms of ranking accuracy, top-k nodes identification and discrimination capability. Moreover, the computational complexity of ILC has not significantly increased compared to LC, and remains the linear order of magnitude O(m). Additionally, ILC has excellent robustness and universality such that there is no need to adjust parameters according to different network structures.}
}
@article{PRINA2024132735,
title = {Machine learning as a surrogate model for EnergyPLAN: Speeding up energy system optimization at the country level},
journal = {Energy},
volume = {307},
pages = {132735},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132735},
url = {https://www.sciencedirect.com/science/article/pii/S036054422402509X},
author = {Matteo Giacomo Prina and Mattia Dallapiccola and David Moser and Wolfram Sparber},
keywords = {Energy system modelling, Energy scenarios, Energy planning, Machine learning},
abstract = {In the field of energy system modelling, increasing complexity and optimization analysis are essential for understanding the most effective decarbonization options. However, the growing need for intricate models leads to increased computational time, which can hinder progress in research and policy-making. This study aims to address this issue by integrating machine learning algorithms with EnergyPLAN and EPLANopt, a coupling of EnergyPLAN software and a multi-objective evolutionary algorithm, to expedite the optimization process while maintaining accuracy. By saving computational time, we can increase the number of evaluations, thereby enabling deeper exploration of uncertainty in energy system modelling. Although machine learning models have been widely employed as surrogate models to accelerate optimization problems, their application in energy system modeling at the national scale, while preserving high temporal resolution and extensive sector-coupling, remains scarce. Several machine learning models were evaluated, and an artificial neural network was selected as the most effective surrogate model. The findings demonstrate that incorporating this surrogate model within the optimization process reduces computational time by 64 % compared to the conventional EPLANopt approach, while maintaining an accuracy level close to that obtained by running EPLANopt without the surrogate model.}
}
@article{ALIABADI2000243,
title = {Stabilized-finite-element/interface-capturing technique for parallel computation of unsteady flows with interfaces},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {3},
pages = {243-261},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(00)00200-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782500002000},
author = {Shahrouz Aliabadi and Tayfun E. Tezduyar},
abstract = {We present the stabilized-finite-element/interface-capturing (SFE/IC) method developed for parallel computation of unsteady flow problems with two-fluid interfaces and free surfaces. The SFE/IC method involves stabilized formulations, an interface-sharpening technique, and the enforcement of global mass conservation for each fluid. The SFE/IC method has been efficiently implemented on the CRAY T3E parallel supercomputer. A number of 2D test problems are presented to demonstrate how the SFE/IC method works and the accuracy it attains. We also show how the SFE/IC method can be very effectively applied to 3D simulation of challenging flow problems, such as two-fluid interfaces in a centrifuge tube and operational stability of a partially filled tanker truck driving over a bump.}
}
@article{WANG2016357,
title = {Research on Application of Abduction to Fire Investigation},
journal = {Procedia Engineering},
volume = {135},
pages = {357-362},
year = {2016},
note = {2015 International Conference on Performance-based Fire and Fire Protection Engineering (ICPFFPE 2015)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.01.142},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816001466},
author = {Shi Wang and Zhong-jun Shu},
keywords = {Fire investigation, Abduction, Logical thinking},
abstract = {To solve the problem of fire investigation caused by lack of exacting logical reasoning, it is of significance in helping that abduction, an important logical thinking should be introduced to the field of fire investigation. This paper first analyzes the fundamental reasoning forms of abduction as well as its general situation of application. Combined with practical work experience, the mode of application of abduction to fire investigation is put forward. The author shows it in detail by analyzing a real fire case. It is north noting that some matters needing attention in application are presented in the end. This paper will be conductive to constructing the right logical reasoning model in fire investigation.}
}
@article{DINU20242902,
title = {An integrated benchmark for verbal creativity testing of LLMs and humans},
journal = {Procedia Computer Science},
volume = {246},
pages = {2902-2911},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.380},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024141},
author = {Anca Dinu and Andra Maria Florescu},
keywords = {LLM creativity, AI creativity, verbal creativity tests, human-machine comparison},
abstract = {Until fairly recently, creativity was a human-specific characteristic. Computational creativity or artificial creativity was established as a domain in the late 90’s, with different fields such as verbal, musical, or graphical creativity. With the latest technological advances and the appearance of Large Language Models (LLMs), creativity as a feature of machines gained more and more interest in the scientific community. The scope of this study is twofold: to design a comprehensive benchmark for verbal creativity assessment of LLMs and then to run the same creativity tests on different LLMs as well as on humans, for a direct comparison. We aimed to raise the replicability and extensibility of the creativity assessment of LLMs. Hence, we adapted different types of creativity tests and different criteria from psychology to fit the LLMs profile. We also employed computer-assisted evaluation methods, by using the Open Creativity Scoring with Artificial Intelligence (OCSAI), as we wanted to focus exclusively on automated approaches to assessing creativity. We quantitatively and qualitatively analyzed the data set of both human and machine-generated answers and interpreted the results. Finally, we provide both the original verbal creativity test that we have designed, and the curated data comprising all the collected answers, from the LLMs and from the humans that participated in this research.}
}
@article{MARCHETTI20121517,
title = {Mindwandering heightens the accessibility of negative relative to positive thought},
journal = {Consciousness and Cognition},
volume = {21},
number = {3},
pages = {1517-1525},
year = {2012},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2012.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1053810012001420},
author = {Igor Marchetti and Ernst H.W. Koster and Rudi {De Raedt}},
keywords = {Mindwandering, Negative cognitions, Mood, Depression, Individual differences},
abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.}
}
@article{HU2022116276,
title = {Multi granularity based label propagation with active learning for semi-supervised classification},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015840},
author = {Shengdan Hu and Duoqian Miao and Witold Pedrycz},
keywords = {Semi-supervised learning, Granular computing, Multi granularity, Label propagation, Active learning, Three-way decision},
abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.}
}
@article{BEHR199399,
title = {Computation of incompressible flows with implicit finite element implementations on the Connection Machine},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {108},
number = {1},
pages = {99-118},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90155-Q},
url = {https://www.sciencedirect.com/science/article/pii/004578259390155Q},
author = {M. Behr and A. Johnson and J. Kennedy and S. Mittal and T. Tezduyar},
abstract = {Two implicit finite element formulations for incompressible flows have been implemented on the Connection Machine supercomputers and successfully applied to a set of time-dependent problems. The stabilized space-time formulation for moving boundaries and interfaces, and a new stabilized velocity-pressure-stress formulation are both described, and significant aspects of the implementation of these methods on massively parallel architectures are discussed. Several numerical results for flow problems involving moving as well as fixed cylinders and airfoils are reported. The parallel implementation, taking full advantage of the computational speed of the new generation of supercomputers, is found to be a significant asset in fluid dynamics research. Its current capability to solve large-scale problems, especially when coupled with the potential for growth enjoyed by massively parallel computers, make the implementation a worthwhile enterprise.}
}
@article{GLASSMAN20101412,
title = {Pragmatism, connectionism and the internet: A mind’s perfect storm},
journal = {Computers in Human Behavior},
volume = {26},
number = {6},
pages = {1412-1418},
year = {2010},
note = {Online Interactivity: Role of Technology in Behavior Change},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210000956},
author = {Michael Glassman and Min Ju Kang},
keywords = {Internet, Dewey, Connectionism, Democracy},
abstract = {This paper explores that natural relationships between Pragmatic theory of knowing, the dynamic structuring of the mind and thinking suggested by connectionist theory, and the way information is distributed and organized through the world wide web (www). We suggest that these three “innovations” can be brought together to offer a better understanding of the way the human mind works. The internet and the information revolution may finally offer the opportunity to use and develop inductive learning practices and information based social inquiry in ways Pragmatic philosophers envisioned a hundred years ago, while the recent rise of connectionist and cognitive architecture works provides a concrete context for such developments. This confluence of process represents the type of synergy that only history can offer. The information revolution – exemplified by both the rise of connectionism and the internet – is the apotheosis of the Pragmatic revolution – bringing together radical empiricism and democratization of information in community practice. We offer three important realizations in our understanding of how information is organized and thinking progresses made possible by burgeoning virtual communities on the internet – open source thinking, scale-free networks, and interrelationships in the development of blogs to illustrate our thesis.}
}
@article{DEKKER2017554,
title = {Rasmussen's legacy and the long arm of rational choice},
journal = {Applied Ergonomics},
volume = {59},
pages = {554-557},
year = {2017},
note = {The Legacy of Jens Rasmussen},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2016.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0003687016300254},
author = {Sidney W.A. Dekker},
keywords = {Rasmussen, Rational choice, Human error, Second victim, Incidents},
abstract = {Rational choice theory says that operators and others make decisions by systematically and consciously weighing all possible outcomes along all relevant criteria. This paper first traces the long historical arm of rational choice thinking in the West to Judeo-Christian thinking, Calvin and Weber. It then presents a case study that illustrates the consequences of the ethic of rational choice and individual responsibility. It subsequently examines and contextualizes Rasmussen's legacy of pushing back against the long historical arm of rational choice, showing that bad outcomes are not the result of human immoral choice, but the product of normal interactions between people and systems. If we don't understand why people did what they did, Rasmussen suggested, it is not because people behaved inexplicably, but because we took the wrong perspective.}
}
@article{20213190,
title = {Eunji Cheong},
journal = {Neuron},
volume = {109},
number = {20},
pages = {3190-3192},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007005},
abstract = {In Korea, the pandemic has elevated scientists as trusted sources for both policy decisions and dinner table conversation. In an interview with Neuron, Eunji Cheong discusses how we need to support future generations by fostering scientific thinking, patience, and flexibility.}
}
@article{HONDA201718,
title = {The difference in foresight using the scanning method between experts and non-experts},
journal = {Technological Forecasting and Social Change},
volume = {119},
pages = {18-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730313X},
author = {Hidehito Honda and Yuichi Washida and Akihito Sudo and Yuichiro Wajima and Keigo Awata and Kazuhiro Ueda},
keywords = {Foresight, Scanning method, Divergent thinking, Difference between experts and non-experts, Creativity},
abstract = {We examined the factors that produce differences in generating scenarios on the near future using the scanning method. Participants were asked to briefly read (scan) 151 articles about new technology, the latest customs, fashion, social change, value system transition, or emerging social problems, and then to generate three scenarios about the near future based on the articles. We compared the generated scenarios between scanning method experts and non-experts with no prior experience with the scanning method. We found that experts generated more unique scenarios than non-experts did, and that experts and non-experts differed in the diversity of articles referenced when generating scenarios. We discuss the relationship between the present findings and previous findings on divergent thinking.}
}
@article{OBERKAMPF2002209,
title = {Verification and validation in computational fluid dynamics},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {3},
pages = {209-272},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(02)00005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0376042102000052},
author = {William L. Oberkampf and Timothy G. Trucano},
abstract = {Verification and validation (V&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V&V, and develops a number of extensions to existing ideas. The review of the development of V&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors’ contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V&V in production and commercial software.}
}
@article{BRIMKOV20071631,
title = {Digital hyperplane recognition in arbitrary fixed dimension within an algebraic computation model},
journal = {Image and Vision Computing},
volume = {25},
number = {10},
pages = {1631-1643},
year = {2007},
note = {Discrete Geometry for Computer Imagery 2005},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2006.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0262885606002988},
author = {Valentin E. Brimkov and Stefan Dantchev},
keywords = {Digital hyperplane, Digital plane recognition, Integer programming, Euclidean algorithm},
abstract = {In this paper we present an algorithm for the integer linear programming (ILP) problem within an algebraic model of computation and use it to solve the following digital plane segment recognition problem: Given a set of points M={p1,p2,…,pm}⊆Rn, decide whether M is a portion of a digital hyperplane and, if so, determine its analytical representation. In our setting p1, p2, …,pm may be arbitrary points (possibly, with rational and/or irrational coefficients) and the dimension n may be any arbitrary fixed integer. We reduce this last problem to an ILP to which our general integer programming algorithm applies. It performs O(mlogD) arithmetic operations, where D is a bound on the norm of the domain elements. For the special case of problem dimension two, we propose an elementary algorithm that takes advantage of the specific geometry of the problem and appears to be optimal. It implies an efficient algorithm for digital line segment recognition.}
}
@article{REDKO2016105,
title = {Epistemological foundations of investigation of cognitive evolution},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {105-115},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300597},
author = {Vladimir G. Red’ko},
keywords = {Modeling of cognitive evolution, Cognitive agents, Animal cognitive features, Epistemological foundations},
abstract = {Epistemological foundations for modeling of cognitive evolution are characterized. Cognitive evolution is the evolution of cognitive abilities of biological organisms. The important result of this evolution is the human thinking, which is used at scientific cognition of nature. The related epistemological viewpoints of David Hume, Immanuel Kant, Konrad Lorenz, and Eugene Wigner are outlined. The sketch program for future investigations of cognitive evolution is proposed; initial models of these studies are outlined. According to the presented analysis, it is possible to believe the following. Investigations of cognitive evolution are directed to analyze the fundamental problems: “Why is human thinking applicable to cognition of nature?”, “How did human thinking origin in the process of biological evolution?” There are powerful backgrounds for considered investigations: (1) models of autonomous cognitive agents, (2) biological investigations of animal cognitive features. Studies of cognitive evolution would have broad interdisciplinary relations. These studies should contribute significantly to the development of the scientific point of view.}
}
@incollection{TOPLAK202253,
title = {3 - Development of the ability to detect and override miserly information processing},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {53-87},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000116},
author = {Maggie E. Toplak},
keywords = {Miserly information processing, Dual process models, Children and youth, Development, Ratio bias, Belief bias syllogisms, Cognitive reflection},
abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.}
}
@article{ROBERTS2024100502,
title = {New approach methodologies (NAMs) in drug safety assessment: A vision of the future},
journal = {Current Opinion in Toxicology},
volume = {40},
pages = {100502},
year = {2024},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2024.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2468202024000445},
author = {Ruth A. Roberts},
keywords = {3Rs, NAMs, FTIH, Drug development, ICH, Nanobots},
abstract = {Much progress has been made in reducing and refining animal use in toxicology testing, but progress in the use of new approach methodologies (NAMs) to replace animals is disappointing. There are many highly sophisticated NAMs available, but societal, regulatory and political barriers to their implementation remain. Change requires vision, starting with imagining a future where we are successful. Specifically, this would comprise the registration of safe and effective medicines without animal tests. How do we achieve this vision? Thinking differently, in silico methods could be used to provide a detailed assessment of target- and modality-related toxicological risks, coupled with modelling of exposure. In vitro NAMs such as microphysiological systems, microelectrode array and ion channel panels could then be employed to address hypothetical risks. Finally, the safety of first time in human trials could be assessed and assured using circulating nanobots that measure conventional clinical pathology parameters alongside new biomarkers such as circulating tissue DNA. This may seem the stuff of fantasy, but imagination is key to shaping a better future and all change starts with a vision, however far-fetched it may seem today.}
}
@article{BAYNE201332,
title = {Thought},
journal = {New Scientist},
volume = {219},
number = {2935},
pages = {32-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62293-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913622939},
author = {Tim Bayne},
abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create}
}
@article{FREYBERG20231,
title = {The morphological paradigm in robotics},
journal = {Studies in History and Philosophy of Science},
volume = {100},
pages = {1-11},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123000742},
author = {Sascha Freyberg and Helmut Hauser},
keywords = {Bionics, Embodied cognition, Morphology, Morphological computation, Soft robotics, Principles of orientation and control},
abstract = {In the paper, we are going to show how robotics is undergoing a shift in a bionic direction after a period of emphasis on artificial intelligence and increasing computational efficiency, which included isolation and extreme specialization. We assemble these new developments under the label of the morphological paradigm. The change in its paradigms and the development of alternatives to the principles that dominated robotics for a long time contains a more general epistemological significance. The role of body, material, environment, interaction and the paradigmatic status of biological and evolutionary systems for the principles of control are crucial here. Our focus will be on the introduction of the morphological paradigm in a new type of robotics and to contrast the interests behind this development with the interests shaping former models. The article aims to give a clear account of the changes in principles of orientation and control as well as concluding general observation in terms of historical epistemology, suggesting further political-epistemological analysis.}
}
@article{HAMMIA2024517,
title = {Enhancing Real-time Simultaneous Localization and Mapping with FPGA-based EKF-SLAM's Hardware Architecture},
journal = {Procedia Computer Science},
volume = {236},
pages = {517-526},
year = {2024},
note = {International Symposium on Green Technologies and Applications (ISGTA’2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924010779},
author = {Slama Hammia and Anas Hatim and Abdelilah Haijoub},
keywords = {EKF-SLAM, Simultaneous Localization and Mapping, Green Technologies, FPGA},
abstract = {Simultaneous Localization and Mapping enable a mobile robot that is exploring an uncharted environment to localize itself and calculate its path within the map. In the context of green technologies and applications, there is a growing need for efficient SLAM solutions that not only provide accurate localization and mapping but also minimize power consumption. EKF-SLAM is a SLAM solution based on the Extended Kalman Filter, it is well known In the domain of robotics for its ability to handle non-linear models, its ability to handle noise related to the sensors, and its extremely high degree of precision. To guarantee real-time performance, the EKF-SLAM implementation requires a high-performance hardware architecture. In light of this challenge, researchers are thinking about using parallel processing platforms like FPGAs, which can provide the required level of performance and meet strict constraints on physics, computing capacity, and electrical power. This study describes a hardware architecture's implementation design for EKF-SLAM on an FPGA platform. The entire design is built on the Cyclone 2 FPGA, which has a maximum speed of 114 MHz and 18577 LUTs, creating a highly efficient hardware architecture.}
}
@incollection{KRAAK2020141,
title = {Geovisualization},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {141-151},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10552-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105529},
author = {Menno-Jan Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated multiple views, Geocomputation, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual analytics, Visual exploration, Visual representation, Visual thinking},
abstract = {Due to technological developments and societal needs cartography, scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GIScience have undergone fundamental changes in recent years. Interactivity and dynamics allow not only maps and diagrams to present known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. Such environments allow for simultaneous alternative views of the data and stimulate visual thinking, resulting in geovisualization.}
}
@article{LI2023104935,
title = {Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network},
journal = {Tunnelling and Underground Space Technology},
volume = {133},
pages = {104935},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104935},
url = {https://www.sciencedirect.com/science/article/pii/S0886779822005764},
author = {Wei Li and Zhoujing Ye and Yajian Wang and Hailu Yang and Songli Yang and Zhenlong Gong and Linbing Wang},
keywords = {Underground Pipeline Network, Mixed Reality, IoT Cloud Platform, Data Communication, Operation and Maintenance},
abstract = {The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN.}
}
@article{DENNER2012240,
title = {Computer games created by middle school girls: Can they be used to measure understanding of computer science concepts?},
journal = {Computers & Education},
volume = {58},
number = {1},
pages = {240-249},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511001849},
author = {Jill Denner and Linda Werner and Eloy Ortiz},
keywords = {Construction of computer games, Secondary education, Programming, After-school},
abstract = {Computer game programming has been touted as a promising strategy for engaging children in the kinds of thinking that will prepare them to be producers, not just users of technology. But little is known about what they learn when programming a game. In this article, we present a strategy for coding student games, and summarize the results of an analysis of 108 games created by middle school girls using Stagecast Creator in an after school class. The findings show that students engaged in moderate levels of complex programming activity, created games with moderate levels of usability, and that the games were characterized by low levels of code organization and documentation. These results provide evidence that game construction involving both design and programming activities can support the learning of computer science concepts.}
}
@article{FLANIGAN2017179,
title = {Implicit intelligence beliefs of computer science students: Exploring change across the semester},
journal = {Contemporary Educational Psychology},
volume = {48},
pages = {179-196},
year = {2017},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X16300479},
author = {Abraham E. Flanigan and Markeya S. Peteranetz and Duane F. Shell and Leen-Kiat Soh},
keywords = {Motivation, Implicit intelligence beliefs, Computer science, Self-regulation, Engagement},
abstract = {This study investigated introductory computer science (CS1) students’ implicit beliefs of intelligence. Referencing Dweck and Leggett’s (1988) framework for implicit beliefs of intelligence, we examined how (1) students’ implicit beliefs changed over the course of a semester, (2) these changes differed as a function of course enrollment and students’ motivated self-regulated engagement profile, and (3) implicit beliefs predicted student learning based on standardized course grades and performance on a computational thinking knowledge test. For all students, there were significant increases in entity beliefs and significant decreases in incremental beliefs across the semester. However, examination of effect sizes suggests that significant findings for change across time were driven by changes in specific subpopulations of students. Moreover, results showed that students endorsed incremental belief more strongly than entity belief at both the beginning and end of the semester. Furthermore, the magnitude of changes differed based on students’ motivated self-regulated engagement profiles. Additionally, students’ achievement outcomes were weakly predicted by their implicit beliefs of intelligence. Finally, results showed that the relationship between changes in implicit intelligence beliefs and student achievement varied across different CS1 courses. Theoretical implications for implicit intelligence beliefs and recommendations for STEM educators are discussed.}
}
@article{RUSSWINKEL2011336,
title = {Predicting temporal errors in complex task environments: A computational and experimental approach},
journal = {Cognitive Systems Research},
volume = {12},
number = {3},
pages = {336-354},
year = {2011},
note = {Special Issue on Complex Cognition},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000143},
author = {Nele Russwinkel and Leon Urbas and Manfred Thüring},
keywords = {Cognitive modelling, Time perception, Working memory, Expectations, Surprise, ACT-R},
abstract = {Management in complex environments requires knowledge about temporal contingencies. Expectations about durations enable us to prepare for important events in good time, but also to detect irregularities. Unfortunately, time perception is not invariant. Situational aspects as well as features of the task at hand may dramatically change our sense of time. Particularly under varying workload conditions, temporal distortions may lead to performance errors. A valid and reliable model of time perception must account for these characteristics. Based on the cognitive architecture ACT-R (Anderson et al., 2004), we developed a computational model in line with this requirement. Specific emphasis was placed on mechanisms of coordinative working memory which seem to influence time encoding and perception. The model’s assumptions were tested in three steps. First, the model was applied to account for time distortions ‘a posteriori’. Effects of varying working memory demands reported by Dutke (2005) were replicated and explained by simulations of the model. Second, the model was used for predicting effects ‘a priori’. Augmenting Dutke’s (2005) approach by switching between different degrees of memory demands, predictions of time distortions were derived from the model. These predictions were compared with experimental data. Central assumptions of the model were supported, but there were also some deviations that the model had not captured. Based on the conclusions from the results of the experiment, a second a priori testing addressed temporal expectations in a complex task using a micro-world scenario. The results support the interpretation of the previous experiment and provide new insights for modelling time perception. In summary, our results indicate that coordinative working memory – in contrast to general attention – causes differences in timing performance. This characteristic is captured by our approach. The model we propose heavily relies on mechanisms of working memory and can be applied to explain effects for different time intervals, under a variety of experimental conditions and in different task environments.}
}
@article{FRANTZ2003265,
title = {Herbert Simon. Artificial intelligence as a framework for understanding intuition},
journal = {Journal of Economic Psychology},
volume = {24},
number = {2},
pages = {265-277},
year = {2003},
note = {The Economic Psychology of Herbert A. Simon},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(02)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167487002002076},
author = {Roger Frantz},
keywords = {Herbert Simon, Intuition, Artificial intelligence, Bounded rationality, Economics and psychology},
abstract = {Herbert Simon made overlapping substantive contributions to the fields of economics, psychology, cognitive science, artificial intelligence, decision theory, and organization theory. Simon’s work was motivated by the belief that neither the human mind, human thinking and decision making, nor human creativity need be mysterious. It was after he helped create “thinking” machines that Simon came to understand human intuition as subconscious pattern recognition. In doing so he showed that intuition need not be associated with magic and mysticism, and that it is complementary with analytical thinking. This paper will show how the overlaps in his work and especially his work on AI affected his view towards intuition.}
}
@article{FU20012567,
title = {Analytical and computational description of effect of grain size on yield stress of metals},
journal = {Acta Materialia},
volume = {49},
number = {13},
pages = {2567-2582},
year = {2001},
issn = {1359-6454},
doi = {https://doi.org/10.1016/S1359-6454(01)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/S1359645401000623},
author = {H.-H. Fu and D.J. Benson and M.A. Meyers},
keywords = {Nanocrystalline materials, Grain size, Hall–Petch},
abstract = {Four principal factors contribute to grain-boundary strengthening: (a) the grain boundaries act as barriers to plastic flow; (b) the grain boundaries act as dislocation sources; (c) elastic anisotropy causes additional stresses in grain-boundary surroundings; (d) multislip is activated in the grain-boundary regions, whereas grain interiors are initially dominated by single slip, if properly oriented. As a result, the regions adjoining grain boundaries harden at a rate much higher than grain interiors. A phenomenological constitutive equation predicting the effect of grain size on the yield stress of metals is discussed and extended to the nanocrystalline regime. At large grain sizes, it has the Hall–Petch form, and in the nanocrystalline domain the slope gradually decreases until it asymptotically approaches the flow stress of the grain boundaries. The material is envisaged as a composite, comprised of the grain interior, with flow stress σfG, and grain boundary work-hardened layer, with flow stress σfGB. The predictions of this model are compared with experimental measurements over the mono, micro, and nanocrystalline domains. Computational predictions are made of plastic flow as a function of grain size incorporating differences of dislocation accumulation rate in grain-boundary regions and grain interiors. The material is modeled as a monocrystalline core surrounded by a mantle (grain-boundary region) with a high work hardening rate response. This is the first computational plasticity calculation that accounts for grain size effects in a physically-based manner. A discussion of statistically stored and geometrically necessary dislocations in the framework of strain-gradient plasticity is introduced to describe these effects. Grain-boundary sliding in the nanocrystalline regime is predicted from calculations using the Raj–Ashby model and incorporated into the computations; it is shown to predispose the material to shear localization.}
}
@article{ERBOZ202587,
title = {Electromagnetic radiation and biophoton emission in neuronal communication and neurodegenerative diseases},
journal = {Progress in Biophysics and Molecular Biology},
volume = {195},
pages = {87-99},
year = {2025},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2024.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610724001159},
author = {Aysin Erboz and Elif Kesekler and Pier Luigi Gentili and Vladimir N. Uversky and Orkid Coskuner-Weber},
keywords = {Electromagnetic radiation, Biophotons, Neurodegenerative diseases, Neuron communication, Intrinsically disordered proteins},
abstract = {The intersection of electromagnetic radiation and neuronal communication, focusing on the potential role of biophoton emission in brain function and neurodegenerative diseases is an emerging research area. Traditionally, it is believed that neurons encode and communicate information via electrochemical impulses, generating electromagnetic fields detectable by EEG and MEG. Recent discoveries indicate that neurons may also emit biophotons, suggesting an additional communication channel alongside the regular synaptic interactions. This dual signaling system is analyzed for its potential in synchronizing neuronal activity and improving information transfer, with implications for brain-like computing systems. The clinical relevance is explored through the lens of neurodegenerative diseases and intrinsically disordered proteins, where oxidative stress may alter biophoton emission, offering clues for pathological conditions, such as Alzheimer's and Parkinson's diseases. The potential therapeutic use of Low-Level Laser Therapy (LLLT) is also examined for its ability to modulate biophoton activity and mitigate oxidative stress, presenting new opportunities for treatment. Here, we invite further exploration into the intricate roles the electromagnetic phenomena play in brain function, potentially leading to breakthroughs in computational neuroscience and medical therapies for neurodegenerative diseases.}
}
@article{OGBUNU2024R913,
title = {C. Brandon Ogbunu},
journal = {Current Biology},
volume = {34},
number = {20},
pages = {R913-R915},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2024.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0960982224012338},
author = {C. Brandon Ogbunu}
}
@article{PAZBARUCH2023101294,
title = {Cognitive abilities and creativity: The role of working memory and visual processing},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101294},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101294},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000640},
author = {Nurit Paz-Baruch and Rotem Maor},
keywords = {Cognitive abilities, Creativity, Working memory, Visual processing},
abstract = {Recent studies have revealed the importance of cognitive abilities in creative thinking. However, most research addressed adults and only a few studies have examined the ways these correlations are manifested among young children. The present study explores the role of various cognitive abilities in creativity among school children. Measures of creativity, visual-spatial working memory (VSWM), verbal short-term memory (STM), working memory (WM), and visual processing (VP) were administered to 331 students in Grades 4 and 5. Cluster analysis was used to group students' creativity levels. A multivariate analysis of variance (MANOVA) was conducted to test differences in cognitive abilities across the three clusters. Group differences between high and moderate level creativity students and low creativity students were found regarding VP abilities in the following tests: VSWM, visual discrimination (VD), and Raven's Colored Progressive Matrices (RCPM). Group differences between high creativity students and low creativity students were also found on verbal STM and WM. Additionally, structural equation modeling (SEM) analysis revealed that VP can significantly account for unique variances associated with creativity, while verbal STM and WM are not significantly related to creativity. These findings enlighten the cognitive processes underlying creativity in young children.}
}
@article{KLATT2009536,
title = {Perspectives for process systems engineering—Personal views from academia and industry},
journal = {Computers & Chemical Engineering},
volume = {33},
number = {3},
pages = {536-550},
year = {2009},
note = {Selected Papers from the 17th European Symposium on Computer Aided Process Engineering held in Bucharest, Romania, May 2007},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2008.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135408001737},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, Modeling, Design, Optimization, Control, Operations, Numerical algorithms, Software, Computer-aided process engineering (CAPE)},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Its major achievements include methodologies and tools to support process modeling, simulation and optimization (MSO). Mature, commercially available technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. MSO technologies have become a commodity, they are not a distinguishing feature of the PSE field any more. Consequently, PSE has to reassess and to reposition its future research agenda. Emphasis should be put on model-based applications in all PSE domains including product and process design, control and operations. Furthermore, systems thinking and systems problem solving have to be prioritized rather than the mere application of computational problem solving methods. This essay reflects on the past, present and future of PSE from an academic and industrial point of view. It redefines PSE as an active and future-proof research field which can play an active role in providing enabling technologies for product and process innovations in the chemical industries and beyond.}
}
@article{MORRIS2015,
title = {Efficacy of a Web-Based, Crowdsourced Peer-To-Peer Cognitive Reappraisal Platform for Depression: Randomized Controlled Trial},
journal = {Journal of Medical Internet Research},
volume = {17},
number = {3},
year = {2015},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.4167},
url = {https://www.sciencedirect.com/science/article/pii/S1438887115000783},
author = {Robert R Morris and Stephen M Schueller and Rosalind W Picard},
keywords = {Web-based intervention, crowdsourcing, randomized controlled trial, depression, cognitive behavioral therapy, mental health, social networks},
abstract = {Background
Self-guided, Web-based interventions for depression show promising results but suffer from high attrition and low user engagement. Online peer support networks can be highly engaging, but they show mixed results and lack evidence-based content.
Objective
Our aim was to introduce and evaluate a novel Web-based, peer-to-peer cognitive reappraisal platform designed to promote evidence-based techniques, with the hypotheses that (1) repeated use of the platform increases reappraisal and reduces depression and (2) that the social, crowdsourced interactions enhance engagement.
Methods
Participants aged 18-35 were recruited online and were randomly assigned to the treatment group, “Panoply” (n=84), or an active control group, online expressive writing (n=82). Both are fully automated Web-based platforms. Participants were asked to use their assigned platform for a minimum of 25 minutes per week for 3 weeks. Both platforms involved posting descriptions of stressful thoughts and situations. Participants on the Panoply platform additionally received crowdsourced reappraisal support immediately after submitting a post (median response time=9 minutes). Panoply participants could also practice reappraising stressful situations submitted by other users. Online questionnaires administered at baseline and 3 weeks assessed depression symptoms, reappraisal, and perseverative thinking. Engagement was assessed through self-report measures, session data, and activity levels.
Results
The Panoply platform produced significant improvements from pre to post for depression (P=.001), reappraisal (P<.001), and perseverative thinking (P<.001). The expressive writing platform yielded significant pre to post improvements for depression (P=.02) and perseverative thinking (P<.001), but not reappraisal (P=.45). The two groups did not diverge significantly at post-test on measures of depression or perseverative thinking, though Panoply users had significantly higher reappraisal scores (P=.02) than expressive writing. We also found significant group by treatment interactions. Individuals with elevated depression symptoms showed greater comparative benefit from Panoply for depression (P=.02) and perseverative thinking (P=.008). Individuals with baseline reappraisal deficits showed greater comparative benefit from Panoply for depression (P=.002) and perseverative thinking (P=.002). Changes in reappraisal mediated the effects of Panoply, but not the expressive writing platform, for both outcomes of depression (ab=-1.04, SE 0.58, 95% CI -2.67 to -.12) and perseverative thinking (ab=-1.02, SE 0.61, 95% CI -2.88 to -.20). Dropout rates were similar for the two platforms; however, Panoply yielded significantly more usage activity (P<.001) and significantly greater user experience scores (P<.001).
Conclusions
Panoply engaged its users and was especially helpful for depressed individuals and for those who might ordinarily underutilize reappraisal techniques. Further investigation is needed to examine the long-term effects of such a platform and whether the benefits generalize to a more diverse population of users.
Trial Registration
ClinicalTrials.gov NCT02302248; https://clinicaltrials.gov/ct2/show/NCT02302248 (Archived by WebCite at http://www.webcitation.org/6Wtkj6CXU).}
}
@article{KIMSEY1994113,
title = {Parallel computation of impact dynamics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {113-121},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00079-4},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000794},
author = {K.D. Kimsey and M.A. Olson},
abstract = {This paper discusses a parallel algorithm and data structures for implementing multimaterial, two-step Eulerian finite difference solution schemes on hypercube architectures. Selected problems in impact dynamics have been modeled on the Connection Machine model CM5, and the results are compared with computational results reported in the literature, as well as direct comparison with experimental data.}
}
@incollection{ESFELD2015131,
title = {Atomism and Holism: Philosophical Aspects},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {131-135},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.63003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868630039},
author = {Michael Esfeld},
keywords = {Atomism, Collectivism, Confirmation, Externalism, Holism, Human nature, Individualism, Internalism, Meaning, Ontological dependence, Rationality, Rule-following, Thought},
abstract = {In the philosophy of the social sciences, atomism is the view that human beings can be thinking, rational beings independently of social relations. Holism, by contrast, is the view that social relations are essential to human beings insofar as they are thinking, rational beings. This article first provides an overview of different sorts of atomism and holism (see Section Types of Atomism and Holism). It then briefly sketches the historical background of these notions in modern philosophy (Section The Historical Background of Atomism and Holism). The main part is a systematic characterization of atomism and holism (see Section A Characterization of Atomism and Holism) and a summary of the most important arguments for both these positions (see Section Arguments for Atomism and Holism).}
}
@article{MATHESON2020116697,
title = {The role of the motor system in generating creative thoughts},
journal = {NeuroImage},
volume = {213},
pages = {116697},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116697},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301841},
author = {Heath E. Matheson and Yoed N. Kenett},
keywords = {Motor system, Creativity, Simulations, Embodied cognition, Grounded cognition, Divergent thinking, Improvisation},
abstract = {Neurocognitive research is pertinent to developing mechanistic models of how humans generate creative thoughts. Such models usually overlook the role of the motor cortex in creative thinking. The framework of embodied or grounded cognition suggests that creative thoughts (e.g. using a shoe as a hammer, improvising a piano solo) are partially served by simulations of motor activity associated with tools and their use. The major hypothesis stemming from the embodied or grounded account is that, while the motor system is used to execute actions, simulations within this system also support higher-order cognition, creativity included. That is, the cognitive process of generating creative output, not just executing it, is deeply embedded in motor processes. Here, we highlight a collection of neuroimaging research that implicates the motor system in generating creative thoughts, including some evidence for its functionally necessary role in generating creative output. Specifically, the grounded or embodied framework suggests that generating creative output may, in part, rely on motor simulations of possible actions, and that these simulations may by partially implemented in the motor regions themselves. In such cases, action simulations (i.e. reactivating or re-using the motor system), do not result in overt action but instead are used to support higher-order cognitive goals like generating creative uses or improvising.}
}
@incollection{ZIMMERMAN2005255,
title = {VIII - Development of Theory with Computation},
editor = {M. Olivucci},
series = {Theoretical and Computational Chemistry},
publisher = {Elsevier},
volume = {16},
pages = {255-278},
year = {2005},
booktitle = {Computational Photochemistry},
issn = {1380-7323},
doi = {https://doi.org/10.1016/S1380-7323(05)80025-1},
url = {https://www.sciencedirect.com/science/article/pii/S1380732305800251},
author = {Howard E. Zimmerman}
}
@article{SUI2001529,
title = {Terrae Incognitae and Limits of Computation: Whither GIScience?},
journal = {Computers, Environment and Urban Systems},
volume = {25},
number = {6},
pages = {529-533},
year = {2001},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(01)00027-8},
url = {https://www.sciencedirect.com/science/article/pii/S0198971501000278},
author = {Daniel Sui}
}
@article{HICKOK2011407,
title = {Sensorimotor Integration in Speech Processing: Computational Basis and Neural Organization},
journal = {Neuron},
volume = {69},
number = {3},
pages = {407-422},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311000675},
author = {Gregory Hickok and John Houde and Feng Rong},
abstract = {Sensorimotor integration is an active domain of speech research and is characterized by two main ideas, that the auditory system is critically involved in speech production and that the motor system is critically involved in speech perception. Despite the complementarity of these ideas, there is little crosstalk between these literatures. We propose an integrative model of the speech-related “dorsal stream” in which sensorimotor interaction primarily supports speech production, in the form of a state feedback control architecture. A critical component of this control system is forward sensory prediction, which affords a natural mechanism for limited motor influence on perception, as recent perceptual research has suggested. Evidence shows that this influence is modulatory but not necessary for speech perception. The neuroanatomy of the proposed circuit is discussed as well as some probable clinical correlates including conduction aphasia, stuttering, and aspects of schizophrenia.}
}
@article{WOLFE197853,
title = {The rise of network thinking in anthropology},
journal = {Social Networks},
volume = {1},
number = {1},
pages = {53-64},
year = {1978},
issn = {0378-8733},
doi = {https://doi.org/10.1016/0378-8733(78)90012-6},
url = {https://www.sciencedirect.com/science/article/pii/0378873378900126},
author = {Alvin W. Wolfe},
abstract = {The encyclopedic inventory of the first half of the twentieth century, “Anthropology Today”, published in 1953, gave little inkling that within a few decades developing trends in social theory, in field experience, in electronic data processing, and in mathematics would combine to bring to prominence a distinctive theoretical approach using a quite formal network model for social systems. Now, sophisticated mathematics and computer programming permit sophisticated network models — networks seen as sets of links, networks seen as generated structures, and networks seen as flow processes. Although network thinking has shown a dramatic rise from the “Anthropology Today” of 1953 to the current anthropology of 1978, it is predicted to soar in the next quarter century, much of the weighty burden of network analysis having been lifted from us by ever more rapid electronic data processing.}
}
@article{CHEN2014740,
title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
journal = {Procedia Computer Science},
volume = {31},
pages = {740-746},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.322},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004992},
author = {Ailing Chen and Wei Liu and Zhihui Wu and Jun Zhang},
keywords = {Prototype heuristics, epiphany, extenics, Theory of Creativity, sytematic scheme ;},
abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.}
}
@article{FERNANDEZFONTECHA2022101067,
title = {Examining the relations between semantic memory structure and creativity in second language},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101067},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101067},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000700},
author = {Almudena Fernández-Fontecha and Yoed N. Kenett},
keywords = {Creativity, Semantic network, L2, Bilingualism, Semantic fluency},
abstract = {Creativity is related to a higher flexible semantic memory structure, which could explain greater fluency of ideas. Extensive research has identified a positive connection between creativity and bi-/multilingualism mainly in contexts where two languages or more concur in daily communicative interactions. Yet, creativity has received scant attention in regard to L2 (second or foreign language) acquisition that mainly takes place in classroom situations. The scarce research points to a positive relationship between creativity and L2 fluency – understood as the number of words produced. We apply computational network science analysis and Forward Flow methods to examine lexical organization patterns of a low creativity (LC) and high creativity (HC) group of 12th grade Spanish English as a Foreign Language (EFL) learners. The participants completed two fluency tasks, where they generated animal names in their L2, and also L1 – used here as a control measure. EFL proficiency was controlled. Our analyses revealed that the HC individuals were more fluent in L1 and L2, generated more remote responses, and exhibited a more flexible and efficiently structured semantic memory in both languages, with a greater effect of creativity in L2. Contrary to previous research, the L2 semantic memory network exhibited a less random organization. Differences in the L2 learning conditions are adduced as likely causes of this result.}
}
@article{OXMAN2006229,
title = {Theory and design in the first digital age},
journal = {Design Studies},
volume = {27},
number = {3},
pages = {229-265},
year = {2006},
note = {Digital Design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000840},
author = {Rivka Oxman},
keywords = {digital design, design theory, design methodology, design thinking},
abstract = {Digital design and its growing impact on design and production practices have resulted in the need for a re-examination of current design theories and methodologies in order to explain and guide future research and development. The present research postulates the requirements for a conceptual framework and theoretical basis of digital design; reviews the recent theoretical and historical background; and defines a generic schema of design characteristics through which the paradigmatic classes of digital design are formulated. The implication of this research for the formulation of ‘digital design thinking’ is presented and discussed.}
}
@incollection{YETURU20233,
title = {Chapter 1 - Object-oriented basis of artificial intelligence methodologies},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {3-46},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169716123000251},
author = {Kalidas Yeturu},
keywords = {Object oriented, Vector representation, Induction, Deduction, Machine learning, Artificial intelligence},
abstract = {If O2 is for I, then what is O2 for AI? What is oxygen for humans is what is object-oriented thinking for artificial intelligent systems. Much the same way as humans need O2 knowingly or unknowingly, the first step in designing an AI system requires the application of object-oriented principles either explicitly or implicitly. The basis of the definition of state in AI is a description of the concept of interest as an object with properties. The idea of an object extends beyond typical noun forms that describe elements of the real world. The verb forms are included as well with -able suffixes such as runnable, serializable, and executable. In the software world, the first step in modeling a business requirement is the identification of objects of interest and defining their properties and interactions. For instance, in the case of web services, a service is an object; in the case of database systems, a table or a transaction is an object; or in the case of large-scale integration, electronic components are objects. The concept of an object extends beyond the software realm and into the mathematical world in an implicit form. Functional analysis is an old topic in mathematics where each function is an object indeed. The concept of space in mathematics relates closely to the possible value ranges of all attributes of an object. Mathematical operators are the same as methods of objects. It is day-to-day practical life in any modern operating system software dealing with process objects and applications such as Python scripts involving function objects. In this chapter, the application of object-oriented thinking to convert a business requirement to a machine learning (ML) formulation is presented with examples. The five steps of supervised ML formulation based on vector representations of input and output, mapping function, loss function, and data set are clarified. The scope and limitation of ML formulation as against general AI methodology are discussed to demystify popular myths. This chapter also reveals the secret behind the success of deep learning methodology as automatic differentiation involving function objects.}
}
@incollection{GARDNER202427,
title = {Chapter 2 - Ethics and the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {27-49},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000045},
author = {Nicole Gardner},
keywords = {Design, process, Ethics, Ethics by design, Material ethics, Philosophy of technology, Postphenomenology, Practical ethics, Smart city, Urban technology, Value-sensitive design},
abstract = {Examining who (and what) stands to gain and lose in the smart city, and what kind of urban life smart cities create, are questions of a philosophical and ethical import. Since the early 2010s scholars and journalists have critiqued the smart city paradigm and examined its various projects and experimental initiatives to surface its ethical dimensions and significance. The smart city's rhetorical swing from tech-centric to human-centric and from smart city to smart citizen is construed here as a further effort to create the image of a more ethical smart city. Consequently, the topic of ethics has also intersected with the smart city in particular ways, and predominantly through the lens of data governance and the protection of privacy rights. This chapter argues for an expanded approach to smart city ethics. It proposes a focus on urban technology design to bridge the gap between a micro-ethical focus on data ethics and macro-level political-ethical critique. Bringing philosophical thinking on technology together with a design-led approach to urban technology is argued to provide a further way to draw out a potentially different set of ethical concerns and to explore how urban life can be lived with technology.}
}
@article{BROSSEAU2003373,
title = {Computational electromagnetics and the rational design of new dielectric heterostructures},
journal = {Progress in Materials Science},
volume = {48},
number = {5},
pages = {373-456},
year = {2003},
issn = {0079-6425},
doi = {https://doi.org/10.1016/S0079-6425(02)00013-0},
url = {https://www.sciencedirect.com/science/article/pii/S0079642502000130},
author = {C. Brosseau and A. Beroual},
abstract = {Dielectric properties of heterogeneous materials for various condensed-matter systems have been gaining world-wide attention over the past 50 or so years in the design (or engineering) of materials structures for desired properties and functional purposes. These applications range from cable and current limiters to sensors. These multiscale systems lead to challenging problems of connecting micro- or meso-structural features to macroscopic materials response, i.e. permittivity, conductivity. This article first reviews progress made at that time of the underlying physics of dielectric heterostructures and points out the missing elements that have led to a resurgence of interest in these and related materials. Recent advances in computational electromagnetics provide unparalleled control over morphology in this class of materials to produce a seemingly unlimited number of exquisitely structured materials endowed with tailored electromagnetic, and other physical properties. In the text to follow, we illustrate how an ab initio computational technique can be used to accurately characterize structure–dielectric property relationships of periodic heterostructures in the quasistatic limit. More specifically, we have carried out two-dimensional (2D) and three-dimensional (3D) numerical studies of two-component materials in which equal-sized inclusions, with shape and orientation and possibly fused together, are fixed in a periodic square (2D) or cubic (3D) array. Boundary-integral equations (BIE) are derived from Green's theorem and are solved for the local field with appropriate periodicity conditions on a unit cell of the structures using the field calculation package PHI3D. A number of illustrative examples shows how this computational technique can provide very accurate predictions for the complex effective permittivity of translationally-invariant heterostructures. The performance of the method is also compared with those of other computational and analytical techniques. We comment on how this computational method helps identify some important characteristics for rationalizing and predicting the structure of composite materials in terms of the nature, size, shape and orientation of their constituents.}
}
@article{JOEL2002535,
title = {Actor–critic models of the basal ganglia: new anatomical and computational perspectives},
journal = {Neural Networks},
volume = {15},
number = {4},
pages = {535-547},
year = {2002},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(02)00047-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608002000473},
author = {Daphna Joel and Yael Niv and Eytan Ruppin},
keywords = {Basal ganglia, Dopamine, Reinforcement learning, Actor–critic, Dimensionality reduction, Evolutionary computation, Behavioral switching, Striosomes/patches},
abstract = {A large number of computational models of information processing in the basal ganglia have been developed in recent years. Prominent in these are actor–critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the temporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and learning guided by a prediction error signal in the actor. We selectively review several actor–critic models of the basal ganglia with an emphasis on two important aspects: the way in which models of the critic reproduce the temporal dynamics of dopamine firing, and the extent to which models of the actor take into account known basal ganglia anatomy and physiology. To complement the efforts to relate basal ganglia mechanisms to reinforcement learning (RL), we introduce an alternative approach to modeling a critic network, which uses Evolutionary Computation techniques to ‘evolve’ an optimal RL mechanism, and relate the evolved mechanism to the basic model of the critic. We conclude our discussion of models of the critic by a critical discussion of the anatomical plausibility of implementations of a critic in basal ganglia circuitry, and conclude that such implementations build on assumptions that are inconsistent with the known anatomy of the basal ganglia. We return to the actor component of the actor–critic model, which is usually modeled at the striatal level with very little detail. We describe an alternative model of the basal ganglia which takes into account several important, and previously neglected, anatomical and physiological characteristics of basal ganglia–thalamocortical connectivity and suggests that the basal ganglia performs reinforcement-biased dimensionality reduction of cortical inputs. We further suggest that since such selective encoding may bias the representation at the level of the frontal cortex towards the selection of rewarded plans and actions, the reinforcement-driven dimensionality reduction framework may serve as a basis for basal ganglia actor models. We conclude with a short discussion of the dual role of the dopamine signal in RL and in behavioral switching.}
}
@article{ALOUPIS2015135,
title = {Classic Nintendo games are (computationally) hard},
journal = {Theoretical Computer Science},
volume = {586},
pages = {135-160},
year = {2015},
note = {Fun with Algorithms},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515001735},
author = {Greg Aloupis and Erik D. Demaine and Alan Guo and Giovanni Viglietta},
keywords = {Nintendo games, Video games, Computational complexity, NP-hardness, PSPACE-hardness},
abstract = {We prove NP-hardness results for five of Nintendo's largest video game franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokémon. Our results apply to generalized versions of Super Mario Bros. 1–3, The Lost Levels, and Super Mario World; Donkey Kong Country 1–3; all Legend of Zelda games; all Metroid games; and all Pokémon role-playing games. In addition, we prove PSPACE-completeness of the Donkey Kong Country games and several Legend of Zelda games.}
}
@incollection{CAMERON200789,
title = {Designing Computational Clusters for Performance and Power},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {69},
pages = {89-153},
year = {2007},
booktitle = {Architectural Issues},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)69002-5},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806690025},
author = {Kirk W. Cameron and Rong Ge and Xizhou Feng},
abstract = {Power consumption in computational clusters has reached critical levels. High-end cluster performance improves exponentially while the power consumed and heat dissipated increase operational costs and failure rates. Yet, the demand for more powerful machines continues to grow. In this chapter, we motivate the need to reconsider the traditional performance-at-any-cost cluster design approach. We propose designs where power and performance are considered critical constraints. We describe power-aware and low power techniques to reduce the power profiles of parallel applications and mitigate the impact on performance.}
}
@article{DRAGGIOTIS1998157,
title = {On the computation of multigluon amplitudes},
journal = {Physics Letters B},
volume = {439},
number = {1},
pages = {157-164},
year = {1998},
issn = {0370-2693},
doi = {https://doi.org/10.1016/S0370-2693(98)01015-6},
url = {https://www.sciencedirect.com/science/article/pii/S0370269398010156},
author = {Petros Draggiotis and Ronald H.P. Kleiss and Costas G. Papadopoulos},
abstract = {A computational algorithm based on recursive equations is developed in order to estimate multigluon production processes at high energy hadron colliders. The partonic reactions gg→(n−2)g with n≤9 are studied and comparisons with known approximations are presented.}
}
@incollection{YANG20133,
title = {1 - Swarm Intelligence and Bio-Inspired Computation: An Overview},
editor = {Xin-She Yang and Zhihua Cui and Renbin Xiao and Amir Hossein Gandomi and Mehmet Karamanoglu},
booktitle = {Swarm Intelligence and Bio-Inspired Computation},
publisher = {Elsevier},
address = {Oxford},
pages = {3-23},
year = {2013},
isbn = {978-0-12-405163-8},
doi = {https://doi.org/10.1016/B978-0-12-405163-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124051638000016},
author = {Xin-She Yang and Mehmet Karamanoglu},
keywords = {Algorithm, ant algorithm, bee algorithm, bat algorithm, bio-inspired, cuckoo search, firefly algorithm, harmony search, particle swarm optimization, swarm intelligence, metaheuristics},
abstract = {Swarm intelligence (SI) and bio-inspired computing in general have attracted great interest in almost every area of science, engineering, and industry over the last two decades. In this chapter, we provide an overview of some of the most widely used bio-inspired algorithms, especially those based on SI such as cuckoo search, firefly algorithm, and particle swarm optimization. We also analyze the essence of algorithms and their connections to self-organization. Furthermore, we highlight the main challenging issues associated with these metaheuristic algorithms with in-depth discussions. Finally, we provide some key, open problems that need to be addressed in the next decade.}
}
@article{BENTON20001135,
title = {Computational modelling of interleaved first- and second-order motion sequences and translating 3f+4f beat patterns},
journal = {Vision Research},
volume = {40},
number = {9},
pages = {1135-1142},
year = {2000},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(00)00026-2},
url = {https://www.sciencedirect.com/science/article/pii/S0042698900000262},
author = {Christopher P. Benton and Alan Johnston and Peter W. McOwan},
keywords = {Motion perception, Computational modelling, Second-order, Feature tracking},
abstract = {Despite detailed psychophysical, neurophysiological and electrophysiological investigation, the number and nature of independent and parallel motion processing mechanisms in the visual cortex remains controversial. Here we use computational modelling to evaluate evidence from two psychophysical studies collectively thought to demonstrate the existence of three separate and independent motion processing channels. We show that the pattern of psychophysical results can largely be accounted for by a single mechanism. The results demonstrate that a low-level luminance based approach can potentially provide a wider account of human motion processing than generally thought possible.}
}

@article{FEDORENKO2014120,
title = {Reworking the language network},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {3},
pages = {120-126},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S136466131300288X},
author = {Evelina Fedorenko and Sharon L. Thompson-Schill},
keywords = {domain specificity, domain generality, language network, cognitive control, fMRI},
abstract = {Prior investigations of functional specialization have focused on the response profiles of particular brain regions. Given the growing emphasis on regional covariation, we propose to reframe these questions in terms of brain ‘networks’ (collections of regions jointly engaged by some mental process). Despite the challenges that investigations of the language network face, a network approach may prove useful in understanding the cognitive architecture of language. We propose that a language network plausibly includes a functionally specialized ‘core’ (brain regions that coactivate with each other during language processing) and a domain-general ‘periphery’ (a set of brain regions that may coactivate with the language core regions at some times but with other specialized systems at other times, depending on task demands). Framing the debate around network properties such as this may prove to be a more fruitful way to advance our understanding of the neurobiology of language.}
}
@article{RIVAS20011369,
title = {Computational identification of noncoding RNAs in E. coli by comparative genomics},
journal = {Current Biology},
volume = {11},
number = {17},
pages = {1369-1373},
year = {2001},
issn = {0960-9822},
doi = {https://doi.org/10.1016/S0960-9822(01)00401-8},
url = {https://www.sciencedirect.com/science/article/pii/S0960982201004018},
author = {Elena Rivas and Robert J. Klein and Thomas A. Jones and Sean R. Eddy},
abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs 1, 2. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{ZAHRAH2024100481,
title = {Unmasking hate in the pandemic: A cross-platform study of the COVID-19 infodemic},
journal = {Big Data Research},
volume = {37},
pages = {100481},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2214579624000558},
author = {Fatima Zahrah and Jason R.C. Nurse and Michael Goldsmith},
keywords = {Social media analysis, Cross-platform analysis, Online hate, COVID-19},
abstract = {The past few decades have established how digital technologies and platforms have provided an effective medium for spreading hateful content, which has been linked to several catastrophic consequences. Recent academic studies have also highlighted how online hate is a phenomenon that strategically makes use of multiple online platforms. In this article, we seek to advance the current research landscape by harnessing a cross-platform approach to computationally analyse content relating to the 2020 COVID-19 pandemic. More specifically, we analyse content on hate-specific environments from Twitter, Reddit, 4chan and Stormfront. Our findings show how content and posting activity can change across platforms, and how the psychological components of online content can differ depending on the platform being used. Through this, we provide unique insight into the cross-platform behaviours of online hate. We further define several avenues for future research within this field so as to gain a more comprehensive understanding of the global hate ecosystem.}
}
@article{ROHLFS2025128701,
title = {Generalization in neural networks: A broad survey},
journal = {Neurocomputing},
volume = {611},
pages = {128701},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128701},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014723},
author = {Chris Rohlfs},
keywords = {Literature review, Deep learning, Overfitting, Causality, Domain generalization, Transfer learning, Foundation models, Multimodal, Semantic knowledge, Abstraction, Biologically-inspired},
abstract = {This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from training to test data are discussed, with suggestive evidence presented that, at least for the ImageNet dataset, popular classification models show substantial overfitting. An empirical example and perspectives from statistics highlight how models’ (2) distribution generalization can benefit from consideration of causal relationships and counterfactual scenarios. Transfer learning approaches and results for (3) domain generalization are summarized, as is the wealth of domain generalization benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the emergence of transformer-based foundation models such as those used for language processing. Studies performing (5) modality generalization are reviewed, including those that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Higher-level (6) scope generalization results are surveyed, including graph-based approaches to represent symbolic knowledge in networks and attribution strategies for improving networks’ explainability. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.}
}
@article{NIKZAINAL2024101739,
title = {Prof. Serena Nik-Zainal},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101739},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004695},
author = {Serena Nik-Zainal},
abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.}
}
@incollection{OBRIEN2014141,
title = {7 - Reasoning with graphs},
editor = {Jamie O’Brien},
booktitle = {Shaping Knowledge},
publisher = {Chandos Publishing},
pages = {141-174},
year = {2014},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-751-4},
doi = {https://doi.org/10.1533/9781780634326.141},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347514500078},
author = {Jamie O’Brien},
keywords = {graph databases, logic and computing, reasoning, spatial data structures, visualization},
abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.}
}
@incollection{WARE2021425,
title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {425-456},
year = {2021},
series = {Interactive Technologies},
isbn = {978-0-12-812875-6},
doi = {https://doi.org/10.1016/B978-0-12-812875-6.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128756000128},
author = {Colin Ware},
keywords = {Interactive visualization design, Visual thinking, Visual thinking design patterns, Visual working memory, Visualization design, Visualization types},
abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.}
}
@article{CARBONARO20101098,
title = {Computer-game construction: A gender-neutral attractor to Computing Science},
journal = {Computers & Education},
volume = {55},
number = {3},
pages = {1098-1111},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510001399},
author = {Mike Carbonaro and Duane Szafron and Maria Cutumisu and Jonathan Schaeffer},
keywords = {Computing Science, Females in Science, Computer game construction},
abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.}
}
@article{NEWMAN20031668,
title = {Frontal and parietal participation in problem solving in the Tower of London: fMRI and computational modeling of planning and high-level perception},
journal = {Neuropsychologia},
volume = {41},
number = {12},
pages = {1668-1682},
year = {2003},
issn = {0028-3932},
doi = {https://doi.org/10.1016/S0028-3932(03)00091-5},
url = {https://www.sciencedirect.com/science/article/pii/S0028393203000915},
author = {Sharlene D Newman and Patricia A Carpenter and Sashank Varma and Marcel Adam Just},
keywords = {Planning, fMRI, Spatial working memory, Problem solving, Tower of London, Computational modeling, 4CAPS},
abstract = {This study triangulates executive planning and visuo-spatial reasoning in the context of the Tower of London (TOL) task by using a variety of methodological approaches. These approaches include functional magnetic resonance imaging (fMRI), functional connectivity analysis, individual difference analysis, and computational modeling. A graded fMRI paradigm compared the brain activation during the solution of problems with varying path lengths: easy (1 and 2 moves), moderate (3 and 4 moves) and difficult (5 and 6 moves). There were three central findings regarding the prefrontal cortex: (1) while both the left and right prefrontal cortices were equally involved during the solution of moderate and difficult problems, the activation on the right was differentially attenuated during the solution of the easy problems; (2) the activation observed in the right prefrontal cortex was highly correlated with individual differences in working memory (measured independently by the reading span task); and (3) different patterns of functional connectivity were observed in the left and right prefrontal cortices. Results obtained from the superior parietal region also revealed left/right differences; only the left superior parietal region revealed an effect of difficulty. These fMRI results converged upon two hypotheses: (1) the right prefrontal area may be more involved in the generation of a plan, whereas the left prefrontal area may be more involved in plan execution; and (2) the right superior parietal region is more involved in attention processes while the left homologue is more of a visuo-spatial workspace. A 4CAPS computational model of the cognitive processes and brain activation in the TOL task integrated these hypothesized mechanisms, and provided a reasonably good fit to the observed behavioral and brain activation data. The multiple research approaches presented here converge on a deepening understanding of the combination of perceptual and conceptual processes in this type of visual problem solving.}
}
@incollection{MAYER2010273,
title = {Problem Solving and Reasoning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {273-278},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00487-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004875},
author = {R.E. Mayer},
keywords = {Convergent thinking, Creativity, Deductive reasoning, Directed thinking, Divergent thinking, Einstellung, Everyday thinking, Expert problem solving, Functional fixedness, Ill-defined problem, Inductive reasoning, Insight, Means-ends analysis, Nonroutine problem, Problem solving, Problem space, Productive thinking, Reasoning, Reproductive thinking, Routine problem, Thinking, Transfer, Well-defined problem},
abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.}
}
@article{WEYDMANN2025111173,
title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {136},
pages = {111173},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624002410},
author = {Gibson Weydmann and Igor Palmieri and Reinaldo A.G. Simões and Samara Buchmann and Eduardo Schmidt and Paulina Alves and Lisiane Bizarro},
keywords = {Overweight, Reinforcement Learning, Working Memory, Computational Modelling},
abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.}
}
@article{ROSS2021100069,
title = {Kinenoetic analysis: Unveiling the material traces of insight},
journal = {Methods in Psychology},
volume = {5},
pages = {100069},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000266},
author = {Wendy Ross and Frédéric Vallée-Tourangeau},
keywords = {Insight, Case study, Observation},
abstract = {Research on insight problem solving sets itself a challenging goal: How to explain the origin of a new idea. It compounds the difficulty of this challenge by traditionally seeking to explain the phenomenon in strictly mental terms. Rather, we suggest that thoughts and actions are bound to objects, inviting a granular description of the world within which thinking proceeds. As the reasoner transforms the world, the physical traces of these changes can be mapped in space and time. Not only can the reasoner see these changes, and act upon them, the researcher can develop new inscription devices that captures the trajectory of the creative arc along spatial and temporal coordinates. Kinenoetic is a term we employ to capture the idea that knowledge comes from the movement of objects and that this knowledge is both at the level of the problem-solver and at the level of the researcher. This form of knowledge can only be constructed in problem solving environments where reasoners can manipulate physical elements. A kinenoetic analysis tracks and maps the changes to the object-qua-models of proto solutions, and in the process unveils the physical genesis of new ideas and creativity. Our aim here is to lay out a method for using the objects commonly employed in interactive problem-solving research, tracing the process of thought to elucidate underlying cognitive mechanisms. Thus, the focus turns from the effects of objects on thoughts, to tracing object-thought mutualities as they are enacted and made visible.}
}
@article{KITTAS2010401,
title = {Evolution of the rate of biological aging using a phenotype based computational model},
journal = {Journal of Theoretical Biology},
volume = {266},
number = {3},
pages = {401-407},
year = {2010},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2010.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022519310003619},
author = {Aristotelis Kittas},
keywords = {Evolution, Aging, Computer simulations, Age-structured populations, Modelling},
abstract = {In this work I introduce a simple model to study how natural selection acts upon aging, which focuses on the viability of each individual. It is able to reproduce the Gompertz law of mortality and can make predictions about the relation between the level of mutation rates (beneficial/deleterious/neutral), age at reproductive maturity and the degree of biological aging. With no mutations, a population with low age at reproductive maturity R stabilizes at higher density values, while with mutations it reaches its maximum density, because even for large pre-reproductive periods each individual evolves to survive to maturity. Species with very short pre-reproductive periods can only tolerate a small number of detrimental mutations. The probabilities of detrimental (Pd) or beneficial (Pb) mutations are demonstrated to greatly affect the process. High absolute values produce peaks in the viability of the population over time. Mutations combined with low selection pressure move the system towards weaker phenotypes. For low values in the ratio Pd/Pb, the speed at which aging occurs is almost independent of R, while higher values favor significantly species with high R. The value of R is critical to whether the population survives or dies out. The aging rate is controlled by Pd and Pb and the amount of the viability of each individual is modified, with neutral mutations allowing the system more “room” to evolve. The process of aging in this simple model is revealed to be fairly complex, yielding a rich variety of results.}
}
@article{ROSSITER20083713,
title = {Compromises between feasibility and performance within linear MPC},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3713-3718},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00627},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639526X},
author = {J.A. Rossiter and Yihang Ding},
keywords = {Constraints, Feasibility, Performance, Computational Efficiency, Contours},
abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.}
}
@article{BELVEDERE201218,
title = {A computational index derived from whole-genome copy number analysis is a novel tool for prognosis in early stage lung squamous cell carcinoma},
journal = {Genomics},
volume = {99},
number = {1},
pages = {18-24},
year = {2012},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888754311002424},
author = {Ornella Belvedere and Stefano Berri and Rebecca Chalkley and Caroline Conway and Fabio Barbone and Federica Pisa and Kenneth MacLennan and Catherine Daly and Melissa Alsop and Joanne Morgan and Jessica Menis and Peter Tcherveniakov and Kostas Papagiannopoulos and Pamela Rabbitts and Henry M. Wood},
keywords = {Lung cancer, Copy number, Survival, Next-generation sequencing},
abstract = {Squamous cell carcinoma of the lung is remarkable for the extent to which the same chromosomal abnormalities are detected in individual tumours. We have used next generation sequencing at low coverage to produce high resolution copy number karyograms of a series of 89 non-small cell lung tumours specifically of the squamous cell subtype. Because this methodology is able to create karyograms from formalin-fixed paraffin-embedded material, we were able to use archival stored samples for which survival data were available and correlate frequently occurring copy number changes with disease outcome. No single region of genomic change showed significant correlation with survival. However, adopting a whole-genome approach, we devised an algorithm that relates to total genomic damage, specifically the relative ratios of copy number states across the genome. This algorithm generated a novel index, which is an independent prognostic indicator in early stage squamous cell carcinoma of the lung.}
}
@article{YANG202075,
title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
journal = {Information Sciences},
volume = {541},
pages = {75-97},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306009},
author = {Xin Yang and Yingying Zhang and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way granular computing, Sequential three-way decision, Local neighborhood, Temporal-spatial, Multi-granularity},
abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.}
}
@article{ARCHAMBAULT2024102865,
title = {Ethical dimensions of algorithmic literacy for college students: Case studies and cross-disciplinary connections},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {3},
pages = {102865},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102865},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000260},
author = {Susan Gardner Archambault and Shalini Ramachandran and Elisa Acosta and Sheree Fu},
keywords = {Algorithmic literacy, Information literacy, Algorithmic bias, AI ethics, Algorithmic fairness, Computer science education},
abstract = {This article addresses three key questions related to the ethical facets of algorithmic literacy. First, it synthesizes existing literature to identify six core ethical components, including bias, privacy, transparency, accountability, accuracy, and non-maleficence. Second, a crosswalk maps the intersections of these principles across the Association of College and Research Libraries' Framework for Information Literacy for Higher Education and the Association of Computing Machinery's Code of Ethics and Professional Conduct and Joint Statement on Principles for Responsible Algorithmic Systems. This analysis reveals significant overlap on issues like unfairness and transparency, helping prioritize topics for instruction. Finally, case studies showcase pedagogical strategies for teaching ethical considerations, informed by the crosswalk. Workshops for diverse undergraduates and computer science students employed reallife instances of algorithmic bias to prompt reflection on unintended harm, contestability, and responsible development. Pre-post surveys indicated expanded critical perspectives after the interventions. By systematically examining shared values and testing instructional approaches, this study provides practical tools to shape ethical thinking on algorithms. It also demonstrates promising practices for responsibly advancing algorithmic literacy across disciplines. Ultimately, fostering interdisciplinary awareness and multipronged educational initiatives can empower students to question algorithmic authority and biases.}
}
@article{RONAYNE2021318,
title = {Evaluating the sunk cost effect},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {318-327},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001293},
author = {David Ronayne and Daniel Sgroi and Anthony Tuckwell},
keywords = {Sunk cost effect, Sunk cost fallacy, Endowment effect, Cognitive ability, Psychological scales, Scale validation},
abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.}
}
@article{OMHOLT2003107,
title = {Eberhard O. Voit, Computational Analysis of Biochemical Systems. A Practical Guide for Biochemists and Molecular Biologists, Cambridge University Press, 2000, 531 pages (ISBN 0-521-78579-0; paperback)},
journal = {Mathematical Biosciences},
volume = {181},
number = {1},
pages = {107-109},
year = {2003},
issn = {0025-5564},
doi = {https://doi.org/10.1016/S0025-5564(02)00153-0},
url = {https://www.sciencedirect.com/science/article/pii/S0025556402001530},
author = {Stig W Omholt}
}
@article{MUSSO2015267,
title = {A single dual-stream framework for syntactic computations in music and language},
journal = {NeuroImage},
volume = {117},
pages = {267-283},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915004000},
author = {Mariacristina Musso and Cornelius Weiller and Andreas Horn and Volkmer Glauche and Roza Umarova and Jürgen Hennig and Albrecht Schneider and Michel Rijntjes},
abstract = {This study is the first to compare in the same subjects the specific spatial distribution and the functional and anatomical connectivity of the neuronal resources that activate and integrate syntactic representations during music and language processing. Combining functional magnetic resonance imaging with functional connectivity and diffusion tensor imaging-based probabilistic tractography, we examined the brain network involved in the recognition and integration of words and chords that were not hierarchically related to the preceding syntax; that is, those deviating from the universal principles of grammar and tonal relatedness. This kind of syntactic processing in both domains was found to rely on a shared network in the left hemisphere centered on the inferior part of the inferior frontal gyrus (IFG), including pars opercularis and pars triangularis, and on dorsal and ventral long association tracts connecting this brain area with temporo-parietal regions. Language processing utilized some adjacent left hemispheric IFG and middle temporal regions more than music processing, and music processing also involved right hemisphere regions not activated in language processing. Our data indicate that a dual-stream system with dorsal and ventral long association tracts centered on a functionally and structurally highly differentiated left IFG is pivotal for domain–general syntactic competence over a broad range of elements including words and chords.}
}
@article{LOWE20219898316,
title = {In-Depth Computational Analysis of Natural and Artificial Carbon Fixation Pathways},
journal = {BioDesign Research},
volume = {2021},
pages = {9898316},
year = {2021},
issn = {2693-1257},
doi = {https://doi.org/10.34133/2021/9898316},
url = {https://www.sciencedirect.com/science/article/pii/S2693125724000566},
author = {Hannes Löwe and Andreas Kremling},
abstract = {In the recent years, engineering new-to-nature CO2- and C1-fixing metabolic pathways made a leap forward. New, artificial pathways promise higher yields and activity than natural ones like the Calvin-Benson-Bassham (CBB) cycle. The question remains how to best predict their in vivo performance and what actually makes one pathway “better” than another. In this context, we explore aerobic carbon fixation pathways by a computational approach and compare them based on their specific activity and yield on methanol, formate, and CO2/H2 considering the kinetics and thermodynamics of the reactions. Besides pathways found in nature or implemented in the laboratory, this included two completely new cycles with favorable features: the reductive citramalyl-CoA cycle and the 2-hydroxyglutarate-reverse tricarboxylic acid cycle. A comprehensive kinetic data set was collected for all enzymes of all pathways, and missing kinetic data were sampled with the Parameter Balancing algorithm. Kinetic and thermodynamic data were fed to the Enzyme Cost Minimization algorithm to check for respective inconsistencies and calculate pathway-specific activities. The specific activities of the reductive glycine pathway, the CETCH cycle, and the new reductive citramalyl-CoA cycle were predicted to match the best natural cycles with superior product-substrate yield. However, the CBB cycle performed better in terms of activity compared to the alternative pathways than previously thought. We make an argument that stoichiometric yield is likely not the most important design criterion of the CBB cycle. Still, alternative carbon fixation pathways were paretooptimal for specific activity and product-substrate yield in simulations with C1 substrates and CO2/H2 and therefore hold great potential for future applications in Industrial Biotechnology and Synthetic Biology.}
}
@article{SU2024108233,
title = {Musical protein: Mapping the time sequence of music onto the spatial architecture of proteins},
journal = {Computer Methods and Programs in Biomedicine},
volume = {252},
pages = {108233},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724002281},
author = {Jun Su and Peng Zhou},
keywords = {Musical protein, Life of music, Bioinformatics, Stave, Note, Piano, Conversion of music to protein},
abstract = {Background and objective
Music, the ubiquitous language across human cultures, is traditionally considered as a form of art but has been linked to biomolecules in recent years. However, previous efforts have only been addressed on sonification of nucleic acids and proteins to produce so-called life music, the soundscape from the basic building blocks of life. In this study, we attempted to, for the first time, conduct a reverse operation of this process, i.e. conversion of music to protein (CoMtP).
Methods
A novel notion termed musical protein (MP) –– the protein defined by music –– was proposed and, on this basis, we described a computational strategy to map the time sequence of music onto the spatial architecture of proteins, which considered that each note in the stave of a music (target) can be simply characterized by two acoustical quantities and that each residue in the primary sequence of a protein (hit) was represented by amino acid descriptors.
Results
A simulated annealing (SA) algorithm was applied to iteratively generate the best matched MP hit for a music target and structural bioinformatics was then used to model spatial advanced structure for the resulting MP. We also demonstrated that some small MPs derived from music segments may have potential biological functions, which, for example, can serve as antimicrobial peptides (AMPs) to inhibit clinical bacterial strains with moderate or high antibacterial potency.
Conclusions
This work may benefit many aspects; for example, it would open a door for the hearing-impaired persons to ‘listen’ music in a biological vision and could be a mean of exposing students to the concepts of biomolecules at an earlier age through the use of auditory characteristics. The CoMtP would also facilitate the rational design of proteins with biological and medicinal significance.}
}
@article{ARUN2009S1116,
title = {P03-117 A bedside schizophrenia thought disorder scale},
journal = {European Psychiatry},
volume = {24},
pages = {S1116},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71349-5},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713495},
author = {C.P. Arun},
abstract = {Present classification systems for thought disorder lack consistency and require one to remember long-winded definitions limiting their use to research settings. As an extension of recent work in this area (World Congress, 2008), we classify the characteristic thought disorder patterns seen in schizophrenia according to the location of the lesion in notional "threads" of mental computational processes that string speech together. These threads must take both semantics and syntax into consideration in performing their function. When we speak - just as when we write - there is a natural hierarchy topic thread (the topic of the ‘essay’) and multiples of paragraph threads, sentence threads, clause threads, word threads and phoneme threads. Intuitively, we grade the severity of thought disorder depending upon whether a particular thread gets stuck (S), reconnects abnormally (R) or is absent altogether: I.paragraph thread R: Disjointed sentences S: Circumstantiality;II.topic threadR: Tangentiality S: Preoccupatory thinking;III.sentence threads R: Knight's move thinking S: Clause perseveration;IV.clause threads R: Word salad S: Word perseveration, fusion;V.word threads R: Incoherent sounds/ neologisms/ paraphasias S: Phoneme/syllable perseveration;VI.phoneme threads - Failure of production: Mutism.Of course, one must record all the lesions that are present at any given time. This scale incorporates a intuitive progression from mild to severe thought disorder in Schizophrenia. Using the STDS would allow the straightforward ‘bedside’ quantification of the severity of thought disorder and enforce discipline into the thought assessment section of the Mental State Examination.}
}
@article{BLACK202010653,
title = {A revolution in biochemistry and molecular biology education informed by basic research to meet the demands of 21st century career paths},
journal = {Journal of Biological Chemistry},
volume = {295},
number = {31},
pages = {10653-10661},
year = {2020},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.AW120.011104},
url = {https://www.sciencedirect.com/science/article/pii/S0021925817501040},
author = {Paul N. Black},
keywords = {biochemistry, molecular biology, teaching, learning, primary research, leadership, environment, inclusive excellence, STEM education, biochemistry and molecular biology teaching and learning},
abstract = {The National Science Foundation estimates that 80% of the jobs available during the next decade will require math and science skills, dictating that programs in biochemistry and molecular biology must be transformative and use new pedagogical approaches and experiential learning for careers in industry, research, education, engineering, health-care professions, and other interdisciplinary fields. These efforts require an environment that values the individual student and integrates recent advances from the primary literature in the discipline, experimentally directed research, data collection and analysis, and scientific writing. Current trends shaping these efforts must include critical thinking, experimental testing, computational modeling, and inferential logic. In essence, modern biochemistry and molecular biology education must be informed by, and integrated with, cutting-edge research. This environment relies on sustained research support, commitment to providing the requisite mentoring, access to instrumentation, and state-of-the-art facilities. The academic environment must establish a culture of excellence and faculty engagement, leading to innovation in the classroom and laboratory. These efforts must not lose sight of the importance of multidimensional programs that enrich science literacy in all facets of the population, students and teachers in K-12 schools, nonbiochemistry and molecular biology students, and other stakeholders. As biochemistry and molecular biology educators, we have an obligation to provide students with the skills that allow them to be innovative and self-reliant. The next generation of biochemistry and molecular biology students must be taught proficiencies in scientific and technological literacy, the importance of the scientific discourse, and skills required for problem solvers of the 21st century.}
}
@article{LIU2006207,
title = {Evolutionary design in a multi-agent design environment},
journal = {Applied Soft Computing},
volume = {6},
number = {2},
pages = {207-220},
year = {2006},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S156849460500013X},
author = {Hong Liu and Mingxi Tang},
keywords = {Multi-agent system, Evolutionary computing, Generic algorithm, Computer-aided design, Creative design},
abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.}
}
@article{WILLIAMS2023145,
title = {Stabilizing expectations when shifting from analytical to intuitive reasoning: The role of prediction errors in reasoning},
journal = {Cortex},
volume = {161},
pages = {145-153},
year = {2023},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223000412},
author = {Chad C. Williams and Cameron D. Hassall and Olave E. Krigolson},
keywords = {Reasoning, Prediction errors, Cognitive control, Theta, EEG},
abstract = {As humans, we rely on intuitive reasoning for most of our decisions. However, when there is a novel or atypical decision to be made, we must rely on a slower and more deliberative thought process—analytical reasoning. As we gain experience with these novel or atypical decisions, our reasoning shifts from analytical to intuitive, which parallels a reduction in the need for cognitive control. Here, we sought to confirm this claim by employing electroencephalographic (EEG) measures of cognitive control as participants performed a simple perceptual decision-making task. Specifically, we had participants categorize “blobs” into families based on their visual attributes so we could examine how their reasoning changed with learning. In a key manipulation, halfway through the experiment we introduced novel blob families to categorize, thus temporarily increasing the need for analytical reasoning (i.e., cognitive control). Congruent with past research, we focused our EEG analyses on frontal theta activity as it has been linked to cognitive control and analytical thinking. As hypothesized, we found a transition from analytical to intuitive decision-making systems with learning as indexed by a decrease in frontal theta power. Further, when the novel blobs were introduced at the midpoint of the experiment, we found that decisions about these stimuli recruited analytical reasoning as indicated by increased theta power in comparison to decisions about well-practiced stimuli. We propose our findings to reflect prediction errors to decision demands—a monitoring process that determines whether our expectations of demands are met. Shifting from analytical to intuitive reasoning thus reflects the stabilization of our expectations of decision demands, which can be violated with unexpected demands when encountering novel stimuli.}
}
@article{TAY20211,
title = {Modelability across time as a signature of identity construction on YouTube},
journal = {Journal of Pragmatics},
volume = {182},
pages = {1-15},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621002307},
author = {Dennis Tay},
keywords = {Identity construction, Social media, LIWC, Modelability, ARIMA, Time series analysis},
abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.}
}
@article{LORE2024105149,
title = {Using multiple, dynamically linked representations to develop representational competency and conceptual understanding of the earthquake cycle},
journal = {Computers & Education},
volume = {222},
pages = {105149},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105149},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001635},
author = {Christopher Lore and Hee-Sun Lee and Amy Pallant and Jie Chao},
keywords = {Teaching/learning strategies, Simulations, Pedagogical issues, Applications in subject areas},
abstract = {Using computational methods to produce and interpret multiple scientific representations is now a common practice in many science disciplines. Research has shown students have difficulty in moving across, connecting, and sensemaking from multiple representations. There is a need to develop task-specific representational competencies for students to reason and conduct scientific investigations using multiple representations. In this study, we focus on three representational competencies: 1) linking between representations, 2) disciplinary sensemaking from multiple representations, and 3) conceptualizing domain-relevant content derived from multiple representations. We developed a block code-based computational modeling environment with three different representations and embedded it within an online activity for students to carry out investigations around the earthquake cycle. The three representations include a procedural representation of block codes, a geometric representation of land deformation build-up, and a graphical representation of deformation build-up over time. We examined the extent of students' representational competencies and which competencies are most correlated with students’ future performance in a computationally supported geoscience investigation. Results indicate that a majority of the 431 students showed at least some form of representational competence. However, a relatively small number of students showed sophisticated levels of linking, sensemaking, and conceptualizing from the representations. Five of seven representational competencies, the most prominent being code sensemaking (η2 = 0.053, p < 0.001), were significantly correlated to student performance on a summative geoscience investigation.}
}
@article{ANDERSON2023102027,
title = {Nurse scholars of the Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program},
journal = {Nursing Outlook},
volume = {71},
number = {5},
pages = {102027},
year = {2023},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2023.102027},
url = {https://www.sciencedirect.com/science/article/pii/S002965542300132X},
author = {Cindy M. Anderson and Nina Ardery and Daniel Pesut and Carmen Alvarez and Tamryn F. Gray and Karen M. Rose and Jasmine L. Travers and Janiece Taylor and Kathy D. Wright},
keywords = {Health professions diversity, Equity, Inclusive excellence, Robert Wood Johnson Foundation, Nurse faculty scholars, Harold Amos, Medical faculty development, Legacy leadership},
abstract = {Background
The challenge to increase the diversity, inclusivity, and equity of nurse scientists is a critical issue to enhance nursing knowledge development, health care, health equity, and health outcomes in the United States.
Purpose
The purpose of this paper is to highlight the current nurse scholars in the Robert Wood Johnson Foundation (RWJF) Harold Amos Medical Faculty Development Program (AMFDP).
Discussion
Profiles and the programs of research and scholarship of the current AMFDP nurse scholars are described and discussed. Scholars share lessons learned, and how the AMFDP program has influenced their thinking and commitments to future action in service of nursing science, diversity efforts, legacy leadership, issues of health equity.
Conclusion
RWJF has a history of supporting the development of nursing scholars. AMFDP is an example of legacy leadership program that contributes to a culture of health and the development of next-generation nursing science scholars.}
}
@article{ECONOMOU1994131,
title = {Activities, issues and perspectives in computational physics: a view from Greece},
journal = {Computational Materials Science},
volume = {2},
number = {1},
pages = {131-136},
year = {1994},
issn = {0927-0256},
doi = {https://doi.org/10.1016/0927-0256(94)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0927025694900558},
author = {E.N. Economou}
}
@article{GOLDBERG2012261,
title = {An efficient tree-based computation of a metric comparable to a natural diffusion distance},
journal = {Applied and Computational Harmonic Analysis},
volume = {33},
number = {2},
pages = {261-281},
year = {2012},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520311001266},
author = {Maxim J. Goldberg and Seonja Kim},
keywords = {Tree, Diffusion, Distance, Metric},
abstract = {Using diffusion to define distances between points on a manifold (or a sampled data set) has been successfully employed in various applications such as data organization and approximately isometric embedding of high dimensional data in low dimensional Euclidean space. Recently, P. Jones has proposed a diffusion distance which is both intuitively appealing and scales appropriately with increasing time. In the first part of our paper, we present an efficient tree-based approach to computing an approximation to Jonesʼs diffusion distance. We also show our approximation is comparable to Jonesʼs distance. Neither Jonesʼs distance, nor our approximation, satisfies the triangle inequality; in particular, in the case of heat flow on Rn, Jonesʼs separation distance gives a scaled square of the Euclidean distance. In the second part of our paper, we present a general construction to obtain an “almost” metric from a general distance. We also discuss a numerical procedure to implement our construction. Additionally, we show that in the case of heat flow on Rn, we recover (scaled) Euclidean distance from Jonesʼs distance.}
}
@article{PARK2023101271,
title = {The impact of research and representation of site analysis for creative design approach in architectural design studio},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101271},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101271},
url = {https://www.sciencedirect.com/science/article/pii/S187118712300041X},
author = {Eun Joo Park and Keunhye Lee and Eunki Kang},
keywords = {Architectural design studio, Creative thinking, Design education, Design methodology, Site analysis},
abstract = {In the context of architectural education, design studio projects generally begin with the research of design themes and contexts; however, few attempts have been made to appreciate site analysis as reliable architectural design research. This study aims to explore strategies that link site analysis and design application to bridge the gap between research and representation as a framework for applying architectural design education. To bridge this knowledge gap, this study describes four phases of site analysis—(1) site selection, (2) site survey, (3) problem identification, and (4) suggestion for the design approach —in which visual expression of the representation technique was explored to develop a creative design approach through observation and analysis of students’ work. A curriculum that adopts the four phases of site analysis was developed based on the SPC and expanded for second-year architecture students in the university. The results showed that there are differences between architecture students in schematizing specific ideas and analysis methods, and that a substantial change in the process occurs when including visual expression in site analysis. In addition, the combination of group-based work and site analysis led to problem-solving that showed co-evolution. Finally, the study describes how research shapes site analysis and explains how representation can contribute to understanding the research. Site analysis consists of an initial attempt to explore research related to creative approaches, and may benefit both architecture educators and students.}
}
@incollection{BUNGE1980155,
title = {CHAPTER 7 - Thinking and Knowing},
editor = {MARIO BUNGE},
booktitle = {The Mind–Body Problem},
publisher = {Pergamon},
pages = {155-173},
year = {1980},
isbn = {978-0-08-024720-5},
doi = {https://doi.org/10.1016/B978-0-08-024720-5.50012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080247205500128},
author = {MARIO BUNGE},
abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.}
}
@incollection{MARON1965118,
title = {On Cybernetics, Information Processing, and Thinking},
editor = {Norbert Wiener and J.P. Schadé},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {17},
pages = {118-138},
year = {1965},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(08)60158-2},
url = {https://www.sciencedirect.com/science/article/pii/S0079612308601582},
author = {M.E. Maron},
abstract = {Publisher Summary
It is the purpose of this chapter to examine the origins, development, and present status of those key cybernetic notions that provide an information-flow framework within which to attack one aspect of the question of how a person thinks— that is,.the question of the information mechanisms and processes that underlie and are correlated with thinking. After an introductory survey of the scope and ramifications of the information sciences, the cybernetic way of looking at the information processing in the nervous system is examined, so as to see in what sense it provides new and sharp tools of analysis for the neurophysiologist. With this as background, the problem of artificial intelligence is considered and with that the logical and linguistic difficulties in talking about the relationship between thinking and brain activity. An information-flow model of an artificial brain mechanism is described whose activity; it is argued is the correlate to activity, such as perceiving, learning, thinking, knowing, etc. This leads finally to a consideration of the impact of these notions on theoretical neurophysiology and its attempt to frame suitable hypotheses and on epistemology that is concerned with the logical analysis of measures, methods, and techniques, which can justify the activity of knowing.}
}
@article{MACHADO2023101290,
title = {A multiple criteria framework to assess learning methodologies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101290},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101290},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000603},
author = {Rafaela Heloisa Carvalho Machado and Samuel Vieira Conceição and Renata Pelissari and Sarah Ben Amor and Thiago Lombardi Resende},
keywords = {Active learning methodologies, Skills, Multiple criteria decision making, MCDA, MCDM},
abstract = {New job skills required by the professional market have been causing significant changes in the learning process of undergraduate students. Different learning methodologies can be adopted to assist in the development of those skills, and the process of choosing the most suitable learning methodology for each situation may be complex, involving multiple and conflicting criteria. In order to support the choice of learning methodologies for the development of the “4C skills”, i.e, collaboration, communication, creativity and critical thinking, we propose a new framework based on the multiple criteria decision-making approach PROMETHEE II (Preference Ranking Organization Method for Enrichment of Evaluations), considering as criteria the “4C skills”, student motivation, level of learning, student comfort, decision-making capacity and time required for class preparation. Passive methods and active learning methodologies such as Guided Reciprocal Peer Questioning (GRPQ), Think-Pair-Share (TPS), and Problem Based Learning (PBL) are compared. Each methodology was applied to three groups of students of Industrial Engineering of a Brazilian University, totaling 138 students. As a result, PBL obtained the best assessment in the three groups, followed by GRPQ. The proposed framework validates the assessment of learning methodologies, providing a structure and guideline for its replication in other educational institutions.}
}
@article{CEKIRGE199465,
title = {An appropriate algorithm in parallel computations for three-dimensional hydrodynamics},
journal = {Mathematical and Computer Modelling},
volume = {20},
number = {1},
pages = {65-84},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90219-4},
url = {https://www.sciencedirect.com/science/article/pii/0895717794902194},
author = {H.M. Cekirge and J. Berlin and R.A. Bernatz and M. Koch},
keywords = {Methods of characteristics, Tidal currents, Parallel computations, Three-dimensional hydrodynamics, Tidal currents in the Arabian Gulf},
abstract = {There are a number of numerical methods for solving three-dimensional hydrodynamical models. An important aspect of any method is its efficient use of parallel computer architectures in an effort to minimize the clock time requirements in certain simulations such as oil spill modeling which uses three-dimensional hydrodynamics. The vertical-horizontal splitting (VHS) algorithm, using the method of characteristics for the two-dimensional horizontal plane and a generalization of the Crank-Nicholson method for vertical integration, is well-suited for the parallel architecture of the CM-2 machine.}
}
@article{BEAR2020104057,
title = {What comes to mind?},
journal = {Cognition},
volume = {194},
pages = {104057},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719302306},
author = {Adam Bear and Samantha Bensinger and Julian Jara-Ettinger and Joshua Knobe and Fiery Cushman},
keywords = {Sampling, Decision-making, Consciousness, Computation},
abstract = {When solving problems, like making predictions or choices, people often “sample” possibilities into mind. Here, we consider whether there is structure to the kinds of thoughts people sample by default—that is, without an explicit goal. Across three experiments we found that what comes to mind by default are samples from a probability distribution that combines what people think is likely and what they think is good. Experiment 1 found that the first quantities that come to mind for everyday behaviors and events are quantities that combine what is average and ideal. Experiment 2 found, in a manipulated context, that the distribution of numbers that come to mind resemble the mathematical product of the presented statistical distribution and a (softmax-transformed) prescriptive distribution. Experiment 3 replicated these findings in a visual domain. These results provide insight into the process generating people’s conscious thoughts and invite new questions about the value of thinking about things that are both likely and good.}
}
@article{BERRUTO2024858,
title = {Engineering agricultural soil microbiomes and predicting plant phenotypes},
journal = {Trends in Microbiology},
volume = {32},
number = {9},
pages = {858-873},
year = {2024},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2024.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X2400043X},
author = {Chiara A. Berruto and Gozde S. Demirer},
keywords = {rhizosphere engineering, plant microbiome, machine learning, community modeling, host–microbe interactions, microbiome-associated phenotype},
abstract = {Plant growth-promoting rhizobacteria (PGPR) can improve crop yields, nutrient use efficiency, plant tolerance to stressors, and confer benefits to future generations of crops grown in the same soil. Unlocking the potential of microbial communities in the rhizosphere and endosphere is therefore of great interest for sustainable agriculture advancements. Before plant microbiomes can be engineered to confer desirable phenotypic effects on their plant hosts, a deeper understanding of the interacting factors influencing rhizosphere community structure and function is needed. Dealing with this complexity is becoming more feasible using computational approaches. In this review, we discuss recent advances at the intersection of experimental and computational strategies for the investigation of plant–microbiome interactions and the engineering of desirable soil microbiomes.}
}
@article{VIERTEL2019109,
title = {A Computational model of the mammalian external tufted cell},
journal = {Journal of Theoretical Biology},
volume = {462},
pages = {109-121},
year = {2019},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318304752},
author = {Ryan Viertel and Alla Borisyuk},
keywords = {External tufted cell, Bursting, Glomerulus, Olfactory bulb, Hodgkin Huxley model},
abstract = {We introduce a novel detailed conductance-based model of the bursting activity in external tufted (ET) cells of the olfactory bulb. We investigate the mechanisms underlying their bursting, and make experimentally-testable predictions. The ionic currents included in the model are specific to ET cells, and their kinetic and other parameters are based on experimental recordings. We validate the model by showing that its bursting characteristics under various conditions (e.g. blocking various currents) are consistent with experimental observations. Further, we identify the bifurcation structure and dynamics that explain bursting behavior. This analysis allows us to make predictions of the response of the cell to current pulses at different burst phases. We find that depolarizing (but not hyperpolarizing) inputs received during the interburst interval can advance burst timing, creating the substrate for synchronization by excitatory connections. It has been hypothesized that such synchronization among the ET cells within one glomerulus might help coordinate the glomerular output. Next we investigate model parameter sensitivity and identify parameters that play the most prominent role in controlling each burst characteristic, such as the burst frequency and duration. Finally, the response of the cell to periodic inputs is examined, reflecting the sniffing-modulated input that these cell receive in vivo. We find that individual cells can be better entrained by inputs with higher, rather than lower, frequencies than the intrinsic bursting frequency of the cell. Nevertheless, a heterogeneous population of ET cells (as may be found in a glomerulus) is able to produce reliable periodic population responses even at lower input frequencies.}
}
@article{JIANG2024109208,
title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
journal = {Aerospace Science and Technology},
volume = {150},
pages = {109208},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.109208},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824003390},
author = {Lulu Jiang and Guanhua Chen and Xiaopeng Xue and Xin Pan and Gang Chen},
keywords = {Supersonic Parachute, Mars atmosphere, Aerodynamic Optimization, Shape design, Wide Speed Range},
abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{NUNEZV2024101173,
title = {Recommendation system using bio-inspired algorithms for urban orchards},
journal = {Internet of Things},
volume = {26},
pages = {101173},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001148},
author = {Juan M. {Núñez V.} and Juan M. Corchado and Diana M. Giraldo and Sara Rodríguez-González and Fernando {De la Prieta}},
keywords = {Internet of Things, Bio-inspired algorithms, Urban orchards, Lettuce crops, Social design},
abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.}
}
@article{DIACOPOULOS2020103911,
title = {A systematic review of mobile learning in social studies},
journal = {Computers & Education},
volume = {154},
pages = {103911},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103911},
url = {https://www.sciencedirect.com/science/article/pii/S036013152030110X},
author = {Mark Michael Diacopoulos and Helen Crompton}
}
@article{RICHARDSON1991305,
title = {Computational physics on the CM-2 supercomputer},
journal = {Physics Reports},
volume = {207},
number = {3},
pages = {305-320},
year = {1991},
issn = {0370-1573},
doi = {https://doi.org/10.1016/0370-1573(91)90149-G},
url = {https://www.sciencedirect.com/science/article/pii/037015739190149G},
author = {John L. Richardson},
abstract = {The Connection Machine Supercomputer system is described with emphasis on the solution to large scale physics problems. Numerous parallel algorithms as well as their implementation are given that demonstrate the use of the Connection Machine for physical simulations. Applications discussed include classical mechanics, quantum mechanics, electromagnetism, fluid flow, statistical physics and quantum field theories. The visualization of physical phenomena is also discussed and in the lectures video tapes demonstrating this capability are shown. Connection Machine performance and I/O characteristics are also described as well as the CM-2 software.}
}
@article{TEO2024102655,
title = {Age-appropriate adaptation of creativity tasks for infants aged 12–24 months},
journal = {MethodsX},
volume = {12},
pages = {102655},
year = {2024},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2024.102655},
url = {https://www.sciencedirect.com/science/article/pii/S2215016124001092},
author = {Ling Zheng Teo and Victoria Leong},
keywords = {Precursors of creativity, Infancy, Measurements of creativity},
abstract = {Creativity is an important skill that relates to innovation, problem-solving and artistic achievement. However, relatively little is known about the early development of creative potential in very young children, in part due to a paucity of tasks suitable for use during infancy. Current measures of creativity in early childhood include the Unusual Box Test, Torrance's Thinking Creatively in Action and Movement (TCAM) task and the Toca Kitchen Monsters task. These tasks are designed for children aged above 12, 36 and 18 months respectively, but very few measures of creativity can be used for infants aged below 2. Accordingly, here we report age-appropriate adaptations of TCAM and Toca Kitchen Monsters tasks for infants as young as 12 to 24 months. Considerations taken into account include (1) infants’ cognitive capacities (i.e., attention span, language comprehension skills, motor skills, and approach to play), and (2) practicality of the stimuli, including suitability for use amid the COVID-19 pandemic. The modified creativity battery for infants includes three tasks: Music Play, Object Play and Exploratory Play tasks. The task protocols elaborated in this paper are intended to facilitate studies on the early development of creativity in infants aged between 12 and 24 months. Primary highlights include:•Age-appropriate adaptation of creativity tasks for use with infants aged between 12 and 24 months.•Consideration of infants’ cognitive capacities and stimulus practicality.•Innovative use of movement as expression of infants’ creative behaviour.}
}
@incollection{GALLICCHIO201127,
title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
editor = {Christo Christov},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {85},
pages = {27-80},
year = {2011},
booktitle = {Computational chemistry methods in structural biology},
issn = {1876-1623},
doi = {https://doi.org/10.1016/B978-0-12-386485-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123864857000028},
author = {Emilio Gallicchio and Ronald M. Levy},
keywords = {Quasi-chemical description, Statistical mechanics, Potential of mean force, PDT, MM/PBSA, free energy perturbation, BEDAM, double decoupling},
abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.}
}
@article{PAN2025125506,
title = {CISL-PD: A deep learning framework of clinical intervention strategies for Parkinson’s disease based on directional counterfactual Dual GANs},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125506},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125506},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402373X},
author = {Changrong Pan and Yu Tian and Lingyan Ma and Tianshu Zhou and Shuyu Ouyang and Jingsong Li},
keywords = {Parkinson’s disease, Intervention strategies, Counterfactual generation, Generative Adversarial Network},
abstract = {Parkinson’s disease (PD) is a prevalent chronic neurodegenerative disorder characterized by both motor and non-motor symptoms. The significant heterogeneity among PD patients poses a major challenge for treatment interventions. Current clinical interventions for PD primarily target motor symptoms, often neglecting non-motor symptoms, which can lead to unnecessary complications in non-motor symptoms while treating motor symptoms. Therefore, it is crucial to provide comprehensive and precise intervention strategies that encompass both symptom types. To address this issue, we develop a deep learning framework of clinical intervention strategies for PD (CISL-PD) based on counterfactual thinking. This framework introduces Directional Counterfactual Dual Generative Adversarial Networks (DCD-GANs), which apply various counterfactual constraints to longitudinal data to generate practical and plausible counterfactual instances aligned with clinical reality. By analyzing these counterfactual instances and their differences from the original instances, we explore PD intervention strategies with duration-specific fine regulation of multidimensional features. Experiments conducted on 374 PD patients from the Parkinson’s Progression Markers Initiative (PPMI) demonstrate that the counterfactual instances generated by DCD-GANs surpass other state-of-the-art models in terms of similarity (0.307 ± 0.246), sparsity (0.513 ± 0.161), smoothness (0.238 ± 0.135), and trend consistency (0.100 ± 0.089). From these generated counterfactual instances, we develop three clinically feasible intervention strategies that address both motor and non-motor symptoms and identify corresponding patterns of PD with distinct progression differences. Validation on an independent cohort of 351 patients from the National Institute of Neurological Disorders and Stroke Parkinson’s Disease Biomarkers Program (PDBP) confirmed the framework’s robustness and generalizability. By offering precise, multidimensional intervention strategies that can address both motor and non-motor symptoms, the CISL-PD framework has the potential to enhance patient outcomes, reduce complications, improve overall quality of life, and guide clinical decision-making.}
}
@article{IYER2024e32546,
title = {Inspiring a convergent engineering approach to measure and model the tissue microenvironment},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32546},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32546},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085773},
author = {Rishyashring R. Iyer and Catherine C. Applegate and Opeyemi H. Arogundade and Sushant Bangru and Ian C. Berg and Bashar Emon and Marilyn Porras-Gomez and Pei-Hsuan Hsieh and Yoon Jeong and Yongdeok Kim and Hailey J. Knox and Amir Ostadi Moghaddam and Carlos A. Renteria and Craig Richard and Ashlie Santaliz-Casiano and Sourya Sengupta and Jason Wang and Samantha G. Zambuto and Maria A. Zeballos and Marcia Pool and Rohit Bhargava and H. Rex Gaskins},
keywords = {Bioengineering, Interdisciplinary research, Bioimaging, Biomaterials, Biosensing, Computational biology, Biomedical devices, Biotechnology},
abstract = {Understanding the molecular and physical complexity of the tissue microenvironment (TiME) in the context of its spatiotemporal organization has remained an enduring challenge. Recent advances in engineering and data science are now promising the ability to study the structure, functions, and dynamics of the TiME in unprecedented detail; however, many advances still occur in silos that rarely integrate information to study the TiME in its full detail. This review provides an integrative overview of the engineering principles underlying chemical, optical, electrical, mechanical, and computational science to probe, sense, model, and fabricate the TiME. In individual sections, we first summarize the underlying principles, capabilities, and scope of emerging technologies, the breakthrough discoveries enabled by each technology and recent, promising innovations. We provide perspectives on the potential of these advances in answering critical questions about the TiME and its role in various disease and developmental processes. Finally, we present an integrative view that appreciates the major scientific and educational aspects in the study of the TiME.}
}
@article{ZHANG2024100667,
title = {Research on the application value of Multimedia-Based virtual reality technology in drama education activities},
journal = {Entertainment Computing},
volume = {50},
pages = {100667},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100667},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000351},
author = {Bingyu Zhang and Wenwen Jiang},
keywords = {Multimedia, Virtual reality, Academic technology, Drama education, and ANOVA},
abstract = {The research and moral use of academic technology focuses on developing, implementing, and overseeing the use of suitable technical resources and procedures to enhance learning and achievement. Multimedia has found its position in some form as an educational technology platform in the contemporary environment of academic universities. The use of virtual reality software as an intellectual tool and learning provider allows students to perform cognitive rehabilitation of preexisting information frameworks. People are paying more and more attention to how preschoolers' holistic skills develop as education reform progresses. Drama education is incorporated into the school curriculum to enhance young children's artistic, intellectual, and linguistic skills. Therefore, this study aims to examine the potential of multimedia-based virtual reality technology (MVRT) in drama education. The participants in this study were students from different universities in China. Students were exposed to multimedia-based virtual reality technology, and its efficacy was assessed using a statistical analytic approach called Analysis of variance (ANOVA). Drama understanding rate, educational improvement ratio, teaching quality rate, student achievement ratio, computation time, and parental support rate are among the performance metrics used to assess performance. Multimedia-based virtual reality technology (MVRT) for drama education showed outstanding success, with a 98% improvement ratio in educational outcomes and higher teaching quality. Students exhibited improved performance, supported by solid parental approval, demonstrating the effectiveness of MVRT in enhancing educational experiences.}
}
@article{MORABIA2020164,
title = {Pandemics and methodological developments in epidemiology history},
journal = {Journal of Clinical Epidemiology},
volume = {125},
pages = {164-169},
year = {2020},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620306454},
author = {Alfredo Morabia},
keywords = {Epidemiology, History, Pandemics, Covid-19, Plague, Cholera, Tuberculosis, Influenza, HIV/AIDS},
abstract = {The crisis spurred by the pandemic of COVID-19 has revealed weaknesses in our epidemiologic methodologic corpus, which scientists are struggling to compensate. This article explores whether this phenomenon is characteristic of pandemics or not. Since the emergence of population-based sciences in the 17th century, we can observe close temporal correlations between the plague and the discovery of population thinking, cholera and population-based group comparisons, tuberculosis and the formalization of cohort studies, the 1918 Great Influenza and the creation of an academic epidemiologic counterpart to the public health service, the HIV/AIDS epidemic, and the formalization of causal inference concepts. The COVID-19 pandemic seems to have promoted the widespread understanding of population thinking both with respect to ways of flattening an epidemic curve and the societal bases of health inequities. If the latter proves true, it will support my hypothesis that pandemics did accelerate profound changes in epidemiologic methods and concepts.}
}
@article{GROSBERG2009359,
title = {Computational models of heart pumping efficiencies based on contraction waves in spiral elastic bands},
journal = {Journal of Theoretical Biology},
volume = {257},
number = {3},
pages = {359-370},
year = {2009},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2008.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022519308006103},
author = {Anna Grosberg and Morteza Gharib},
keywords = {Cardiac modeling, Left ventricular twist, Myocardium macro-structure, Finite element simulations},
abstract = {We present a framework for modeling biological pumping organs based on coupled spiral elastic band geometries and active wave-propagating excitation mechanisms. Two pumping mechanisms are considered in detail by way of example: one of a simple tube, which represents a embryonic fish heart and another more complicated structure with the potential to model the adult human heart. Through finite element modeling different elastic contractions are induced in the band. For each version the pumping efficiency is measured and the dynamics are evaluated. We show that by combining helical shapes of muscle bands with a contraction wave it is possible not only to achieve efficient pumping, but also to create desired dynamics of the structure. As a result we match the function of the model pumps and their dynamics to physiological observations.}
}
@article{CANIZARES2024100020,
title = {Taming the Rhinoceros: A brief history of a ubiquitous tool},
journal = {Perspectives in Architecture and Urbanism},
volume = {1},
number = {2},
pages = {100020},
year = {2024},
issn = {2950-2675},
doi = {https://doi.org/10.1016/j.pau.2024.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2950267524000241},
author = {Galo Canizares},
keywords = {Software, History, Parametric design, Digital fabrication, Theory},
abstract = {At the turn of the millennium, architects and educators, propelled by the demise of critical theory, found a space to speculate about technology’s role in the future of architecture. As Michael Speaks wrote in 2002, “if philosophy was the intellectual dominant of early twentieth century vanguards and theory the intellectual dominant of late twentieth century vanguards, then intelligence has become the intellectual dominant of twenty-first century post-vanguards” (Speaks, 2010, p. 211). This emphasis on intelligence fostered a progressive narrative around the increasing reliance on software in design processes. This paper examines architectural practice during this period, with a specific focus on the rise of a new set of values, priorities, and factors that transformed architectural thinking and making. In contrast to existing accounts of digital design’s history, this paper places less importance on outputs and more on the shifts in modes of working and enacting design labor. More specifically, it narrows in on a software application that, as will be argued, drastically changed both cultural values and design knowledge: Rhinoceros. Beyond simply facilitating the production of geometrically intricate and complex architectural assemblies, Rhinoceros helped shape the discourse on parametric, computational, and algorithmic design, redefining the role of the designer as a creative technologist. In doing so, it also engendered a specific community of practice, which in turn produced its own culture and folklore. The spread of this software greatly contributed to the rise of two new kinds of architectural technologists: the “parametric designer” and the “digital fabricator,” two actors who would significantly impact how architecture was imagined and produced from the mid-2000s through the 2010s.}
}
@article{DUAN2021107596,
title = {Equidistant k-layer multi-granularity knowledge space},
journal = {Knowledge-Based Systems},
volume = {234},
pages = {107596},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107596},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008583},
author = {Jiangli Duan and Guoyin Wang and Xin Hu},
keywords = {Granular computing, Multi-granularity knowledge space, Knowledge space distance, Hierarchical quotient space},
abstract = {A multi-granularity knowledge space is a computational model that simulates human thinking and solves complex problems. However, as the amount of data increases, the multi-granularity knowledge space will have a larger number of layers, which will reduce its problem-solving ability. Therefore, we define a knowledge space distance measurement and propose two algorithms to select k representative layers from the multi-granularity knowledge space, where k is specified by the user according to the needs in problem solving, and k representative layers are approximately equidistant. First, we propose a knowledge space distance to measure the distance between any two layers in a multi-granularity knowledge space with superset-subset relationships, and the rationality of the knowledge space distance is verified by theory and experiment. Second, relying on the knowledge space distance and knowledge space distance variance, we propose two algorithms (i.e., a deterministic algorithm and a heuristic algorithm) to select an approximate equidistant k-layer multi-granularity knowledge space. Third, in addition to verifying the effectiveness of the knowledge space distance, the knowledge space distance variance, the deterministic algorithm and the heuristic algorithm, we verify that the equidistant k-layer multi-granularity knowledge space is more efficient than the original multi-granularity knowledge space.}
}
@article{LOPEZBRAU2023105524,
title = {People can use the placement of objects to infer communicative goals},
journal = {Cognition},
volume = {239},
pages = {105524},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105524},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001580},
author = {Michael Lopez-Brau and Julian Jara-Ettinger},
keywords = {Computational modeling, Social objects, Theory of Mind},
abstract = {Beyond words and gestures, people have a remarkable capacity to communicate indirectly through everyday objects: A hat on a chair can mean it is occupied, rope hanging across an entrance can mean we should not cross, and objects placed in a closed box can imply they are not ours to take. How do people generate and interpret the communicative meaning of objects? We hypothesized that this capacity is supported by social goal inference, where observers recover what social goal explains an object being placed in a particular location. To test this idea, we study a category of common ad-hoc communicative objects where a small cost is used to signal avoidance. Using computational modeling, we first show that goal inference from indirect physical evidence can give rise to the ability to use object placement to communicate. We then show that people from the U.S. and the Tsimane’—a farming-foraging group native to the Bolivian Amazon—can infer the communicative meaning of object placement in the absence of a pre-existing convention, and that people’s inferences are quantitatively predicted by our model. Finally, we show evidence that people can store and retrieve this meaning for use in subsequent encounters, revealing a potential mechanism for how ad-hoc communicative objects become quickly conventionalized. Our model helps shed light on how humans use their ability to interpret other people’s behavior to embed social meaning into the physical world.}
}
@article{FRITSCH2024100297,
title = {Teaching advanced topics in econometrics using introductory textbooks: The case of dynamic panel data methods},
journal = {International Review of Economics Education},
volume = {47},
pages = {100297},
year = {2024},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2024.100297},
url = {https://www.sciencedirect.com/science/article/pii/S147738802400015X},
author = {Markus Fritsch and Andrew Adrian Yu Pua and Joachim Schnurbus},
keywords = {Teaching econometrics, instrumental variables, linear dynamic panel data methods, cigarette demand, lagged variables},
abstract = {We show how to use the introductory econometrics textbook by Stock and Watson (2019) as a starting point for teaching and studying dynamic panel data methods. The materials are intended for undergraduate students taking their second econometrics course, undergraduate students in seminar-type courses, independent study courses, capstone, or thesis projects, and beginning graduate students in a research methods course. First, we distill the methodological core necessary to understand dynamic panel data methods. Second, we design an empirical and a theoretical case study to highlight the capabilities, downsides, and hazards of the method. The empirical case study is based on the cigarette demand example in Stock and Watson (2019) and illustrates that economic and methodological issues are interrelated. The theoretical case study shows how to evaluate current empirical practices from a theoretical standpoint. We designed both case studies to boost students’ confidence in working with technical material and to provide instructors with more opportunities to let students develop econometric thinking and to actively communicate with applied economists. Although we focus on Stock and Watson (2019) and the statistical software R, we also show how to modify the material for use with another introductory textbook by Wooldridge (2020) and Stata, and highlight some possible further pathways for instructors and students to reuse and extend our materials.}
}
@incollection{KOZA2002275,
title = {Chapter 10 - Genetic Programming: Biologically Inspired Computation That Exhibits Creativity in Producing Human-Competitive Results},
editor = {Peter J Bentley and David W. Corne},
booktitle = {Creative Evolutionary Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {275-298},
year = {2002},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-673-9},
doi = {https://doi.org/10.1016/B978-155860673-9/50048-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606739500483},
author = {John R. Koza and Forrest H. Bennett and David Andre and Martin A. Keane},
abstract = {Publisher Summary
One of the central challenges of computer science is to get a computer to solve a problem without programming it explicitly. The challenge is to create an automatic system whose input is a high-level statement of a problem's requirements and whose output is a satisfactory solution to the given problem. This challenge is the common goal of such fields of research as artificial intelligence and machine learning. Paraphrasing Arthur Samuel, this challenge addresses the question: How can computers are made to do what needs to be done, without being told exactly how to do it? As Samuel further explained: “The aim is to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.” This chapter provides an affirmative answer to the following two questions: Starting only with a high-level statement of the problem's requirements, can computers automatically discover the solution to nontrivial problems? And, can automatically created solutions be competitive with the products of human creativity and inventiveness? In answering these questions, this chapter focuses on a biologically inspired domain-independent problem-solving technique of evolutionary computation, called genetic programming. For each problem, genetic programming automatically creates entities that improve on previously patented inventions, or duplicate the functionality of previously patented inventions or duplicate the functionality of previously patented inventions. The chapter also discusses the importance of illogic in achieving creativity and inventiveness.}
}
@article{EBERBACH2007200,
title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
journal = {Theoretical Computer Science},
volume = {383},
number = {2},
pages = {200-243},
year = {2007},
note = {Complexity of Algorithms and Computations},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507003192},
author = {Eugene Eberbach},
keywords = {Problem solving, Process algebras, Anytime algorithms, SuperTuring models of computation, Bounded rational agents, $-calculus, Intractability, Undecidability, Completeness, Optimality, Search optimality, Total optimality},
abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.}
}
@article{BIBRI2018758,
title = {A foundational framework for smart sustainable city development: Theoretical, disciplinary, and discursive dimensions and their synergies},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {758-794},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717313069},
author = {Simon Elias Bibri},
keywords = {Smart sustainable cities, Theories, Academic disciplines, Academic discourses, Multidimensional framework, Interdisciplinarity and transdisciplinarity, Systems thinking, Complexity science, Sustainability, Computing and ICT},
abstract = {In the subject of smart sustainable cities, the underlying theories are a foundation for practice. Moreover, scholarly research in the field of smart sustainable cities operates out of the understanding that advances in the underlying knowledge necessitate pursuing multifaceted questions that can only be resolved from the vantage point of interdisciplinarity or transdisciplinarity. Indeed, research problems in this field are inherently too complex to be addressed by single disciplines. The PhD study addressing the topic of smart sustainable city development falls within the broad research field of sustainability transition and sustainability science where ICT is seen as a salient factor given its transformational, disruptive, and synergetic effects as an enabling, integrative, and constitutive technology. In light of this, the approach to the PhD study is of an applied theoretical kind, and its aim is to investigate and analyze how to advance and sustain the contribution of sustainable urban forms to the goals of sustainable development with support of ICT of pervasive computing. This is to primarily create a framework for strategic smart sustainable city development based on scientific principles, theories, and academic disciplines and discourses used to guide urban actors in their practice towards sustainability and analyze its impact. This involves the application of a set of integrative foundational elements drawn from urban planning, urban design, sustainability, sustainable development, sustainability science, data science, computer science, complexity science, systems theory, systems thinking, and ICT. Accordingly, it is deemed of high significance to devise a multidimensional framework consisting of relevant theories and academic disciplines and discourses that underpin the development of smart sustainable cities as a set of future practices. This framework in turn emphasizes the interdisciplinary and transdisciplinary nature and orientation of the topic of smart sustainable cities and thus the relevance of pursuing an interdisciplinary and transdisciplinary approach into studying this topic. Therefore, this paper endeavors to systematize the very complex and dense scientific area of smart sustainable cities in terms of identifying, distilling, and structuring the core dimensions of a foundational framework for smart sustainable city development as a set of future practices. In doing so, it focuses on a number of fundamental theories along with academic disciplines and discourses, with the aim of setting a framework that analytically relates city development, sustainability, and ICT, while emphasizing how and to what extent sustainability and ICT have particularly become influential in city development in modern society. In addition, this paper offers an in–depth interdisciplinary and transdisciplinary discussion covering topics of high relevance to the PhD study and at the heart of the very synergic relationship between the theoretical, disciplinary, and discursive dimensions of the foundational framework underpinning smart sustainable city development. These dimensions thus form the basis for the framework for strategic smart sustainable city development that is under investigation and will be developed based on a backcasting approach to strategic planning. This study provides an important lens through which to understand a set of influential theories and established academic disciplines and discourses with high potential for integration, fusion, and practicality in relation to the practice of smart sustainable city development.}
}
@article{ALVARADO20043,
title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
journal = {Expert Systems with Applications},
volume = {26},
number = {1},
pages = {3-8},
year = {2004},
note = {Intelligent Computing in the Petroleum Industry, ICPI-02},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403001039},
author = {Matı&#x0301;as Alvarado and Leonid Cheremetov and Francisco Cantú}
}
@incollection{RZESZEWSKI2024219,
title = {Chapter 10 - Augmented reality content and relations of power in smart spaces},
editor = {Zhihan Lyu},
booktitle = {Smart Spaces},
publisher = {Academic Press},
pages = {219-234},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-13462-3},
doi = {https://doi.org/10.1016/B978-0-443-13462-3.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313462300008X},
author = {Michal Rzeszewski and Leighton Evans},
keywords = {Augmented reality, Smart space, Agency of place, Power relations},
abstract = {This chapter’s main aim is to interrogate how augmented reality (AR) content can change the relations of power within a place and transform the perception of place from being a material background for social interactions into a living agent that can have its own agency. We use the concept of sociotechnical imaginary and thematic analysis of AR-related media content to explore main narrations in the current discourse on AR and smart urban spaces. We identify two dominant themes: “smart information in place” and “subversion of meaning” that combine two ways of thinking about relation between place and AR. In the first one, AR acts not as an augmentation of place, but as a reducer of the experience of place down to seeing the world as information and data for the achievement of efficiencies in late capitalism. In the second one, AR can be seen as transcending the traditional constellations of power relations that shape places and spaces of modern cities, skewing them toward nonhuman actors, of which AR may be the most visible and influential. Through the visible differentiation of users, alteration of perception and the forcing of presence and behavior, AR is a technology that makes visible the systems and processes of control and mediation of space and place. As such, the smart space itself will become visible through the presence, use, and functioning of AR in a manner that has not been the case previously. We posit, therefore, that AR can be seen as a physical manifestation of the agency of place. This position can have consequences for the practical development of smart spaces and for theoretical consideration of the human-technology interaction in urban space in its material and digital dimensions.}
}
@article{LOCK2023102310,
title = {Conserving complexity: A complex systems paradigm and framework to study public relations’ contribution to grand challenges},
journal = {Public Relations Review},
volume = {49},
number = {2},
pages = {102310},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123000255},
author = {Irina Lock},
keywords = {Grand challenges, Public issues, Public relations, Strategic communication, Complex adaptive systems, Digital communication, Complexity},
abstract = {Sustainable development poses a grand challenge for society, addressed by organisations through their public relations activities. Grand challenges are complex by nature and call for nontrivial solutions whose effects show at the level of society. That is why studying public relations’ contribution to grand challenges requires a macro perspective that accounts for the dynamic interaction between individual, organisational, and system levels in a digital communication environment. This paper offers a new paradigm to analyse organisations’ significant and at times undue impact on grand challenges through public relations. It develops a framework inspired by complex adaptive systems thinking and adopts its ten properties for public relations: emergence, adaptivity, heterogeneous actors, nonlinear effects, feedback mechanisms, self-organisation, phase transitions, networks, scaling, and cooperation. The paper applies the framework to the example of sustainable development. It shows why research on grand challenges requires a holistic perspective and how it can help study digitally born communication phenomena. The proposed complex systems paradigm provides space for critical, social scientific, and interpretative research lines in public relations. Inquiries start from the grand challenge and study the communicative interactions between organisations and other actors from existing theory while accounting for the ten properties of complex adaptive systems. The paper outlines how future research can enrich the study of public relations and discusses its limits.}
}
@article{SARTON195551,
title = {The astral religion of antiquity and the “thinking machines” of to-day},
journal = {Vistas in Astronomy},
volume = {1},
pages = {51-60},
year = {1955},
issn = {0083-6656},
doi = {https://doi.org/10.1016/0083-6656(55)90012-X},
url = {https://www.sciencedirect.com/science/article/pii/008366565590012X},
author = {George Sarton}
}
@article{JU2022101062,
title = {Proposal for a STEAM education program for creativity exploring the roofline of a hanok using GeoGebra and 4Dframe},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101062},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101062},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000657},
author = {Hyunshik Ju and Hogul Park and Eun Young Jung and Seoung-Hey Paik},
keywords = {STEAM education program, Creativity, Catenary curve, Korean traditional architecture, Modelling, GeoGebra, 4Dframe},
abstract = {This research was conducted to confirm the feasibility of a STEAM education program in which the mathematics, physics, and Korean traditional arts underlying the hanok roofline are investigated using educational tools of GeoGebra and 4Dframe. This paper contends that this program has the potential to engage students in knowledge restructuring regarding the perception of the hanok’s architectural beauty, Newton's concept of gravity, and mathematical functions. The Octagonal Pavilion in Tapgol Park in Seoul, South Korea, a representative hanok, was used as an educational resource. GeoGebra is used to determine that the roofline of the Octagonal Pavilion generally follows the formula of the catenary curve and then the roofline is modelled using 4Dframe. The catenary form of the roofline of the hanok is linked to the Korean sense of beauty in the pursuit of naturalness under the influence of gravity and organically harmonizes with the environment. The class described in this study, in which the curve of the roofline of the Octagonal Pavilion is explored using GeoGebra and 4Dframe, can help develop creative and critical thinking in students in the context of STEAM education. The findings of this study have the potential to expand the scope of STEAM education to include content for creative education.}
}
@article{PAPADOPOULOS2019210,
title = {Using mobile puzzles to exhibit certain algebraic habits of mind and demonstrate symbol-sense in primary school students},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {210-227},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231730189X},
author = {Ioannis Papadopoulos},
keywords = {Algebraic habits of mind, mobile puzzles, Symbol sense},
abstract = {Given the growing concern for developing students’ algebraic ideas and thinking in earlier grades (NCTM, 2000) it is important for students to have experiences that better prepare them for their formal introduction to algebra. Mobile puzzles seem to be an opportunity for exhibiting certain algebraic habits of mind as well as for demonstrating symbol-sense which might support students in their transition from arithmetic to algebra. These puzzles include multiple balanced collections of objects whose weights must be determined by the solver. The arms/beams must be perfectly balanced for it to hang properly. Therefore, they represent, in a pictorial way, systems of equations. Each arm/beam that balances two sets of objects (representing variables as unknown “weights”) represents an equation. The data derived from Grade-6 students who were asked to solve a collection of tasks reflect the presence of the “Puzzling and Persevering” and “Seeking and Using Structure” habits of mind. At the same time these data incorporate instances of some main components of symbol-sense such as “friendliness with symbols”, “manipulating and ‘reading through’ symbolic expressions”, and “choice of symbols”. Also discussed is the way this experience contributes to an intuitive application of the conventional rules for solving equations that will be later introduced to the students as the standard algebraic “moves”.}
}
@article{LEBARON2000679,
title = {Agent-based computational finance: Suggested readings and early research},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {5},
pages = {679-702},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00022-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000226},
author = {Blake LeBaron},
keywords = {Agents, Heterogeneous information, Simulated markets, Learning, Evolution},
abstract = {The use of computer simulated markets with individual adaptive agents in finance is a new, but growing field. This paper explores some of the early works in the area concentrating on a set of some of the earliest papers. Six papers are summarized in detail, along with references to many other pieces of this wide ranging research area. It also covers many of the questions that new researchers will face when getting into the field, and hopefully can serve as a kind of minitutorial for those interested in getting started.}
}
@incollection{KAMAREDDINE2012801,
title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
editor = {Dov M. Gabbay and Akihiro Kanamori and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {6},
pages = {801-845},
year = {2012},
booktitle = {Sets and Extensions in the Twentieth Century},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-51621-3.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444516213500116},
author = {Fairouz Kamareddine and Twan Laan and Robert Constable}
}
@incollection{CORMACK2005325,
title = {4.1 - Computational Models of Early Human Vision},
editor = {AL BOVIK},
booktitle = {Handbook of Image and Video Processing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Burlington},
pages = {325-IX},
year = {2005},
series = {Communications, Networking and Multimedia},
isbn = {978-0-12-119792-6},
doi = {https://doi.org/10.1016/B978-012119792-6/50083-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780121197926500838},
author = {Lawrence K. Cormack}
}
@article{KESIC2024101072,
title = {Complexity and biocomplexity: Overview of some historical aspects and philosophical basis},
journal = {Ecological Complexity},
volume = {57},
pages = {101072},
year = {2024},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2023.101072},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X23000442},
author = {Srdjan Kesić},
keywords = {Cybernetics, General systems theory, Complexity, Modeling, Biocomplexity, Emergence, Autopoiesis},
abstract = {Complexity has radically changed human understanding of the world environment and continues challenging our best scientific theories. In a rapidly changing research landscape, historical and philosophical insights into Complexity can heighten awareness of the proper theoretical perspectives scientists should adopt to advance the study of biocomplexity, including ecological complexity. The present work aims to deepen this awareness and disclose how researchers should generally approach, scientifically and philosophically, the question of what Complexity is, which is of great importance not only to the scientific community but also far beyond. First, this article reviews some critical historical turning points that led to Complexity. Second, the paper discusses philosophical-scientific approaches to the emergence as one of the most critical features of complex systems. The critical ideas behind attempts to understand the generators of complexity in nature are then presented, focusing on the living world. Finally, the review focuses on understanding the ecosystem- and organism-oriented perspectives of biocomplexity. We conclude that the genuine problem of the origin of complexity theory and biocomplexity will continue to inspire generations of researchers to search for new, more comprehensive mathematical and computational frameworks to explain biological hierarchies in order to further advance the scientific understanding of life.}
}
@article{WANG201854,
title = {Studying cognitive development in cultural context: A multi-level analysis approach},
journal = {Developmental Review},
volume = {50},
pages = {54-64},
year = {2018},
note = {Towards a Cultural Developmental Science},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0273229717301041},
author = {Qi Wang},
keywords = {Culture, Cognition, Episodic thinking, Memory, Multiple levels of analysis},
abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.}
}
@article{PEARSON1994203,
title = {Report on University of Wales Institute of non-Newtonian Fluid Mechanics Mini-Symposium on “Continuum and Microstructural Modelling in Computational Rheology” Seiont Manor, Gwynedd, 11–12 April 1994},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {55},
number = {2},
pages = {203-205},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)80006-5},
url = {https://www.sciencedirect.com/science/article/pii/0377025794800065},
author = {J.R.A. Pearson}
}
@article{CAO2024102160,
title = {Self-assembly of peptides: The acceleration by molecular dynamics simulations and machine learning},
journal = {Nano Today},
volume = {55},
pages = {102160},
year = {2024},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2024.102160},
url = {https://www.sciencedirect.com/science/article/pii/S174801322400015X},
author = {Nana Cao and Kang Huang and Jianjun Xie and Hui Wang and Xinghua Shi},
keywords = {Peptides, Self-assembly, Molecular dynamics, Machine learning},
abstract = {Peptides, biopolymeric compounds connected by peptide bonds, have garnered significant attention in recent years as their potential wide applications in fields such as drug delivery, tissue engineering, and antibiotics. Peptides exhibit excellent biocompatibility and stability due to their structural similarities to many bioactive substances found in human bodies. The self-assembly of peptides has piqued considerable interest with groundbreaking advancements achieved in experimental research. However, it is still a big challenge to establish comprehensive theoretical model to accurately describe the behavior of peptide self-assembly. Current peptide self-assembly designs primarily rely on experimental outcomes and general rules, which is inefficient and susceptible to human errors. In recent years, thanks to rapid advancements in computer techniques and theoretical methods, computational research has become a vital tool in complementing experimental research with rapid development witted in this field. This review delves into the description of peptide self-assembly, covering relevant sequences, structures, morphologies, rules, and application areas. It places particular emphasis on the recent progress in computational methods such as molecular dynamics (MD) simulations and machine learning (ML) techniques in the study. Finally, we provide a perspective on the application of computational methods to expedite exploration in the realm of multi-peptide self-assembly.}
}
@article{FUXJAGER2023105340,
title = {Systems biology as a framework to understand the physiological and endocrine bases of behavior and its evolution—From concepts to a case study in birds},
journal = {Hormones and Behavior},
volume = {151},
pages = {105340},
year = {2023},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2023.105340},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X23000387},
author = {Matthew J. Fuxjager and T. Brandt Ryder and Nicole M. Moody and Camilo Alfonso and Christopher N. Balakrishnan and Julia Barske and Mariane Bosholn and W. Alice Boyle and Edward L. Braun and Ioana Chiver and Roslyn Dakin and Lainy B. Day and Robert Driver and Leonida Fusani and Brent M. Horton and Rebecca T. Kimball and Sara Lipshutz and Claudio V. Mello and Eliot T. Miller and Michael S. Webster and Morgan Wirthlin and Roy Wollman and Ignacio T. Moore and Barney A. Schlinger},
keywords = {Systems biology, Animal behavior, Organismal physiology, Adaptive evolution, Manakin birds, Androgenic hormones, Robustness},
abstract = {Organismal behavior, with its tremendous complexity and diversity, is generated by numerous physiological systems acting in coordination. Understanding how these systems evolve to support differences in behavior within and among species is a longstanding goal in biology that has captured the imagination of researchers who work on a multitude of taxa, including humans. Of particular importance are the physiological determinants of behavioral evolution, which are sometimes overlooked because we lack a robust conceptual framework to study mechanisms underlying adaptation and diversification of behavior. Here, we discuss a framework for such an analysis that applies a “systems view” to our understanding of behavioral control. This approach involves linking separate models that consider behavior and physiology as their own networks into a singular vertically integrated behavioral control system. In doing so, hormones commonly stand out as the links, or edges, among nodes within this system. To ground our discussion, we focus on studies of manakins (Pipridae), a family of Neotropical birds. These species have numerous physiological and endocrine specializations that support their elaborate reproductive displays. As a result, manakins provide a useful example to help imagine and visualize the way systems concepts can inform our appreciation of behavioral evolution. In particular, manakins help clarify how connectedness among physiological systems—which is maintained through endocrine signaling—potentiate and/or constrain the evolution of complex behavior to yield behavioral differences across taxa. Ultimately, we hope this review will continue to stimulate thought, discussion, and the emergence of research focused on integrated phenotypes in behavioral ecology and endocrinology.}
}
@article{NOH2006554,
title = {Computational tools for isotopically instationary 13C labeling experiments under metabolic steady state conditions},
journal = {Metabolic Engineering},
volume = {8},
number = {6},
pages = {554-577},
year = {2006},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2006.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717606000449},
author = {Katharina Nöh and Aljoscha Wahl and Wolfgang Wiechert},
keywords = {Instationary C metabolic flux analysis, C labeling experiment, C labeling dynamics, Parameter identifiability, Optimal experimental design, },
abstract = {13C metabolic flux analysis (MFA) has become an important and powerful tool for the quantitative analysis of metabolic networks in the framework of metabolic engineering. Isotopically instationary 13C MFA under metabolic stationary conditions is a promising refinement of classical stationary MFA. It accounts for the experimental requirements of non-steady-state cultures as well as for the shortening of the experimental duration. This contribution extends all computational methods developed for classical stationary 13C MFA to the instationary situation by using high-performance computing methods. The developed tools allow for the simulation of instationary carbon labeling experiments (CLEs), sensitivity calculation with respect to unknown parameters, fitting of the model to the measured data, statistical identifiability analysis and an optimal experimental design facility. To explore the potential of the new approach all these tools are applied to the central metabolism of Escherichia coli. The achieved results are compared to the outcome of the stationary counterpart, especially focusing on statistical properties. This demonstrates the specific strengths of the instationary method. A new ranking method is proposed making both an a priori and an a posteriori design of the sampling times available. It will be shown that although still not all fluxes are identifiable, the quality of flux estimates can be strongly improved in the instationary case. Moreover, statements about the size of some immeasurable pool sizes can be made.}
}
@article{MASOUD201293,
title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {90},
pages = {93-108},
year = {2012},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2012.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1386142512000418},
author = {Mamdouh S. Masoud and Alaa E. Ali and Medhat A. Shaker and Gehan S. Elasala},
keywords = {Uric, Complexes, Synthesis, Spectroscopy, Thermal analysis, Computational},
abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.}
}
@article{VANDENHURK2023106030,
title = {Consideration of compound drivers and impacts in the disaster risk reduction cycle},
journal = {iScience},
volume = {26},
number = {3},
pages = {106030},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106030},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223001074},
author = {Bart J.J.M. {van den Hurk} and Christopher J. White and Alexandre M. Ramos and Philip J. Ward and Olivia Martius and Indiana Olbert and Kathryn Roscoe and Henrique M.D. Goulart and Jakob Zscheischler},
keywords = {Earth sciences, Social sciences, Decision science},
abstract = {Summary
Consideration of compound drivers and impacts are often missing from applications within the Disaster Risk Reduction (DRR) cycle, leading to poorer understanding of risk and benefits of actions. The need to include compound considerations is known, but lack of guidance is prohibiting practitioners from including these considerations. This article makes a step toward practitioner guidance by providing examples where consideration of compound drivers, hazards, and impacts may affect different application domains within disaster risk management. We discern five DRR categories and provide illustrative examples of studies that highlight the role of “compound thinking” in early warning, emergency response, infrastructure management, long-term planning, and capacity building. We conclude with a number of common elements that may contribute to the development of practical guidelines to develop appropriate applications for risk management.}
}
@article{NISHI2022314,
title = {Health and landscape approaches: A comparative review of integrated approaches to health and landscape management},
journal = {Environmental Science & Policy},
volume = {136},
pages = {314-325},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122002027},
author = {Maiko Nishi and Shizuka Hashimoto},
keywords = {Landscape approaches, One Health, Ecohealth, Planetary Health, Social-ecological systems, Sustainability transformation},
abstract = {Landscape approaches are integrated place-based approaches and provide cross-sectoral opportunities to facilitate sustainability transformations. The COVID-19 outbreak has profound ramifications for multiple dimensions of landscapes, ranging from mobility and lifestyle to value to environment and society. Therefore, integrated approaches to “health” have been more vigorously promoted in the policy arena dealing with human–nature interactions. The ecosystem principles of the Convention on Biological Diversity, which resonate with landscape approaches, are generally aligned with integrated approaches to health. However, commonalities and distinctions between these integrated approaches in both political and scientific domains have not been clarified. Drawing on a narrative review of the literature on “One Health,” “Ecohealth,” and “Planetary Health” as major health-oriented approaches in comparison with landscape approaches, the aspects of landscape approaches to be complemented in addressing health-related challenges were examined in this study. In addition to the review on the intellectual roots and evolutionary pathways, a comparative analysis of these relevant approaches was conducted in terms of three realms including theoretical assumptions, knowledge bases, and research paradigms. The results of the comparative review show that all approaches share systems thinking, interdisciplinarity, cross-sectoral collaboration, and holistic paradigm but differ with respect to their focused management problems, disciplines, and sectors as well as ontological and epistemological underpinnings. Pointing to the recent theoretical and methodological development in integrating health in placemaking, the results of this study suggest that pragmatic landscape approaches could be strengthened by using health-related research paradigms to achieve better constructivism–positivism meeting grounds regarding health–landscape intersections.}
}
@article{OLIVER2014289,
title = {Crack-path field and strain-injection techniques in computational modeling of propagating material failure},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {274},
pages = {289-348},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514000139},
author = {J. Oliver and I.F. Dias and A.E. Huespe},
keywords = {Fracture, Computational material failure, Strong discontinuities, Crack-path field, Strain injection, Finite elements with embedded discontinuities},
abstract = {The work presents two new numerical techniques devised for modeling propagating material failure, i.e. cracks in fracture mechanics or slip-lines in soil mechanics. The first one is termed crack-path-field technique and is conceived for the identification of the path of those cracks, or slip-lines, represented by strain-localization based solutions of the material failure problem. The second one is termed strain-injection, and consists of a procedure to insert, during specific stages of the simulation and in selected areas of the domain of analysis, goal oriented specific strain fields via mixed finite element formulations. In the approach, a first injection, of elemental constant strain modes (CSM) in quadrilaterals, is used, in combination of the crack-path-field technique, for obtaining reliable information that anticipates the position of the crack-path. Based on this information, in a subsequent stage, a discontinuous displacement mode (DDM) is efficiently injected, ensuring the required continuity of the crack-path across sides of contiguous elements. Combination of both techniques results in an efficient and robust procedure based on the staggered resolution of the crack-path-field and the mechanical failure problems. It provides the classical advantages of the “intra-elemental” methods for capturing complex propagating displacement discontinuities in coarse meshes, as E-FEM or X-FEM methods, with the non-code-invasive character of the crack-path-field technique. Numerical representative simulations of a wide range of benchmarks, in terms of the type of material and the failure problem, show the broad applicability, accuracy and robustness of the proposed methodology. The finite element code used for the simulations is open-source and available at http://www.cimne.com/compdesmat/.}
}
@article{VALLESPERIS2024102448,
title = {Digital citizenship at school: Democracy, pragmatism and RRI},
journal = {Technology in Society},
volume = {76},
pages = {102448},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102448},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002531},
author = {Núria Vallès-Peris and Miquel Domènech},
keywords = {Science and technology studies, Digital citizenship, Responsible research and innovation, Democracy, Pragmatism},
abstract = {This paper presents a strategy for fostering digital citizenship at school that transcends the mere use of digital devices or instructional methods focused solely on their use. The core premise of this proposal rests on the need for an ethical-political debate concerning digitization in education. In addition, it emphasizes the need to cultivate a form of digital literacy that blends science and technology with the humanities, and erases the traditional boundaries between making and thinking. The proposed approach encapsulates two primary concerns: firstly, it asserts that digital literacy serves as a foundation for meaningful participation in digital societies; secondly, it underscores the importance of democratizing digital technologies by incorporating the perspectives, needs, and concerns of children. Drawing inspiration from the theories of pragmatism and responsible research and innovation (RRI), we present a conceptual framework for digital citizenship. To operationalize this approach, we adapt John Dewey's pragmatic model of inquiry as a method that can be applied within the school setting. This pragmatic methodology serves as a conduit for developing hands-on experience geared towards developing digital citizenship. The practical implementation of this methodology is illustrated through an actualized experience with 10- and 11-year-old children in a public primary school, regarding the issue of care robots. This paper advocates for a symbiotic relationship between theoretical understanding and practical application, and puts forward a concrete proposal for the integration of digital citizenship in schools in the form of a four-phase procedural model, based on the creation of what we term ‘the encounter’ between the educational community and the research and development community.}
}
@incollection{SHI2021117,
title = {Chapter 4 - Mind model},
editor = {Zhongzhi Shi},
booktitle = {Intelligence Science},
publisher = {Elsevier},
pages = {117-149},
year = {2021},
isbn = {978-0-323-85380-4},
doi = {https://doi.org/10.1016/B978-0-323-85380-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385380400004X},
author = {Zhongzhi Shi},
keywords = {Mind model, Turing machine, physical symbol system, SOAR model, ACT-R model, CAM model, cognitive cycle, PMJ model},
abstract = {The technology of building mind model is often called mind modeling, which aims to explore and study the human thinking mechanism.}
}
@article{INTRONE201479,
title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
journal = {Decision Support Systems},
volume = {64},
pages = {79-89},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001262},
author = {Joshua Introne and Luca Iandoli},
keywords = {Computer-supported argumentation, Evidence-based reasoning, Dempster–Shafer belief aggregation, Housing market prediction},
abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.}
}
@article{SPINU2022100205,
title = {A matter of trust: Learning lessons about causality will make qAOPs credible},
journal = {Computational Toxicology},
volume = {21},
pages = {100205},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000517},
author = {Nicoleta Spînu and Mark T.D. Cronin and Judith C. Madden and Andrew P. Worth},
keywords = {Model credibility, Adverse Outcome Pathway, qAOP, Causality, Next Generation Risk Assessment},
abstract = {Toxicology in the 21st Century has seen a shift from chemical risk assessment based on traditional animal tests, identifying apical endpoints and doses that are “safe”, to the prospect of Next Generation Risk Assessment based on non-animal methods. Increasingly, large and high throughput in vitro datasets are being generated and exploited to develop computational models. This is accompanied by an increased use of machine learning approaches in the model building process. A potential problem, however, is that such models, while robust and predictive, may still lack credibility from the perspective of the end-user. In this commentary, we argue that the science of causal inference and reasoning, as proposed by Judea Pearl, will facilitate the development, use and acceptance of quantitative AOP models. Our hope is that by importing established concepts of causality from outside the field of toxicology, we can be “constructively disruptive” to the current toxicological paradigm, using the “Causal Revolution” to bring about a “Toxicological Revolution” more rapidly.}
}
@article{SUJAN2023105994,
title = {Operationalising FRAM in Healthcare: A critical reflection on practice},
journal = {Safety Science},
volume = {158},
pages = {105994},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105994},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522003332},
author = {M. Sujan and L. Pickup and M.S. {de Vos} and R. Patriarca and L. Konwinski and A. Ross and P. McCulloch},
keywords = {Patient Safety, FRAM, Resilience Engineering, System Safety},
abstract = {Resilience Engineering principles are becoming increasingly popular in healthcare to improve patient safety. FRAM is the best-known Resilience Engineering method with several examples of its application in healthcare available. However, the guidance on how to apply FRAM leaves gaps, and this can be a potential barrier to its adoption and potentially lead to misuse and disappointing results. The article provides a self-reflective analysis of FRAM use cases to provide further methodological guidance for successful application of FRAM to improve patient safety. Five FRAM use cases in a range of healthcare settings are described in a structured way including critical reflection by the original authors of those studies. Individual reflections are synthesised through group discussion to identify lessons for the operationalisation of FRAM in healthcare. Four themes are developed: (1) core characteristics of a FRAM study, (2) flexibility regarding the underlying epistemological paradigm, (3) diversity with respect to the development of interventions, and (4) model complexity. FRAM is a systems analysis method that offers considerable flexibility to accommodate different epistemological positions, ranging from realism to phenomenology. We refer to these as computational FRAM and reflexive FRAM, respectively. Practitioners need to be clear about their analysis aims and their analysis position. Further guidance is needed to support practitioners to tell a convincing and meaningful “system story” through the lens of FRAM.}
}
@article{CAMPAGNOLO2007387,
title = {The Methods of Approximation and Lifting in Real Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {167},
pages = {387-423},
year = {2007},
note = {Proceedings of the Third International Conference on Computability and Complexity in Analysis (CCA 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000229},
author = {Manuel L. Campagnolo and Kerry Ojakian},
keywords = {Computable Analysis, Real Recursive Functions, Elementary Computable},
abstract = {The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [M.L. Campagnolo, C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002) 977–100]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [O. Bournez and E. Hainry, Elementarily computable functions over the real numbers and R-sub-recursive functions, Theoretical Computer Science 348 (2005) 130–147], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.}
}
@article{YANG2001167,
title = {Computational verb systems: computing with perceptions of dynamics},
journal = {Information Sciences},
volume = {134},
number = {1},
pages = {167-248},
year = {2001},
note = {Computing with Words},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(01)00096-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020025501000962},
author = {Tao Yang},
abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.}
}
@article{DEBRIGARD2021104574,
title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
journal = {Cognition},
volume = {209},
pages = {104574},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303930},
author = {Felipe {De Brigard} and Paul Henne and Matthew L. Stanley},
keywords = {Imagination, Counterfactual thinking, Plausibility, Similarity, Possible worlds},
abstract = {People frequently entertain counterfactual thoughts, or mental simulations about alternative ways the world could have been. But the perceived plausibility of those counterfactual thoughts varies widely. The current article interfaces research in the philosophy and semantics of counterfactual statements with the psychology of mental simulations, and it explores the role of perceived similarity in judgments of counterfactual plausibility. We report results from seven studies (N = 6405) jointly supporting three interconnected claims. First, the perceived plausibility of a counterfactual event is predicted by the perceived similarity between the possible world in which the imagined situation is thought to occur and the actual world. Second, when people attend to differences between imagined possible worlds and the actual world, they think of the imagined possible worlds as less similar to the actual world and tend to judge counterfactuals in such worlds as less plausible. Lastly, when people attend to what is identical between imagined possible worlds and the actual world, they think of the imagined possible worlds as more similar to the actual world and tend to judge counterfactuals in such worlds as more plausible. We discuss these results in light of philosophical, semantic, and psychological theories of counterfactual thinking.}
}
@article{LEE2005261,
title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
journal = {International Journal of Mass Spectrometry},
volume = {240},
number = {3},
pages = {261-272},
year = {2005},
note = {Mass Spectrometry of Biopolymers: From Model Systems to Ribosomes},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2004.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1387380604003975},
author = {Jeehiun K. Lee},
keywords = {Nucleic acids, RNA, DNA, Enzyme nucleobase, Acidity, Proton affinity},
abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.}
}
@article{ZHAO2023100891,
title = {Meet the authors: Yuxuan Zhao, Enmeng Lu, and Yi Zeng},
journal = {Patterns},
volume = {4},
number = {12},
pages = {100891},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100891},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002933},
author = {Yuxuan Zhao and Enmeng Lu and Yi Zeng},
abstract = {Yuxuan Zhao, associate professor, Enmeng Lu, research engineer, and Yi Zeng, professor and lab director, have proposed a brain-inspired bodily self-perception model based on biological findings on monkeys and humans. This model can reproduce various rubber hand illusion (RHI) experiments, which helps reveal the RHI’s computational and biological mechanisms. They talk about their view of data science and research plans for brain-inspired robot self-modeling and ethical robots.}
}
@article{JIANG2024102530,
title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102530},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102530},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001782},
author = {Shaofei Jiang and Jingwei Yang and Jing Xie and Xuesong Xu and Yubo Dou and Liting Jing},
keywords = {Product innovation design, Patent text, Knowledge graph, RFSB ontology model, Implicit relationship completion},
abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.}
}
@article{HUBERMAN19981169,
title = {Computation as economics},
journal = {Journal of Economic Dynamics and Control},
volume = {22},
number = {8},
pages = {1169-1186},
year = {1998},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(98)00008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188998000086},
author = {Bernardo A. Huberman},
abstract = {We use computers to study economics, but few people realize that we can use economics to study and design computational systems. The reason is that computer networks can be regarded as a community of processes that in their interactions, strategies and lack of perfect knowledge face the same issues as people in markets. This paper describes how computers have evolved to a point where economics approaches are useful for designing them and understanding their dynamics. Examples are given of existing computer systems that use market mechanisms and of novel phenomena, such as clustered volatility, that we uncovered when studying their evolution.}
}
@article{ATREIDES202135,
title = {E-governance with ethical living democracy},
journal = {Procedia Computer Science},
volume = {190},
pages = {35-39},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012461},
author = {Kyrtin Atreides},
keywords = {mASI, AGI, e-governance, mediated artificial superintelligence, collective superintelligence, direct digital democracy, liquid democracy, EAP, Effective Altruistic Principles, Ethical Living Democracy, ELD},
abstract = {A new form of e-governance is proposed based on systems seen in biological life at all scales. This model of e-governance offers the performance of collective superintelligence, equally high ethical quality, and a substantial reduction in resource requirements for government functions. In addition, the problems seen in modern forms of government such as misrepresentation, corruption, lack of expertise, short-term thinking, political squabbling, and popularity contests may be rendered virtually obsolete by this approach. Lastly, this model of government generates a digital ecosystem of intelligent life which mirrors physical citizens, serving to bridge the emotional divide between physical and digital life, while also producing the first form of government able to keep pace with accelerating technological progress.}
}
@article{MORGAN20052564,
title = {The visual computation of 2-D area by human observers},
journal = {Vision Research},
volume = {45},
number = {19},
pages = {2564-2570},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905002075},
author = {M.J. Morgan},
keywords = {Psychophysics, Shape, Weber fraction},
abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.}
}
@article{MALLETT20241105,
title = {New strategies for the cognitive science of dreaming},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {12},
pages = {1105-1117},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400264X},
author = {Remington Mallett and Karen R. Konkoly and Tore Nielsen and Michelle Carr and Ken A. Paller},
keywords = {sleep, dreams, memory, neuroscience, natural language processing},
abstract = {Dreams have long captivated human curiosity, but empirical research in this area has faced significant methodological challenges. Recent interdisciplinary advances have now opened up new opportunities for studying dreams. This review synthesizes these advances into three methodological frameworks and describes how they overcome historical barriers in dream research. First, with observable dreaming, neural decoding and real-time reporting offer more direct measures of dream content. Second, with dream engineering, targeted stimulation and lucidity provide routes to experimentally manipulate dream content. Third, with computational dream analysis, the generation and exploration of large dream-report databases offer powerful avenues to identify patterns in dream content. By enabling researchers to systematically observe, engineer, and analyze dreams, these innovations herald a new era in dream science.}
}
@article{OLIVEIRA2022102347,
title = {Beyond energy services: A multidimensional and cross-disciplinary agenda for home energy management research},
journal = {Energy Research & Social Science},
volume = {85},
pages = {102347},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004382},
author = {Sonja Oliveira and Lidia Badarnah and Merate Barakat and Anna Chatzimichali and Ed Atkins},
keywords = {Architecture, Biomimetics, Computational design, Cross-disciplinary methods, Home energy management},
abstract = {Home Energy Management (HEM) has a significantly growing impact on strategic energy policy, digital equity, as well as housing development and transport issues. With the proliferation of home working, reliance on electricity for heating and cooling and the increasing needs for electric charging for transportation, there is an urgent need to develop novel ways for efficient management of home energy use. Current efforts focus on HEM technologies at individual household levels, without considering the social or spatial context or their collective community-wide interrelated dependencies. We propose a multifaceted agenda at the intersection of disciplinary domains to tackle this problem by using a multidimensional lens that draws on energy behaviour, architectural research, biomimetics, and computational design, simultaneously. Optimal and effective behavioural patterns can be extracted and abstracted from nature, informing a more collective and interrelated behavioural dependencies approach that considers the complex multidimensional energy use patterns of different housing typologies. This paper discusses the analytical benefits of this new research approach through a study of home energy management behaviour. The approach though could be expanded to consider other similar empirical contexts whereby sustainable multidimensional resource management is sought such as water use, food distribution as well as transport and mobility.}
}
@article{CAGLAYAN2015131,
title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {131-153},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315000504},
author = {Günhan Caglayan},
keywords = {Undergraduate mathematics education, Dynamic geometry software, Visualization, Representation, Connection, Linear algebra, Eigenvectors, Eigenvalues, Matrices},
abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.}
}
@incollection{GRIFFIN2020303,
title = {Information Graphics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-314},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10563-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105633},
author = {Amy L. Griffin},
keywords = {Big data, Cognition, Exploratory data analysis, Geovisualization, Graphics, Information dashboard, Information visualization, Interactivity, Perception, Semiotics, Storytelling, Visual analytics},
abstract = {Information graphics include a wide variety of static and dynamic visual representations of information such as diagrams, statistical graphics, and maps. These displays take different forms depending on the purpose for which the graphic is being constructed (e.g., for thinking or communication), characteristics of the data, potential visual display forms, and whether or not the information graphic is interactive. Choosing an appropriate method for representing data requires a basic understanding of the perceptual and cognitive processing that occurs in the human visual system. Information graphics can be constructed using a wide array of tools, but are increasingly constructed using computer code and distributed through the internet.}
}
@article{ASSIOURAS2025104063,
title = {The evolution of artificial empathy in the hospitality metaverse era},
journal = {International Journal of Hospitality Management},
volume = {126},
pages = {104063},
year = {2025},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.104063},
url = {https://www.sciencedirect.com/science/article/pii/S027843192400375X},
author = {Ioannis Assiouras and Cornelia Laserer and Dimitrios Buhalis},
keywords = {Empathy, Artificial empathy, Artificial intelligence, Metaverse, Hospitality, Artificial intelligence agents},
abstract = {As hospitality enters the metaverse era, artificial empathy becomes essential for developing of artificial intelligence (AI) agents. Using the empathy cycle model, computational empathy frameworks and interdisciplinary research, this conceptual paper proposes a model explaining how artificial empathy will evolve in the hospitality metaverse era. The paper also addresses customer empathy and responses towards AI agents and other human actors with in the hospitality context. It explores how metaverse characteristics such as immersiveness, sociability, experiential nature, interoperability, blended virtual and physical environments as well as environmental fidelity will shape computational models and evolution of artificial empathy. Findings suggests that metaverse enables AI agents to form a seamless cycle of detection, resonation, and response to consumers’ affective states, facilitating the evolution of artificial empathy. Additionally, the paper outlines conditions under which the artificial empathy cycle may be disrupted and proposes future research questions that can advance our understanding of artificial empathy.}
}
@article{LAKAMSANI1995993,
title = {Mapping molecular dynamics computations on to hypercubes},
journal = {Parallel Computing},
volume = {21},
number = {6},
pages = {993-1013},
year = {1995},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)00006-A},
url = {https://www.sciencedirect.com/science/article/pii/016781919500006A},
author = {Vamsee Lakamsani and Laxmi N. Bhuyan and D.Scott Linthicum},
keywords = {Mapping problem, Recursive mincut, Molecular dynamics, Compact MD graph, Hypercube},
abstract = {We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules. We model the application as a task graph which we call a compact MD graph. Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.}
}
@article{BLOCK20191003,
title = {What Is Wrong with the No-Report Paradigm and How to Fix It},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {12},
pages = {1003-1013},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319302360},
author = {Ned Block},
keywords = {consciousness, perception, rivalry, frontal, global workspace, higher order},
abstract = {Is consciousness based in prefrontal circuits involved in cognitive processes like thought, reasoning, and memory or is it based in sensory areas in the back of the neocortex? The no-report paradigm has been crucial to this debate because it aims to separate the neural basis of the cognitive processes underlying post-perceptual decision and report from the neural basis of conscious perception itself. However, the no-report paradigm is problematic because, even in the absence of report, subjects might engage in post-perceptual cognitive processing. Therefore, to isolate the neural basis of consciousness, a no-cognition paradigm is needed. Here, I describe a no-cognition approach to binocular rivalry and outline how this approach can help to resolve debates about the neural basis of consciousness.}
}
@incollection{WU2012223,
title = {10 - Computational modeling and ab initio calculations in MAX phases – II},
editor = {I.M. Low},
booktitle = {Advances in Science and Technology of Mn+1AXn Phases},
publisher = {Woodhead Publishing},
pages = {223-270},
year = {2012},
isbn = {978-1-84569-991-8},
doi = {https://doi.org/10.1533/9780857096012.223},
url = {https://www.sciencedirect.com/science/article/pii/B9781845699918500102},
author = {E. Wu},
keywords = {computational modeling,  calculations, density function theory, energy band, electronic properties, density of states},
abstract = {Abstract:
This chapter reviews the latest researches and advances in the uses of the computational modeling and ab initio calculations on the study of the MAX phases and their properties. The fundamentals and approaches of the density functional theory in the ab initio quantum mechanical calculations and the importance of the theory in the study of the MAX phases are introduced. The studies of the electronic structures and properties, in particular, the energy band structures and total and/or partial density of states of the MAX phases, by using the means of the density function theory are illustrated and discussed. The stability and occurrence of the MAX phases predicted and confirmed by the density functional theory based energetic calculations are addressed. The ab initio calculated elastic and other physical properties of the MAX phases, and the effects of pressure, defects and impurities on the various structural and physical properties are also discussed.}
}

@article{TURNQUIST2024112726,
title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
journal = {Journal of Computational Physics},
volume = {500},
pages = {112726},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112726},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123008215},
author = {Axel G.R. Turnquist},
keywords = {Optimal transport, Optimal information transport, Diffeomorphic density matching, Moving mesh methods, Convergent numerical methods, Compact manifolds},
abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.}
}
@incollection{SAWLEY1995181,
title = { - A serial data-parallel multi-block method for compressible flow computations},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {181-188},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50148-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501481},
author = {M.L. Sawley and J.K. Tegnér and C.M. Bergman},
abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.}
}
@article{NAZIDIZAJI2015318,
title = {Does the smartest designer design better? Effect of intelligence quotient on students’ design skills in architectural design studio},
journal = {Frontiers of Architectural Research},
volume = {4},
number = {4},
pages = {318-329},
year = {2015},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263515000394},
author = {Sajjad Nazidizaji and Ana Tomé and Francisco Regateiro},
keywords = {Architectural design studio, Intelligence quotient (IQ), Design education, Human factors, Design thinking},
abstract = {Understanding the cognitive processes of the human mind is necessary to further learn about design thinking processes. Cognitive studies are also significant in the research about design studio. The aim of this study is to examine the effect of designers intelligence quotient (IQ) on their designs. The statistical population in this study consisted of all Deylaman Institute of Higher Education architecture graduate students enrolled in 2011. Sixty of these students were selected via simple random sampling based on the finite population sample size calculation formula. The students’ IQ was measured using Raven’s Progressive Matrices. The students’ scores in Architecture Design Studio (ADS) courses from first grade (ADS-1) to fifth grade (ADS-5) and the mean scores of the design courses were used in determining the students’ design ability. Inferential statistics, as well as correlation analysis and mean comparison test for independent samples with SPSS, were also employed to analyze the research data. Results indicated that the students’ IQ, ADS-1 to ADS-4 scores, and the mean scores of the students’ design courses were not significantly correlated. By contrast, the students’ IQ and ADS-5 scores were significantly correlated. As the complexity of the design problem and designers’ experience increased, the effect of IQ on design seemingly intensified.}
}
@article{ZHU2023126915,
title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
journal = {Neurocomputing},
volume = {562},
pages = {126915},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126915},
url = {https://www.sciencedirect.com/science/article/pii/S092523122301038X},
author = {ZiJie Zhu and RenDong Ying and Fei Wen and PeiLin Liu},
keywords = {Action recognition, Self-attention, 3D-skeleton, Graph convolutional networks},
abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.}
}
@article{AMACHER2020100022,
title = {Specificity in PDZ-peptide interaction networks: Computational analysis and review},
journal = {Journal of Structural Biology: X},
volume = {4},
pages = {100022},
year = {2020},
issn = {2590-1524},
doi = {https://doi.org/10.1016/j.yjsbx.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2590152420300040},
author = {Jeanine F. Amacher and Lionel Brooks and Thomas H. Hampton and Dean R. Madden},
keywords = {Protein-protein interactions, PDZ, Peptide-binding domains, Therapeutic targets},
abstract = {Globular PDZ domains typically serve as protein–protein interaction modules that regulate a wide variety of cellular functions via recognition of short linear motifs (SLiMs). Often, PDZ mediated-interactions are essential components of macromolecular complexes, and disruption affects the entire scaffold. Due to their roles as linchpins in trafficking and signaling pathways, PDZ domains are attractive targets: both for controlling viral pathogens, which bind PDZ domains and hijack cellular machinery, as well as for developing therapies to combat human disease. However, successful therapeutic interventions that avoid off-target effects are a challenge, because each PDZ domain interacts with a number of cellular targets, and specific binding preferences can be difficult to decipher. Over twenty-five years of research has produced a wealth of data on the stereochemical preferences of individual PDZ proteins and their binding partners. Currently the field lacks a central repository for this information. Here, we provide this important resource and provide a manually curated, comprehensive list of the 271 human PDZ domains. We use individual domain, as well as recent genomic and proteomic, data in order to gain a holistic view of PDZ domains and interaction networks, arguing this knowledge is critical to optimize targeting selectivity and to benefit human health.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{SEVERENGIZ2018429,
title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
journal = {Procedia Manufacturing},
volume = {21},
pages = {429-437},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830180X},
author = {Mustafa Severengiz and Ina Roeder and Kristina Schindler and Günther Seliger},
keywords = {summative assessment, gamification, higher education, engineering education},
abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.}
}
@article{BONCHEKDOKOW201444,
title = {Towards computational models of intention detection and intention prediction},
journal = {Cognitive Systems Research},
volume = {28},
pages = {44-79},
year = {2014},
note = {Special Issue on Mindreading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041713000399},
author = {Elisheva Bonchek-Dokow and Gal A. Kaminka},
keywords = {Intention recognition, Intention prediction, Cognitive modeling},
abstract = {Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.}
}
@incollection{DUNBAR200113746,
title = {Scientific Reasoning and Discovery, Cognitive Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {13746-13749},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01602-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016028},
author = {K. Dunbar},
abstract = {The cognitive mechanisms underlying scientific thinking and discovery have been investigated using approaches from cognitive psychology, cognitive science, and artificial intelligence. In this article, six overlapping approaches are discussed. First, historical analyses and interviews have provided important information on the types of thinking involved in particular discoveries or used by individual scientists. Second, scientific reasoning has been thought of as a form of inductive thinking, and as a form of problem solving. Researchers using this approach have delineated some of the problem solving and inductive reasoning strategies used in science. Third, much research on errors in scientific reasoning, particularly on the topic of ‘confirmation bias’ has revealed some of the circumstances under which science can go awry. Fourth, many researchers have investigated how children's thinking is similar to, or different from, that of scientists. A fifth approach has been to investigate scientists reasoning live or ‘in vivo’ in their own labs. This work has shown how processes such as analogy, distributed cognition, and specific types of inductive and deductive reasoning strategies are used together by scientists. Finally, the incorporation of cognitive mechanisms into computer programs that make discoveries is seen as an important development in the cognitive psychology of scientific thinking.}
}
@article{LYU2023366,
title = {Application of Deep Learning in the Search and a Certain Celestial Body},
journal = {Procedia Computer Science},
volume = {228},
pages = {366-372},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018665},
author = {Yang Lyu and Donglin Su},
keywords = {Deep Learning, Search Analysis, Research Applications, a Certain Celestial Body},
abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.}
}
@article{XU2024473,
title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
journal = {International Journal of Hydrogen Energy},
volume = {62},
pages = {473-487},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924009170},
author = {Pan Xu and Jian Wen and Ke Li and Simin Wang and Yanzhong Li},
keywords = {Hydrogen liquefaction, Continuous catalytic ortho-para hydrogen conversion technology, Ortho-para hydrogen conversion catalyst, Packing layer, Plate-fin heat exchanger},
abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.}
}
@article{CAO2024101200,
title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101200},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001341},
author = {Rosa Cao and Daniel Yamins},
keywords = {Evolution, Contravariance, Intelligibility, Function, Optimization, No-miracles, Instrumentalism, Realism, Philosophy, Constraints, Evolutionary landscape, Models, Explanation, Evo-devo, Development, Learning, Deep learning, Abstraction},
abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.}
}
@article{KRONICK2011435,
title = {Compensatory beliefs and intentions contribute to the prediction of caloric intake in dieters},
journal = {Appetite},
volume = {57},
number = {2},
pages = {435-438},
year = {2011},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2011.05.306},
url = {https://www.sciencedirect.com/science/article/pii/S0195666311004636},
author = {Ilana Kronick and Randy P. Auerbach and Christine Stich and Bärbel Knäuper},
keywords = {Compensatory beliefs, Compensatory intentions, Restraint, Disinhibition, Caloric intake, Experience sampling methodology},
abstract = {One cognitive process that impacts dieters’ decision to indulge is the activation of compensatory beliefs. Compensatory beliefs (CBs) are convictions that the consequences of engaging in an indulgent behaviour (eating cake) can be neutralized by the effects of another behaviour (skipping dinner). Using experience sampling methodology, this study hypothesized that, in addition to the cognitive processes associated with restraint and disinhibition, compensatory thinking contributes to the prediction of caloric intake. Results indicated that higher scores on CB, CI and TFEQ-D predicted a greater number of portions eaten signifying that, along with disinhibition, compensatory thinking predicts caloric intake in dieters.}
}
@article{DIAS2022140,
title = {Utilization of the Arena simulation software and Lean improvements in the management of metal surface treatment processes},
journal = {Procedia Computer Science},
volume = {204},
pages = {140-147},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007554},
author = {A.S.M.E. Dias and R.M.G. Antunes and A. Abreu and V. Anes and H.V.G. Navas and T. Morgado and J.M.F. Calado},
keywords = {Process management, Arena software, Lean tools, Case study, Metal surface treatments},
abstract = {For companies to stand out in increasingly competitive, dynamic and global markets, they must have customer satisfaction goals, create value through their processes, products and services and also aim for innovation. In this context, computer sciences combined with engineering processes constitutes a powerful way for companies to be able to improve process management, to interact with such markets in an efficient and effective way. The main objective of this article is to use Arena simulation software, to quantitatively predict the impact of improvements applied in metal surface treatment processes, based on tools to support Lean thinking. A case study in a Portuguese company in the metalworking sector is presented, in which it is verified that the proposed improvements in terms of the factory layout and resource management, suggested by the comparison between simulations of the current state of the company and the improved one, streamline the processes of finishing in metals, namely zinc coating and lacquering which prevent the occurrence of oxidation and the consequent corrosion of the base metals, by adding other metals and materials to their surface, which adhere and protect it. Through the results obtained, it is concluded that the reduction of waiting times and transport of stocks without production and of work-in-progress, as well as the increase of the productive capacity, make the company more able to guarantee the satisfaction of the requirements of its customers and improve its positioning in the market compared to its competitors.}
}
@article{SUDHESHWAR2024108305,
title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
journal = {Environment International},
volume = {183},
pages = {108305},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2023.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412023005780},
author = {Akshat Sudheshwar and Christina Apel and Klaus Kümmerer and Zhanyun Wang and Lya G. Soeteman-Hernández and Eugenia Valsami-Jones and Claudia Som and Bernd Nowack},
keywords = {Safe-by-Design (SbD), Safe and Sustainable-by-Design (SSbD), Literature mapping, SSbD implementation},
abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.}
}
@incollection{MILLER2023169,
title = {Chapter 9 - Doctoral and professional programs},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {169-196},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000134},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Basic/applied/clinical, Career/job, Collaboration/teams, Critical thinking, -index, PhD/PharmD, Postdoc/postdoctoral, Problem identification, Research design, Writing/publishing},
abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.}
}
@article{JAYAPRAKASAM2015229,
title = {PSOGSA-Explore: A new hybrid metaheuristic approach for beampattern optimization in collaborative beamforming},
journal = {Applied Soft Computing},
volume = {30},
pages = {229-237},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615000435},
author = {S. Jayaprakasam and S.K.A. Rahim and Chee Yen Leow},
keywords = {Collaborative beamforming, Random array, Sidelobe suppression, Particle swarm optimization (PSO), Gravitational search algorithm (GSA)},
abstract = {A conventional collaborative beamforming (CB) system suffers from high sidelobes due to the random positioning of the nodes. This paper introduces a hybrid metaheuristic optimization algorithm called the Particle Swarm Optimization and Gravitational Search Algorithm-Explore (PSOGSA-E) to suppress the peak sidelobe level (PSL) in CB, by the means of finding the best weight for each node. The proposed algorithm combines the local search ability of the gravitational search algorithm (GSA) with the social thinking skills of the legacy particle swarm optimization (PSO) and allows exploration to avoid premature convergence. The proposed algorithm also simplifies the cost of variable parameter tuning compared to the legacy optimization algorithms. Simulations show that the proposed PSOGSA-E outperforms the conventional, the legacy PSO, GSA and PSOGSA optimized collaborative beamformer by obtaining better results faster, producing up to 100% improvement in PSL reduction when the disk size is small.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@article{GOERTZEL199595,
title = {Self-reference, computation, and mind},
journal = {Journal of Social and Evolutionary Systems},
volume = {18},
number = {1},
pages = {95-101},
year = {1995},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(95)90018-7},
url = {https://www.sciencedirect.com/science/article/pii/1061736195900187},
author = {Ben Goertzel and Harold Bowman}
}
@article{JOHNSTON2003325,
title = {Biological computation of image motion from flows over boundaries},
journal = {Journal of Physiology-Paris},
volume = {97},
number = {2},
pages = {325-334},
year = {2003},
note = {Neurogeometry and visual perception},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2003.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0928425703000664},
author = {A. Johnston and P.W. McOwan and C.P. Benton},
keywords = {Optic flow, Cortex, Differential forms, Vision, Motion},
abstract = {A theory of early motion processing in the human and primate visual system is presented which is based on the idea that spatio-temporal retinal image data is represented in primary visual cortex by a truncated 3D Taylor expansion that we refer to as a jet vector. This representation allows all the concepts of differential geometry to be applied to the analysis of visual information processing. We show in particular how the generalised Stokes theorem can be used to move from the calculation of derivatives of image brightness at a point to the calculation of image brightness differences on the boundary of a volume in space–time and how this can be generalised to apply to integrals of products of derivatives. We also provide novel interpretations of the roles of direction selective, bi-directional and pan-directional cells and of type I and type II cells in V5/MT.}
}
@incollection{SALTZER20091,
title = {Chapter 1 - Systems},
editor = {Jerome H. Saltzer and M. Frans Kaashoek},
booktitle = {Principles of Computer System Design},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-42},
year = {2009},
isbn = {978-0-12-374957-4},
doi = {https://doi.org/10.1016/B978-0-12-374957-4.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123749574000104},
author = {Jerome H. Saltzer and M. Frans Kaashoek},
abstract = {Publisher Summary
This chapter introduces some of the vocabulary and concepts used in designing computer systems. It also introduces the “systems perspective,” a way of thinking about systems that is global and encompassing rather than focused on particular issues. The usual course of study of computer science and engineering begins with linguistic constructs for describing computations (software) and physical constructs for realizing computations (hardware). To develop applications that have these requirements, the designer must look beyond the software and hardware and view the computer system as a whole. In doing so, the designer encounters many new problems—so many that the limit on the scope of computer systems generally arises neither from laws of physics nor from theoretical impossibility, but rather from limitations of human understanding.}
}
@incollection{GISZTER2007323,
title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
editor = {Paul Cisek and Trevor Drew and John F. Kalaska},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {165},
pages = {323-346},
year = {2007},
booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(06)65020-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612306650206},
author = {Simon Giszter and Vidyangi Patil and Corey Hart},
keywords = {primitives, motor synergies, force-fields, modularity, feedback, motor pattern analysis, decomposition, rhythm generation, pattern shaping},
abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.}
}
@article{LONGIN2022103280,
title = {Augmenting perception: How artificial intelligence transforms sensory substitution},
journal = {Consciousness and Cognition},
volume = {99},
pages = {103280},
year = {2022},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2022.103280},
url = {https://www.sciencedirect.com/science/article/pii/S1053810022000125},
author = {Louis Longin and Ophelia Deroy},
keywords = {Sensory substitution, Sensory extension, Intelligent sensory augmentation, Information quality, Senses, Artificial intelligence},
abstract = {What happens when artificial sensors are coupled with the human senses? Using technology to extend the senses is an old human dream, on which sensory substitution and other augmentation technologies have already delivered. Laser tactile canes, corneal implants and magnetic belts can correct or extend what individuals could otherwise perceive. Here we show why accommodating intelligent sensory augmentation devices not just improves but also changes the way of thinking and classifying former sensory augmentation devices. We review the benefits in terms of signal processing and show why non-linear transformation is more than a mere improvement compared to classical linear transformation.}
}
@article{SONI2024100016,
title = {Advancements in MXene-based electrocatalysts for hydrogen evolution reaction processes: A comprehensive review},
journal = {Journal of Alloys and Compounds Communications},
volume = {3},
pages = {100016},
year = {2024},
issn = {2950-2845},
doi = {https://doi.org/10.1016/j.jacomc.2024.100016},
url = {https://www.sciencedirect.com/science/article/pii/S295028452400016X},
author = {Kunjal Soni and Rakesh Kumar Ameta},
keywords = {MXenes, 2D materials, Two-electron transfer process, Hydrogen evolution process, Electrocatalysts},
abstract = {MXenes are a newly emerging family of two-dimensional (2D) materials that include carbonitrides, nitrides, and carbides of transition metals. They have attracted much interest from scientists and researchers due to their potential use in electrocatalysts, where a two-electron transfer process is applied. Their remarkable properties, such as strong chemical and structural stability, high electrical conductivity, and large active surface area, make them effective for their potential in advanced hydrogen evolution reactions (HER). This thorough analysis starts by carefully outlining the forward-thinking advances in MXene synthesis and development. It then explores the theoretical and empirical aspects of MXene-based HER electrocatalysts. This review paper presents methods for improving the HER catalytic activity of MXene, including terminal modification, metal-atom doping, and the creation of various nanostructures to increase the density of active sites. The study clarifies current issues and new opportunities and provides a valuable framework for the future development of effective MXene-based electrocatalysts for HERs.}
}
@article{ZHOU20241018,
title = {A 21st Century View of Allowed and Forbidden Electrocyclic Reactions},
journal = {The Journal of Organic Chemistry},
volume = {89},
number = {2},
pages = {1018-1034},
year = {2024},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.3c02103},
url = {https://www.sciencedirect.com/science/article/pii/S0022326324000720},
author = {Qingyang Zhou and Garrett Kukier and Igor Gordiy and Roald Hoffmann and Jeffrey I. Seeman and K. N. Houk},
abstract = {In 1965, Woodward and Hoffmann proposed a theory to predict the stereochemistry of electrocyclic reactions, which, after expansion and generalization, became known as the Woodward–Hoffmann Rules. Subsequently, Longuet-Higgins and Abrahamson used correlation diagrams to propose that the stereoselectivity of electrocyclizations could be explained by the correlation of reactant and product orbitals with the same symmetry. Immediately thereafter, Hoffmann and Woodward applied correlation diagrams to explain the mechanism of cycloadditions. We describe these discoveries and their evolution. We now report an investigation of various electrocyclic reactions using DFT and CASSCF. We track the frontier molecular orbitals along the intrinsic reaction coordinate and modeled trajectories and examine the correlation between HOMO and LUMO for thermally forbidden systems. We also investigate the electrocyclizations of several highly polarized systems for which the Houk group had predicted that donor–acceptor substitution can induce zwitterionic character, thereby providing low-energy pathways for formally forbidden reactions. We conclude with perspectives on the field of pericyclic reactions, including a refinement as the meaning of Woodward and Hoffmann’s “Violations. There are none!” Lastly, we comment on the burgeoning influence of computations on all fields of chemistry.}
}
@article{OMORI19991157,
title = {Emergence of symbolic behavior from brain like memory with dynamic attention},
journal = {Neural Networks},
volume = {12},
number = {7},
pages = {1157-1172},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(99)00054-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608099000544},
author = {T. Omori and A. Mochizuki and K. Mizutani and M. Nishizaki},
keywords = {Symbolic behavior, Associative memory, Attention, PATON, Inference, Hippocampus, Model, Computational theory},
abstract = {An important feature of human intelligence is the use of symbols. This is seen in our daily use of language and logical thinking. However, the use of symbols is not limited to humans. We observe planned action sequences in primate behavior and prediction-based action in higher mammals. For the representation and operation of symbols by the brain neural circuit, no specific construction principle or computational theory is known so far. In this paper, we regard the brain as a complex of associative memory and dynamic attentional system, and starting from two hypotheses on information representation and operation in the brain, we propose a model of primitive symbolic behavior emergence that is consistent with the conventional symbolic processing model. We also describe a computational theory of the symbolic processing model in associative memory. Through computer simulation studies on a language-like memory search and map learning by a moving robot, we discuss the validity of the model.}
}
@article{TALAMI2025115242,
title = {Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design},
journal = {Energy and Buildings},
volume = {329},
pages = {115242},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115242},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824013586},
author = {Riccardo Talami and Jonathan Wright and Bianca Howard},
keywords = {Building optimization, Building performance, Algorithm, Building simulation, Building design},
abstract = {The complexity of performance-based building design stems from the evaluation of numerous candidate design options, driven by the plethora of variables, objectives, and constraints inherent in multi-disciplinary projects. This necessitates optimization approaches to support the identification of well performing designs while reducing the computational time of performance evaluation. In response, this paper proposes and evaluates a sequential approach for multi-objective design optimization of building geometry, fabric, HVAC system and controls for building performance. This approach involves sequential optimizations with optimal solutions from previous stages passed to the next. The performance of the sequential approach is benchmarked against a full factorial search, assessing its effectiveness in finding global optima, solution quality, reliability to scale and variations of problem formulations, and computational efficiency compared to the NSGA-II algorithm. 24 configurations of the sequential approach are tested on a multi-scale case study, simulating 874 to 4,147,200 design options for an office building, aiming to minimize energy demand while maintaining thermal comfort. A two-stage sequential process-(building geometry + fabric) and (HVAC system + controls) identified the same Pareto-optimal solutions as the full factorial search across all four scales and variations of problem formulations, demonstrating 100 % effectiveness and reliability. This approach required 100,700 function evaluations, representing a 91.2 % reduction in computational effort compared to the full factorial search. In contrast, NSGA-II achieved only 73.5 % of the global optima with the same number of function evaluations. This research indicates that a sequential optimization approach is a highly efficient and robust alternative to the standard NSGA-II algorithm.}
}
@article{VANHOOIJDONK2022101044,
title = {Creativity and change of context: The influence of object-context (in)congruency on cognitive flexibility},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101044},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101044},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000475},
author = {Mare {van Hooijdonk} and Simone M. Ritter and Marcel Linka and Evelyn Kroesbergen},
keywords = {Creativity, Spatial context, Object congruence, Cognitive flexibility, Stimulating creativity},
abstract = {Specific environmental features, such as natural settings or spatial design, can foster creativity. The effect of object-context congruency on creativity has not yet been investigated. While congruence between an object and its visual context provides meaning to the object, it may hamper creativity due to mental fixation effects. In the current study, virtual reality technology (VR) was employed to examine the hypothesis that people display more cognitive flexibility - a key element of creativity, representing the ability to overcome mental fixation - when thinking about an object while being in an incongruent than in a congruent environment. Participants (N = 184) performed an Alternative Uses Task, in which they had to name as many uses for a book as possible, while being immersed in a virtual environment that was either object-context congruent (i.e., places where you would expect a book; e.g., a library or a living room; n = 91) or object-context incongruent (i.e., places where a book is not expected; e.g., a clothing store or a car workshop; n = 93). The effect of object (in)congruency was also assessed for three other indices of creativity: fluency (i.e., the number of ideas generated), originality and usefulness. In line with our hypothesis, participants scored higher on pure cognitive flexibility in the object-context incongruent than in the object-context congruent environment. Moreover, participants in the object-context incongruent environment condition generated more original ideas. The theoretical and practical implications of the current findings are discussed.}
}
@article{PUPUNWIWAT2011827,
title = {Conceptual Selective RFID Anti-Collision Technique Management},
journal = {Procedia Computer Science},
volume = {5},
pages = {827-834},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.114},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100439X},
author = {Prapassara Pupunwiwat and Peter Darcy and Bela Stantic},
keywords = {Radio Frequency Identification (RFID), Anti-Collision, Decision Tree, Six Thinking Hats},
abstract = {Radio Frequency Identification (RFID) uses wireless radio frequency technology to automatically identify tagged objects. Despite the extensive development of RFID technology, tag collisions still remains a major drawback. The collision issue can be solved by using anti-collision techniques. While existing research has focused on improving anti-collision methods alone, it is also essential that a suitable type of anti-collision algorithm is selected for the specific circumstance. In this work, we evaluate anti-collision techniques and perform a comparative analysis in order to find the advantages and disadvantages of each approach. To identify the best anti-collision selection method in various scenarios, we have proposed two strategies for selective anti-collision technique management: a “Novel Decision Tree Strategy” and a “Six Thinking Hats Strategy”. We have shown that the selection of the correct technique for specific scenarios improve the quality of the data collection which, in turn, will increase the integrity of the data after being transformed, aggregated, and used for event processing.}
}
@article{JUDD1997907,
title = {Computational economics and economic theory: Substitutes or complements?},
journal = {Journal of Economic Dynamics and Control},
volume = {21},
number = {6},
pages = {907-942},
year = {1997},
note = {Society of Computational Economics Conference},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(97)00010-9},
url = {https://www.sciencedirect.com/science/article/pii/S0165188997000109},
author = {Kenneth L. Judd},
keywords = {Computational approach, Theoretical analysis},
abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.}
}
@article{ZHAO2024109027,
title = {Towards the definition of spatial granules},
journal = {Fuzzy Sets and Systems},
volume = {490},
pages = {109027},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001738},
author = {Liquan Zhao and Yiyu Yao},
keywords = {Granularity, Fineness, Subsethood, Coarse-fine relation, Quotient join, Quotient meet, Granular space, Spatial granular computing},
abstract = {Three basic issues of granular computing are construction or definition of granules, measures of granules, and computation or reasoning with granules. This paper reviews the main theories of granular computing and introduces the definition of spatial granules. A granule is composed of one or more atomic granules. The rationality of this definition is explained from the four aspects: simplicity, applicability, measurability and visualization. A one-to-one correspondence is established between the granules and the points in the unit hypercube, and the coarsening and refining of the granules are the descending and ascending dimensions of the points, respectively. The weak fuzzy tolerance relation and weak fuzzy equivalence relation are defined so as to study on all fuzzy binary relations. The notion of layer granularity/fineness is introduced and each granule can be easily denoted by two numbers, which can be used to pre-process macro knowledge space and greatly improve the search speed. This paper also discusses the main properties of granules including the necessary and sufficient conditions of coarse-fine relation and the main principles of granular space.}
}
@article{AMEL2023104187,
title = {Toward an automatic detection of cardiac structures in short and long axis views},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104187},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006413},
author = {Laidi Amel and Mohammed Ammar and Mostafa {El Habib Daho} and Said Mahmoudi},
keywords = {Cardiac MRI Segmentation, Shape Descriptors, Particle Swarm Optimization, Residual Network, Interpretability},
abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.}
}
@article{CHEN202321,
title = {Varieties of specification: Redefining over- and under-specification},
journal = {Journal of Pragmatics},
volume = {216},
pages = {21-42},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S037821662300200X},
author = {Guanyi Chen and Kees {van Deemter}},
keywords = {Referring expressions, Over-specification, Under-specification},
abstract = {A long tradition of research in theoretical, experimental and computational pragmatics has investigated over-specification and under-specification in referring expressions. Along broadly Gricean lines, these studies compare the amount of information expressed by a referring expression against the amount of information that is required. Often, however, these studies offer no formal definition of what “required” means, and how the comparison should be performed. In this paper, we use a simple set-theoretic perspective to define some communicatively important types of over-/under-specification. We argue that our perspective enables an enhanced understanding of reference phenomena that can pay important dividends for the analysis of reference in corpora and for the evaluation of computational models of referring. To illustrate and substantiate our claims, we analyse two corpora, containing Chinese and English referring expressions respectively, using the new perspective. The results show that interesting new monolingual and cross-linguistic insights can be obtained from our perspective.}
}
@article{FILIPPOU2016892,
title = {Modelling the impact of study behaviours on academic performance to inform the design of a persuasive system},
journal = {Information & Management},
volume = {53},
number = {7},
pages = {892-903},
year = {2016},
note = {Special Issue on Papers Presented at Pacis 2015},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616300507},
author = {Justin Filippou and Christopher Cheong and France Cheong},
keywords = {Study behaviour, Persuasive systems, Linear modelling, Higher education},
abstract = {Information technology is deeply ingrained in most aspects of everyday life and can be designed to influence users to behave in a certain way. Influencing students to improve their study behaviour would be a useful application of this technology. As a preamble to the design of a persuasive system for learning, we collected data to identify the study behaviours of students and recent alumni. We then developed two models to measure which behaviours have the most significant impact on learning performance. Current students reported more foundational behaviours whereas alumni demonstrated more higher-order thinking traits.}
}
@article{FAELENS2021106510,
title = {Social media use and well-being: A prospective experience-sampling study},
journal = {Computers in Human Behavior},
volume = {114},
pages = {106510},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106510},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302624},
author = {Lien Faelens and Kristof Hoorelbeke and Bart Soenens and Kyle {Van Gaeveren} and Lieven {De Marez} and Rudi {De Raedt} and Ernst H.W. Koster},
keywords = {Social media, Social comparison, Self-esteem, Repetitive negative thinking, Negative affect},
abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.}
}
@article{ROY2020106210,
title = {Fixed subgroups and computation of auto-fixed closures in free-abelian times free groups},
journal = {Journal of Pure and Applied Algebra},
volume = {224},
number = {4},
pages = {106210},
year = {2020},
issn = {0022-4049},
doi = {https://doi.org/10.1016/j.jpaa.2019.106210},
url = {https://www.sciencedirect.com/science/article/pii/S0022404919302178},
author = {Mallika Roy and Enric Ventura},
keywords = {Free-abelian times free, Automorphism, Fixed subgroup, Periodic subgroup, Auto-fixed closure},
abstract = {The classical result by Dyer–Scott about fixed subgroups of finite order automorphisms of Fn being free factors of Fn is no longer true in Zm×Fn. Within this more general context, we prove a relaxed version in the spirit of Bestvina–Handel Theorem: the rank of fixed subgroups of finite order automorphisms is uniformly bounded in terms of m,n. We also study periodic points of endomorphisms of Zm×Fn, and give an algorithm to compute auto-fixed closures of finitely generated subgroups of Zm×Fn. On the way, we prove the analog of Day's Theorem for real elements in Zm×Fn, contributing a modest step into the project of doing so for any right angled Artin group (as McCool did with respect to Whitehead's Theorem in the free context).}
}
@article{VERNON2019122,
title = {Internal simulation in embodied cognitive systems: Comment on “Muscleless motor synergies and actions without movements: From motor neuroscience to cognitive robotics” by Vishwanathan Mohan et al.},
journal = {Physics of Life Reviews},
volume = {30},
pages = {122-125},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300429},
author = {David Vernon},
keywords = {Internal simulation, Embodied cognition, Cognitive robotics, Episodic future thinking}
}
@article{EDELMAN1997296,
title = {Computational theories of object recognition},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {8},
pages = {296-304},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01090-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010905},
author = {S. Edelman},
abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.}
}
@incollection{KISS2019109,
title = {Process Systems Engineering from an industrial and academic perspective},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {109-114},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343500199},
author = {Anton A. Kiss and Johan Grievink},
keywords = {PSE, industry, education, research, interface, perspectives},
abstract = {Process Systems Engineering (PSE) deals with decision-making, at all levels and scales, by understanding complex process systems using a holistic view. Computer Aided Process Engineering (CAPE) is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. The ‘PSE’ term suffers from a branding issue to the point that PSE does not get the recognition it deserves. This work aims to provide an informative industrial and academic perspective on PSE, arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be prioritized ahead of just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@incollection{KIHLSTROM2018,
title = {Cognitive Psychology: Overview☆},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2018},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21702-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245217021},
author = {John F. Kihlstrom and Lillian Park},
keywords = {Cognition, Sensation, Perception, Attention, Memory, Categorization, Learning, Language, Reasoning, Judgment, Decision-making, Choice, Cognitive development, Cognitive neuroscience, Cognitive sociology},
abstract = {Cognitive psychology seeks to understand how we acquire knowledge about ourselves and the world, how this knowledge is represented in the mind and brain, and how we use knowledge to guide behavior. Major topics in cognitive psychology include sensation and perception, attention, memory, categorization, learning, language and communication, and thinking, reasoning, judgment, and decision-making. Cognitive development is discussed from both an ontogenetic and phylogenetic point of view. Cognitive neuroscience explores the neural substrates of cognitive processes. The cognitive point of view has been extended to personality, social, and clinical psychology, as well as to sociology, anthropology, and other social-science disciplines.}
}
@article{KOTIR2024140042,
title = {Field experiences and lessons learned from applying participatory system dynamics modelling to sustainable water and agri-food systems},
journal = {Journal of Cleaner Production},
volume = {434},
pages = {140042},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.140042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623042002},
author = {Julius H. Kotir and Renata Jagustovic and George Papachristos and Robert B. Zougmore and Aad Kessler and Martin Reynolds and Mathieu Ouedraogo and Coen J. Ritsema and Ammar Abdul Aziz and Ron Johnstone},
keywords = {Africa, Group model building, Systems thinking, Stakeholder engagement, System modelling, Sustainable development},
abstract = {Achieving the objectives of sustainable development in water and agri-food systems requires the utilisation of decision-support tools in stakeholder-driven processes to construct and simulate various scenarios and evaluate the outcomes of associated policy interventions. While it is common practice to involve stakeholders in participatory modelling processes, their comprehensive documentation and the lessons learned remain scarce. In this paper, we share our experience of engaging stakeholders throughout the entire system dynamics modelling process. We draw on two projects implemented in the Volta River Basin, West Africa, to understand the dynamics of water and agri-food systems under changing environmental and socioeconomic conditions. We outline eight key insights and lessons as practical guides derived from each stage of the participatory modelling process, including the pre-workshop stage, problem definition, model conceptualization, simulation model formulation, model testing and verification, and policy design and evaluation. Our findings demonstrate that stakeholders can actively contribute to all phases of the system dynamics modelling process, including parameter estimation, sensitivity analysis, and numerical simulation experiments. However, we encountered notable challenges, including the time-intensive nature of the process, the struggle to reach a consensus on the modelled problem, and the difficulty of translating the conceptual model into a simulation model using stock and flow diagrams – all of which were addressed through a structured facilitation process. While the projects were anchored in the specific context of West Africa, the key lessons and insights highlighted have broader significance, particularly for researchers employing PSDM in regions characterised by multifaceted human-environmental systems and where stakeholder involvement is crucial for holistic understanding and effective policy interventions. This paper contributes practical guidance for future efforts with participatory modelling, particularly in regions worldwide grappling with sustainable development challenges in water and agri-food systems, and where stakeholder involvement is crucial for holistic understanding of the multiple challenges and for designing effective policy interventions.}
}
@article{RYLE1953189,
title = {Thinking},
journal = {Acta Psychologica},
volume = {9},
pages = {189-196},
year = {1953},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(53)90012-2},
url = {https://www.sciencedirect.com/science/article/pii/0001691853900122},
author = {Gilbert Ryle}
}
@incollection{MAIDA201639,
title = {Chapter 2 - Cognitive Computing and Neural Networks: Reverse Engineering the Brain},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {39-78},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300529},
author = {A.S. Maida},
keywords = {Brain simulation, Deep belief networks, Convolutional networks, Liquid computing, Biological neural networks, Neocortex},
abstract = {Cognitive computing seeks to build applications which model and mimic human thinking. One approach toward achieving this goal is to develop brain-inspired computational models. A prime example of such a model is the class of deep convolutional networks which is currently used in pattern recognition, machine vision, and machine learning. We offer a brief review of the mammalian neocortex, the minicolumn, and the ventral pathway. We provide descriptions of abstract neural circuits that have been used to model these areas of the brain. This include Poisson spiking networks, liquid computing networks, spiking models of feature discovery in the ventral pathway, spike-timing-dependent plasticity learning, restricted Boltzmann machines, deep belief networks, and deep convolutional networks. In summary, this chapter explores abstractions of neural networks found within the mammalian neocortex that support cognition and the beginnings of cognitive computation.}
}
@incollection{ADRIAANS2008133,
title = {LEARNING AND THE COOPERATIVE COMPUTATIONAL UNIVERSE},
editor = {Pieter Adriaans and Johan {van Benthem}},
booktitle = {Philosophy of Information},
publisher = {North-Holland},
address = {Amsterdam},
pages = {133-167},
year = {2008},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51726-5.50010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517265500108},
author = {Pieter Adriaans}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@article{STATHOPOULOS20031565,
title = {Wind loads on low buildings: in the wake of Alan Davenport's contributions},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {91},
number = {12},
pages = {1565-1585},
year = {2003},
note = {ENGINEERING SYMPOSIUM To Honour ALAN G. DAVENPORT for his 40 Years of Contributions},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2003.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167610503001302},
author = {Ted Stathopoulos},
keywords = {Building, Code, Computational wind engineering, Design, Load, Pressure, Standard, Time series, Wind},
abstract = {The paper reviews the evolution of knowledge and its current state regarding the evaluation of wind loads on low buildings by placing particular emphasis on Alan Davenport's contributions. These contributions have paved the way to the current state-of-the-art and have influenced the thinking of not only Alan's closest collaborators but also of other researchers in this area around the world. The paper will provide a brief historical perspective, followed by some detailed description of the University of Western Ontario's research on wind loads on low buildings carried out in the 1970s. Visualizing the wake of Davenport's contributions in this area, the paper will refer to the influence of this knowledge in the formulation of design load provisions in contemporary wind standards and codes of practice. The paper will also discuss the status of computational wind engineering as well as the so-called computer-aided wind engineering in the evaluation of wind pressures on low buildings.}
}
@article{ALVARADO20043,
title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
journal = {Expert Systems with Applications},
volume = {26},
number = {1},
pages = {3-8},
year = {2004},
note = {Intelligent Computing in the Petroleum Industry, ICPI-02},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403001039},
author = {Matı&#x0301;as Alvarado and Leonid Cheremetov and Francisco Cantú}
}
@article{FERREIRA20131446,
title = {Fostering the Creative Development of Computer Science Students in Programming and Interaction Design},
journal = {Procedia Computer Science},
volume = {18},
pages = {1446-1455},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.312},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913004559},
author = {Deller James Ferreira},
keywords = {Creativity, Programming, Interaction design},
abstract = {This study explores the enhancement of creativity in undergraduate students studying computer science. We assume that everybody has creative potential. As a teacher, we can explicitly encourage creative thinking, providing space to let students collaboratively discover and explore their creativity. This paper presents a dialogical framework to help the teacher fostering creativity among students of computer science in programming and interaction design. The framework presented here involves underlying dialogic processes from seven collaborative and creative dimensions that allow students to develop creativity. The use of the pedagogical framework makes it possible to teachers create significant interaction design and computer programming experiences to students, motivating them to activate mental processes underlying creativity. Students can simultaneously activate two or more ideas, images, or thoughts and have them interact, prompt thought experiments, change cognitive perspectives, raise new points of view, and risk category mistakes.}
}
@article{GOLDBERG20007,
title = {The Design of Innovation: Lessons from Genetic Algorithms, Lessons for the Real World},
journal = {Technological Forecasting and Social Change},
volume = {64},
number = {1},
pages = {7-12},
year = {2000},
issn = {0040-1625},
doi = {https://doi.org/10.1016/S0040-1625(99)00079-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040162599000797},
author = {David E Goldberg},
abstract = {This article considers some of the connections between genetic algorithms (GAs)—search procedures based on the mechanics of natural selection and natural genetics—and human innovation. Simply stated, innovation has been a source of inspiration for thinking about genetic algorithms, and as the algorithms have improved, GAs have become increasingly interesting computational models of the processes of innovation. The article reviews the basics of genetic algorithm operation and connects the basic mechanics to two processes of innovation: continual improvement and discontinuous change. Thereafter, some of the technical lessons of genetic algorithm processing are reviewed and their implications are briefly explored in the context of organizational change.}
}
@article{BLOSS2016,
title = {Reimagining Human Research Protections for 21st Century Science},
journal = {Journal of Medical Internet Research},
volume = {18},
number = {12},
year = {2016},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.6634},
url = {https://www.sciencedirect.com/science/article/pii/S1438887116003204},
author = {Cinnamon Bloss and Camille Nebeker and Matthew Bietz and Deborah Bae and Barbara Bigby and Mary Devereaux and James Fowler and Ann Waldo and Nadir Weibel and Kevin Patrick and Scott Klemmer and Lori Melichar},
keywords = {ethics committees, research, biomedical research, telemedicine, informed consent, behavioral research},
abstract = {Background
Evolving research practices and new forms of research enabled by technological advances require a redesigned research oversight system that respects and protects human research participants.
Objective
Our objective was to generate creative ideas for redesigning our current human research oversight system.
Methods
A total of 11 researchers and institutional review board (IRB) professionals participated in a January 2015 design thinking workshop to develop ideas for redesigning the IRB system.
Results
Ideas in 5 major domains were generated. The areas of focus were (1) improving the consent form and process, (2) empowering researchers to protect their participants, (3) creating a system to learn from mistakes, (4) improving IRB efficiency, and (5) facilitating review of research that leverages technological advances.
Conclusions
We describe the impetus for and results of a design thinking workshop to reimagine a human research protections system that is responsive to 21st century science.}
}
@article{NIRMALADEVI2025126553,
title = {DCNN-SBiL: EEG signal based mild cognitive impairment classification using compact convolutional network},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126553},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126553},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001757},
author = {A. {Nirmala Devi} and M. Latha},
keywords = {Mild cognitive impairment, Deep learning, Compact convolutional neural network, EEG signal, Dual attention, Alzheimer’s disease, Improved tuneable Q wavelet transform},
abstract = {Mild cognitive impairment (MCI) is a state that falls between the more severe decline of dementia and the typical aging-related loss of memory and thinking. MCI must be diagnosed earlier to avoid complete memory loss. Several Machine Learning (ML) and Deep Learning (DL) models employ standard feature extraction approaches to achieve effective MCI categorization. However, it has some drawbacks, including lower accuracy, longer time consumption, less feature learning, and increased model complexity. The proposed method introduces a novel deep learning model to address the limitations of existing MCI classification approaches. Initially, the Electroencephalography (EEG) signal is pre-processed using the Sequential Savitzky-Golay filtering model (SEQ-SG), which improves the signal’s quality and removes unnecessary noise. The Improved Tuneable Q Wavelet Transform (ITQWT) feature extraction model is used to extract relevant features. The Coati Stochastic Optimization (CSO) algorithm selects the most optimal channel features from the EEG signal. Finally, the proposed deep learning model, Dual Attention Assisted Compact Convolutional Network with Stacked Bi-LSTM (DCCN-SBiL), is used to classify EEG signals into three categories: Alzheimer’s disease, MCI, and normal. The proposed model is optimized using the Gazelle Optimization Algorithm (GOA), which tunes the classification model’s hyperparameters. The proposed classification model is evaluated using the Mendeley Dataset, which contains EEG signals from Alzheimer's disease, MCI and Normal. The proposed model has shown great performance in many performance parameters, including 97.25% accuracy, 95.94% recall, 96.03% precision, and 94.65% specificity in MCI classification.}
}
@article{GIRARD2005215,
title = {From brainstem to cortex: Computational models of saccade generation circuitry},
journal = {Progress in Neurobiology},
volume = {77},
number = {4},
pages = {215-251},
year = {2005},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2005.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S030100820500153X},
author = {B. Girard and A. Berthoz},
keywords = {Saccade generation circuitry, Computational models, Brainstem, Superior colliculus, Cerebellum, Basal ganglia, Cortex},
abstract = {The brain circuitry of saccadic eye movements, from brainstem to cortex, has been extensively studied during the last 30 years. The wealth of data gathered allowed the conception of numerous computational models. These models proposed descriptions of the putative mechanisms generating this data, and, in turn, made predictions and helped to plan new experiments. In this article, we review the computational models of the five main brain regions involved in saccade generation: reticular formation saccadic burst generators, superior colliculus, cerebellum, basal ganglia and premotor cortical areas. We present the various topics these models are concerned with: location of the feedback loop, multimodal saccades, long-term adaptation, on the fly trajectory correction, strategy and metrics selection, short-term spatial memory, transformations between retinocentric and craniocentric reference frames, sequence learning, to name the principle ones. Our objective is to provide a global view of the whole system. Indeed, narrowing too much the modelled areas while trying to explain too much data is a recurrent problem that should be avoided. Moreover, beyond the multiple research topics remaining to be solved locally, questions regarding the operation of the whole structure can now be addressed by building on the existing models.}
}
@article{ALY2014206,
title = {Atmospheric boundary-layer simulation for the built environment: Past, present and future},
journal = {Building and Environment},
volume = {75},
pages = {206-221},
year = {2014},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360132314000444},
author = {Aly Mousaad Aly},
keywords = {Aerodynamics, Aeroelasticity, Atmospheric boundary-layer, Built environment, Experimental/computational wind engineering},
abstract = {This paper summarizes the state-of-the-art techniques used to simulate hurricane winds in atmospheric boundary-layer (ABL) for wind engineering testing. The wind tunnel simulation concept is presented along with its potential applications, advantages and challenges. ABL simulation at open-jet simulators is presented along with an application example followed by a discussion on the advantages and challenges of testing at these facilities. Some of the challenges and advantages of using computational fluid dynamics (CFD) are presented with an application example. The paper show that the way the wind can be simulated is complex and matching one parameter at full-scale may lead to a mismatch of other parameters. For instance, while large-scale testing is expected to improve Reynolds number and hence approach the full-scale scenario, it is challenging to generate large-scale turbulence in an artificially created wind. New testing protocols for low-rise structures and small-size architectural features are presented as an answer to challenging questions associated with both wind tunnel and open-jet testing. Results show that it is the testing protocol that can be adapted to enhance the prediction of full-scale physics in nature. Thinking out of the box and accepting non-traditional ABL is necessary to compensate for Reynolds effects and to allow for convenient experimentation. New research directions with focus on wind, rain and waves as well as other types of non-synoptic winds are needed, in addition to a more focus on the flow physics in the lower part of the ABL, where the major part of the infrastructure exists.}
}
@article{KNAPP2017370,
title = {Energy-efficient Legionella control that mimics nature and an open-source computational model to aid system design},
journal = {Applied Thermal Engineering},
volume = {127},
pages = {370-377},
year = {2017},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S135943111633664X},
author = {Samuel Knapp and Bo Nordell},
keywords = {Thermal model, Heat exchanger, Pasteurization, Legionnaire’s disease, Microsoft Excel},
abstract = {Although there is no direct connection, the incidence of Legionnaire’s disease has increased concurrently with increased usage of energy efficient domestic hot water (DHW) systems, which serve as ideal growth environments for Legionella pneumophila, the bacteria responsible for Legionnaire’s disease. The Duck Foot Heat Exchange Model (DFHXM) was developed to aid design of energy efficient thermal pasteurization systems with Legionella control specifically in mind. The model simulates a system design imitating the countercurrent heat exchange in the feet of ducks, an evolutionary adaption reducing environmental heat losses in cold climates. Such systems use a heat exchanger to preheat fluids prior to pasteurization and cool the same fluid after pasteurization. Thus, the design requires minimal addition of heat to achieve pasteurization temperatures and to cover environmental heat losses. This article describes the underlying principles and use of the freely available Microsoft Excel model, as well as compares results from the DFHXM to measurements of an experimental pilot system. Simulation outputs agreed well with experimental results for transient and steady-state temperatures, the largest discrepancy in steady-state temperatures being 4.6%. Lastly, we discuss the flexibility of the DFHXM to simulate a wide variety of designs with special emphasis on Legionella control and solar-thermal water disinfection.}
}
@article{WEICHBROTH20223798,
title = {A note on the affective computing systems and machines: a classification and appraisal},
journal = {Procedia Computer Science},
volume = {207},
pages = {3798-3807},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.441},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013345},
author = {Paweł Weichbroth and Wiktor Sroka},
keywords = {Affective Computing, Artificial Emotional Intelligence, Classification, System, Machine},
abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.}
}
@article{OLSON1995183,
title = {Emergent computation and the modeling and management of ecological systems},
journal = {Computers and Electronics in Agriculture},
volume = {12},
number = {3},
pages = {183-209},
year = {1995},
issn = {0168-1699},
doi = {https://doi.org/10.1016/0168-1699(94)00022-I},
url = {https://www.sciencedirect.com/science/article/pii/016816999400022I},
author = {Richard L. Olson and Ronaldo A. Sequeira},
keywords = {Emergent computation, Ecosystem dynamics, Ecosystem management},
abstract = {This paper introduces the emergent computational paradigm, discusses its applicability and potential in ecosystem management, and reviews the literature. Emergent computation is significantly different from the “classic” computational paradigm, where control is top-down and centralized. In emergent systems, overall system dynamics emerge from the local interactions of independent agents. In such systems, overall global control is minimized or eliminated altogether. Applications in ecosystem management include use of “artificial ecosystems” as surrogate experimental systems, and genetics-based machine learning systems to evolve management rule-sets for complex domains. Cellular automata, neural networks, genetic algorithms and classifier systems are discussed as examples of the emergent approach. Finally, an in-depth literature review of artificial ecosystems is provided.}
}
@article{YAP19973,
title = {Towards exact geometric computation},
journal = {Computational Geometry},
volume = {7},
number = {1},
pages = {3-23},
year = {1997},
issn = {0925-7721},
doi = {https://doi.org/10.1016/0925-7721(95)00040-2},
url = {https://www.sciencedirect.com/science/article/pii/0925772195000402},
author = {Chee-Keng Yap},
abstract = {Exact computation is assumed in most algorithms in computational geometry. In practice, implementors perform computation in some fixed-precision model, usually the machine floating-point arithmetic. Such implementations have many well-known problems, here informally called “robustness issues”. To reconcile theory and practice, authors have suggested that theoretical algorithms ought to be redesigned to become robust under fixed-precision arithmetic. We suggest that in many cases, implementors should make robustness a non-issue by computing exactly. The advantages of exact computation are too many to ignore. Many of the presumed difficulties of exact computation are partly surmountable and partly inherent with the robustness goal. This paper formulates the theoretical framework for exact computation based on algebraic numbers. We then examine the practical support needed to make the exact approach a viable alternative. It turns out that the exact computation paradigm encompasses a rich set of computational tactics. Our fundamental premise is that the traditional “BigNumber” package that forms the work-horse for exact computation must be reinvented to take advantage of many features found in geometric algorithms. Beyond this, we postulate several other packages to be built on top of the BigNumber package.}
}
@article{LI2024109502,
title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
journal = {International Journal of Heat and Fluid Flow},
volume = {109},
pages = {109502},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24002273},
author = {Libo Li and Jiyuan Bi and Jingkai Ma and Xiaoxu Zhang and Qiuwang Wang and Ting Ma},
keywords = {Printed circuit heat exchanger, Anisotropic thermal conductivity, Numerical simulation, Thermal resistance, Heat exchanger efficiency},
abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.}
}
@article{MAHMUD2025111321,
title = {RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data},
journal = {Pattern Recognition},
volume = {161},
pages = {111321},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010720},
author = {Mohammad Sultan Mahmud and Hua Zheng and Diego Garcia-Gil and Salvador García and Joshua Zhexue Huang},
keywords = {Clustering approximation, Ensemble clustering, Incremental clustering, Ensemble learning},
abstract = {Large-scale data clustering needs an approximate approach for improving computation efficiency and data scalability. In this paper, we propose a novel method for ensemble clustering of large-scale datasets that uses the Random Sample Partition and Clustering Approximation (RSPCA) to tackle the problems of big data computing in cluster analysis. In the RSPCA computing framework, a big dataset is first partitioned into a set of disjoint random samples, called RSP data blocks that remain distributions consistent with that of the original big dataset. In ensemble clustering, a few RSP data blocks are randomly selected, and a clustering operation is performed independently on each data block to generate the clustering result of the data block. All clustering results of selected data blocks are aggregated to the ensemble result as an approximate result of the entire big dataset. To improve the robustness of the ensemble result, the ensemble clustering process can be conducted incrementally using multiple batches of selected RSP data blocks. To improve computation efficiency, we use the I-niceDP algorithm to automatically find the number of clusters in RSP data blocks and the k-means algorithm to determine more accurate cluster centroids in RSP data blocks as inputs to the ensemble process. Spectral and correlation clustering methods are used as the consensus functions to handle irregular clusters. Comprehensive experiment results on both real and synthetic datasets demonstrate that the ensemble of clustering results on a few RSP data blocks is sufficient for a good global discovery of the entire big dataset, and the new approach is computationally efficient and scalable to big data.}
}
@incollection{SCHOMMERS201991,
title = {Chapter 2 - Theoretical and Computational Methods},
editor = {Wolfram Schommers},
booktitle = {Basic Physics of Nanoscience},
publisher = {Elsevier},
pages = {91-202},
year = {2019},
isbn = {978-0-12-813718-5},
doi = {https://doi.org/10.1016/B978-0-12-813718-5.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128137185000028},
author = {Wolfram Schommers},
keywords = {Simulation methods, interaction potentials, anharmonicities, temperature effects, molecular dynamics, nanosystems, structures, dynamics},
abstract = {It is underlined that typical nanosystems are adequately described only by the fundamental laws of theoretical physics. It is in particular argued that phenomenological models are in most cases not sophisticated enough. For the description of such nanosystems the theoretical and computational tools have to be selected carefully and have in particular to be improved in many cases. In this connection the interaction laws (potentials) between the atoms, forming a nanosystem, are critical functions because the structure and dynamics of such systems are very sensitive to small variations in the potentials. This point has been studied in detail. Various potential laws have been introduced and discussed in connection with applications. The most relevant simulations methods are quoted and their relevance for nanotechnology is discussed. In particular, the molecular dynamics method is described in detail. We give typical examples, which demonstrate the fact the molecular dynamics method is a powerful and reliable tool for the investigation of typical nanosystems with their large variety of structures and complex dynamical states. The examples deal with wear at the nanotechnological level and with metallic nanoclusters as building blocks.}
}
@article{VANDUN2023113880,
title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
journal = {Decision Support Systems},
volume = {165},
pages = {113880},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113880},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001518},
author = {Christopher {van Dun} and Linda Moder and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Business process improvement, Business process redesign, Generative adversarial networks, Generative machine learning, Process mining},
abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{PORNSUWANCHAROEN20181034,
title = {Meditation mathematical formalism and Lorentz factor calculation based-on Mindfulness foundation},
journal = {Results in Physics},
volume = {11},
pages = {1034-1038},
year = {2018},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2211379718325294},
author = {N. Pornsuwancharoen and I.S. Amiri and J. Ali and P. Youplao and P. Yupapin},
keywords = {Meditation science, Mindfulness Foundation, Buddhism philosophy, Mathematics foundation, Natural science},
abstract = {Mindfulness foundation is an excellent method of the human spiritual development by the reasonable thinking and consideration, which was established by Lord Buddha a long time ago. There are four ways of thinking and consideration-(i) form (body), (ii) sensation, (iii) spiritual and (iv) Dhamma. In this paper, we propose the use of the form consideration for the spiritual development, in which the form can be considered thoroughly inside the body by the spiritual projection. By using the nonlinear microring resonator known as a Panda-ring resonator, the electromagnetic (EM) signals called polaritons can be generated by the coupling interaction between the intense EM fields and the ionic diploes within the almost closed system, where the dipoles can obtain from the coupling between the gold grating and the strong electromagnetic fields. In the manipulation, cells, tissues, and organs inside the human body can communicate with the spiritual (polaritonic) signals and investigation. The simulation results obtained have shown that the Lorentz factor of 0.99999959 is obtained. The successively filtering of the signal circulation within the body during the meditation can be formulated and the meditation behaviors modeled. The aura, the stopping, and the cold body states can be configured and explained.}
}
@article{KALAY199837,
title = {P3: Computational environment to support design collaboration},
journal = {Automation in Construction},
volume = {8},
number = {1},
pages = {37-48},
year = {1998},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(98)00064-8},
url = {https://www.sciencedirect.com/science/article/pii/S0926580598000648},
author = {Yehuda E Kalay},
keywords = {Collaborative design, Design environment, Product model, Performance model, Process model},
abstract = {The work reported in this paper addresses the paradoxical state of the construction industry (also known as A/E/C, for Architecture, Engineering and Construction), where the design of highly integrated facilities is undertaken by severely fragmented teams, leading to diminished performance of both processes and products. The construction industry has been trying to overcome this problem by partitioning the design process hierarchically or temporally. While these methods are procedurally efficient, their piecemeal nature diminishes the overall performance of the project. Computational methods intended to facilitate collaboration in the construction industry have, so far, focused primarily on improving the flow of information among the participants. They have largely met their stated objective of improved communication, but have done little to improve joint decision-making, and therefore have not significantly improved the quality of the design project itself. We suggest that the main impediment to effective collaboration and joint decision-making in the A/E/C industry is the divergence of disciplinary `world-views', which are the product of educational and professional processes through which the individuals participating in the design process have been socialized into their respective disciplines. To maximize the performance of the overall project, these different world-views must be reconciled, possibly at the expense of individual goals. Such reconciliation can only be accomplished if the participants find the attainment of the overall goals of the project more compelling than their individual disciplinary goals. This will happen when the participants have become cognizant and appreciative of world-views other than their own, including the objectives and concerns of other participants. To achieve this state of knowledge, we propose to avail to the participants of the design team highly specific, contextualized information, reflecting each participant's valuation of the proposed design actions. P3 is a semantically-rich computational environment, which is intended to fulfill this mission. It consists of: (1) a shared representation of the evolving design project, connected (through the World Wide Web) to (2) individual experts and their discipline-specific knowledge repositories; and (3) a computational project manager makes the individual valuations visible to all the participants, and helps them deliberate and negotiate their respective positions for the purpose of improving the overall performance of the project. The paper discusses the theories on which the three components are founded, their function, and the principles of their implementation.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{BAYRAKTARSARI2024110835,
title = {Architectural spatial layout design for hospitals: A review},
journal = {Journal of Building Engineering},
volume = {97},
pages = {110835},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110835},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224024033},
author = {Aysegul Ozlem {Bayraktar Sari} and Wassim Jabi},
keywords = {Architectural spatial layout design, Hospital spatial layout design, Computational design, Facility layout planning, Machine learning (ML) driven layout design, Systematic review},
abstract = {The design of hospital spatial layouts is a critical aspect of healthcare architecture, directly influencing patient outcomes, staff efficiency, and the overall quality of care. A well-designed hospital layout is essential for ensuring smooth operations, minimizing errors, and improving both patient and staff experiences. This paper reviews the significant advances in the field, particularly focusing on the transition from traditional design methods to the integration of computational techniques and machine learning (ML) in hospital layout planning. Despite these technological advancements, there remains a notable gap in the full adoption and optimization of these methods to effectively address the inherent complexities of healthcare environments. This review identifies that while computational methods and machine learning-driven approaches have brought precision and innovation to hospital design, the challenge lies in balancing these technologies with the expertise and insights of human designers. Moreover, the need for interdisciplinary collaboration between architects, healthcare professionals, and engineers is emphasized as crucial for the successful implementation of advanced design strategies. Insights from this review highlight the potential of future research to bridge the existing gaps, proposing directions for the continuous integration of technology in hospital layout design.}
}
@article{SAVIN2024108324,
title = {Reviewing studies of degrowth: Are claims matched by data, methods and policy analysis?},
journal = {Ecological Economics},
volume = {226},
pages = {108324},
year = {2024},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108324},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924002210},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Economic growth, Environmental policy, GDP, Political feasibility, Post-growth},
abstract = {In the last decade many publications have appeared on degrowth as a strategy to confront environmental and social problems. We undertake a systematic review of their content, data and methods. This involves the use of computational linguistics to identify main topics investigated. Based on a sample of 561 studies we conclude that: (1) content covers 11 main topics; (2) the large majority (almost 90%) of studies are opinions rather than analysis; (3) few studies use quantitative or qualitative data, and even fewer ones use formal modelling; (4) the first and second type tend to include small samples or focus on non-representative cases; (5) most studies offer ad hoc and subjective policy advice, lacking policy evaluation and integration with insights from the literature on environmental/climate policies; (6) of the few studies on public support, a majority concludes that degrowth strategies and policies are socially-politically infeasible; (7) various studies represent a “reverse causality” confusion, i.e. use the term degrowth not for a deliberate strategy but to denote economic decline (in GDP terms) resulting from exogenous factors or public policies; (8) few studies adopt a system-wide perspective – instead most focus on small, local cases without a clear implication for the economy as a whole. We illustrate each of these findings for concrete studies.}
}
@article{COOPER19821,
title = {Energy conservation in buildings: Part 2-A commentary on British government thinking},
journal = {Applied Energy},
volume = {10},
number = {1},
pages = {1-45},
year = {1982},
issn = {0306-2619},
doi = {https://doi.org/10.1016/0306-2619(82)90058-7},
url = {https://www.sciencedirect.com/science/article/pii/0306261982900587},
author = {Ian Cooper},
abstract = {Like my previous paper in this journal this commentary is focused on government statements published during the period 1974 to 1979. It is intended as an introductory guide aimed at two overlapping audiences. First, it is addressed to those interested in the reasoning which lies behind the Government's technical arguments on energy conservation in buildings. Secondly, it is directed towards those who seek to understand the social implications and consequences of this area of government endeavour. Not all the statements examined in this commentary represent official expressions of government policy. Some, indeed, are prefaced in their originals by specific disclaimers to this effect. Rather, they should be read as examples of arguments voiced by a variety of individuals and groups who are capable of informing, influencing or making decisions that affect this field of government activity. It should not be supposed that the government statements brought together in this commentary are necessarily consistent or coherent. Instead, in some cases at least, they seem incompatible and may even be irreconcilable. But, given that the source material is drawn from a wide range of documents with a broad range of authors and was published over a number of years, the extent of their unanimity is remarkable. As an introductory guide, this commentary is not offered as exhaustive, as representative of all aspects or shades of government thinking on this subject. On the contrary, only statements published in documents emanating from, or associated with, the Department of Energy have, for the most part, been cited. For the sake of brevity, statements published by other government departments with responsibility for the conservation of energy in buildings—such as the Department of the Environment—have not been drawn upon.}
}
@article{LUNGU2008255,
title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
journal = {Economic Modelling},
volume = {25},
number = {2},
pages = {255-273},
year = {2008},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0264999307000818},
author = {L. Lungu and K.G.P. Matthews and A.P.L. Minford},
keywords = {Rational expectations, Partial current information, Signal extraction, Macroeconomic modelling},
abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.}
}
@article{INTRONE201479,
title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
journal = {Decision Support Systems},
volume = {64},
pages = {79-89},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001262},
author = {Joshua Introne and Luca Iandoli},
keywords = {Computer-supported argumentation, Evidence-based reasoning, Dempster–Shafer belief aggregation, Housing market prediction},
abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{CANADAS201687,
title = {Second graders articulating ideas about linear functional relationships},
journal = {The Journal of Mathematical Behavior},
volume = {41},
pages = {87-103},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300055},
author = {María C. Cañadas and Bárbara M. Brizuela and Maria Blanton},
keywords = {Quantities, Functional thinking, Early algebra, Elementary students},
abstract = {In this paper, we explore the ideas that second grade students articulate about functional relationships. We adopt a function-based approach to introduce elementary school children to algebraic content. We present results from a design-based research study carried out with 21 second-grade students (approximately 7 years of age). We focus on a lesson from our classroom teaching experiment in which the students were working on a problem that involved a linear functional relationship (y=2x). From the analysis of students’ written work and classroom video, we illustrate two different approaches that students adopt to express the relationship between two quantities. Students show fluency recontextualizing the problem posed, moving between extra-mathematical and intra-mathematical contexts.}
}
@article{BARELI2013472,
title = {Sketching profiles: Awareness to individual differences in sketching as a means of enhancing design solution development},
journal = {Design Studies},
volume = {34},
number = {4},
pages = {472-493},
year = {2013},
note = {Special Issue: Articulating Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000197},
author = {Shoshi Bar-Eli},
keywords = {design processes, design research, design behavior, problem solving, design tools},
abstract = {This paper focuses on the differences between interior design students' design processes as derived from an analysis of their sketching and design behavior. By implementing qualitative methodologies in the analysis of the sketches produced in the conceptual phase of the design process, the experiment allows identifying sketching characteristics and profiles. The motivation is to show that sketches can serve as a tool to differentiate between designers and recognize their personal approach and design strategies. The results point to three distinct sketching profiles that characterize designers' use of sketches as a tool for thinking and communicating ideas during their solution generation process. Awareness to differences between students' sketches and design behavior may support the development of pedagogical concepts, strategies and tools.}
}
@article{GONG2023105530,
title = {Continuous time causal structure induction with prevention and generation},
journal = {Cognition},
volume = {240},
pages = {105530},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105530},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001646},
author = {Tianwei Gong and Neil R. Bramley},
keywords = {Causal learning, Time, Prevention, Structure induction, Summary statistics},
abstract = {Most research into causal learning has focused on atemporal contingency data settings while fewer studies have examined learning and reasoning about systems exhibiting events that unfold in continuous time. Of these, none have yet explored learning about preventative causal influences. How do people use temporal information to infer which components of a causal system are generating or preventing activity of other components? In what ways do generative and preventative causes interact in shaping the behavior of causal mechanisms and their learnability? We explore human causal structure learning within a space of hypotheses that combine generative and preventative causal relationships. Participants observe the behavior of causal devices as they are perturbed by fixed interventions and subject to either regular or irregular spontaneous activations. We find that participants are capable learners in this setting, successfully identifying the large majority of generative, preventative and non-causal relationships but making certain attribution errors. We lay out a computational-level framework for normative inference in this setting and propose a family of more cognitively plausible algorithmic approximations. We find that participants’ judgment patterns can be both qualitatively and quantitatively captured by a model that approximates normative inference via a simulation and summary statistics scheme based on structurally local computation using temporally local evidence.}
}
@article{FLINT2025100948,
title = {Expansion of analytical methods in auditing education},
journal = {Journal of Accounting Education},
volume = {70},
pages = {100948},
year = {2025},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100948},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000642},
author = {Michele S. Flint},
keywords = {Auditing education, Analytical procedures, Data analytics, Beneish M−score, Altman Z-score, Sloan Accrual},
abstract = {Data analytics is changing the audit environment and carries significant implications for auditing education. Both international auditing education (International Accounting Education Standards Board (IAESB), 2019a; IAESB, 2019b) and U.S.-based regulatory bodies (American Institute of Certified Public Accountants (AICPA), 2021c; AICPA & National Association of State Boards of Accountancy (NASBA), 2021) have made efforts to address the growing expectations for auditing education, citing fraud risk and going concern risk. While auditing courses have progressed to include some computerized audit software for case studies, the study of analytical procedures has been limited to the application of basic financial ratios, trend analyses and common-size financial statements. Demands for advanced analytics place most emphasis on computerized query and computational methods; however, several advanced analytical models, namely the Altman Z-score, Beneish M−score and the Sloan Accrual formula provide opportunities for greater insight on specific audit risks and do not require advanced computer-based skills. The ability to link audit procedures, specifically analytical procedures to the audit objectives of financial risk and going concern risk strengthens the rationale for introduction of these advanced models within the context of auditing education. This paper discusses the inherent value in these analytical models, links them to audit objectives, proposes the inclusion of these three analytical models as a component of auditing education, and suggests that future study be undertaken to assess implementation and student learning. In addition, we recommend future study of other analytical models that may provide further insight for auditing students.}
}
@article{DASILVA2022402,
title = {A Predictive, Context-Dependent Stochastic Model for Engineering Applications},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {402-407},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.227},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002282},
author = {Márcio J. {da Silva} and Gustavo Künzel and Carlos E. Pereira},
keywords = {Data Mining, Predictive Situation, Context Testing, Industrial Alarm System, Recommendation Systems},
abstract = {This work explores the architecture of a context-dependent probabilistic model. We identify opportunities for providing reminders to operators in their environment as a means to address information overload. Hence, there is a need to represent a state of knowledge and help them stay vigilant during their jobs. Along with the architectural improvements, which further specialize information flows and develop a data-driven approach, continual learning techniques covered events in a probabilistic graphical model called Context-Dependent Recommendation Systems (CD-RS). We demonstrated, as a result, the use of statistical thinking and Design of Experiments (DoE), which are most clear in conducting a suitable experiment. Moreover, the validation of the model and experiments of the novel architecture based on the collected data from a real case study demonstrates the value of the proposed methods.}
}
@article{ELVEVAG2023115098,
title = {Reflections on measuring disordered thoughts as expressed via language},
journal = {Psychiatry Research},
volume = {322},
pages = {115098},
year = {2023},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0165178123000513},
author = {Brita Elvevåg},
keywords = {Assessment, Language, Memory},
abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.}
}
@article{STORLIE20091735,
title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
journal = {Reliability Engineering & System Safety},
volume = {94},
number = {11},
pages = {1735-1763},
year = {2009},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2009.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832009001112},
author = {Curtis B. Storlie and Laura P. Swiler and Jon C. Helton and Cedric J. Sallaberry},
keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.}
}
@article{ZHAO2025128946,
title = {The effect of the head number for multi-head self-attention in remaining useful life prediction of rolling bearing and interpretability},
journal = {Neurocomputing},
volume = {616},
pages = {128946},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128946},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401717X},
author = {Qiwu Zhao and Xiaoli Zhang and Fangzhen Wang and Panfeng Fan and Erick Mbeka},
keywords = {Remaining useful life prediction, Machine learning, Multi-head self-attention mechanism, Interpretability, Graph theory, Functional networks},
abstract = {As one of the machine learning (ML) models, the multi-head self-attention mechanism (MSM) is competent in encoding high-level feature representations, providing computing superiorities, and systematically processing sequences bypassing the recurrent neural networks (RNN) models. However, the model performance and computational results are affected by head number, and the lack of impact interpretability has become a primary obstacle due to the complex internal working mechanisms. Therefore, the effects of the head number of the MSM on the accuracy of the result, the robustness of the model, and computation efficiency are investigated in the remaining useful life (RUL) prediction of rolling bearings. The results show that the accuracy of prediction results will be reduced caused by large or few head numbers. In addition, the more heads are selected, the more robust and higher the predictive efficiency of the model is achieved. The above effects are explained relying on the visualization of the attention weight distribution and functional networks, which are constructed and solved by the equivalent fully connected layer and graph theory analysis, respectively. The model's attention coefficient distribution during training and prediction shows that the representative information will be captured inadequately if fewer heads are selected, which causes MSM to neglect to assign large attention coefficients to degraded information. On the contrary, representational degradation information and redundant information will be acquired by models with too many heads. MSM will be disturbed by this redundant information in the attention weight distribution, resulting in incorrect allocation of attention. Both of these cases will reduce the accuracy of the prediction results. In addition, the selection rules of the head number are established based on the feature complexity that is measured by the sample entropy (SamEn). The local range for head selection is also found based on the relationship between head number and feature complexity; The effects of the head number of the MSM on the robustness of the model and computation efficiency are explained by the changes in the three parameters (average of the clustering coefficients, global efficiency, and of the average shortest path length) of the graph, which is constructed after solving the function network. The research provides a reference for rolling bearing prediction with high computational accuracy, calculation efficiency, and strong robustness using MSM.}
}
@article{OH2023100602,
title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
journal = {International Journal of Child-Computer Interaction},
volume = {38},
pages = {100602},
year = {2023},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000399},
author = {HyunJoo Oh and Sherry Hsi and Noah Posner and Colin Dixon and Tymirra Smith and Tingyu Cheng},
keywords = {Paper-based computing, Codesign, Inclusive CS education, Learning through making},
abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.}
}
@article{RAHMAN20125541,
title = {Developing Mathematical Communication Skills of Engineering Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {46},
pages = {5541-5547},
year = {2012},
note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.06.472},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812022082},
author = {Roselainy Abdul Rahman and Yudariah Mohammad Yusof and Hamidreza Kashefi and Sabariah Baharun},
keywords = {Communication, Mathematical Thinking, Multivariable Calculus, Student's Obstacles},
abstract = {In Malaysia and also elsewhere in the world the demands for graduates who have employability skills such as ability to think critically, solve problems and can communicate are highly sought in the workplace. In the early 2006, the development of such skills was recognized as integral goals of undergraduate education at Universiti Teknologi Malaysia. Since then rigorous efforts have been made to inculcate these skills amongst the undergraduates. In this paper, we will share some of our experiences in coping with the challenges of changing our teaching practices to accommodate this quest though focusing on communication. For mathematics learning to occur, we believed that students should participate actively in the knowledge construction and be able to take charge of their own learning. Taking these aspects into consideration, we had developed a framework of active learning and used it to guide our instruction in engineering mathematics at UTM. Here we will discuss the strategies that we had designed and employed in engaging students with the subject matter as well as to initiate and support student's thinking and communication in the language of mathematics. Some student's responses that gave indications of their struggle, progress and growth encountered in the research implementation will also be presented.}
}
@article{MUSGRAVE2017137,
title = {Understanding and advancing graduate teaching assistants’ mathematical knowledge for teaching},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {137-149},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316302012},
author = {Stacy Musgrave and Marilyn P. Carlson},
keywords = {Graduate student teaching assistant, Mathematical meanings, Average rate of change, Precalculus},
abstract = {Graduate student teaching assistants (GTAs) usually teach introductory level courses at the undergraduate level. Since GTAs constitute the majority of future mathematics faculty, their image of effective teaching and preparedness to lead instructional improvements will impact future directions in undergraduate mathematics curriculum and instruction. In this paper, we argue for the need to support GTAs in improving their mathematical meanings of foundational ideas and their ability to support productive student thinking. By investigating GTAs’ meanings for average rate of change, a key content area in precalculus and calculus, we found evidence that even mathematically sophisticated GTAs possess impoverished meanings of this key idea. We argue for the need, and highlight one approach, for supporting GTAs to improve their understanding of foundational mathematical ideas and how these ideas are learned.}
}
@article{LI2023106299,
title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
journal = {Materials Today Communications},
volume = {35},
pages = {106299},
year = {2023},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2023.106299},
url = {https://www.sciencedirect.com/science/article/pii/S235249282300990X},
author = {Jiawei Li and Zhengxin Chen and Jiang Wu and Jia Lin and Ping He and Rui Zhu and Cheng Peng and Hai Zhang and Wenhao Li and Xu Fang and Hongtao Shen},
keywords = {Random forest model, Chemical formula, Components, Bandgap, Machine learning},
abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.}
}
@article{LI2023119775,
title = {Neural representations of self-generated thought during think-aloud fMRI},
journal = {NeuroImage},
volume = {265},
pages = {119775},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119775},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922008965},
author = {Hui-Xian Li and Bin Lu and Yu-Wei Wang and Xue-Ying Li and Xiao Chen and Chao-Gan Yan},
keywords = {Self-generated thoughts, Think-aloud fMRI, Natural language processing, Representational similarity analysis},
abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.}
}
@incollection{VALLERO2021601,
title = {Chapter 14 - The Future},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {601-613},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000040},
author = {Daniel A. Vallero},
keywords = {Precautionary principle, Evidence-based risk assessment, Exposome, Translational science, Scientific workflow, Ontologies, Resilience, Data-driven decision-making, Precision science, Life cycle risk assessment (LCRA)},
abstract = {Solving and preventing environmental problems will continue to rely on systems thinking that translates and combines data, information, knowledge, and wisdom from numerous scientific and other perspectives. The chapter provides insights into possible directions for environmental systems science, especially ways to address complexities at every scale from cellular to planetary. Environmental scientists and engineers will engage in precision science and customized approaches to reduce risks, improve reliability and resilience, and ensure sustainability.}
}
@article{GUSTAFSON199557,
title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
journal = {Mathematical and Computer Modelling},
volume = {22},
number = {9},
pages = {57-75},
year = {1995},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(95)00168-2},
url = {https://www.sciencedirect.com/science/article/pii/0895717795001682},
author = {K. Gustafson},
keywords = {Navier-Stokes equations, Driven cavity problem, Pressure boundary condition, Vortex shedding, Hopf bifurcation},
abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.}
}
@article{GALLISTEL2021104533,
title = {The physical basis of memory},
journal = {Cognition},
volume = {213},
pages = {104533},
year = {2021},
note = {Special Issue in Honour of Jacques Mehler, Cognition’s founding editor},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104533},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303528},
author = {C.R. Gallistel},
keywords = {Engram, Communication channel, Plastic synapse, Molecules},
abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.}
}
@article{ZOHDI20073927,
title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {37},
pages = {3927-3950},
year = {2007},
note = {Special Issue Honoring the 80th Birthday of Professor Ivo Babuška},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S004578250700117X},
author = {T.I. Zohdi},
keywords = {Particle–fluid interaction, Multiple fields, Iterative methods},
abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.}
}
@article{XU2025121541,
title = {Adaptive sequential three-way decisions for dynamic time warping},
journal = {Information Sciences},
volume = {690},
pages = {121541},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121541},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524014555},
author = {Jianfeng Xu and Ruihua Wang and Yuanjian Zhang and Weiping Ding},
keywords = {Time-series data, Sequential three-way decisions, Dynamic time warping, Uncertainty},
abstract = {Dynamic time warping (DTW) algorithm is widely used in diversified applications due to its excellent anti-deformation and anti-interference in measuring time-series based similarity. However, the high time complexity of DTW restrains the applicability of real-time case. The existing DTW acceleration studies suffer from a loss of accuracy. How to accelerate computation while maintaining satisfying computational accuracy remains challenging. Motivated by sequential three-way decisions, this paper develops a novel model with adaptive sequential three-way decisions for dynamic time warping (AS3-DTW). Firstly, we systematically summarize distance differences under the context of adjacent tripartite search spaces for DTW, and propose five patterns of granularity adjustments of the search spaces. Furthermore, we present the corresponding calculation method of DTW adjacent tripartite search spaces distances difference. Finally, we construct a novel dynamism on adaptively adjusting time warping by combining sequence-based multi-granularity with sequential three-way decisions. Experimental results show that AS3-DTW effectively achieves promising trade-off between computational speed and accuracy on multiple UCR datasets when compared with the state-of-the-art algorithms.}
}
@article{NOWACK2024459,
title = {Science and reflections: With some thoughts to young applied scientists and engineers},
journal = {Earthquake Science},
volume = {37},
number = {5},
pages = {459-493},
year = {2024},
issn = {1674-4519},
doi = {https://doi.org/10.1016/j.eqs.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1674451924000648},
author = {Robert L. Nowack},
keywords = {geophysics, computational and data science, applied science and engineering},
abstract = {I provide some science and reflections from my experiences working in geophysics, along with connections to computational and data sciences, including recent developments in machine learning. I highlight several individuals and groups who have influenced me, both through direct collaborations as well as from ideas and insights that I have learned from. While my reflections are rooted in geophysics, they should also be relevant to other computational scientific and engineering fields. I also provide some thoughts for young, applied scientists and engineers.}
}
@article{KHAZAEI2025115420,
title = {Renewable energy portfolio in Mexico for Industry 5.0 and SDGs: Hydrogen, wind, or solar?},
journal = {Renewable and Sustainable Energy Reviews},
volume = {213},
pages = {115420},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115420},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125000930},
author = {Moein Khazaei and Fatemeh Gholian-Jouybari and Mahdi {Davari Dolatabadi} and Aryan {Pourebrahimi Alamdari} and Hamidreza Eskandari and Mostafa Hajiaghaei-Keshteli},
keywords = {Renewable energies, Industry 5.0, Sustainable development goals, Portfolio selection, Nearshoring},
abstract = {Despite a surge in Foreign Direct Investment (FDI) in Mexico, like nearshoring, the slow growth in international investment in renewables challenges the country's progress in achieving Sustainable Development Goals (SDGs) related to clean energy. To the best of current knowledge, this research is one of the first to explore the integration of renewable energy (Green/Blue/Turquoise Hydrogen, Solar, and Wind plants) in Mexico, emphasizing a diverse portfolio of projects aligned with SDGs and Industry 5.0. While previous works have focused on the nexus between energy, Industry 4.0, and sustainability, the present study advances this discourse by incorporating Industry 5.0 principles and a comprehensive methodological approach. Through a comprehensive methodology involving Value-Focused Thinking (VFT), fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL), and multi-objective mathematical programming, the study identifies key criteria encompassing social, economic, environmental, and technological dimensions. The resulting criteria form a robust framework for evaluating project sustainability. The fuzzy DEMATEL analysis reveals intricate interrelations among criteria, emphasizing the need for balanced considerations. Results highlighted job creation, income equality, and microfinance support as key social considerations, while energy-related criteria emphasized sustainable practices. The proposed multi-objective programming model and COmbined COmpromise SOlution (COCOSO) method facilitated the selection of eight projects, with one project as the top-ranked option across various scoring strategies. Overall, this research provides a nuanced roadmap for effective decision-making in renewable energy projects, offering insights into project strengths, weaknesses, and potential areas for improvement.}
}
@incollection{2004201,
title = {Chapter 6 Alternatives to purely Lagrangian computations},
editor = {Jonas A. Zukas},
series = {Studies in Applied Mechanics},
publisher = {Elsevier},
volume = {49},
pages = {201-250},
year = {2004},
booktitle = {Introduction to Hydrocodes},
issn = {0922-5382},
doi = {https://doi.org/10.1016/S0922-5382(04)80007-2},
url = {https://www.sciencedirect.com/science/article/pii/S0922538204800072},
abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.}
}
@article{WOLFF2024893,
title = {The mediodorsal thalamus in executive control},
journal = {Neuron},
volume = {112},
number = {6},
pages = {893-908},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000023},
author = {Mathieu Wolff and Michael M. Halassa},
keywords = {thalamus, mediodorsal, prefrontal cortex, cognition, cognitive flexibility, computation, neural architectures},
abstract = {Summary
Executive control, the ability to organize thoughts and action plans in real time, is a defining feature of higher cognition. Classical theories have emphasized cortical contributions to this process, but recent studies have reinvigorated interest in the role of the thalamus. Although it is well established that local thalamic damage diminishes cognitive capacity, such observations have been difficult to inform functional models. Recent progress in experimental techniques is beginning to enrich our understanding of the anatomical, physiological, and computational substrates underlying thalamic engagement in executive control. In this review, we discuss this progress and particularly focus on the mediodorsal thalamus, which regulates the activity within and across frontal cortical areas. We end with a synthesis that highlights frontal thalamocortical interactions in cognitive computations and discusses its functional implications in normal and pathological conditions.}
}
@article{MERRITT2024103670,
title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
journal = {Energy Research & Social Science},
volume = {116},
pages = {103670},
year = {2024},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103670},
url = {https://www.sciencedirect.com/science/article/pii/S2214629624002615},
author = {Eileen G. Merritt and Andrea E. Weinberg and Candace Lapan and Sara E. Rimm-Kaufman},
keywords = {Environmental service-learning, Energy literacy, Elementary students, Science education, Randomized controlled trial},
abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.}
}
@article{HARDING2024295,
title = {A new predictive coding model for a more comprehensive account of delusions},
journal = {The Lancet Psychiatry},
volume = {11},
number = {4},
pages = {295-302},
year = {2024},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00411-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662300411X},
author = {Jessica Niamh Harding and Noham Wolpe and Stefan Peter Brugger and Victor Navarro and Christoph Teufel and Paul Charles Fletcher},
abstract = {Summary
Attempts to understand psychosis—the experience of profoundly altered perceptions and beliefs—raise questions about how the brain models the world. Standard predictive coding approaches suggest that it does so by minimising mismatches between incoming sensory evidence and predictions. By adjusting predictions, we converge iteratively on a best guess of the nature of the reality. Recent arguments have shown that a modified version of this framework—hybrid predictive coding—provides a better model of how healthy agents make inferences about external reality. We suggest that this more comprehensive model gives us a richer understanding of psychosis compared with standard predictive coding accounts. In this Personal View, we briefly describe the hybrid predictive coding model and show how it offers a more comprehensive account of the phenomenology of delusions, thereby providing a potentially powerful new framework for computational psychiatric approaches to psychosis. We also make suggestions for future work that could be important in formalising this novel perspective.}
}
@article{HIPOLITO2017432,
title = {Mind-life continuity: A qualitative study of conscious experience},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {432-444},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301165},
author = {Inês Hipólito and Jorge Martins},
keywords = {Conscious experience, Qualitative study, Meditation, , Mind-life continuity thesis},
abstract = {There are two fundamental models to understanding the phenomenon of natural life. One is the computational model, which is based on the symbolic thinking paradigm. The other is the biological organism model. The common difficulty attributed to these paradigms is that their reductive tools allow the phenomenological aspects of experience to remain hidden behind yes/no responses (behavioral tests), or brain ‘pictures’ (neuroimaging). Hence, one of the problems regards how to overcome methodological difficulties towards a non-reductive investigation of conscious experience. It is our aim in this paper to show how cooperation between Eastern and Western traditions may shed light for a non-reductive study of mind and life. This study focuses on the first-person experience associated with cognitive and mental events. We studied phenomenal data as a crucial fact for the domain of living beings, which, we expect, can provide the ground for a subsequent third-person study. The intervention with Jhana meditation, and its qualitative assessment, provided us with experiential profiles based upon subjects' evaluations of their own conscious experiences. The overall results should move towards an integrated or global perspective on mind where neither experience nor external mechanisms have the final word.}
}
@article{YAN2025109327,
title = {An approach to calculate conceptual distance across multi-granularity based on three-way partial order structure},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109327},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109327},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002147},
author = {Enliang Yan and Pengfei Zhang and Tianyong Hao and Tao Zhang and Jianping Yu and Yuncheng Jiang and Yuan Yang},
keywords = {Partial order structure, Concept-cognitive learning, Knowledge distance, Three-way decision, Granular computing, Concept graph},
abstract = {The computation of concept distances aids in understanding the interrelations among entities within knowledge graphs and uncovering implicit information. The existing studies predominantly focus on the conceptual distance of specific hierarchical levels without offering a unified framework for comprehensive exploration. To overcome the limitations of unidimensional approaches, this paper proposes a method for calculating concept distances at multiple granularities based on a three-way partial order structure. Specifically: (1) this study introduces a methodology for calculating inter-object similarity based on the three-way attribute partial order structure (APOS); (2) It proposes the application of the similarity matrix to delineate the structure of categories; (3) Based on the similarity matrix describing the three-way APOS of categories, we establish a novel method for calculating inter-category distance. The experiments on eight datasets demonstrate that this approach effectively differentiates various concepts and computes their distances. When applied to classification tasks, it exhibits outstanding performance.}
}
@article{KELLEY2021439,
title = {Applying Independent Core Observer Model Cognitive Architecture to a Collective Intelligence System},
journal = {Procedia Computer Science},
volume = {190},
pages = {439-449},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012977},
author = {David Kelley},
keywords = {Collective Intelligence Systems, Independent Core Observer Model, Artificial General Intelligence, mediated Artificial Superintelligence, Hive Mind, AGI, ICOM, mASI.},
abstract = {This paper shows how the Independent Core Observer Model (ICOM) Cognitive Architecture for Artificial General Intelligence (AGI) can be applied to building a collective intelligence system called a mediated Artificial Superintelligence (mASI). The details include breaking down the ICOM implementation in the form of the mASI system and the general performance of initial studies with the mASI. Details of the primary difference between the Independent Core Observer Model Cognitive Architecture and the mASI architecture variant include inserting humanity in the contextual engine components of ICOM, creating a type of collective intelligence. Humans can ‘mediate’ new system-generated thinking keeping the thought process accessible and slow enough for humans to oversee and understand. This also allows the modification of emotional valences of the thought process of the mASI system to help the system generate complex contextual models (knowledge graphs) of new ideas and which speeds up the learning process. With the humans acting as control rods in a reactor and emotional drivers, the mASI system maintains safety where the system would cease to function if humans walked away.}
}
@article{ACISECHE2023109386,
title = {A perspective on the sharing of docking data},
journal = {Data in Brief},
volume = {49},
pages = {109386},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004985},
author = {Samia Aci-Sèche and Stéphane Bourg and Pascal Bonnet and Joseph Rebehmed and Alexandre G. {de Brevern} and Julien Diharce},
keywords = {3D coordinates, Docking, Files, SDF, Sharing, FAIR principles},
abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.}
}

@incollection{ERICSSON200112256,
title = {Protocol Analysis in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12256-12262},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01598-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015989},
author = {K.A. Ericsson},
abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.}
}
@incollection{DIX2003381,
title = {CHAPTER 14 - Upside-Down ∀s and Algorithms—Computational Formalisms and Theory},
editor = {John M. Carroll},
booktitle = {HCI Models, Theories, and Frameworks},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {381-429},
year = {2003},
series = {Interactive Technologies},
isbn = {978-1-55860-808-5},
doi = {https://doi.org/10.1016/B978-155860808-5/50014-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608085500149},
author = {Alan Dix}
}
@article{CAI2004135,
title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {2},
pages = {135-167},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000124},
author = {Jinfa Cai},
abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.}
}
@article{LOTURCO2022104059,
title = {The knowledge and skill content of production complexity},
journal = {Research Policy},
volume = {51},
number = {8},
pages = {104059},
year = {2022},
note = {Special Issue on Economic Complexity},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2020.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0048733320301372},
author = {Alessia {Lo Turco} and Daniela Maggioni},
keywords = {Occupational complexity, Services, Regional growth, STEM},
abstract = {In this paper we investigate the labour content of complex products. By exploiting O*NET information on the skill and knowledge required by occupations, we find that the product complexity measure suggested by Hausmann and Hidalgo (2009) is highly intensive in STEM knowledge and in Science, Mathematics and Critical Thinking skill requirements. We then propose a new measure of occupational complexity based on these occupational features. Among other advantages, this indicator has the merit to measure complexity for service industries that, so far, has never been measured. In an empirical model of the growth of USA Metropolitan Areas (MSAs), we find that MSAs whose initial industrial structure embeds a higher level of occupational complexity experience higher real per capita GDP growth over the 2001–2017 period. The occupational complexity measure is a stronger predictor of growth than other metrics of industries’ occupational and task content as well as compared to indicators of local occupational and industrial composition. When we separately compute occupational complexity of service and manufacturing industries and delve into their specific role for long run growth, we find a prominent role of the occupation complexity embedded in local services with respect to the one embedded in local manufacturing. Our baseline evidence is corroborated in the context of the NUTS3 regions of France over the period 2010–2017.}
}
@article{BERNALMANRIQUE202086,
title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
journal = {Journal of Contextual Behavioral Science},
volume = {17},
pages = {86-94},
year = {2020},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144720301551},
author = {Koryn N. Bernal-Manrique and María B. García-Martín and Francisco J. Ruiz},
keywords = {Acceptance and commitment therapy, Interpersonal skills, Emotional disorders, Psychological flexibility, Repetitive negative thinking},
abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{AMADEI2020120149,
title = {Revisiting positive peace using systems tools},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120149},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120149},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309756},
author = {Bernard Amadei},
keywords = {Complex systems, Systems thinking, System dynamics, Cross-impact analysis, Network analysis, Positive peace, Peace geometry},
abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.}
}
@article{HERNIMAN2021373,
title = {Interrelationships between depressive symptoms and positive and negative symptoms of recent onset schizophrenia spectrum disorders: A network analytical approach},
journal = {Journal of Psychiatric Research},
volume = {140},
pages = {373-380},
year = {2021},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2021.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0022395621003071},
author = {Sarah E. Herniman and Lisa J. Phillips and Stephen J. Wood and Sue M. Cotton and Edith J. Liemburg and Kelly A. Allott},
keywords = {Comorbidity, Co-occurrence, Early psychosis, Depression, Affect},
abstract = {Objective
There is a need to better understand the interrelationships between positive and negative symptoms of recent-onset schizophrenia spectrum disorders (SSD) and co-occurring depressive symptoms. Aims were to determine: (1) whether depressive symptoms are best conceptualised as distinct from, or intrinsic to, positive and negative symptoms; and (2) bridging symptoms.
Methods
Network analysis was applied to data from 198 individuals with depressive and psychotic symptoms in SSD from the Psychosis Recent Onset GRoningen Survey (PROGR-S). Measures were: Montgomery–Åsberg Depression Rating Scale and Positive and Negative Syndrome Scale.
Results
Positive symptoms were just as likely to be associated with depressive and negative symptoms, and had more strong associations with depressive than negative symptoms. Negative symptoms were more likely to be associated with depressive than positive symptoms, and had more strong associations with depressive than positive symptoms. Suspiciousness and stereotyped thinking bridged between positive and depressive symptoms, and apparent sadness and lassitude between negative and depressive symptoms.
Conclusions
Depressive symptoms might be best conceptualised as intrinsic to positive and negative symptoms pertaining to deficits in motivation and interest in the psychotic phase of SSD. Treatments targeting bridges between depressive and positive symptoms, and depressive and such negative symptoms, might prevent or improve co-occurring depressive symptoms, or vice-versa, in the psychotic phase of SSD.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{HEILMAN20041,
title = {Computational models of epileptiform activity in single neurons},
journal = {Biosystems},
volume = {78},
number = {1},
pages = {1-21},
year = {2004},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2004.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264704000978},
author = {Avram D. Heilman and James Quattrochi},
keywords = {Paroxysmal depolarizing shifts (PDS), Sustained depolarizations (SD), Hippocampus, Autaptic CA1/CA3 pyramidal neuron, Voltage-gated Ca channels, Ca-dependent K channels},
abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.}
}
@article{GREGORY198254,
title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
journal = {Design Studies},
volume = {3},
number = {1},
pages = {54},
year = {1982},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(82)90084-9},
url = {https://www.sciencedirect.com/science/article/pii/0142694X82900849},
author = {Sydney Gregory}
}
@article{LI202231,
title = {Application Analysis of Artificial Intelligent Neural Network Based on Intelligent Diagnosis},
journal = {Procedia Computer Science},
volume = {208},
pages = {31-35},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014478},
author = {Yukun Li},
keywords = {Intelligent diagnosis, artificial intelligence network, automobile fault diagnosis, the neural network},
abstract = {In recent years, the continuous development of computer science and AI technology makes the application prospect of artificial intelligence in fault diagnosis emerge. As a simulation technology of human thinking pattern, intelligent diagnosis technology can check and manage the monitoring target in real time to ensure the accuracy of data information. This paper introduces the basic principles of key artificial intelligence technologies in the field of sports, such as convolutional neural network, object detection, object tracking and action recognition. Then it analyzes the application status of intelligent diagnosis technology and artificial intelligence network under intelligent diagnosis, and puts forward the application of artificial intelligence neural network in automobile fault diagnosis based on examples. In the construction of the neural network system, the real-time collection of vehicle operation data can be analyzed, once the fault is found, the driver can be notified in time to avoid safety accidents. The author summarizes the existing research results on the application of artificial intelligence algorithm in intelligent diagnosis, in order to provide help for the subsequent research.}
}
@incollection{MILLER2023125,
title = {Chapter 7 - Graduate and postgraduate education at a crossroads},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {125-155},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000092},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Pharmaceutical, Pharmacology, Postdoctoral, Postgraduate, Master’s degree, Doctorate},
abstract = {In this chapter, we introduce a proverbial crossroads in graduate and postgraduate education and jobs. We use medicinal chemistry as a core example for many topics, representative of what could also be said about pharmacology and other critical disciplines involved in drug discovery. Many factors are at play today for drug hunters, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has in the past been called “the gathering storm.” Core drug discovery disciplines are discussed, along with the importance of diversity and interdisciplinary skills and the value of academia-industry symbiosis. Challenges in making sure we continue to educate and engage the best and the brightest to tackle important biomedical problems are considered, especially in the context of personalized medicine and its interfaces with big data, bioinformatics, pharmacogenomics, and more. Finally, we scratch the surface on how to navigate graduate school, postdocs, employers, and careers.}
}
@article{KONG20241462,
title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
journal = {Chinese Journal of Physics},
volume = {89},
pages = {1462-1473},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324001655},
author = {Otto C.W. Kong},
keywords = {Quantum information, Quantum locality, Deutsch–Hayden descriptors, Noncommutative values of observables},
abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.}
}
@article{BADIA2024101049,
title = {Analysing the radiation reliability, performance and energy consumption of low-power SoC through heterogeneous parallelism},
journal = {Sustainable Computing: Informatics and Systems},
volume = {44},
pages = {101049},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2024.101049},
url = {https://www.sciencedirect.com/science/article/pii/S2210537924000945},
author = {Jose M. Badia and German Leon and Mario Garcia-Valderas and Jose A. Belloch and Almudena Lindoso and Luis Entrena},
keywords = {Heterogeneous parallelism, System-on-Chip, Fault tolerance, Energy consumption, Neutron irradiation},
abstract = {This study focuses on the low-power Tegra X1 System-on-Chip (SoC) from the Jetson Nano Developer Kit, which is increasingly used in various environments and tasks. As these SoCs grow in prevalence, it becomes crucial to analyse their computational performance, energy consumption, and reliability, especially for safety-critical applications. A key factor examined in this paper is the SoC’s neutron radiation tolerance. This is explored by subjecting a parallel version of matrix multiplication, which has been offloaded to various hardware components via OpenMP, to neutron irradiation. Through this approach, this researcher establishes a correlation between the SoC’s reliability and its computational and energy performance. The analysis enables the identification of an optimal workload distribution strategy, considering factors such as execution time, energy efficiency, and system reliability. Experimental results reveal that, while the GPU executes matrix multiplication tasks more rapidly and efficiently than the CPU, using both components only marginally reduces execution time. Interestingly, GPU usage significantly increases the SoC’s critical section, leading to an escalated error rate for both Detected Unrecoverable Errors (DUE) and Silent Data Corruptions (SDC), with the CPU showing a higher average number of affected elements per SDC.}
}
@article{M2023120604,
title = {Design of a Cognitive Knowledge Representation Model to Assess the Reasoning Levels of Primary School Children},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120604},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120604},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011065},
author = {Srivani M. and Abirami Murugappan},
keywords = {Customized AI based teaching, Cognitive performance test, Reasoning coefficient, Cognition level, Knowledge representation, Cognitive metrics},
abstract = {Background and aim:
In recent days, the research on student’s intelligence level modelling is a challenging Artificial Intelligence (AI) task, which gains more attraction because it provides actionable insights to the tutor by analysing the intelligence level of the learners. Each learner’s knowledge, comprehension, and intellectual capacities are unique. It is critical to identify these capacities and provide learners, particularly slow learners, with the necessary knowledge. Cognitive Performance Test (CPT) is an essential component for assessing the knowledge level of students. The reasoning level or coefficient deals with the analysis of the thinking capability in a logical way. It also reflects the child’s learning potential. The main aim of the proposed system is to design a Cognitive Knowledge Representation Model (CKRM), which fuses Cognitive Performance Metrics (CPM) calculation and Reasoning Coefficient Calculation (RCC) algorithms to assess the student’s intelligence level. The result of the proposed system is stratification of students to three different ranges of reasoning coefficient.
Methods:
The CKRM consists of the following phases: data collection, statistical Exploratory Data Analysis (EDA), model building and analysis, which involve the assessment of the knowledge level using CPT and calculation of reasoning coefficient using First Order Logic (FOL), and finally model evaluation using cognitive evaluation metrics. CPM and RCC algorithms have been proposed in this paper to calculate the student’s reasoning coefficient by using the forward chaining FOL inference engine. The dataset is a real time data which consists of the academic and cognitive performance details of school students from classes 1 to 6 for the year 2019 to 2020. The academic data are collected from the Educational Management Information System (EMIS) maintained by the school. The cognitive performance data are collected by conducting the tests for the students using the memory training application called Lumosity.
Results:
The proposed system’s performance is evaluated using ten Machine Learning (ML) algorithms in which the Quadratic Discriminant Analysis achieved an accuracy of 0.97 for classes 1, 2, and 3. For classes 4, 5, and 6, nearly twelve ML algorithms are evaluated in which Random Forest (RF) Classifier achieved an accuracy of 0.98. Six math expert committee teachers concluded that the reasoning coefficient value was acceptable with an average accuracy of 0.92 for classes 1, 2, 3 and 0.9 for classes 4, 5, 6. In comparison to the pre-existing models employed in the prior research, it was determined that the created CKRM (academic and cognitive) was superior. The cognitive metrics such as taskability, Response Time (RT), knowledge capacity and utilization has also been evaluated. The average values of taskability, RT, knowledge capacity and knowledge utilization are 0.85, 0.81, 0.55, and 0.44.
Conclusion:
The ultimate goal is to make customized teaching easier; hence, this article involves determining a student’s cognitive level by estimating their reasoning coefficient. The suggested approach analyses and categorizes students’ cognitive abilities, such as memory, reasoning, problem solving, thinking, and logical reasoning, using three different reasoning coefficients. This approach assists teachers in determining the degree of intelligence of their students.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{LIU2024105391,
title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
journal = {Automation in Construction},
volume = {162},
pages = {105391},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001274},
author = {Yuming Liu and Aidi Hizami Bin Alias and Nuzul Azam Haron and Nabilah Abu Bakar and Hao Wang},
keywords = {Construction robotics, BERTopic model, BIM, Human–robot collaboration, Deep reinforcement learning, Dual-track quantitative analysis},
abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.}
}
@article{LI2021104369,
title = {Elementary effects analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of social distancing and mask usage},
journal = {Computers in Biology and Medicine},
volume = {134},
pages = {104369},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104369},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001633},
author = {Kelvin K.F. Li and Stephen A. Jarvis and Fayyaz Minhas},
keywords = {COVID-19, Agent-based modelling, Coronavirus, Simulation, SARS-COV-2, netlogo, Python, Epidemiology, Survival, Infectious diseases, VIRUS, Stochastic processes, Stochasticity, Social distancing, Masks, Isolation, Lockdown},
abstract = {COVID-19 was declared a pandemic by the World Health Organisation (WHO) on March 11th, 2020. With half of the world's countries in lockdown as of April due to this pandemic, monitoring and understanding the spread of the virus and infection rates and how these factors relate to behavioural and societal parameters is crucial for developing control strategies. This paper aims to investigate the effectiveness of masks, social distancing, lockdown and self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings from an agent-based simulation modelling showed that whilst requiring a lockdown is widely believed to be the most efficient method to quickly reduce infection numbers, the practice of social distancing and the usage of surgical masks can potentially be more effective than requiring a lockdown. Our multivariate analysis of simulation results using the Morris Elementary Effects Method suggests that if a sufficient proportion of the population uses surgical masks and follows social distancing regulations, then SARS-CoV-2 infections can be controlled without requiring a lockdown.}
}
@incollection{ALISEDA2007431,
title = { - Logical, Historical and Computational Approaches},
editor = {Theo A.F. Kuipers},
booktitle = {General Philosophy of Science},
publisher = {North-Holland},
address = {Amsterdam},
pages = {431-513},
year = {2007},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-044451548-3/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444515483500100},
author = {Atocha Aliseda and Donald Gillies},
abstract = {Publisher Summary
This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the ﬁrst time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important inﬂuences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientiﬁc revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the ﬁrst time.}
}
@article{FATTAHITABASI20221151,
title = {Design and mechanism of building responsive skins: State-of-the-art and systematic analysis},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {6},
pages = {1151-1176},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095263522000565},
author = {Saba {Fattahi Tabasi} and Saeed Banihashemi},
keywords = {Responsive skin, Architectural design, Mechanism design},
abstract = {The demand to satisfy environmental and economic performance requirements of buildings highlights the application of the responsive skin facades in offering superior performance, as compared to conventional façades. With this respect, responsive skins have become a growing field of research during the recent decade while a thorough review of studies investigating their design and technology aspects is still missing. To fill the identified gap, this study aims to present a systematic literature review and state of the art in an untouched research area of the responsive skins, integrated with their geometric and mechanism design approaches. To this end, a total of 89 studies, collected from two major bibliographic databases of Scopus and Google Scholar from the first of 2010 to the mid of 2021, were reviewed and several classifications and analyses on the associated design thinking, skin systems and responsive mechanisms were presented. The gap analysis of the findings indicates that the lack of controllable substitution design for mechanical skins is one of the reasons preventing the application of responsive skins in construction industry. Furthermore, the gap between simulation and constructability and the relationship between the designed skin geometry with climatic analysis and performance provide basis for future studies.}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{MASHALEH20242245,
title = {IoT Smart Devices Risk Assessment Model Using Fuzzy Logic and PSO},
journal = {Computers, Materials and Continua},
volume = {78},
number = {2},
pages = {2245-2267},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.047323},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001267},
author = {Ashraf S. Mashaleh and Noor Farizah Binti Ibrahim and Mohammad Alauthman and Mohammad Almseidin and Amjad Gawanmeh},
keywords = {IoT botnet detection, risk assessment, fuzzy logic, particle swarm optimization (PSO), cybersecurity, interconnected devices},
abstract = {Increasing Internet of Things (IoT) device connectivity makes botnet attacks more dangerous, carrying catastrophic hazards. As IoT botnets evolve, their dynamic and multifaceted nature hampers conventional detection methods. This paper proposes a risk assessment framework based on fuzzy logic and Particle Swarm Optimization (PSO) to address the risks associated with IoT botnets. Fuzzy logic addresses IoT threat uncertainties and ambiguities methodically. Fuzzy component settings are optimized using PSO to improve accuracy. The methodology allows for more complex thinking by transitioning from binary to continuous assessment. Instead of expert inputs, PSO data-driven tunes rules and membership functions. This study presents a complete IoT botnet risk assessment system. The methodology helps security teams allocate resources by categorizing threats as high, medium, or low severity. This study shows how CICIoT2023 can assess cyber risks. Our research has implications beyond detection, as it provides a proactive approach to risk management and promotes the development of more secure IoT environments.}
}
@article{COELHOLOPES2023103555,
title = {The structure of a strategic crisis management model: The context and characteristics of a brazilian community college},
journal = {International Journal of Disaster Risk Reduction},
volume = {87},
pages = {103555},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.103555},
url = {https://www.sciencedirect.com/science/article/pii/S2212420923000353},
author = {Gisele Silveira {Coelho Lopes} and Carlos Ricardo Rossetto and Micheline {Ramos de Oliveira} and Jorge Oneide Sausen and Rudimar {Antunes da Rocha}},
keywords = {Crisis management, Organizational strategy, Community university},
abstract = {This study presents the structure of a strategic crisis management model, considering the context and characteristics of a Brazilian community college. Strategic crisis management associated with coordination and control mechanisms theoretically underpinned the problem under study. It is a single case study, with grounded theory as the strategy for data treatment and content analysis to interpret and present the findings. In the model, the strategic and tactical stages systematized the dynamics of crisis management in a coordinated manner. Strategic crisis management was considered a continuous process rather than a strictly punctual one. The dynamism of the model's operationalization considered some premises that guided the behavior of the leadership and the crisis management team: i) pragmatic strategic thinking shaped by rationality; ii) quick responses in facing the crisis; iii) simplicity in actions; iv) reversible decisions susceptible to flexibilization; v) creativity and boldness to innovate and set new standards; v) collaboration for common causes.}
}
@article{LI2025e42756,
title = {Classifying breast intraductal proliferative lesions via a knowledge distillation framework using convolutional neural network-based nuclei-segmentation-assisted classification (KDCNN-NSAC)},
journal = {Heliyon},
pages = {e42756},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42756},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025011375},
author = {Xiangmin Li and Jiamei Chen and Bo Luo and Minyan Xia and Xu Zhang and Hangjia Zhu and Yutian Zhang-Cai and Yongshun Chen and Yang Yang and Yaofeng Wen},
keywords = {Breast intraductal proliferative lesions, Breast cancer, Knowledge distillation, Convolutional neural network, Nuclei segmentation, Classification},
abstract = {ABSTRACT
Background and objective
Diagnosis of breast intraductal proliferative lesions (BIDPLs) in hematoxylin-eosin (HE) images remains a time-consuming and intractable topic because of subjective processes and subtle morphological differences. Convolutional neural networks (CNNs) show great potential for providing objective analysis strategies for HE images. In this study, we proposed a novel knowledge distillation (KD) framework using CNN-based nuclei segmentation-assisted classification (KDCNN-NSAC).
Methods
The diagnosis of BIDPLs is treated as multiple class classification tasks in the BReAst Carcinoma Subtyping dataset. The KDCNN-NSAC fully leveraged the epithelial and stromal nuclei-level features in training phases and performed region-of-interest (ROI)-level classifications in predicting phases. Then, the whole slide image (WSI) was diagnosed based on the risk ratings of the ROIs within it, instead of processing a WSI.
Results
The principal results showed that in ROI-level classifications, KDCNN-NSAC outperformed the state-of-the-art methods for 7-class classification with an average F1 score of 63.26% and achieves F1 score of 98.36% and 94.21%, respectively, in distinguishing BIDPLs from invasive cancer and normal tissue. The WSI-level predictions obtained a high degree of consistency with the pathologists’ annotation (kappa value of 0.88). Ablation experiments showed that nuclei segmentation and classification components improve the performance of the baseline model in KDCNN-NSAC by 3%.
Conclusions
The KDCNN-NSAC makes the model focus on important cellular information and predicts the WSI in accordance with the pathologists’ diagnostic thinking, thus improving model explainability. Moreover, the introduce of KDCNN-NSAC will help achieve superior performance in diagnosing BIDPLs.}
}
@article{PARTTO2012442,
title = {Explaining failures in innovative thought processes in engineering design},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {442-449},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009330},
author = {Minna Pärttö and Pertti Saariluoma},
keywords = {Microinnovation processes, engineering design, thought errors, thought failures},
abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.}
}
@article{MIASNIKOVA202126,
title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
journal = {Neuroscience Research},
volume = {166},
pages = {26-33},
year = {2021},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2020.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010219305772},
author = {Aleksandra Miasnikova and Gleb Perevoznyuk and Olga Martynova and Mikhail Baklushev},
keywords = {Abstract reasoning, Salience, Phase synchronization, Cross-frequency coupling, Phase-to-phase coupling, EEG},
abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.}
}
@article{AIGBAVBOA20173003,
title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
journal = {Energy Procedia},
volume = {105},
pages = {3003-3010},
year = {2017},
note = {8th International Conference on Applied Energy, ICAE2016, 8-11 October 2016, Beijing, China},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.743},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217308068},
author = {Clinton Aigbavboa and Ifije Ohiomah and Thulisile Zwane},
keywords = {Climate change, sustainable thinking, sustainable construction practices, South Africa},
abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{GAO20241233,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
volume = {92},
pages = {1233-1239},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.}
}
@incollection{PAUL2005431,
title = {Chapter 15 - Models of Computation for Systems-on-Chips},
editor = {Ahmed Amine Jerraya and Wayne Wolf},
booktitle = {Multiprocessor Systems-on-Chips},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {431-463},
year = {2005},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012385251-9/50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852519500311},
author = {JoAnn M. Paul and Donald E. Thomas},
abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.}
}
@article{CASTELLO202354,
title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {54-62},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000210},
author = {E. Castelló and C. Santiviago and J. Ferreira and R. Coniglio and E. Budelli and V. Larnaudie and M. Passeggi and I. López},
keywords = {Engineering education, Professional skills, Unconventional laboratory practice, Industrial Internships, Autonomous learning},
abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.}
}
@article{CHEN1998475,
title = {A computationally attractive nonlinear predictive control scheme with guaranteed stability for stable systems},
journal = {Journal of Process Control},
volume = {8},
number = {5},
pages = {475-485},
year = {1998},
note = {ADCHEM '97 IFAC Symposium: Advanced Control of Chemical Processes},
issn = {0959-1524},
doi = {https://doi.org/10.1016/S0959-1524(98)00021-3},
url = {https://www.sciencedirect.com/science/article/pii/S0959152498000213},
author = {H. Chen and F. Allgöwer},
keywords = {nonlinear predictive control, constraints, stability, terminal conditions},
abstract = {We introduce in this paper a nonlinear model predictive control scheme for open-loop stable systems subject to input and state constraints. Closed-loop stability is guaranteed by an appropriate choice of the finite prediction horizon, independent of the specification of the desired control performance. In addition, this control scheme is likely to allow ‘real time’ implementation, because of its computational attractiveness. The theoretical results are demonstrated and discussed with a CSTR control application.}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{HO1993567,
title = {Recent Applications of Symbolic Computation in Control System Design},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 2},
pages = {567-570},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 2 Robust Control, Design and Software, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49006-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017490069},
author = {D.W.C. Ho and J. Lam and S.K. Tin and C.Y. Han},
keywords = {Symbolic Computation, Computer-aided Control System Design},
abstract = {In this paper we describe several recent applications of symbolic computation to control system design based on MACSYMA. These include routines to calculate the transfer function of a control system in block diagram representation and to compute and simplify state space realizations of multivariable control systems. The programs provide a quick way to formulate design problem and automate the calculation in the initial stage of a control system design process. An example on the application of these routines in the setting up of generalized plant state space realization for use in H∞/H2 optimial control is provided}
}
@incollection{RUBIN2023125,
title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {125-145},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000062},
author = {Matthew Rubin and Ella Miron-Spektor and Joshua Keller},
keywords = {Creativity, Innovation, Paradox, Mindset, Paradoxical leadership, Culture},
abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.}
}
@incollection{JAIN2015181,
title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
editor = {Sarah A. Woodson and Frédéric H.T. Allain},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {558},
pages = {181-212},
year = {2015},
booktitle = {Structures of Large RNA Molecules and Their Complexes},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915000208},
author = {Swati Jain and David C. Richardson and Jane S. Richardson},
keywords = {RNA crystallography, RNA backbone conformers, Ribose pucker, Clash correction, MolProbity, PHENIX, ERRASER, wwPDB validation},
abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{ALKABI202368,
title = {Proposed artificial intelligence algorithm and deep learning techniques for development of higher education},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {68-73},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000039},
author = {Amin {Al Ka'bi}},
keywords = {Artificial intelligence (AI), Communication systems, Higher education, Neural networks, Attention process, Deep learning},
abstract = {Artificial intelligence (AI) has been increasingly impacting various aspects of our daily lives, including education. With the rise of digital technologies, higher education has also been experiencing a transformation, and AI has been playing a crucial role in this transformation. The application of AI in higher education has been rapidly increasing, with a focus on improving student engagement, increasing efficiency, and enhancing the learning experience. The use of AI in higher education is not without its challenges and ethical considerations. One of the biggest challenges is ensuring the accuracy and fairness of AI algorithms, as well as avoiding potential biases. In addition, there are concerns about the privacy of student data, as well as the potential for AI to replace human instructors and support staff. Another challenge is ensuring that AI is used in a way that supports the overall goals of higher education, such as promoting critical thinking and creativity, rather than just being used as a tool for automating tasks and increasing efficiency. In this article, we will discuss the various ways in which AI is being applied in higher education where a proposed model for improving the cognitive capability of students is proposed and compared to other existing algorithms. It will be shown that the proposed model shows better performance compared to other models.}
}
@article{JORAJURIA2022664,
title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
journal = {Neurocomputing},
volume = {492},
pages = {664-675},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018956},
author = {Tania Jorajuría and Mina {Jamshidi Idaji} and Zafer İşcan and Marisol Gómez and Vadim V. Nikulin and Carmen Vidaurre},
keywords = {Brain-computer interface, Steady-state visual evoked potential, Spatio-spectral decomposition, Higher order discriminant analysis, Analytical regularization, Tensor-based feature reduction},
abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.}
}
@article{KLIMUSOVA2016652,
title = {Psychometric Properties of the Learning Potential Test},
journal = {Procedia - Social and Behavioral Sciences},
volume = {217},
pages = {652-656},
year = {2016},
note = {Future Academy Multidisciplinary Conference “ICEEPSY & CPSYC & icPSIRS & BE-ci” 13–17 October 2015 Istanbul},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2016.02.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877042816001142},
author = {Helena Klimusová and Petr Květon},
keywords = {psychometrics, admission test},
abstract = {The use of cognitive ability tests to help select highly performing students is becoming a standard in most major Czech universities. Such tests need to show good psychometric properties. To highlight the importance of these properties to the process of selection, this study explores the psychometric properties of the Learning Potential test, which is used as a selection criterion within the admission procedure at a major Czech university. This study's objective is to assess the psychometric properties of the Learning Potential Test and provide an insight into its structure. The Cronbach's alpha were computed to assess the internal consistency of the test. The structure of the items was explored by the factor analysis methods. Factor analysis indicated the anticipated structure of the test with two major factors - critical thinking/verbal reasoning abilities and numerical/spatial/analytical abilities. Since the role of admission tests in the process of selection new university students is crucial, it is essential to periodically reassess its psychometric characteristics to ensure that our test remain relevant and applicable.}
}
@article{CHAN2022102109,
title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
journal = {Learning and Individual Differences},
volume = {93},
pages = {102109},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102109},
url = {https://www.sciencedirect.com/science/article/pii/S1041608021001461},
author = {Jenny Yun-Chen Chan and Erin R. Ottmar and Ji-Eun Lee},
keywords = {Pause time, Strategy efficiency, Algebra problem-solving, Online learning environment, Metacognitive skills},
abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.}
}
@article{CARLISLE1996248,
title = {Software Caching and Computation Migration in Olden},
journal = {Journal of Parallel and Distributed Computing},
volume = {38},
number = {2},
pages = {248-255},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0145},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901458},
author = {Martin C. Carlisle and Anne Rogers},
abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.}
}
@article{KING1980313,
title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
journal = {Artificial Intelligence},
volume = {13},
number = {3},
pages = {313-322},
year = {1980},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(80)90005-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370280900053},
author = {Margaret King}
}
@article{ZORAN2025101135,
title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
journal = {International Journal of Gastronomy and Food Science},
volume = {39},
pages = {101135},
year = {2025},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2025.101135},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X25000368},
author = {Amit Raphael Zoran},
abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.}
}
@article{HAGSTROM1998385,
title = {Experiments with approximate radiation boundary conditions for computational aeroacoustics},
journal = {Applied Numerical Mathematics},
volume = {27},
number = {4},
pages = {385-402},
year = {1998},
note = {Special Issue on Absorbing Boundary Conditions},
issn = {0168-9274},
doi = {https://doi.org/10.1016/S0168-9274(98)00021-X},
url = {https://www.sciencedirect.com/science/article/pii/S016892749800021X},
author = {Thomas Hagstrom and John Goodrich},
keywords = {Nonreflecting Boundary Conditions, Aeroacoustics},
abstract = {We present a series of numerical experiments on the accuracy of approximate radiation boundary conditions for computational aeroacoustics based on Padé approximants. Our test problem is described by an infinite periodic array of pressure pulses, for which we can independently evaluate the exact solution by numerical quadrature. It is demonstrated that acceptable long time accuracy can be achieved, but only if conditions of high order are employed. As predicted by theory, the order required for a given accuracy is proportional to the time of the simulation.}
}
@article{TARMIZI2010384,
title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {384-392},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810021592},
author = {Rohani Ahmad Tarmizi and Sahar Bayat},
keywords = {Problem-based learning, E-learning, Statistic learning performance, Mental load},
abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{CUI2024124662,
title = {Cooperative interference to achieve interval many-objective evolutionary algorithm for association privacy secure computing migration},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124662},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124662},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401529X},
author = {Zhihua Cui and Zhenyu Shi and Qi Li and Tianhao Zhao and Wensheng Zhang and Jinjun Chen},
keywords = {Mobile edge computing, Computing migration, Physical layer security(PLS), Interval many-objective optimization, Evolutionary algorithm},
abstract = {In this paper, we study secure computing migration scenarios in uncertain environments with the presence of multiple malicious eavesdroppers (MEs). Specifically, when edge servers (ESs) execute tasks delivered by smart devices (SDs), SDs may move beyond the coverage of ESs, and computing migration (CM) of unfinished tasks is required to ensure service continuity. There is a risk of privacy leakage during task migration, and MEs use colluding eavesdropping to eavesdrop on the migrated tasks, and we consider eavesdropping on the associated tasks through data sharing among MEs to improve the eavesdropping efficiency. For eavesdropping in MEs, we achieve eavesdropping strikes using cooperative interference by jammers, which benefit by providing jamming services. In addition, uncertain computational scenarios directly affect the efficiency of task execution, and we consider the uncertainty factor in the malicious eavesdropping environment. To this end, this paper proposes the secure computational migration of associative privacy in uncertain environments (SCMAPUE) model, which transforms uncertainties into interval parameters, and optimizes the five objectives of migration delay, maximum completion time, energy consumption, load balancing and migration reliability to achieve efficient task execution and reliable migration. Aiming at the model characteristics, this paper designs an interval many-objective evolutionary algorithm for reliable migration (IMaOEA-RM), which employs a condition-based interval confidence strategy and a multi-access secure migration selection strategy to improve the convergence of the algorithm, and utilizes a dual-migration crossover strategy in order to adjust the jammer partners and improve the population diversity. Simulation results show that our proposed IMaOEA-RM algorithm can provide a more reliable and efficient migration scheme than existing algorithms.}
}
@article{CHAKRAVARTY2010606,
title = {The creative brain – Revisiting concepts},
journal = {Medical Hypotheses},
volume = {74},
number = {3},
pages = {606-612},
year = {2010},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2009.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0306987709006926},
author = {Ambar Chakravarty},
abstract = {Summary
Creativity is a complex neuro-psycho-philosophical phenomenon which is difficult to define literally. Fundamentally it involves the ability to understand and express novel orderly relationships. The creative process involves four stages – preparation, incubation, illumination and verification. A high level of general intelligence, domain specific knowledge and special skills are necessary pre-requisites. It is possible that in addition, some creative people might have architectural alternations of specific portions of the posterior neocortex. Associated with such pre-requisites, the process of creative innovation (incubation and illumination stages) necessitates the need for an ability of divergent thinking, a novelty seeking behavior, some degree of suppression of latent inhibition and a subtle degree of frontal dysfunction. The author hypothesizes that these features are often inter-linked and subtle frontally disinhibited behavior is conducive towards creativity by allowing uninterrupted flow of creative thought possessing and opening up new avenues towards problem solving. Perhaps the most essential feature of the creative brain is its degree of connectivity – both inter-hemispheric and intra-hemispheric. Connectivity correlates or binds together functions of apparently structurally isolated domains on brain modules sub-serving different functions. It is felt that creative cognition is a self rewarding process where divergent thinking would promote connectivity through development of new synapses. In addition, the phenomenon of synaesthesia has often been observed in creative visual artists. Creative innovation often occurs during low arousal states and creative people often manifests features of affective disorders. This suggests a role of neurotransmitters in creative innovation. Dopaminergic pathways are involved in the novelty seeking attitude of creative people while norepinephrine levels are depressed during discovery of novel orderly relationships. The relationship between mood and catecholamines and that of creative cognition is often in an inverted U-shaped form. It is hypothesized that that subtle frontal dysfunction is a pre-requisite for creative cognition but here again the relationship is also in an inverted U-form.}
}
@article{BOYDDAVIS2018185,
title = {‘A dialogue between the real-world and the operational model’ – The realities of design in Bruce Archer’s 1968 doctoral thesis},
journal = {Design Studies},
volume = {56},
pages = {185-204},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300893},
author = {Stephen {Boyd Davis} and Simone Gristwood},
keywords = {design history, philosophy of design, science of design, design research, systematic method},
abstract = {The article centres on a single document, the 1968 doctoral thesis of L. Bruce Archer. It traces Archer’s earlier publications and the sources that informed and inspired his thinking as a way of understanding his influential work at the Royal College of Art from 1962. Analysis suggests that Archer’s ambition for a rigorous ‘science of design’ inspired by linear algorithmic approaches was increasingly threatened with disruption by his experience of large, complex design projects. Reflecting on Archer's engagement with other models of designing, the article ends with Archer’s retrospective view and an account of his significantly altered opinions. Archer is located as both a theorist and someone fascinated by the commercial and practical aspects of designing.}
}
@article{KUAI2020101103,
title = {The extensible Data-Brain model: Architecture, applications and directions},
journal = {Journal of Computational Science},
volume = {46},
pages = {101103},
year = {2020},
note = {20 years of computational science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320300752},
author = {Hongzhi Kuai and Ning Zhong},
keywords = {Artificial intelligence (AI), Brain informatics, Brain computing, Data-Brain, Brain big data, Web intelligence (WI), Intelligence systems},
abstract = {One of the key ideas in realizing human-like intelligence is to understand information-processing mechanisms in the human brain. Brain Informatics is a rapidly expanding interdisciplinary field to systematically utilize brain-related data, information and knowledge coming from the entire research process for in-depth brain investigation. In the past few years, a data-centric conceptual brain model, namely Data-Brain, has been proposed, providing the foundation for the systematic Brain Informatics methodology. The Data-Brain model constitutes a conceptual framework and detailed guideline for managing and analyzing brain big data. The development of Data-Brain model also demands the support from advanced technologies. This paper presents an extensible version of the Data-Brain with advanced computing techniques in the connected world. It provides a global understanding of how multidisciplinary techniques work together to tackle brain computing challenges. Particularly, the integrated K-I-D (Knowledge-Information-Data) loop is proposed, constructing a cycle as the thinking space to help pursue the systematic brain investigation, by which the extensible Data-Brain model continuously iterates and evolves through the never-ending learning. Such synergistic evolvement will power future progress for building intelligence systems and applications connected with the study of complex human brain.}
}
@article{KING2023104028,
title = {Identifying risk controls for future advanced brain-computer interfaces: A prospective risk assessment approach using work domain analysis},
journal = {Applied Ergonomics},
volume = {111},
pages = {104028},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104028},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023000662},
author = {Brandon J. King and Gemma J.M. Read and Paul M. Salmon},
keywords = {Brain-computer interfaces, Risk assessment, System modelling},
abstract = {Brain-computer interface (BCI) technologies are progressing rapidly and may eventually be implemented widely within society, yet their risks have arguably not yet been comprehensively identified, nor understood. This study analysed an anticipated invasive BCI system lifecycle to identify the individual, organisational, and societal risks associated with BCIs, and controls that could be used to mitigate or eliminate these risks. A BCI system lifecycle work domain analysis model was developed and validated with 10 subject matter experts. The model was subsequently used to undertake a systems thinking-based risk assessment approach to identify risks that could emerge when functions are either undertaken sub-optimally or not undertaken at all. Eighteen broad risk themes were identified that could negatively impact the BCI system lifecycle in a variety of unique ways, while a larger number of controls for these risks were also identified. The most concerning risks included inadequate regulation of BCI technologies and inadequate training of BCI stakeholders, such as users and clinicians. In addition to specifying a practical set of risk controls to inform BCI device design, manufacture, adoption, and utilisation, the results demonstrate the complexity involved in managing BCI risks and suggests that a system-wide coordinated response is required. Future research is required to evaluate the comprehensiveness of the identified risks and the practicality of implementing the risk controls.}
}
@article{DALLAQUA2021422,
title = {ForestEyes Project: Conception, enhancements, and challenges},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {422-435},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001965},
author = {Fernanda B.J.R. Dallaqua and Álvaro L. Fazenda and Fabio A. Faria},
keywords = {Citizen Science, Deforestation area detection, Rainforest, Tropical forest, Volunteered Thinking},
abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.}
}
@article{CHOI199617,
title = {Computation and semiotic practice as compositional process},
journal = {Computers & Mathematics with Applications},
volume = {32},
number = {1},
pages = {17-35},
year = {1996},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(96)00084-3},
url = {https://www.sciencedirect.com/science/article/pii/0898122196000843},
author = {I. Choi},
keywords = {Dynamical systems, Semiotics, Computer music, Synergetics, Cognitive systems, Music composition, Chaos, Music synthesis},
abstract = {In sound computation, computational processes are brought into the acoustic domain by a set of formalized instructions for controlling parameters in synthesis engines and compositional algorithms. From the acoustic events, listeners often extract patterns or “musical objects” in their perception to the extent that certain associations are made external to the computational process. Perceived, musical objects rapidly become immutable, and that immutability may be considered a compositional problem. The problem is how to approach a compositional project for bringing new insights into play while, on one side, using the existing representational system such as symbolic language in computation, and on the other side, facing listeners' perceptual tendency to make external associations. For composers, the problem requires technical solutions as well as an ability to articulate the philosophical issues. This problem also exists in semiotics, a general study of signs, when one has to borrow language from existing linguistic systems in order to express a new thought without being trapped within the immutability of given linguistic sources. Semiotic practice is a discipline which emphasizes the function of semiotics to generate necessary discourse to examine the linguistic system in use and its logocentric tendency—the tendency towards known signs. We define semiotic practice as a signifying process in which meaning may be generated during that particular process under study; thus, meaning in semiotic practice is temporal context-dependent as a function of signifiers. The compositional problems involving sound computation for generating cases to support semiotic practice inquires about two tasks: 1.(1) how to design software which enables acoustic events to be observed as processes rather than observing sounds only as familiar objects or transformations featuring the recognition of objects, and2.(2) how to compose a piece of music so mutability of signs can be observed. To meet these problems, this paper examines perspectives on systems and cognition from multiple views such as semiotics, computation theories, and synergetics. We also discuss software designed for composition in terms of semiotic practice.}
}
@article{DECARO200758,
title = {Methodologies for examining problem solving success and failure},
journal = {Methods},
volume = {42},
number = {1},
pages = {58-67},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002982},
author = {Marci S. DeCaro and Mareike Wieth and Sian L. Beilock},
keywords = {Working memory, Performance, Pressure, Individual differences, Problem solving, Creativity, Short term memory, Stress, Math},
abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.}
}
@article{PELAEZ2025109363,
title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
journal = {Computer Physics Communications},
volume = {306},
pages = {109363},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109363},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524002868},
author = {Raúl P. Peláez and Pablo Ibáñez-Freire and Pablo Palacios-Alonso and Aleksandar Donev and Rafael Delgado-Buscalioni},
keywords = {Molecular dynamics, Hydrodynamics, C++, CUDA, Soft matter},
abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.}
}
@article{HULME2017345,
title = {From control to causation: Validating a ‘complex systems model’ of running-related injury development and prevention},
journal = {Applied Ergonomics},
volume = {65},
pages = {345-354},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730159X},
author = {A. Hulme and P.M. Salmon and R.O. Nielsen and G.J.M. Read and C.F. Finch},
keywords = {Systems ergonomics, STAMP, Sports injury prevention, Running injury},
abstract = {Introduction
There is a need for an ecological and complex systems approach for better understanding the development and prevention of running-related injury (RRI). In a previous article, we proposed a prototype model of the Australian recreational distance running system which was based on the Systems Theoretic Accident Mapping and Processes (STAMP) method. That model included the influence of political, organisational, managerial, and sociocultural determinants alongside individual-level factors in relation to RRI development. The purpose of this study was to validate that prototype model by drawing on the expertise of both systems thinking and distance running experts.
Materials and methods
This study used a modified Delphi technique involving a series of online surveys (December 2016- March 2017). The initial survey was divided into four sections containing a total of seven questions pertaining to different features associated with the prototype model. Consensus in opinion about the validity of the prototype model was reached when the number of experts who agreed or disagreed with survey statement was ≥75% of the total number of respondents.
Results
A total of two Delphi rounds was needed to validate the prototype model. Out of a total of 51 experts who were initially contacted, 50.9% (n = 26) completed the first round of the Delphi, and 92.3% (n = 24) of those in the first round participated in the second. Most of the 24 full participants considered themselves to be a running expert (66.7%), and approximately a third indicated their expertise as a systems thinker (33.3%). After the second round, 91.7% of the experts agreed that the prototype model was a valid description of the Australian distance running system.
Conclusion
This is the first study to formally examine the development and prevention of RRI from an ecological and complex systems perspective. The validated model of the Australian distance running system facilitates theoretical advancement in terms of identifying practical system-wide opportunities for the implementation of sustainable RRI prevention interventions. This ‘big picture’ perspective represents the first step required when thinking about the range of contributory causal factors that affect other system elements, as well as runners' behaviours in relation to RRI risk.}
}
@article{RYDER2022100703,
title = {Rethinking reflective practice: John Boyd’s OODA loop as an alternative to Kolb},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100703},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100703},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722001057},
author = {Mike Ryder and Carolyn Downs},
keywords = {OODA, Reflective practice, Experiential learning, Work-based learning, Employability, Business education},
abstract = {The world is changing and business schools are struggling to keep up. Theories of reflective practice developed by the likes of Schon (1983), Gibbs (1988), Driscoll (1994, 2007) and Kolb (1984, 2015) are outdated and unfit for current purposes. Problems include the chronology of events, the orientation of the observer, the impact of external inputs, and the fact that neither education nor the workplace follow a structured, linear path. In response to these challenges, we propose a new ‘solution’: John Boyd's OODA loop. We argue that OODA loops offer the chance to reshape reflective practice and work-based learning for a world in which individuals must cope with ‘an unfolding evolving reality that is uncertain, ever changing and unpredictable’ (Boyd, 1995, slide 1). By embracing the philosophy of John Boyd and his OODA loop theory, business schools can develop greater resilience and employability in graduates, preparing them to embrace change while also embedding the concept of life-long learning to make them better equipped to face the uncertainty that the modern world brings.}
}
@article{TIAN2018104,
title = {The association between visual creativity and cortical thickness in healthy adults},
journal = {Neuroscience Letters},
volume = {683},
pages = {104-110},
year = {2018},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2018.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0304394018304397},
author = {Fang Tian and Qunlin Chen and Wenfeng Zhu and Yongming Wang and Wenjing Yang and Xingxing Zhu and Xue Tian and Qinglin Zhang and Guikang Cao and Jiang Qiu},
keywords = {Visual creativity, Cortical thickness, Prefrontal cortex, Supplementary motor cortex, Insula},
abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.}
}
@article{PERCHTOLDSTEFAN202398,
title = {Functional EEG Alpha Activation Patterns During Malevolent Creativity},
journal = {Neuroscience},
volume = {522},
pages = {98-108},
year = {2023},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306452223002178},
author = {Corinna M. Perchtold-Stefan and Christian Rominger and Ilona Papousek and Andreas Fink},
keywords = {malevolent creativity, EEG, alpha power, time-course, divergent thinking},
abstract = {On the dark side of creativity, creative ideation is intentionally used to damage others. This first electroencephalogram (EEG) study on malevolent creativity investigated task-related power (TRP) changes in the alpha band while n = 89 participants (52 women, 37 men) generated original ideas for revenge in the psychometric Malevolent Creativity Test. TRP changes were assessed for different stages of the idea generation process and linked to performance indicators of malevolent creativity. This study revealed three crucial findings: 1) Malevolent creativity yielded topographically distinct alpha power increases similar to conventional creative ideation. 2) Time-related activity changes during malevolent creative ideation were reflected in early prefrontal and mid-stage temporal alpha power increases in individuals with higher malevolent creativity performance. This performance-related, time-sensitive pattern of TRP changes during malevolent creativity may reflect early conceptual expansion from prosocial to antisocial perspectives, and subsequent inhibition of dominant semantic associations in favor of novel revenge ideas. 3) The observed, right-lateralized alpha power increases over the entire ideation phase may denote an additional emotional load of creative ideation. Our study highlights the seminal role of EEG alpha oscillations as a biomarker for creativity, also when creative processes operate in a malevolent context.}
}
@article{DELLACQUA20241727,
title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
journal = {Procedia Computer Science},
volume = {239},
pages = {1727-1735},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015953},
author = {Pierangelo Dell’Acqua and Stefania Costantini and Abeer Dyoub and Giovanni De Gasperis and Andrea Monaldini and Andrea Rafanelli},
keywords = {Behavior Trees, Affective Computing, Decision Making},
abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.}
}
@article{KARIMI199745,
title = {A Parallel Algorithm for Routing: Best Solutions at Low Computational Costs},
journal = {Geomatica},
volume = {51},
number = {1},
pages = {45-51},
year = {1997},
issn = {1195-1036},
doi = {https://doi.org/10.5623/geomat-1997-0006},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624002969},
author = {Hassan A. Karimi and Dongming Hwang},
abstract = {Routing is a common activity in network analysis. Of the many routing algorithms available, Dijkstra’s algorithm is one of the most widely used for computing best paths in networks. Dijkstra’s algorithm guarantees best solutions with a time complexity ofO(N2) in the worst case. This time complexity results in very long processing times for computing best paths in large networks, which can be a serious problem for real-time applications. Alternative approaches that give a better time complexity have been suggested, most of which are heuristics. However, these heuristics do not guarantee best solutions. To achieve best solutions at low computational costs, a parallel algorithm for parallel machines is suggested.
L’établissement de parcours s’emploie fréquemment dans l’analyse de réseau. De tous les algorithmes d’établissement de parcours disponibles, l’algorithme de Dijkstra est parmi ceux qu’on emploie le plus souvent afin de calculer les meilleurs itinéraires de réseaux. Cet algorithme assure les meilleures solutions avec une complexité temporelle de 0(N2) dans la pire éventualité. Cette complexité temporelle entraîne des temps de traitement très longs lors du calcul des meilleurs itinéraires dans les grands réseaux, ce qui peut constituer un problème sérieux pour les applications en temps réel. On a suggéré d’autres méthodes qui offrent une meilleure complexité temporelle, la plupart étant des heuristiques. Toutefois, ces heuristiques ne garantissent pas les meilleures solutions. Afin d’en arriver aux meilleurs solutions tout en réduisant les coûts de calcul, on suggère un algorithme parallèle pour des machines parallèles.}
}
@incollection{DRYGAS20201,
title = {1 - Introduction to computational methods and theory of composites},
editor = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
booktitle = {Applied Analysis of Composite Media},
publisher = {Woodhead Publishing},
pages = {1-56},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102670-0},
doi = {https://doi.org/10.1016/B978-0-08-102670-0.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102670000010X},
author = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
keywords = {Self-consistent approximation, structural sum, statistical mechanics methods, self-Similarity and renormalization-group},
abstract = {Overview of traditional approaches based on self-consistent approximations in composite materials is presented. Their restrictions are underlined. Neoclassical approach previously introduced in the Preface, is illustrated and compared to methods applied in statistical mechanics. The structural sums, the key construction of the neoclassical approach, are outlined. Method of series and asymptotic method of approximants, Padé approximants, DLog Padé approximants, Factor, Root, Additive approximants are briefly discussed. Notion of Self-Similarity and renormalization-group is introduced. Minimal difference and minimal derivative methods of calculation for short series are discussed in detail. Critical Index is calculated from various short series. DLog root approximants are introduced and illustrate by several examples, where the DLog Padé approximants fail. DLog additive approximants are introduced and presented iteratively. Multiple examples are presented in the chapter and in the appendix. Method of Log Padé approximants is suggested.}
}
@article{WANG2025102570,
title = {ST-TNet: An spatio-temporal joint transformer network for CSI feedback in FDD-MIMO systems},
journal = {Physical Communication},
volume = {68},
pages = {102570},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102570},
url = {https://www.sciencedirect.com/science/article/pii/S187449072400288X},
author = {Linyu Wang and Yize Cao and Jianhong Xiang},
keywords = {Massive MIMO, CSI feedback, Deep learning, Lightweighting, Transformer, Attention},
abstract = {In recent years, deep learning methods have been shown to have strong potential and superiority in reducing channel state information (CSI) feedback overhead and further improving feedback accuracy to maximize the performance benefits of massive Multiple-Input Multiple-Output (MIMO) in frequency division duplex (FDD) mode. As the CSI matrices are transformed into sequences for input to the Transformer model, the rearrangement leads to the loss of the original physical location relationships. Based on this problem, this paper proposes a transformer decoder based on spatio-temporal joint (ST-T). We employ a spatial attention mechanism to compensate for this information loss and focus on key spatial features more accurately, further exploiting the potential of single- and two-layer transformers in reconstructing CSI matrices. The results are validated by simulations based on DCRNet and CLNet encoders, which show that higher performance can be achieved with lower computational load compared to other lightweight models.}
}
@incollection{NIEVERGELT1993167,
title = {Experiments in Computational Heuristics and Their Lessons for Software and Knowledge Engineering},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {37},
pages = {167-205},
year = {1993},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60405-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808604052},
author = {Jurg Nievergelt},
abstract = {Publisher Summary
This chapter presents examples that illustrate a type of programming project increasingly prominent in the field of knowledge engineering. The examples are chosen on grounds of familiarity, without any claim to represent the field of heuristic programming at large. The chapter begins by describing some software projects in computational heuristics. These projects are presented as case studies of the interaction between software engineering and knowledge engineering that illustrate the decisive importance of a powerful software environment. The chapter presents the case of the smart game board and describes the main software tool for rapid prototyping of game implementations, needed to conduct experiments. It also describes a project involving heuristics and knowledge engineering that has been evolving without interruption for the past five years. It presents paradigms of software development favored by system designers and implementers of exploratory development projects, and explains why these are often diametrically opposed to the conventional software engineering lore.}
}
@article{METTLER2024101932,
title = {Same same but different: How policies frame societal-level digital transformation},
journal = {Government Information Quarterly},
volume = {41},
number = {2},
pages = {101932},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101932},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000248},
author = {Tobias Mettler and Gianluca Miscione and Claus D. Jacobs and Ali A. Guenduez},
keywords = {Digital transformation, IS policy research, Computational content analysis, Narratives},
abstract = {The digital transformation (DT) is not only forcing companies to rethink their business models but is also challenging governments to address the question of how information technology will change society today and in the future. By setting the legal boundaries and acting as an investor and promoter of the domestic digital economy, governments actively influence in which ways this transformational process takes place. The vision and objectives how DT should be realized on state level is portrayed in well-crafted DT policies. Yet, little is known how governments, as strategic actors, see their role in the DT and how they frame these documents. In this paper, we argue that policymaking about DT is isomorphic in the global context, rather than a differentiator for countries to gain a competitive edge. Using machine learning to analyze a vast text corpus of policy documents, we identify the common repertoire of narratives used by governments from all around the globe to picture their vision of the DT and show that DT policies appear to be almost context-free due to their high similarity.}
}
@article{EGOROV2007293,
title = {Neural logic molecular, counter-intuitive},
journal = {Biomolecular Engineering},
volume = {24},
number = {3},
pages = {293-299},
year = {2007},
note = {6th Atlantic Symposium on Computational Biology},
issn = {1389-0344},
doi = {https://doi.org/10.1016/j.bioeng.2007.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389034407000342},
author = {Igor K. Egorov},
keywords = {Creative thinking, Boolean logic gates, Molecular mechanism, Transcription regulation, Somatic hypermutation, Neuron},
abstract = {A hypothesis is proposed that multiple “LOGIC” genes control Boolean logic in a neuron. Each hypothetical LOGIC gene encodes a transcription factor that regulates another LOGIC gene(s). Through transcription regulation, LOGIC genes connect into a complex circuit, such as a XOR logic gate or a two-input flip–flop logic circuit capable of retaining information. LOGIC gene duplication, mutation and recombination may result in the diversification of Boolean logic gates. Creative thinking may sometimes require counter-intuitive reasoning, rather than common sense. Such reasoning is likely to engage novel logic circuits produced by LOGIC somatic mutations. An individual's logic maturates by a mechanism of somatic hypermutation, gene conversion and recombination of LOGIC genes in precursor cells followed by selection of neurons in the brain for functional competence. In this model, a single neuron among billions in the brain may contain a unique logic circuit being the key to a hard intellectual problem. The output of a logic neuron is likely to be a neurotransmitter. This neuron is connected to other neurons in the spiking neural network. The LOGIC gene hypothesis is testable by molecular techniques. Understanding mechanisms of authentic human ingenuity may help to invent digital systems capable of creative thinking.}
}
@article{SUDDENDORF201826,
title = {Prospection and natural selection},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {26-31},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302449},
author = {T Suddendorf and A Bulley and B Miloyan},
abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.}
}
@article{RODRIGUEZ2022104446,
title = {Using scaffolded feedforward and peer feedback to improve problem-based learning in large classes},
journal = {Computers & Education},
volume = {182},
pages = {104446},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000173},
author = {María Fernanda Rodríguez and Miguel Nussbaum and Leyla Yunis and Tomás Reyes and Danilo Alvares and Jean Joublan and Patricio Navarrete},
abstract = {The growing demand for access to higher education has seen institutions turn increasingly towards large classes. Implementing active, problem-based learning in this context can be difficult as it requires the lecturer to attend to every student's individual needs. Given the lack of tools for providing personalized feedback, this represents a significant challenge. The aim of this study is to see how best to support lecturers in giving timely feedback to students in a large class during problem-based learning. To meet this goal, we propose a model that combines feedforward, scaffolded using an automated summarization tool, with peer feedback. In this sense, the lecturer first provides feedforward through a series of general comments before an anonymous peer gives personalized feedback. The results show that, despite not giving personalized feedback, the lecturer is able to provide enriched formative feedforward thanks to the summary generated by the automated system. Furthermore, in more qualitative terms, the students show that they appreciate the opportunity to both give and receive feedback. Finally, the students' critical thinking skills are also shown to improve progressively from one activity to the next. Given the research gap regarding how lecturers use the reports generated by automated summarization tools, our study contributes to the literature by proposing a strategy for lecturers to use such reports to provide feedforward. Additionally, this study also contributes to the literature by proposing a model that can be fully integrated in both synchronous and asynchronous online learning.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{EGIDI2020155,
title = {Desertification risk, economic resilience and social issues: From theory to practice},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {18},
number = {2},
pages = {155-163},
year = {2020},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S2325426221000310},
author = {Gianluca Egidi and Luca Salvati},
keywords = {Population dynamics, Ecosystem functioning, Socio-ecological resilience, Complex adaptive systems, Interpretative framework},
abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.}
}
@article{HARTWIGSEN20212075,
title = {How does hemispheric specialization contribute to human-defining cognition?},
journal = {Neuron},
volume = {109},
number = {13},
pages = {2075-2090},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321002907},
author = {Gesa Hartwigsen and Yoshua Bengio and Danilo Bzdok},
keywords = {human intelligence, artificial general intelligence, computational design principles, deep learning, language, global workspace theory},
abstract = {Summary
Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman’s System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{POLZER2022100181,
title = {The rise of people analytics and the future of organizational research},
journal = {Research in Organizational Behavior},
volume = {42},
pages = {100181},
year = {2022},
issn = {0191-3085},
doi = {https://doi.org/10.1016/j.riob.2023.100181},
url = {https://www.sciencedirect.com/science/article/pii/S0191308523000011},
author = {Jeffrey T. Polzer},
keywords = {People analytics, Algorithms, Decision-making, Networks, Teams, Meetings, Culture, Monitoring, Computational social science, Organizational behavior},
abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{SHUKLA2024117388,
title = {Association of road traffic noise exposure and school childrens’ cognition: A structural equation model approach},
journal = {Environmental Research},
volume = {240},
pages = {117388},
year = {2024},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2023.117388},
url = {https://www.sciencedirect.com/science/article/pii/S0013935123021928},
author = {Avnish Shukla and Bhaven N. Tandel},
keywords = {School children, Cognition, Traffic noise index (TNI), Exploratory factor analysis (EFA), Structural equation modeling (SEM)},
abstract = {This study explores the complex relationship between traffic noise and school children's cognition, acknowledging existing empirical inconsistencies and aiming to contribute to a richer understanding of this pivotal issue. Schools adjacent to noisy roads were selected, and outdoor noise levels were measured employing a Kimo dB300 sound level meter, focusing on noise level indices LAeq, L10, and L90. Subsequent calculations were performed to determine the noise pollution level (Lnp), noise climate (NC), and traffic noise index (TNI), revealing a severe noise exposure when compared to standard guidelines. A perception questionnaire for various noise and acoustic factors influencing cognition was developed, and 1524 student responses were collected. Data analysis incorporated Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) for dimension reduction, revealing three latent factors labelled 'annoyance,' 'behaviour,' and 'cognition'. Further, Structural Equation Modeling (SEM) was utilized to explore multivariate relationships between variables and latent factors. Resultant path coefficients were obtained as 0.12, 0.98, and 0.10 for the impact of 'behaviour' and 'annoyance' on 'cognition' and the correlation between 'annoyance' and 'behaviour', respectively. Findings underscore a potent positive impact of annoyance, stemming from acute ambient noise exposure, on the deterioration of children's cognition. While suggesting that ambient noise may be correlated with adverse health impacts due to its influence on cognition, this study emphasizes the pressing necessity for noise mitigation in roadside schools and stringent enforcement of noise pollution guidelines in academic zones.}
}
@article{SEWALL2020,
title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
journal = {Journal of Microbiology & Biology Education},
volume = {21},
number = {1},
year = {2020},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v21i1.1991},
url = {https://www.sciencedirect.com/science/article/pii/S1935787720000660},
author = {Julia Massimelli Sewall and Andrew Oliver and Kameryn Denaro and Alexander B. Chase and Claudia Weihe and Mi Lay and Jennifer B. H. Martiny and Katrine Whiteson},
abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.}
}
@article{TWORZYDLO1995759,
title = {Knowledge-based methods and smart algorithms in computational mechanics},
journal = {Engineering Fracture Mechanics},
volume = {50},
number = {5},
pages = {759-800},
year = {1995},
issn = {0013-7944},
doi = {https://doi.org/10.1016/0013-7944(94)E0060-T},
url = {https://www.sciencedirect.com/science/article/pii/0013794494E0060T},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. These include algorithmic approaches, based on error estimation, adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object-oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here.}
}
@article{CIPRIANI2024102277,
title = {Personality traits and climate change denial, concern, and proactivity: A systematic review and meta-analysis},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102277},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102277},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000501},
author = {Enrico Cipriani and Sergio Frumento and Angelo Gemignani and Danilo Menicucci},
keywords = {Climate change, Personality, Communication, Big five, Climate change denial, Climate change concern},
abstract = {Climate Change is a global issue which touches the lives of all human beings, each of whom have their own unique outlooks and motivations. Hence, the high degree of complexity which emerges from the involvement of such a large number of people might be better understood through the lenses of their individual differences. We performed a systematic review and meta-analysis following PRISMA guidelines. We searched keywords on Web of Science™ and Scopus®, and included peer-reviewed articles which quantitatively examined correlations between personality and climate attitudes. After screening, 74 papers were included in our review. From these articles, k = 100 samples were extracted and included in meta-analysis models. Our results show that Climate Change Denial is positively correlated with Social Dominance Orientation (r = 0.39) and Right-Wing Authoritarianism (r = 0.42), and negatively with Openness (r = −0.14), Conscientiousness (r = −0.05), Agreeableness (r = −0.11), Consideration of Future Consequences (r = −0.38), and Actively Open-Minded Thinking (r = −0.38). Concern for Climate Change correlates with Openness (r = 0.10), Neuroticism (r = 0.12), Consideration of Future Consequences (r = 0.34), and negatively with Social Dominance Orientation (r = -0.36) and Right-Wing Authoritarianism (r = −0.22). Finally, Proactivity towards Climate Change correlates positively with Openness (r = 0.17), Extraversion (r = 0.09), Agreeableness (r = 0.05), Neuroticism (r = 0.10), Consideration of Future Consequences (r = 0.39), and negatively with Social Dominance Orientation (r = -0.25) and Right-Wing Authoritarianism (r = -0.31). Moderation analysis shows geographical variations in the Social Dominance Orientation and Climate Denial relationship. We conclude that some personality traits – such as Openness – transversally affect climate change attitudes. Moreover, meta-analytic data suggest that the personality involvement in Climate Change may be dependent on the socio-political context of different countries. Future research, policies, and communication campaigns should take these peculiarities into account.}
}
@article{FARAJ2021100337,
title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
journal = {Information and Organization},
volume = {31},
number = {1},
pages = {100337},
year = {2021},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2021.100337},
url = {https://www.sciencedirect.com/science/article/pii/S1471772721000038},
author = {Samer Faraj and Wadih Renno and Anand Bhardwaj},
keywords = {COVID, Digitalization, Technology, Organizing, Breaching experiment},
abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.}
}
@article{BARKER2023100569,
title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
journal = {Molecular & Cellular Proteomics},
volume = {22},
number = {7},
pages = {100569},
year = {2023},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2023.100569},
url = {https://www.sciencedirect.com/science/article/pii/S1535947623000804},
author = {Anna D. Barker and Mario M. Alba and Parag Mallick and David B. Agus and Jerry S.H. Lee},
keywords = {proteomics, cancer biomarkers, protein biomarkers, complex adaptive systems, clinical proteomics},
abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.}
}
@article{IVANOV2023108938,
title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
journal = {International Journal of Production Economics},
volume = {263},
pages = {108938},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108938},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323001706},
author = {Dmitry Ivanov},
keywords = {Supply chain resilience, Intelligent digital twin, Data analytics, Stress-test, Ripple effect, anyLogistix},
abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{LUO202571,
title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
journal = {Methods},
volume = {235},
pages = {71-80},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000349},
author = {Rentao Luo and Jiawei Liu and Lixin Guan and Mengshan Li},
keywords = {Promoter, Deep learning, Attention, Gene sequences, Bioinformatics},
abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.}
}
@article{TALWAR2021102341,
title = {Has financial attitude impacted the trading activity of retail investors during the COVID-19 pandemic?},
journal = {Journal of Retailing and Consumer Services},
volume = {58},
pages = {102341},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102341},
url = {https://www.sciencedirect.com/science/article/pii/S0969698920313497},
author = {Manish Talwar and Shalini Talwar and Puneet Kaur and Naliniprava Tripathy and Amandeep Dhir},
keywords = {Artificial neural network, COVID-19, Financial behavior, Financial attitude, Financial anxiety, Pandemic},
abstract = {Financial attitude influences the financial behavior of retail investors. Although the extant research has acknowledged and examined this relationship, the measures of financial attitude and behavior still vary widely and are generally posed as a series of questions rather than statements. In addition to this, there is insufficient knowledge regarding retail investors' behavior in the face of a health crisis, such as the current COVID-19 pandemic. This study addresses these gaps in the prior literature by examining the relative influence of six dimensions of financial attitude, namely, financial anxiety, optimism, financial security, deliberative thinking, interest in financial issues, and needs for precautionary savings, on the trading activity of retail investors during the pandemic. Data were collected from 404 respondents and analyzed using the artificial neural network (ANN) method. The results revealed that all six dimensions had a positive influence on trading activity, with interest in financial issues exerting the strongest influence, followed by deliberative thinking. The study thus contributes important inferences for researchers and managers.}
}
@article{FALBEN2023105386,
title = {The power of the unexpected: Prediction errors enhance stereotype-based learning},
journal = {Cognition},
volume = {235},
pages = {105386},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105386},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000203},
author = {Johanna K. Falbén and Marius Golubickis and Dimitra Tsamadi and Linn M. Persson and C. Neil Macrae},
keywords = {Stereotyping, Person perception, Reinforcement learning, Prediction errors, Drift diffusion model},
abstract = {Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced.}
}
@article{DAVIS20111046,
title = {Homogeneous steady deformation: A review of computational techniques},
journal = {Journal of Structural Geology},
volume = {33},
number = {6},
pages = {1046-1062},
year = {2011},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191814111000447},
author = {Joshua R. Davis and Sarah J. Titus},
keywords = {Kinematic model, Homogeneous deformation, Velocity gradient, Transpression, Vorticity},
abstract = {Homogeneous steady models are frequently used in the structural geology community to describe rock deformation. We review the literature on these models in a streamlined, coordinate-free framework based on matrix exponentials and logarithms. These mathematical tools allow us to compute progressive and simultaneous deformations easily. As an application, we develop transpression with triclinic symmetry in two ways. The tools let us integrate field data related to position and velocity in computing best-fit models with many degrees of freedom. As an application, we reanalyze a published study to demonstrate the extent to which kinematic vorticity is sensitive to modeling assumptions. The tools also open the door to an increased role for the mathematics of Lie groups (spaces of deformations) in structural geology. We suggest two topics for further study: numerical methods for non-steady deformations, and statistics of deformation tensors.}
}
@article{STOLOWY2022101334,
title = {Competing for narrative authority in capital markets: Activist short sellers vs. financial analysts},
journal = {Accounting, Organizations and Society},
volume = {100},
pages = {101334},
year = {2022},
issn = {0361-3682},
doi = {https://doi.org/10.1016/j.aos.2022.101334},
url = {https://www.sciencedirect.com/science/article/pii/S0361368222000010},
author = {Hervé Stolowy and Luc Paugam and Yves Gendron},
keywords = {Activist short sellers, Expertise, Financial analysts, Framing, Narrative authority},
abstract = {Activist short sellers (AShSs) and financial analysts are information intermediaries who analyze firm disclosures as well as produce and disseminate influential investment narratives. This study aims to better understand narrative challenges surrounding the legitimate expertise of financial analysts. Specifically, we examine how AShSs challenge sell-side financial analysts' narrative authority (i.e., the perception that they produce expert knowledge) in interpreting firms' performance and future prospects. We investigate how analysts respond (or do not respond) to this challenge. We use 442 AShS reports, 12 interviews with AShSs and analysts, and analysts' stock recommendations and target prices. In their criticisms of analysts (found in one-third of reports), AShSs frequently frame analysts as lacking market expertise and critical thinking – two core dimensions of analysts' narrative authority. Sixty-six percent of analysts, although explicitly criticized in AShS reports, do not engage in written responses in their equity research reports because they reportedly either adopt a renunciation attitude to the challenge or they engage in off-the-record discussions with certain market participants. However, 34% of analysts respond overtly by counter-framing AShSs as lacking market expertise and objectivity. After the dissemination of AShS reports, analysts, on average, do not revise their highly visible stock recommendations but they revise target prices downward. Theoretically, this study extends our understanding of the construction of narrative authority in capital markets as we examine a challenge to the expertise of influential information intermediaries.}
}
@article{BIELZA2000725,
title = {Structural, elicitation and computational issues faced when solving complex decision making problems with influence diagrams},
journal = {Computers & Operations Research},
volume = {27},
number = {7},
pages = {725-740},
year = {2000},
issn = {0305-0548},
doi = {https://doi.org/10.1016/S0305-0548(99)00113-6},
url = {https://www.sciencedirect.com/science/article/pii/S0305054899001136},
author = {C. Bielza and M. Gómez and S. Rı́os-Insua and J.A.Fernández {del Pozo}},
keywords = {Decision analysis, Influence diagrams, Implementation issues, Medical decision making, Neonatal jaundice},
abstract = {Influence diagrams have become a popular tool for representing and solving decision making problems under uncertainty (Shachter, Operations Research 1986;34:871–82). We show here some practical difficulties when using them to construct a medical decision support system. Specifically, it is hard to tackle issues related to the problem structuring, like the existence of constraints on the sequence of decisions, and the time evolution modeling; related to the knowledge-acquisition, like probability and utility assignment; and related to computational limitations, in memory storage and evaluation phases, as well as the explanation of results. We have recently developed a complex decision support system for neonatal jaundice management — a very common medical problem — , encountering all these difficulties. In this paper, we describe them and how they have been undertaken, providing insights into the community involved in the design and solution of decision models by means of influence diagrams.
Scope and purpose
Decision Analysis is a very well-known discipline that deals with the practice of Decision Theory (Clemen, Making hard decisions: an introduction to decision analysis, 2nd ed. Pacific Grove, CA: Duxbury, 1996). It comprises various steps usually implemented in a decision support system: definition of the alternatives and objectives, modelization of the structure of the decision problem, as well as the beliefs and preferences of the decision maker. The recommended alternative is the one with maximum expected utility, once all the assignments have been refined via sensitivity analyses. However, there are a number of difficulties faced in practice when solving large problems, that require an attentive study.}
}
@article{PRONK202443,
title = {Qualitative systems mapping in promoting physical activity and cardiorespiratory fitness: Perspectives and recommendations},
journal = {Progress in Cardiovascular Diseases},
volume = {83},
pages = {43-48},
year = {2024},
note = {Cardiorespiratory Fitness and Physical Activity: An Update of Evidence, Global Status and Recommendations},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2024.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033062024000355},
author = {Nicolaas P. Pronk and Bruce Y. Lee},
keywords = {Systems mapping, Causal loop diagram, Physical activity, Cardiorespiratory fitness, Complexity},
abstract = {The purpose of this report is to provide a perspective on the use of qualitative systems mapping, provide examples of physical activity (PA) systems maps, discuss the role of PA systems mapping in the context of iterative learning to derive breakthrough interventions, and provide actionable recommendations for future work. Systems mapping methods and applications for PA are emerging in the scientific literature in the study of complex health issues and can be used as a prelude to mathematical/computational modeling where important factors and relationships can be elucidated, data needs can be prioritized and guided, interventions can be tested and (co)designed, and metrics and evaluations can be developed. Examples are discussed that describe systems mapping based on Group Model Building or literature reviews. Systems maps are highly informative, illustrate multiple components to address PA and physical inactivity issues, and make compelling arguments against single intervention action. No studies were identified in the literature scan that considered cardiorespiratory fitness the focal point of a systems maps. Recommendations for future research and education are presented and it is concluded that systems mapping represents a valuable yet underutilized tool for visualizing the complexity of PA promotion.}
}
@article{KLOOSTER2024110771,
title = {A systematic review on eHealth technology personalization approaches},
journal = {iScience},
volume = {27},
number = {9},
pages = {110771},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019965},
author = {Iris ten Klooster and Hanneke Kip and Lisette {van Gemert-Pijnen} and Rik Crutzen and Saskia Kelders},
keywords = {Health sciences, Health technology},
abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.}
}
@article{GOLTZ2021103417,
title = {Do you listen to music while studying? A portrait of how people use music to optimize their cognitive performance},
journal = {Acta Psychologica},
volume = {220},
pages = {103417},
year = {2021},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S0001691821001670},
author = {Franziska Goltz and Makiko Sadakata},
keywords = {Background music, Cognitive performance, Music perception},
abstract = {The effect of background music (BGM) on cognitive task performance is a popular topic. However, the evidence is not converging: experimental studies show mixed results depending on the task, the type of music used and individual characteristics. Here, we explored how people use BGM while optimally performing various cognitive tasks in everyday life, such as reading, writing, memorizing, and critical thinking. Specifically, the frequency of BGM usage, preferred music types, beliefs about the scientific evidence on BGM, and individual characteristics, such as age, extraversion and musical background were investigated. Although the results confirmed highly diverse strategies among individuals regarding when, how often, why and what type of BGM is used, we found several general tendencies: people tend to use less BGM when engaged in more difficult tasks, they become less critical about the type of BGM when engaged in easier tasks, and there is a negative correlation between the frequency of BGM and age, indicating that younger generations tend to use more BGM than older adults. The current and previous evidence are discussed in light of existing theories. Altogether, this study identifies essential variables to consider in future research and further forwards a theory-driven perspective in the field.}
}
@article{FAN2025126317,
title = {Deep dive into clarity: Leveraging signal-to-noise ratio awareness and knowledge distillation for underwater image enhancement},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126317},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126317},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031841},
author = {Guodong Fan and Jingchun Zhou and Chengpei Xu and Zheng Cheng},
keywords = {Underwater image enhancement, SNR-Aware Transformer, Knowledge distillation},
abstract = {This paper presents an innovative dual-branch solution designed for underwater image enhancement (UIE), leveraging the synergistic combination of Signal-to-Noise Ratio (SNR) aware transformers and convolutional models. SNR-Net dynamically enhances pixel quality through spatial-varying operations. While transformers excel in capturing long-range dependencies, they face challenges in weak local relation learning. To address this, we introduce a SNR prior to guide transformer learning, incorporating a novel self-attention mechanism that avoids tokens from regions with very low SNR. Conversely, CNNs, optimized for exploiting local patterns, suffer from limited receptive fields and weak diversity representation. To overcome this limitation, we enhance the receptive field and multi-scale perception of CNNs by introducing a MR-ResNet module. Additionally, we incorporate a Selective Kernel Merging Module (SKMM), an attention-based feature merging module. These enhancements empower our approach to learn an enriched set of features that selectively combine contextual information from both branches while preserving high-quality spatial details. Finally, through knowledge distillation and contrastive learning, SNR-KD significantly reduces the number of parameters and computations of SNR-Net with minimal impact on performance. Extensive experiments validate the effectiveness of our methods, namely SNR-Net and SNR-KD, demonstrating their state-of-the-art performance compared to other recent UIE methods. The code of our model is publicly available at: https://github.com/Alexande-rChan/SNR-UIE.}
}
@article{SEEMAN202211461,
title = {Understanding chemistry: from “heuristic (soft) explanations and reasoning by analogy” to “quantum chemistry”††Dedicated to Dudley Herschbach in celebration of his 90th year who, when asked whether he was a theoretician or an experimentalist, responded, “The molecules don't know and don't care.”},
journal = {Chemical Science},
volume = {13},
number = {39},
pages = {11461-11486},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc02535c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023014347},
author = {Jeffrey I. Seeman and Dean J. Tantillo},
abstract = {ABSTRACT
“Soft theories,” i.e., “heuristic models based on reasoning by analogy” largely drove chemistry understanding for 150 years or more. But soft theories have their limitations and with the expansion of chemistry in the mid-20th century, more and more inexplicable (by soft theory) experimental results were being obtained. In the past 50 years, quantum chemistry, most often in the guise of applied theoretical chemistry including computational chemistry, has provided (a) the underlying “hard evidence” for many soft theories and (b) the explanations for chemical phenomena that were unavailable by soft theories. In this publication, we define “hard theories” as “theories derived from quantum chemistry.” Both soft and hard theories can be qualitative and quantitative, and the “Houk quadrant” is proposed as a helpful categorization tool. Furthermore, the language of soft theories is often used appropriately to describe quantum chemical results. A valid and useful way of doing science is the appropriate use and application of both soft and hard theories along with the best nomenclature available for successful communication of results and ideas.}
}
@incollection{RUNCO20141,
title = {Chapter 1 - Cognition and Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-38},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000011},
author = {Mark A. Runco},
keywords = {Threshold theory, IQ, Structure of intellect, Associative theory, Problem solving, Problem finding, Incubation, Insight, Intuition, Meta-cognition, Mindfulness, Overinclusive thinking},
abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.}
}
@article{XIA2025108857,
title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
journal = {Optics and Lasers in Engineering},
volume = {186},
pages = {108857},
year = {2025},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2025.108857},
url = {https://www.sciencedirect.com/science/article/pii/S0143816625000442},
author = {Xuhui Xia and Jiale Guo and Zelin Zhang and Lei Wang and Yuyao Guo},
keywords = {Cold-rolled strip steel, Defect classification, Lightweight network, Feature extraction},
abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.}
}
@article{FADLALLA1995987,
title = {Improving the performance of enumerative search methods—part II: Computational experiments},
journal = {Computers & Operations Research},
volume = {22},
number = {10},
pages = {987-994},
year = {1995},
issn = {0305-0548},
doi = {https://doi.org/10.1016/0305-0548(95)00016-F},
url = {https://www.sciencedirect.com/science/article/pii/030505489500016F},
author = {Adam Fadlalla and James R. Evans and Martin S. Levy},
abstract = {Generally, branch and bound algorithms typically use mechanistic search strategies and generally do not fully exploit “local” information inherent in problem structures; that is, specific problem-domain knowledge. Some exceptions are found in [2–5]. Incorporatiing intelligence in branch and bound algorithms has been suggested by Glover [1], but not studied in a rigorous experimental framework. We use the mean tardiness job sequencing problem to explore these issues. This paper is divided into two Parts. In Part I [9], we provided the intuitive motivation for this investigation and an experimental framework. In Part II, we present detailed computational results and statistical analysis. The results indicate that branch and bound algorithms can be enhanced significantly by exploiting local knowledge of problem structure and more judicious search strategies.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}
@article{AGRAWAL2022101673,
title = {Spectrum sensing in cognitive radio networks and metacognition for dynamic spectrum sharing between radar and communication system: A review},
journal = {Physical Communication},
volume = {52},
pages = {101673},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101673},
url = {https://www.sciencedirect.com/science/article/pii/S187449072200043X},
author = {Sumit Kumar Agrawal and Abhay Samant and Sandeep Kumar Yadav},
keywords = {Cognitive radio, Spectrum sensing, Spectrum sharing, Cognitive radar, Metacognition, Metacognitive radar},
abstract = {The massive growth in mobile users and wireless technologies has resulted in increased data traffic and created demand for additional radio spectrum. This growing demand for radio spectrum has resulted in spectrum congestion and mandated the need for coexistence between radar and interfering communication emitters. To address the aforementioned issues, it is critical to review existing policies and evaluate new technologies that can utilize spectrum in an efficient and intelligent manner. Cognitive radio and cognitive radar are two promising technologies that exploit spectrum using dynamic spectrum access techniques. Additionally, introducing the bio-inspired concept ‘metacognition’ in a cognitive process has shown to increase the effectiveness and robustness of the cognitive radio and cognitive radar system. Metacognition is a high-order thinking agent that monitors and regulates the cognition process through a feedback and control process called the perception–action cycle. Extensive research has been done in the field of spectrum sensing in cognitive radio and spectral coexistence between radar and communication systems. This paper provides a detailed classification of spectrum sensing schemes and explains how dynamic spectrum access strategies share the spectrum between radar and communication systems. In addition to this, the fundamentals of cognitive radio, its architecture, spectrum management framework, and metacognition concept in radar are discussed. Furthermore, this paper presents various research issues, challenges, and future research directions associated with spectrum sensing in cognitive radar and dynamic spectrum access strategies in cognitive radar.}
}

@article{BEJAR2005117,
title = {Sensor networks and distributed CSP: communication, computation and complexity},
journal = {Artificial Intelligence},
volume = {161},
number = {1},
pages = {117-147},
year = {2005},
note = {Distributed Constraint Satisfaction},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400150X},
author = {Ramón Béjar and Carmel Domshlak and Cèsar Fernández and Carla Gomes and Bhaskar Krishnamachari and Bart Selman and Magda Valls},
keywords = {Distributed CSP benchmark, Phase transitions, Randomized combinatorial search, Communication network delays, NP-completeness},
abstract = {We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC), and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of SensorDCSP. We found that random delays (due to network traffic or in some cases actively introduced by the agents) combined with a dynamic decentralized restart strategy can improve the performance of DisCSP algorithms. In addition, we introduce GSensorDCSP, a plain-embedded version of SensorDCSP that is closely related to various real-life dynamic tracking systems. We perform both analytical and empirical study of this benchmark domain. In particular, this benchmark allows us to study the attractiveness of solution repairing for solving a sequence of DisCSPs that represent the dynamic tracking of a set of moving objects.}
}
@article{IANDOLI2014298,
title = {Socially augmented argumentation tools: Rationale, design and evaluation of a debate dashboard},
journal = {International Journal of Human-Computer Studies},
volume = {72},
number = {3},
pages = {298-319},
year = {2014},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2013.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581913001043},
author = {Luca Iandoli and Ivana Quinto and Anna {De Liddo} and Simon {Buckingham Shum}},
keywords = {Computer-supported argument visualization, Grounding process, Common ground, Debate dashboard, Collective deliberation, Visual feedback},
abstract = {Collaborative Computer-Supported Argument Visualization (CCSAV) is a technical methodology that offers support for online collective deliberation over complex dilemmas. As compared with more traditional conversational technologies, like wikis and forums, CCSAV is designed to promote more critical thinking and evidence-based reasoning, by using representations that highlight conceptual relationships between contributions, and through computational analytics that assess the structural integrity of the network. However, to date, CCSAV tools have achieved adoption primarily in small-scale educational contexts, and only to a limited degree in real world applications. We hypothesise that by reifying conversations as logical maps to address the shortcomings of chronological streams, CCSAV tools underestimate the importance of participation and interaction in enhancing collaborative knowledge-building. We argue, therefore, that CCSAV platforms should be socially augmented in order to improve their mediation capability. Drawing on Clark and Brennan influential Common Ground theory, we designed a Debate Dashboard, which augmented a CCSAV tool with a set of widgets that deliver meta-information about participants and the interaction process. An empirical study simulating a moderately sized collective deliberation scenario provides evidence that this experimental version outperformed the control version on a range of indicators, including usability, mutual understanding, quality of perceived collaboration, and accuracy of individual decisions. No evidence was found that the addition of the Debate Dashboard impeded the quality of the argumentation or the richness of content.}
}
@article{GERPOTT2024101783,
title = {New ways of seeing: Four ways you have not thought about Registered Reports yet},
journal = {The Leadership Quarterly},
volume = {35},
number = {2},
pages = {101783},
year = {2024},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2024.101783},
url = {https://www.sciencedirect.com/science/article/pii/S1048984324000122},
author = {Fabiola H. Gerpott and Roman Briker and George Banks},
keywords = {Registered Reports, Open Science, Transparency, Quantitative, Qualitative, Leadership},
abstract = {The Leadership Quarterly has helped as a pioneer in accepting Registered Reports (RRs), a submission format where authors provide the introduction, theory section, and methods of their paper for peer review before data collection. Proud but never satisfied, we aim to further boost the number of suitable RR submissions due to our firm belief in their potential for fostering transparent, high-impact research. To inspire authors to explore diverse data collection strategies and methods beyond experiments and survey-based (replication) studies, this work presents four distinct but equally suitable research formats for RRs: meta-analyses, qualitative research, computational approaches, and field intervention studies. Expanding prior research that has explored and promoted general practices and methodological standards for RRs, we offer unique recommendations for preparing an adequate RR proposal along each of these four RR avenues. Additionally, we provide a table of summary resources for authors, reviewers, and editors looking to engage more with RR. In conclusion, we envision a future where other top-tier journals and funding agencies follow The Leadership Quarterly by embracing the incorporation of RRs as a critical component of their strategic approach.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{WISTEN199777,
title = {Distributed computation of dynamic traffic equilibria},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {5},
number = {2},
pages = {77-93},
year = {1997},
note = {Parallel Computing in Transport Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(97)00003-X},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X9700003X},
author = {M.B. Wisten and M.J. Smith},
abstract = {The dynamic traffic assignment problem is formulated in the space of splitting rates rather than link and route flows. A distributed algorithm for computation of dynamic user-equilibria is specified. The algorithm has been implemented on a Meiko Computing Surface with 32 T800 processors and some numerical results are given. We do not yet have a general proof of convergence for the algorithm but we have been able to demonstrate convergence with all test networks used.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{CHEN20171,
title = {Heterogeneity in generalized reinforcement learning and its relation to cognitive ability},
journal = {Cognitive Systems Research},
volume = {42},
pages = {1-22},
year = {2017},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300559},
author = {Shu-Heng Chen and Ye-Rong Du},
keywords = {Generalized reinforcement learning, Experience-weighted attraction learning, Cognitive ability, Granularity},
abstract = {In this paper, we study the connections between working memory capacity (WMC) and learning in the context of economic guessing games. We apply a generalized version of reinforcement learning, popularly known as the experience-weighted attraction (EWA) learning model, which has a connection to specific cognitive constructs, such as memory decay, the depreciation of past experience, counterfactual thinking, and choice intensity. Through the estimates of the model, we examine behavioral differences among individuals due to different levels of WMC. In accordance with ‘Miller’s magic number’, which is the constraint of working memory capacity, we consider two different sizes (granularities) of strategy space: one is larger (finer) and one is smaller (coarser). We find that constraining the EWA models by using levels (granules) within the limits of working memory allows for a better characterization of the data based on individual differences in WMC. Using this level-reinforcement version of EWA learning, also referred to as the EWA rule learning model, we find that working memory capacity can significantly affect learning behavior. Our likelihood ratio test rejects the null that subjects with high WMC and subjects with low WMC follow the same EWA learning model. In addition, the parameter corresponding to ‘counterfactual thinking ability’ is found to be reduced when working memory capacity is low.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{GRIFFITHS2020873,
title = {Understanding Human Intelligence through Human Limitations},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {873-883},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320302151},
author = {Thomas L. Griffiths},
keywords = {artificial intelligence, inductive bias, meta-learning, rational meta-reasoning, cultural evolution},
abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.}
}
@article{PITOWSKY1996161,
title = {Laplace's demon consults an oracle: The computational complexity of prediction},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {27},
number = {2},
pages = {161-180},
year = {1996},
issn = {1355-2198},
doi = {https://doi.org/10.1016/1355-2198(96)85115-X},
url = {https://www.sciencedirect.com/science/article/pii/135521989685115X},
author = {Itamar Pitowsky}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{MUEHLENSIEPEN2022,
title = {Factors Associated With Telemedicine Use Among German General Practitioners and Rheumatologists: Secondary Analysis of Data From a Nationwide Survey},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40304},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122007373},
author = {Felix Muehlensiepen and Pascal Petit and Johannes Knitza and Martin Welcker and Nicolas Vuillerme},
keywords = {telemedicine, rheumatology, primary care, secondary analysis, health services research},
abstract = {Background
Previous studies have demonstrated telemedicine (TM) to be an effective tool to complement rheumatology care and address workforce shortage. With the outbreak of the SARS-CoV-2 pandemic, TM experienced a massive upswing. However, in rheumatology care, the use of TM stagnated again shortly thereafter. Consequently, the factors associated with physicians’ willingness to use TM (TM willingness) and actual use of TM (TM use) need to be thoroughly investigated.
Objective
This study aimed to identify the factors that determine TM use and TM willingness among German general practitioners and rheumatologists.
Methods
We conducted a secondary analysis of data from a German nationwide cross-sectional survey with general practitioners and rheumatologists. Bayesian univariate and multivariate logistic regression analyses were applied to the data to determine which factors were associated with TM use and TM willingness. The predictor variables (covariates) that were studied individually included sociodemographic factors (eg, age and sex), work characteristics (eg, practice location and medical specialty), and self-assessed knowledge of TM. All the variables positively and negatively associated with TM use and TM willingness in the univariate analysis were then considered for Bayesian model averaging analysis after a selection based on the variance inflation factor (≤2.5). All analyses were stratified by sex.
Results
Univariate analysis revealed that out of 83 variables, 36 (43%) and 34 (41%) variables were positively or negatively associated (region of practical equivalence≤5%) with TM use and TM willingness, respectively. The Bayesian model averaging analysis allowed us to identify 13 and 17 factors of TM use and TM willingness, respectively. Among these factors, being female, having very poor knowledge of TM, treating <500 patients per quarter, and not being willing to use TM were negatively associated with TM use, whereas having good knowledge of TM and treating >1000 patients per quarter were positively associated with TM use. In addition, being aged 51 to 60 years, thinking that TM is not important for current and future work, and not currently using TM were negatively associated with TM willingness, whereas owning a smart device and working in an urban area were positively associated with TM willingness.
Conclusions
The results point to the close connection between health care professionals’ knowledge of TM and actual TM use. These results lend support to the integration of digital competencies into medical education as well as hands-on training for health care professionals. Incentive programs for physicians aged >50 years and practicing in rural areas could further encourage TM willingness.}
}
@article{SAJID2023103174,
title = {Thermal case classification of solar-powered cars for binary tetra hybridity nanofluid using Cash and Carp method with Hamilton-Crosser model},
journal = {Case Studies in Thermal Engineering},
volume = {49},
pages = {103174},
year = {2023},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103174},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X2300480X},
author = {Tanveer Sajid and Wasim Jamshed and Nek Muhammad Katbar and Mohamed R. Eid and Assmaa Abd-Elmonem and Nesreen Sirelkhtam Elmki Abdalla and Sayed M. {El Din} and Gilder Cieza Altamirano},
keywords = {Solar sports car, Solar sheet, Reiner-Philippoff tetrhybrid nanofluid, Thermal radiation, Heat generation},
abstract = {Solar energy is the most important source of thermal energy that comes from the sun. This kind of energy has enormous potential applications in fields of technology such as photovoltaic panels, renewable power, solar light poles, and solar pumps used for water extraction. The era in which we are living is all about the applications of solar energy in industrial sectors most importantly in solar sports car manufacturing. This article presents a new way of thinking about the heat transport analyses of photovoltaic hybrid vehicles, by factoring Casson-Sutterby liquid with the inclusion of various effects like variable thermal conduction, thermal radiation, heat generation, and tetrahybrid nanoparticles. To solve the modelled equations in regards to both momentum and energy, another well-computational approach known as the Cash and Carp method was used. The effects of a wide variety of factors on temperature, shear stress, and velocity fields, as well as the surface drag coefficient and Nusselt number, are briefly described and illustrated in the form of tables and figures. It then found that the thermal radiation, heat production, and thermal conductivity parameters and insertion of agglomerative tetrhybrid nanoparticles in the base fluid amplify heat transfer rate, it has been shown that the performance of the solar car increases in terms of heat transition. In comparison to standard nanofluid, tetrahybrid nanofluid is the most effective medium for the transmission of heat. From the regression analysis, it is observed that the error in terms of Nusselt number is smaller 0.0151 for the case ε=1.5, and increases to 0.0151 in the case of ε=2.5. Relative percentage error is smaller 4.62% in the case of heat generation Q=0.7 but a maximum of 15.8% in the case of thermal radiation Rd=2.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{ZHANG2024100479,
title = {Open source implementations of numerical algorithms for computing the complete elliptic integral of the first kind},
journal = {Results in Applied Mathematics},
volume = {23},
pages = {100479},
year = {2024},
issn = {2590-0374},
doi = {https://doi.org/10.1016/j.rinam.2024.100479},
url = {https://www.sciencedirect.com/science/article/pii/S2590037424000499},
author = {Hong-Yan Zhang and Wen-Juan Jiang},
keywords = {Complete elliptic integral of the first kind (CEI-1), Algorithm design, Orthogonal polynomials, Verification-validation-testing (VVT), STEM education, Computer programming},
abstract = {The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions ellipticK and EllipticK provided by MATLAB and Mathematica respectively, are based on Kcs(m) instead of the usual form K(k) such that Kcs(k2)=K(k) and m=k2. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss–Chebyshev method and Gauss–Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing the CEI-1 are designed, verified, validated and tested, which can be utilized in R& D and be reused properly. Numerical results show that our open source implementations based on K(k) are equivalent to the commercial implementation based on Kcs(m). The general algorithms for computing orthogonal polynomials developed are valuable for the STEM education and scientific computation.}
}
@article{LIU2024111728,
title = {DuaPIN: Auxiliary task enhanced dual path interaction network for civil court view generation},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111728},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003630},
author = {Nayu Liu and Luyao Ma and Yiquan Wu and Kaiwen Wei and Cunhang Fan and Yating Zhang},
keywords = {Dual path interaction network, Auxiliary task, Civil court view generation, Natural language processing},
abstract = {Civil court view generation (CCVG) is a novel but important task for legal intelligence that aims to automatically generate a judge’s opinion based on the plaintiff’s claims and fact descriptions to interpret the judgment result. The task is more challenging than criminal court view generation as the latter generates views based only on criminal facts as input, whereas the CCVG must consider both the plaintiff’s claims and civil facts under the principle of “no claim, no trial.” However, current approaches still follow criminal domain practices to solve problems in civil cases. Moreover, the explicit modeling of the potential correspondence between claims and facts has often been neglected, as court views are required to respond to each corresponding claim based on factual evidence. To address the issues, we propose a dual path interaction network augmented by two self-supervised auxiliary tasks (named DuaPIN), which follows a bionic design by simulating the thinking logic of judges when writing opinions. Specifically, we construct a structurally symmetric Transformer-based dual path multi-encoder–decoder model such that the two inputs, claim and fact, contribute equally to the generation of civil court views. Moreover, an auxiliary task enhanced (ATE) training paradigm using multiple DuaPIN decoders is proposed to explicitly model the potential correspondence between claims and facts. Extensive experiments on public legal document dataset demonstrated that DuaPIN achieves competitive performance compared with previous methods and offers certain performance improvements to popular pre-trained language models via the ATE training method.}
}
@article{ZAHEDI2024103730,
title = {How hypnotic suggestions work – A systematic review of prominent theories of hypnosis},
journal = {Consciousness and Cognition},
volume = {123},
pages = {103730},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000977},
author = {Anoushiravan Zahedi and Steven {Jay Lynn} and Werner Sommer},
keywords = {Hypnosis, Theory, Suggestibility, Hypnotizability, Hypnotic Suggestions (HS), Posthypnotic Suggestions (PHS), Direct Verbal Suggestions},
abstract = {In recent decades, hypnosis has increasingly moved into the mainstream of scientific inquiry. Hypnotic suggestions are frequently implemented in behavioral, neurocognitive, and clinical investigations and interventions. Despite abundant reports about the effectiveness of suggestions in altering behavior, perception, cognition, and agency, no consensus exists regarding the mechanisms driving these changes. This article reviews competing theoretical accounts that address the genesis of subjective, behavioral, and neurophysiological responses to hypnotic suggestions. We systematically analyze the broad landscape of hypnosis theories that best represent our estimation of the current status and future avenues of scientific thinking. We start with procedural descriptions of hypnosis, suggestions, and hypnotizability, followed by a comparative analysis of systematically selected theories. Considering that prominent theoretical perspectives emphasize different aspects of hypnosis, our review reveals that each perspective possesses salient strengths, limitations, and heuristic values. We highlight the necessity of revisiting extant theories and formulating novel evidence-based accounts of hypnosis.}
}
@incollection{DORFLER202057,
title = {Artificial Intelligence},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {57-64},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23863-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245238637},
author = {Viktor Dörfler},
keywords = {AI, AI paradigms, Artificial intelligence, Artificial neural networks, Cognition, Creativity, Intuition, Learning, Machine learning, Mind, Mind and machine, Narrow AI, Thinking, Thinking machines, Wide AI},
abstract = {ARTIFICIAL INTELLIGENCE is a label coined to describe machines that can perform something humans would perform through thinking. In this chapter, I am looking at artificial intelligence (AI) specifically in the context of creativity. My view is inevitably a personal one, as for the time being, the answers to the tough questions on AI entail working with beliefs more so than with facts, opinions more so than hard evidence. What matters most in “AI Creativity” is how we define creativity, as this definition will determine whether we can consider AI to be creative, now or in the future. It is also important to explore what kind of impact AI has or may have on human creativity.}
}
@article{WORBOYS199885,
title = {Computation with imprecise geospatial data},
journal = {Computers, Environment and Urban Systems},
volume = {22},
number = {2},
pages = {85-106},
year = {1998},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(98)00023-4},
url = {https://www.sciencedirect.com/science/article/pii/S0198971598000234},
author = {Michael Worboys},
abstract = {Imprecision in spatial data arises from the granularity or resolution at which observations of phenomena are made, and from the limitations imposed by computational representations, processing and presentational media. Precision is an important component of spatial data quality, and a key to appropriate integration of collections of data sets. Previous work of the author provides a theoretical foundation for imprecision of spatial data resulting from finite granularities, and gives the beginnings of an approach to reasoning with such data using methods similar to rough set theory. This paper develops the theory further, and extends the work to a model that includes both spatial and semantic components. Notions such as observation, schema, frame of discernment and vagueness are examined and formalised.}
}
@article{DECARVALHO2025111158,
title = {A game-inspired algorithm for marginal and global clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111158},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009099},
author = {Miguel {de Carvalho} and Gabriel Martos and Andrej Svetlošák},
keywords = {Cluster analysis, Mixture models, Model-based clustering, Similarity-based clustering, Unsupervised learning},
abstract = {An often overlooked pitfall of model-based clustering is that it typically results in the same number of clusters per margin, an assumption that may not be natural in practice. We develop a clustering method that takes advantage of the sturdiness of model-based clustering, while attempting to mitigate this issue. The proposed approach allows each margin to have a varying number of clusters and employs a strategy game-inspired algorithm, named ‘Reign-and-Conquer’, to cluster the data. Since the proposed clustering approach only specifies a model for the margins, but leaves the joint unspecified, it has the advantage of being partially parallelizable; hence, the proposed approach is computationally appealing as well as more tractable for moderate to high dimensions than a ‘full’ (joint) model-based clustering approach. A battery of numerical experiments on simulated data indicates an overall good performance of the proposed methods in a variety of scenarios, and real datasets are used to showcase their usefulness in practice.}
}
@article{ZHAI2023101373,
title = {Can reflective interventions improve students’ academic achievement? A meta-analysis},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101373},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101373},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001414},
author = {Na Zhai and Yong Huang and Xiaomei Ma and Jingchun Chen},
keywords = {Reflection, Reflective intervention, Academic achievement, Meta-analysis},
abstract = {Reflection is widely acknowledged as a crucial skill for successful learning and decision-making. Recent evidence has shown that reflection can enhance motivation for in-depth learning, improve cognitive and metacognitive strategies, and promote self-regulated learning. While some studies have reported the positive effects of reflective interventions on student academic outcomes, conflicting findings exist. To provide a comprehensive understanding of the effectiveness of reflective interventions on academic achievement, this meta-analysis synthesized data from 25 quantitative studies (comprising 29 effect sizes) conducted between 2012 and 2022, with a total of 2,111 participants. The results revealed a significant overall effect of reflective interventions on academic achievement (g = 0.793, p < 0.001). Further moderator analyses indicated that the effectiveness of reflective interventions was influenced by factors such as learning mode, intervention duration, the role of reflective writing, and culture. However, education level, discipline, teacher or expert feedback, peer interaction, and technological scaffolding did not significantly affect the impact of reflective interventions across studies. These findings highlight the importance of fostering reflective thinking and refining the detailed design of reflective interventions to enhance students’ academic achievement.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@incollection{COXON2019179,
title = {Chapter 7 - Transforming Future Mobility},
editor = {Selby Coxon and Robbie Napper and Mark Richardson},
booktitle = {Urban Mobility Design},
publisher = {Elsevier},
pages = {179-214},
year = {2019},
isbn = {978-0-12-815038-2},
doi = {https://doi.org/10.1016/B978-0-12-815038-2.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150382000074},
author = {Selby Coxon and Robbie Napper and Mark Richardson},
keywords = {Innovation methodology, Design thinking, Future mobility},
abstract = {The book having built a picture of a driverless, accessible, positive experience and inventively built mobility landscape, now leverages the techniques of design thinking to demonstrate the tools of a future economy and how they might be applied to a range of future mobility speculations. This chapter demonstrates that a combination of technology developments and design thinking skills can generate inventive compelling solutions to mobility problems. The chapter is illustrated with examples of these research speculations.}
}
@article{YECKEL1998206,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate: II. Spin down},
journal = {Journal of Crystal Growth},
volume = {191},
number = {1},
pages = {206-224},
year = {1998},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(98)00102-X},
url = {https://www.sciencedirect.com/science/article/pii/S002202489800102X},
author = {Andrew Yeckel and Yuming Zhou and Michael Dennis and Jeffrey J. Derby},
keywords = {Fluid flow, Solution growth, Finite element model},
abstract = {Three-dimensional, time-dependent flows that occur in the Lawrence Livermore National Laboratory system for rapid growth of potassium dihydrogen phosphate (KDP) crystals from solution are studied using massively parallel finite element computations. The simulation reveals that excellent global mixing occurs during the spin-down phase of a time-dependent stirring cycle. The large scale fluid motions in the radial and axial directions that promote mixing are caused primarily by effects of platform geometry, but are augmented to some degree by the intrinsic tendency of a decelerating rotational flow to reverse direction within Ekman layers that form at the boundaries. Along with Part I of this work [Y. Zhou and J.J. Derby, J. Crystal Growth 180 (1997) 497], which emphasized spin up and steady rotation, significant advances have been made in our understanding of hydrodynamic phenomena in this system.}
}
@article{THIBODEAU2017852,
title = {How Linguistic Metaphor Scaffolds Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {11},
pages = {852-863},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301535},
author = {Paul H. Thibodeau and Rose K. Hendricks and Lera Boroditsky},
keywords = {analogy, decision making, framing, language and thought, metaphor, reasoning},
abstract = {Language helps people communicate and think. Precise and accurate language would seem best suited to achieve these goals. But a close look at the way people actually talk reveals an abundance of apparent imprecision in the form of metaphor: ideas are ‘light bulbs’, crime is a ‘virus’, and cancer is an ‘enemy’ in a ‘war’. In this article, we review recent evidence that metaphoric language can facilitate communication and shape thinking even though it is literally false. We first discuss recent experiments showing that linguistic metaphor can guide thought and behavior. Then we explore the conditions under which metaphors are most influential. Throughout, we highlight theoretical and practical implications, as well as key challenges and opportunities for future research.}
}
@article{LIU2023109530,
title = {Quantum computing for power systems: Tutorial, review, challenges, and prospects},
journal = {Electric Power Systems Research},
volume = {223},
pages = {109530},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623004194},
author = {Hualong Liu and Wenyuan Tang},
keywords = {Quantum computing, Optimization, Power systems, Renewable energy, Climate neutrality},
abstract = {As a large number of renewable energy resources are connected to power systems, the operation, planning, and optimization of power systems have been becoming more and more complex. Power flow calculation, unit commitment, economic dispatch, energy pricing, and power system planning are essentially computation problems. A lot of computing resources are required for these problems, which are non-trivial, especially for large-scale power systems with the high penetration of renewable energy. Traditionally, the calculation and optimization of power systems are completed by classical computers based on the classical computing theory and the von Neumann architecture. However, with Moore’s law getting closer and closer to the limit, the importance of quantum computing has become increasingly prominent. Quantum computing has been applied to some fields to a certain extent, yet the applications of quantum computing in power systems are rare. As the power industry is the foundation of the national economy, introducing quantum computing into the power system has far-reaching and crucial significance, such as improving the penetration of renewable energy, enhancing the computing efficiency, and helping in achieving the goal of net zero and climate neutrality by 2050. This paper first introduces the core concepts, essential ideas and theories of quantum computing, and then reviews the existing literature on the applications of quantum computing in power systems, and puts forward our critical thinking about the applications of quantum computing in power systems. In brief, this paper is dedicated to a tutorial on quantum computing targeting power system professionals and a review of its applications in power systems. The main contributions of this paper are: (1) introduce quantum computing into the field of power engineering in a thoroughly detailed way and delineate the analysis methodologies of quantum circuits systematically without losing mathematical rigor; (2) based on Dirac’s notation, the related formulae are derived meticulously with sophisticated schematic diagrams; (3) elaborate and derive some critical quantum algorithms in depth, which play an important role in the applications of quantum computing in power systems; (4) critically summarize and comment on the existing literature on the applications of quantum computing in power systems; (5) the future applications and challenges of quantum computing in power systems are prospected and remarked.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{LI2023101752,
title = {The role of inhibition in overcoming arithmetic natural number bias in the Chinese context: Evidence from behavioral and ERP experiments},
journal = {Learning and Instruction},
volume = {86},
pages = {101752},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101752},
url = {https://www.sciencedirect.com/science/article/pii/S095947522300021X},
author = {Xiaodong Li and Ping Xu and Ronghuan Jiang and Shuang Chen},
keywords = {Inhibitory control, Negative priming, Natural number bias, Arithmetic operation, Event-related potential},
abstract = {The natural number bias (NNB) in arithmetic operations refers to the application of natural number properties to reasoning about rational numbers. Previous studies found the NNB interferes with students’ problem-solving. However, few studies have examined it in the Chinese context or the underlying mechanism by which it can be overcome. Addressing these gaps, in Experiments 1a (n = 31) and 1b (n = 30), we found that Chinese students demonstrate the NNB despite linguistic differences between Chinese and western languages. Experiment 2 (n = 38) adopted a negative priming paradigm and found that inhibitory control was necessary to overcome the NNB. Experiment 3 (n = 34) employed the event-related potential technique; we observed increased P2 amplitude when students solved congruent problems, and increased N2 and decreased P3 amplitude when they solved incongruent problems. These results indicated that the NNB is rooted in intuitive thinking, and overcoming this bias relies on inhibition.}
}
@article{HAYES201739,
title = {Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
journal = {Behaviour Research and Therapy},
volume = {98},
pages = {39-57},
year = {2017},
note = {Best Practice Guidelines for Modern Statistical Methods in Applied Clinical Research},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005796716301887},
author = {Andrew F. Hayes and Nicholas J. Rockwood},
keywords = {Mediation analysis, Moderation, Interaction, Regression analysis, Mechanisms},
abstract = {There have been numerous treatments in the clinical research literature about various design, analysis, and interpretation considerations when testing hypotheses about mechanisms and contingencies of effects, popularly known as mediation and moderation analysis. In this paper we address the practice of mediation and moderation analysis using linear regression in the pages of Behaviour Research and Therapy and offer some observations and recommendations, debunk some popular myths, describe some new advances, and provide an example of mediation, moderation, and their integration as conditional process analysis using the PROCESS macro for SPSS and SAS. Our goal is to nudge clinical researchers away from historically significant but increasingly old school approaches toward modifications, revisions, and extensions that characterize more modern thinking about the analysis of the mechanisms and contingencies of effects.}
}
@article{DAS2024100104,
title = {AI and data-driven urbanism: The Singapore experience},
journal = {Digital Geography and Society},
volume = {7},
pages = {100104},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000266},
author = {Diganta Das and Berwyn Kwek},
keywords = {Singapore, Smart cities, Smart nation, Artificial intelligence (AI), Digital urbanism},
abstract = {This paper presents a deep and critical analysis of Singapore's new wave of state-built digital tools and services and how it connects to its larger smart urbanism project, also known as Smart Nation. The COVID-19 pandemic, and particularly Singapore's response, served as a real-world testing ground for smart urbanist strategies. In particular, we analysed the logic that emanates from these novel digital interventions, how they operate on the complex urban built environment and the population, and their effects on urban and citizenry morphologies. Next, we examined a series of state-led technological implementations that have emerged since the Covid-19 pandemic, providing digital solutions that assist citizens with the changing rhythms of everyday living, data-capturing sensors and gantries to aid authorities in contract tracing efforts and enforce vaccination differentiation measures, geospatial digital mapping of demographic data, in withal robotics for automated policing and cleaning activities; and the use of AI and automated data-driven tools in public health to improve service delivery and care to patients. While we are unable to exhaust every piece of technology for the purpose of this paper, these developments, along with their design thinking and operations, we argue, are helpful in revealing the contemporary conjectures of Singaporean digital urban idealism and the governing strategies of the state. By examining Singapore's response, this study aims to contribute to the ongoing discourse on smart urbanism, offering insights into how cities can leverage technology effectively while balancing technological innovation with privacy and public trust.}
}
@article{BARILE2022467,
title = {Platform-based innovation ecosystems: Entering new markets through holographic strategies},
journal = {Industrial Marketing Management},
volume = {105},
pages = {467-477},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122001614},
author = {Sergio Barile and Cristina Simone and Francesca Iandolo and Antonio Laudando},
keywords = {Platforms, Innovation ecosystems, Platform-based innovation ecosystems, Holographic strategies, Digital algorithms, Platform envelopment},
abstract = {The platformization seems to be a demiurgic force, increasingly (re)shaping this millennium and its socio-economic, technological and physical structures, institutions, and human lives. Innovation ecosystems are experiencing this platformization, leading to the rise of platform-based innovation ecosystems. However, the industrial and managerial literature still lacks a shared definition, a consistent theoretical and strategic framework to explain how platform-based innovation ecosystems emerge and replicate from market to market. This conceptual work attempts to fill those gaps by integrating the extant literature on innovation ecosystems in two ways. First, moving from the literature on innovation ecosystems and industry platforms, using systems thinking framing, it explains the platformization of innovation ecosystems through the double lens structure-system. Second, it identifies the holographic strategy as one of the typical patterns featuring platform-based innovation ecosystem envelopment beyond extant market boundaries. These conceptualizations have insightful theoretical, managerial, and policy implications. In particular, the work discusses the ecosystem as a valid unit of analysis for understanding such an unprecedented shaped-by-platform landscape. Then, it describes the growth strategies of the platform-based innovation ecosystem supporting the platform sponsor in mastering multipoint competition. Eventually, the study pinpoints crucial issues for policymakers in regulating the impact that platformization is having on society.}
}
@article{KELTNER2021216,
title = {A taxonomy of positive emotions},
journal = {Current Opinion in Behavioral Sciences},
volume = {39},
pages = {216-221},
year = {2021},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621000991},
author = {Dacher Keltner and Alan Cowen},
abstract = {Within social functionalist theory (SFT), emotions structure attachment relations, cooperative alliances, hierarchies, and collectives. Within this line of thinking, a rich array of positive emotions enable the formation and negotiation of these relationships. Guided by these arguments, we synthesize how top-down confirmatory studies and data-driven, computational studies converge on evidence for 11 positive emotions with distinct experience, expression, and physiology. This taxonomy includes amusement, awe, compassion, contentment, desire, love, joy, interest, pride, relief, and triumph. We conclude by considering how recent taxonomic efforts will advance emotion science in mapping the distinct forms and functions of the positive emotions.}
}
@article{MAHONY2020104668,
title = {New ideas for non-animal approaches to predict repeated-dose systemic toxicity: Report from an EPAA Blue Sky Workshop},
journal = {Regulatory Toxicology and Pharmacology},
volume = {114},
pages = {104668},
year = {2020},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2020.104668},
url = {https://www.sciencedirect.com/science/article/pii/S0273230020300945},
author = {Catherine Mahony and Randolph S. Ashton and Barbara Birk and Alan R. Boobis and Tom Cull and George P. Daston and Lorna Ewart and Thomas B. Knudsen and Irene Manou and Sebastian Maurer-Stroh and Luigi Margiotta-Casaluci and Boris P. Müller and Pär Nordlund and Ruth A. Roberts and Thomas Steger-Hartmann and Evita Vandenbossche and Mark R. Viant and Mathieu Vinken and Maurice Whelan and Zvonar Zvonimir and Mark T.D. Cronin},
keywords = {Repeated dose toxicity testing, Alternatives, Safety assessment, Chemical legislation, , , Read-across, },
abstract = {The European Partnership for Alternative Approaches to Animal Testing (EPAA) convened a ‘Blue Sky Workshop’ on new ideas for non-animal approaches to predict repeated-dose systemic toxicity. The aim of the Workshop was to formulate strategic ideas to improve and increase the applicability, implementation and acceptance of modern non-animal methods to determine systemic toxicity. The Workshop concluded that good progress is being made to assess repeated dose toxicity without animals taking advantage of existing knowledge in toxicology, thresholds of toxicological concern, adverse outcome pathways and read-across workflows. These approaches can be supported by New Approach Methodologies (NAMs) utilising modern molecular technologies and computational methods. Recommendations from the Workshop were based around the needs for better chemical safety assessment: how to strengthen the evidence base for decision making; to develop, standardise and harmonise NAMs for human toxicity; and the improvement in the applicability and acceptance of novel techniques. “Disruptive thinking” is required to reconsider chemical legislation, validation of NAMs and the opportunities to move away from reliance on animal tests. Case study practices and data sharing, ensuring reproducibility of NAMs, were viewed as crucial to the improvement of non-animal test approaches for systemic toxicity.}
}
@article{GARGALO2024108504,
title = {A process systems engineering view of environmental impact assessment in renewable and sustainable energy production: Status and perspectives},
journal = {Computers & Chemical Engineering},
volume = {180},
pages = {108504},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003745},
author = {Carina L. Gargalo and Haoshui Yu and Nikolaus Vollmer and Ahmad Arabkoohsar and Krist V. Gernaey and Gürkan Sin},
keywords = {Renewable and sustainable energy systems, Environmental impact assessment, Process systems engineering, Life cycle assessment, Sustainability},
abstract = {With the increasing concern for climate change, renewable and sustainable energy production has attracted considerable attention from the scientific community, industrial practitioners, and policy and decision-makers. There are many technological alternatives for each sub-category of complex sustainable energy systems. Life cycle assessment (LCA) can be an effective tool to compare the environmental impacts of each pathway and identify the most promising alternatives from an environmental impact perspective. This contribution first reviews the environmental assessment methods and tools developed over the years. Secondly, a comprehensive review of the contribution of the PSE community to the environmental impact analysis of renewable energy systems is performed. It is observed that while LCA is the preferred method, these studies differed widely concerning the choice of impact assessment method used, the level of details shared concerning the underlying LCA calculations, and whether or not sensitivity and uncertainty analyses were carried out, among many others. This makes the comparison of results from different studies difficult and often impossible. It is clear that the PSE community, with its emphasis on systems thinking and holistic approaches, plays a critical role in the design, integration, and operation of complex sustainable energy systems. However, the thorough calculations necessary to ensure a robust and transparent LCA analysis require a shared methodology and a detailed description of the rules. Such explicit, systematic, and transparent methods will set the bar for a minimum requirement for thorough LCA calculations, ensuring fair comparison and discussions of different technical solutions developed in the wider PSE community for sustainable renewables.}
}
@article{ABEYSEKERA2024100213,
title = {ChatGPT and academia on accounting assessments},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {1},
pages = {100213},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124000076},
author = {Indra Abeysekera},
keywords = {Academia, Accounting, Assessments, ChatGPT, Multiple Choice Questions, Sustainable Development Goals of the United Nations},
abstract = {ChatGPT is considered a risk and an opportunity for academia. An area of threat in contemporary settings is whether it can become a student agent for assessments in academia. This study determines how ChatGPT can become a human agent for students on two financial accounting course units, multiple choice question assessments. The study provided five numerical-based and five narrative-based multiple choice questions. There were ten questions for the Introductory Financial Accounting and 10 for the Advanced Financial Accounting course units. ChatGPT received one question at a time requesting a solution. In the Introductory Financial Accounting section, ChatGPT produced incorrect answers because it incorrectly assumed the underlying assumptions contained in those questions. In Advanced Financial Accounting, ChatGPT presented incorrect answers because of the complexity of the task contained in those questions. ChatGPT demonstrated similar competencies in providing solutions to numerical-based and narrative-based questions. ChatGPT obtained the correct answers to sit in the 80th percentile in the Introductory Financial Accounting course unit assessment and the 50th percentile in the Advanced Financial course unit assessment. ChatGPT4 showed improved performance, with the 90th percentile for Introductory Financial Accounting and the 70th percentile for Advanced Financial Accounting. The findings indicate that the knowledge construct requires reflective thinking with ChatGPT in the ecosystem, and what is assumed and assessable knowledge must be revisited.}
}
@article{LI2023113687,
title = {Twins transformer: Cross-attention based two-branch transformer network for rotating bearing fault diagnosis},
journal = {Measurement},
volume = {223},
pages = {113687},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113687},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123012514},
author = {Jie Li and Yu Bao and WenXin Liu and PengXiang Ji and LeKang Wang and Zhongbing Wang},
keywords = {Attention mechanisms, Cross-attention, Fault diagnosis, Transformer},
abstract = {Due to the inherent shortcomings of traditional depth models, the Transformer model based on the self-attention mechanism has become popular in the field of fault diagnosis. The current Transformer's self-attentive mechanism provides an alternative way of thinking, which can make direct association between each signal. However, it can only focus on the association information within a sequence, and it is difficult to understand the information gap between samples. Therefore, this paper proposes the two-branch Twins attention, which for the first time uses cross-attention to focus on information associations between samples. Twins attention uses cross-attention to learn information associations between samples in addition to retaining the information associations within sequences learned by self-attention. The performance of the proposed model was validated on four popular bearing datasets. Compared to the original transformer structure, the average accuracy of each dataset improved by 1.73% to 99.42%, leading the noise experiments.}
}
@article{RAIKOV2018492,
title = {Cognitive Modelling Quality Rising by Applying Quantum and Optical Semantic Approaches},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {492-497},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.309},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318329823},
author = {A. Raikov},
keywords = {big data, quantum semantic, cognitive modelling, deep learning, decision-making, optical computing},
abstract = {Advanced decision support systems require significant acceleration of decision-making under conditions when factors describing the situation are ill-defined and non-metric. As a rule, such conditions arise in the field of politics, culture and in the social sphere. Cognitive modelling technology is applied in these cases. The cognitive models are created by people, experts from different subjects’ fields. The modelling processes take a great deal of time. Furthermore, the result of the modelling has to be verified when the model’s creators cannot get complete information and have to understand the problem very quickly. The factors and their mutual relationships in cognitive models could be verified with Big Data analysis technology. But this approach takes into account only denotational semantics that are based on the mapping of the model on formalised logical constructions, words, objects, schemes. This paper addresses the issue of creating cognitive semantics that take into consideration thinking, feeling and transcendental factors. It is shown that the classical computer or quantum computer cannot ensure cognitive semantics because they are based on discrete representation of data. An optical computing and Optical Semantic approach could be applied. The architecture of the special optical processor is represented.}
}
@article{SELVAKKUMARAN2020111053,
title = {Review of the use of system dynamics (SD) in scrutinizing local energy transitions},
journal = {Journal of Environmental Management},
volume = {272},
pages = {111053},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720309816},
author = {Sujeetha Selvakkumaran and Erik O. Ahlgren},
keywords = {System dynamics, Modelling, Local, Energy transitions, Multi-level perspective},
abstract = {Local energy transition processes are complex socio-technical transitions requiring careful study. The use of System Dynamics (SD) in modelling and analyzing local energy transitions is especially suitable given the characteristics of SD. Our aim is to systematically categorize the different ways SD is used and useful to scrutinize local energy transitions, and to see if we can discern any common themes that can be useful to researchers looking to scrutinize local energy transitions, using SD. The study is exploratory in nature, with peer-reviewed journal and conference articles analyzed using content analysis. The six categories on which the articles are analyzed are: the sector the article studies; the transition that is studied in the article; the modelling depth in the article; the objective of the article; the justification for using SD provided in the article and the levels of interaction with ‘local’. Our findings show most of the local energy transitions have been studied using simulatable Stock and Flow Diagrams in SD methodology. The important sectors in the energy field are represented in terms of SD modelling of local energy transitions, including electricity, transport, district heating etc. Most of the local energy transitions scrutinized by SD in the articles have descriptive objectives, with some prescriptive, and just one evaluative objective. In terms of justification for using SD provided by the articles analyzed in this study, we found four major themes along which the justifications that were provided. They are dynamics, feedbacks, delays and complexity, systematic thinking, bridging disciplines and actor interactions and behaviour. The ‘dynamics, feedbacks, delays and complexity’ theme is the most cited justification for the use of SD in scrutinizing local energy transitions, followed by systematic thinking.}
}
@article{SLANKSTERSCHMIERER202538,
title = {Modernizing toxicology: The importance of accessible NAM training},
journal = {Toxicology Letters},
volume = {406},
pages = {38-39},
year = {2025},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425000402},
author = {Eryn Slankster-Schmierer},
keywords = {Training, Professional development, Education and outreach, New approach methodologies, In vitro toxicology, Computational toxicology, Regulatory toxicology, Alternative approaches, Nonanimal, NAM},
abstract = {Current toxicology curricula and certifications are heavily reliant on animal-based research and lack mandatory education and training in New Approach Methodologies (NAMs). Traditional animal-based toxicological methods come with many concerns, including translatability and reproducibility, which NAMs are aptly positioned to address. The NAM Use for Regulatory Application (NURA) program aims to bridge this educational gap by providing training to toxicologists, method developers, regulators, and legislators on the use of NAMs, helping to build confidence in NAM use and facilitate the shift to more human-based methods.}
}
@incollection{BARDINI202537,
title = {Chapter 4 - From sketch to landscape: Transforming neuronal concepts across technological change},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {37-52},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000043},
author = {Roberta Bardini},
keywords = {Artificial intelligence, Brain complexity, Breakthroughs in neuroscience, Cell theory, Computational biology, Electrophysiology, Emerging technologies in neuroscience, History of neuroscience, History of technology, Interdisciplinary approaches in neuroscience, Machine learning, Multiomics, Nervous system, Neural networks (biological sciences), Neurology, Neuron, Neuron doctrine, Neuroscience, Neuroscience research advances, Patch-clamp, Patch-seq, Systems biology, Techniques in neuroscience},
abstract = {This chapter delves into the evolution of neuronal concepts, tracing their journey through technological change, from the early days of hand-drawn sketches to the advanced techniques of computational modeling in systems neuroscience. Covering a timeline from the 17th century's scientific revolution to the 21st century's technological boom, this analysis highlights key milestones in the history of neurology and the intricate understanding of neuronal functions. It presents a narrative that emphasizes the significant role technological innovations have played in shaping our perception of neuronal cells. With each scientific breakthrough, we gain deeper insights into the neuron's critical role within the brain's complex network. This chapter not only provides a historical perspective but also sets the stage for future discoveries that will continue to revolutionize our understanding of the neuroscience landscape.}
}
@article{ALTAY20111111,
title = {Fuzzy cognitive mapping in factor elimination: A case study for innovative power and risks},
journal = {Procedia Computer Science},
volume = {3},
pages = {1111-1119},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005569},
author = {Ayca Altay and Gülgün Kayakutlu},
keywords = {Fuzzy cognitive maps, Innovation, Factor prioritization, Factor elimination},
abstract = {Factor or criteria prioritization is essential for decision making and planning. In most areas in decision making, integrating the related literature yields an exuberance of criteria which leads a robust decision. Yet, an excess number of criteria may handicap decision making or evaluations in terms of computational time and complexity. In these circumstances, decreasing the number of factors in exchange for a negligible amount of knowledge can emancipate the decision maker yet does not affect the quality of the decision. This elimination can be conducted through qualitative methods such as interviews or quantitative methods. However, quantitative methods are more trustworthy since qualitative methods can be deceptive due to the perceptions of the interviewee. Furthermore, working with larger groups is more prone to neutrality in terms of group thinking. On the subject of innovative power and risks, the literature offers 48 criteria depending on the industry, size or demographics of related companies. Prioritizing and working with these criteria for their decision making applications becomes computationally expensive, especially when embedded in more complex algorithms. In this study, 48 criteria will be reduced using Fuzzy Cognitive Maps and it is believed to provide a sufficient number of criteria with a negligible loss of information and comparisons will be conducted.}
}
@article{JONES2001325,
title = {NMR quantum computation},
journal = {Progress in Nuclear Magnetic Resonance Spectroscopy},
volume = {38},
number = {4},
pages = {325-360},
year = {2001},
issn = {0079-6565},
doi = {https://doi.org/10.1016/S0079-6565(00)00033-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079656500000339},
author = {J.A. Jones}
}
@article{YOON2021100865,
title = {United States and South Korean citizens’ interpretation and assessment of COVID-19 quantitative data},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100865},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100865},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000262},
author = {Hyunkyoung Yoon and Cameron O’Neill Byerley and Surani Joshua and Kevin Moore and Min Sook Park and Stacy Musgrave and Laura Valaas and James Drimalla},
keywords = {COVID-19, Graphs, Representations of quantitative data, Rate of change, Exponential growth},
abstract = {We investigate United States and South Korean citizens’ mathematical schemes and how these schemes supported or hindered their attempts to assess the severity of COVID-19. We selected web and media-based COVID-19 data representations that we hypothesized citizens would interpret differently depending on their mathematical schemes. We included items that we conjectured would be easier or more difficult to interpret with schemes that prior research had reported were more or less productive, respectively. We used the representations during clinical interviews with 25 United States and seven South Korean citizens. We illustrate that citizens’ mathematical schemes (as well as their beliefs) impacted how they assessed the severity of COVID-19. We present vignettes of citizens’ schemes that inhibited interpreting representations of COVID-19 in ways compatible with the displayed quantitative data, schemes that aided them in assessing the severity of COVID-19, and beliefs about the reliability of scientific data that overrode their mathematical conclusions.}
}
@article{SAWLEY1994363,
title = {A comparative study of the use of the data-parallel approach for compressible flow calculations},
journal = {Parallel Computing},
volume = {20},
number = {3},
pages = {363-373},
year = {1994},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(06)80019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106800190},
author = {M.L. Sawley and C.M. Bergman},
keywords = {Computational fluid dynamics, Euler equations, Data-parallel programming, Portability, Performance results},
abstract = {The results are presented of an investigation into the use of the data-parallel programming approach on four different massively-parallel computers: the MasPar MP-1 and MP-2 and the Thinking Machines CM-200 and CM-5. A code to calculate inviscid compressible flow, originally written in FORTRAN 77 for a traditional vector computer, has been re-written entirely in Fortran 90 to take advantage of the compilers available on the massively-parallel computers. It is shown that the discretization of the governing equations on a regular mesh is well adapted to data parallelism. For a typical test problem of supersonic flow through a ramped duct, computational speeds have been achieved using these massively-parallel computers that are superior to those obtained using a single processor of a Cray Y-MP. In addition, this study has enabled the question of code portability between the different computers to be assessed.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{GOSAK2024103888,
title = {The ChatGPT effect and transforming nursing education with generative AI: Discussion paper},
journal = {Nurse Education in Practice},
volume = {75},
pages = {103888},
year = {2024},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2024.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1471595324000179},
author = {Lucija Gosak and Lisiane Pruinelli and Maxim Topaz and Gregor Štiglic},
keywords = {Artificial Intelligence, ChatGPT, Documentation, Education, Nursing, Nursing Diagnosis},
abstract = {Aim
The aim of this study is to present the possibilities of nurse education in the use of the Chat Generative Pre-training Transformer (ChatGPT) tool to support the documentation process.
Background
The success of the nursing process is based on the accuracy of nursing diagnoses, which also determine nursing interventions and nursing outcomes. Educating nurses in the use of artificial intelligence in the nursing process can significantly reduce the time nurses spend on documentation.
Design
Discussion paper.
Methods
We used a case study from Train4Health in the field of preventive care to demonstrate the potential of using Generative Pre-training Transformer (ChatGPT) to educate nurses in documenting the nursing process using generative artificial intelligence. Based on the case study, we entered a description of the patient's condition into Generative Pre-training Transformer (ChatGPT) and asked questions about nursing diagnoses, nursing interventions and nursing outcomes. We further synthesized these results.
Results
In the process of educating nurses about the nursing process and nursing diagnosis, Generative Pre-training Transformer (ChatGPT) can present potential patient problems to nurses and guide them through the process from taking a medical history, setting nursing diagnoses and planning goals and interventions. Generative Pre-training Transformer (ChatGPT) returned appropriate nursing diagnoses, but these were not in line with the North American Nursing Diagnosis Association – International (NANDA-I) classification as requested. Of all the nursing diagnoses provided, only one was consistent with the most recent version of the North American Nursing Diagnosis Association – International (NANDA-I). Generative Pre-training Transformer (ChatGPT) is still not specific enough for nursing diagnoses, resulting in incorrect answers in several cases.
Conclusions
Using Generative Pre-training Transformer (ChatGPT) to educate nurses and support the documentation process is time-efficient, but it still requires a certain level of human critical-thinking and fact-checking.}
}
@article{RANGEL2012970,
title = {Value normalization in decision making: theory and evidence},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {970-981},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812001201},
author = {Antonio Rangel and John A Clithero},
abstract = {A sizable body of evidence has shown that the brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence.}
}
@article{TAKANO201922,
title = {Difficulty in updating positive beliefs about negative cognition is associated with increased depressed mood},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {64},
pages = {22-30},
year = {2019},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005791617302926},
author = {Keisuke Takano and Julie {Van Grieken} and Filip Raes},
keywords = {depression, Rumination, Memory, Reinforcement learning, Q-learning},
abstract = {Background and objectives
Depressed people hold positive beliefs about negative cognition (e.g., rumination is useful to find a solution), which may motivate those individuals to engage in sustained negative thinking. However, in reality, rumination often leads to unfavorable outcomes. Thus, such beliefs create a large discrepancy between one's expectations and the actual outcome. Therefore, we hypothesized that this prediction error would be associated with increased depressed mood.
Methods
We observed how people update their positive beliefs about negative cognition within a volatile environment, in which negative cognition does not always result in a beneficial outcome. Forty-six participants were offered two response options (retrieving a negative or positive personal memory) and subsequently provided either an economic reward or punishment. Retrieving a negative (rather than positive) memory was initially reinforced, although this action-outcome contingency was reversed during the task. In the control condition, positive memory retrieval was initially reinforced, although a contingency reversal was employed to encourage negative memory retrieval.
Results
Model-based computational modeling revealed that participants who showed a delay in switching from negative to positive (but not from positive to negative) responses experienced increased levels of depressed mood. This delay in switching was also found to be associated with depressive symptoms and trait rumination.
Limitations
The non-clinical nature of the sample may limit the clinical implications of the results.
Conclusions
Difficulty in updating positive beliefs (or outcome predictions) for negative cognition may play an important role in depressive symptomatology.}
}
@article{LANGE2024101191,
title = {What are explanatory proofs in mathematics and how can they contribute to teaching and learning?},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101191},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101191},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000683},
author = {Marc Lange},
keywords = {Explanation, Proof, Generalization, Pedagogy, Unification, Coincidence},
abstract = {This paper will examine several simple examples (drawn from the mathematics literature) where there are multiple proofs of the same theorem, but only some of these proofs are widely regarded by mathematicians as explanatory. These examples will motivate an account of explanatory proofs in mathematics. Along the way, the paper will discuss why deus ex machina proofs are not explanatory, what a mathematical coincidence is, and how a theorem's proper setting reflects the naturalness of various mathematical kinds. The paper will also investigate how context influences which features of a theorem are salient and consequently which proofs are explanatory. The paper will discuss several ways in which explanatory proofs can contribute to teaching and learning, including how shifts in context (and hence in a proof’s explanatory power) can be exploited in a classroom setting, leading students to dig more deeply into why some theorem holds. More generally, the paper will examine how “Why?” questions operate in mathematical thinking, teaching, and learning.}
}
@incollection{PESCE2024123,
title = {Chapter Seven - Creativity and consciousness in motion: The roundtrip of “mindful” and “mindless” processes in embodied creativity},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {287},
pages = {123-151},
year = {2024},
booktitle = {The Neurophysiology of Silence (C): Creativity, Aesthetic Experience and Time},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2024.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612324000785},
author = {Caterina Pesce and Nicoletta Tocci},
keywords = {Creative thinking, Motor creativity, Embodiment, Hypofrontality, Flow, Incubation, Mind wandering, Nature, Green exercise, Mindful movements},
abstract = {In this opinion paper, we make a journey across different accounts of creativity that emphasize either the mindful, conscious and cognitive expression of creativity, or its mindless, unconscious and sensorimotor expression. We try to go beyond dichotomy, putting creativity in motion and outlining its embodied and enactive features. Based on the assumption that no creative act is purely conscious or purely unconscious, our discussion on creativity relies on the distinction of three types of creativity that complementarily contribute to the creative process through shifts in the activation of their substrates in the brain: the deliberate, spontaneous and flow types of creativity. The latter is a hybrid and embodied type, in which movement and physical activity meet creativity. We then focus on the most fascinating contribution of unconscious processes and mind wandering to spontaneous and flow modes of creativity, exploring what happens when the individual apparently takes a break from a deliberate and effortful search for solutions and the creative process progresses through an incubation phase. This phase and the overall creative process can be facilitated by physical activity which, depending on its features and context, can disengage the cognitive control network and free the mind from filters that constrain cognitive processes or, conversely, can engage attentional control on sensorimotor and cognitive task components in a mindful way. Lastly, we focus on the unique features of the outer natural environment of physical activity and of the inner environment during mindful movements that can restore capacities and boost creativity.}
}
@article{COOPER2022100755,
title = {Balboa security v. M&M systems: Forensic accounting for determining commercial damages},
journal = {Journal of Accounting Education},
volume = {58},
pages = {100755},
year = {2022},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100755},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000427},
author = {John R. Cooper and Brett S. Kawada},
keywords = {Forensic accounting, Commercial damages, Litigation, Supplier-customer relationship},
abstract = {The ability of accounting students to apply skills beyond traditional accounting in a thoughtful and analytical way is becoming increasingly important, especially in fraud detection and forensic accounting. This case provides an opportunity for students to use critical thinking and problem-solving skills in applying accounting knowledge to a supplier-customer commercial damages litigation matter. Students are provided with a fact pattern of a supplier-customer relationship where they analyze issues related to commercial damages stemming from sources common in real world forensic accounting cases. Students evaluate the facts, which include not only financial data but also interviews with key personnel of parties to the legal action, and demonstrate an understanding of the issues involved in the case through responses of questions regarding overriding forensic accounting and professional practice issues. Students will also prepare a written commercial damages report demonstrating the ability to effectively communicate their analyses.}
}
@article{ZHURAVLEV2023104934,
title = {Three levels of information processing in the brain},
journal = {Biosystems},
volume = {229},
pages = {104934},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104934},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001090},
author = {Aleksandr V. Zhuravlev},
keywords = {Brain, Information, Consciousness, The hard problem of consciousness, Qualia, Entropy},
abstract = {Information, the measure of order in a complex system, is the opposite of entropy, the measure of chaos and disorder. We can distinguish several levels at which information is processed in the brain. The first one is the level of serial molecular genetic processes, similar in some aspects to digital computations (DC). At the same time, higher cognitive activity is probably based on parallel neural network computations (NNC). The advantage of neural networks is their intrinsic ability to learn, adapting their parameters to specific tasks and to external data. However, there seems to be a third level of information processing as well, which involves subjective consciousness and its units, so called qualia. They are difficult to study experimentally, and the very fact of their existence is hard to explain within the framework of modern physical theory. Here I propose a way to consider consciousness as the extension of basic physical laws – namely, total entropy dissipation leading to a system simplification. At the level of subjective consciousness, the brain seems to convert information embodied by neural activity to a more simple and compact form, internally observed as qualia. Whereas physical implementations of both DC and NNC are essentially approximate and probabilistic, qualia-associated computations (QAC) make the brain capable of recognizing general laws and relationships. While elaborating a behavioral program, the conscious brain does not act blindly or gropingly but according to the very meaning of such general laws, which gives it an advantage compared to any artificial intelligence system.}
}
@article{INDLEKOFER20021035,
title = {Number theory—probabilistic, heuristic, and computational approaches},
journal = {Computers & Mathematics with Applications},
volume = {43},
number = {8},
pages = {1035-1061},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)80012-8},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102800128},
author = {K.-H Indlekofer},
keywords = {Probabilistic number theory, Asymptotic results on arithmetic function, Computational number theory, Stone-Cech compactification, Measure and integration on },
abstract = {After the description of the models of Kubilius, Novoselov and Schwarz, and Spilker, respectively, a probability theory for finitely additive probability measures is developed by use of the Stone-Cech compactification of N. The new model is applied to the result of Erdős and Wintner about the limit distribution of additive functions and to the famous result of Szemerédi in combinatorial number theory. Further, it is explained how conjectures on prime values of irreducible polynomials are used in the search for large prime twins and Sophie Germain primes.}
}
@article{ESCOLAGASCON2023111893,
title = {Who falls for fake news? Psychological and clinical profiling evidence of fake news consumers},
journal = {Personality and Individual Differences},
volume = {200},
pages = {111893},
year = {2023},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2022.111893},
url = {https://www.sciencedirect.com/science/article/pii/S0191886922003981},
author = {Álex Escolà-Gascón and Neil Dagnall and Andrew Denovan and Kenneth Drinkwater and Miriam Diez-Bosch},
keywords = {Fake news, Pseudoscientific information, Cognitive biases, Individual differences, Clinical prevention},
abstract = {Awareness of the potential psychological significance of false news increased during the coronavirus pandemic, however, its impact on psychopathology and individual differences remains unclear. Acknowledging this, the authors investigated the psychological and psychopathological profiles that characterize fake news consumption. A total of 1452 volunteers from the general population with no previous psychiatric history participated. They responded to clinical psychopathology assessment tests. Respondents solved a fake news screening test, which allowed them to be allocated to a quasi-experimental condition: group 1 (non-fake news consumers) or group 2 (fake news consumers). Mean comparison, Bayesian inference, and multiple regression analyses were applied. Participants with a schizotypal, paranoid, and histrionic personality were ineffective at detecting fake news. They were also more vulnerable to suffer its negative effects. Specifically, they displayed higher levels of anxiety and committed more cognitive biases based on suggestibility and the Barnum Effect. No significant effects on psychotic symptomatology or affective mood states were observed. Corresponding to these outcomes, two clinical and therapeutic recommendations related to the reduction of the Barnum Effect and the reinterpretation of digital media sensationalism were made. The impact of fake news and possible ways of prevention are discussed.}
}
@article{MANCHO200655,
title = {A tutorial on dynamical systems concepts applied to Lagrangian transport in oceanic flows defined as finite time data sets: Theoretical and computational issues},
journal = {Physics Reports},
volume = {437},
number = {3},
pages = {55-124},
year = {2006},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2006.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0370157306003401},
author = {Ana M. Mancho and Des Small and Stephen Wiggins},
keywords = {Lagrangian transport, Geophysical fluid flows, Finite time hyperbolicity, Finite time Lyapunov exponents, Stable and unstable manifolds, Transport barriers},
abstract = {In the past 15 years the framework and ideas from dynamical systems theory have been applied to a variety of transport and mixing problems in oceanic flows. The motivation for this approach comes directly from advances in observational capabilities in oceanography (e.g., drifter deployments, remote sensing capabilities, satellite imagery, etc.) which reveal space–time structures that are highly suggestive of the structures one visualizes in the global, geometrical study of dynamical systems theory. In this tutorial, we motivate this approach by showing the relationship between fluid transport in two-dimensional time-periodic incompressible flows and the geometrical structures that exist for two-dimensional area-preserving maps, such as hyperbolic periodic orbits, their stable and unstable manifolds and KAM (Kolmogorov–Arnold–Moser) tori. This serves to set the stage for the attempt to “transfer” this approach to more realistic flows modelling the ocean. However, in order to accomplish this several difficulties must be overcome. The first difficulty that confronts us that any attempt to carry out a dynamical systems approach to transport requires us to obtain the appropriate “dynamical system”, which is the velocity field describing the fluid flow. In general, adequate model velocity fields are obtained by numerical solution of appropriate partial differential equations describing the dynamical evolution of the velocity field. Numerical solution of the partial differential equations can only be done for a finite time interval, and since the ocean is generally not time-periodic, this leads to a new type of dynamical system: a finite-time, aperiodically time-dependent velocity field defined as a data set on a space–time grid. The global, geometrical analysis of transport in such dynamical systems requires both new concepts and new analytical and computational tools, as well as the necessity to discard some of the standard ideas and results from dynamical systems theory. The purpose of this tutorial is to describe these new concepts and analytical tools first using simple dynamical systems where quantities can be computed exactly. We then discuss their computational implications and implementation in the context of a model geophysical flow: a turbulent wind-driven double-gyre in the quasigeostrophic approximation.}
}
@article{MASROURI2025102428,
title = {Animal-skin-pattern-inspired multifunctional composites by generative AI},
journal = {Cell Reports Physical Science},
volume = {6},
number = {2},
pages = {102428},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2025.102428},
url = {https://www.sciencedirect.com/science/article/pii/S266638642500027X},
author = {Milad Masrouri and Akshay Vilas Jadhav and Zhao Qin},
keywords = {composite design, bioinspiration, generative AI, molecular dynamics, elastic network, animal pattern, biomimicry, 3D printing, toughness modulus, multifunctional materials},
abstract = {Summary
Bioinspired composite materials offer several advantages by mimicking the structure of natural counterparts. However, their complex hierarchical structure, compared to the limited number of observations, makes it difficult to extract all the structural features and vary the structure to optimize the materials’ functions without losing their natural features. We applied generative artificial intelligence (GenAI) to design composites inspired by animal skin patterns, leveraging a small dataset to generate diverse configurations that closely emulate natural designs. Our computational simulations investigated the structure-mechanics relationship in these materials, revealing significant variations in mechanical functions and identifying patterns that exhibited superior mechanical properties. We validated these outstanding configurations’ performance through tensile tests on specimens produced by a multimaterial printer. We showcase GenAI’s role in structural augmentation that can yield rational bioinspired designs, complemented by an educational web page with interactive games for public access.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{GOULETCOULOMBE2025,
title = {Time-varying parameters as ridge regressions},
journal = {International Journal of Forecasting},
year = {2025},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169207024000931},
author = {Philippe {Goulet Coulombe}},
abstract = {Time-varying parameter (TVP) models are frequently used in economics to capture structural change. I highlight a rather underutilized fact—that these are actually ridge regressions. Instantly, this makes computations, tuning, and implementation much easier than in the state-space paradigm. Among other things, solving the equivalent dual ridge problem is computationally very fast even in high dimensions, and the crucial ‘amount of time variation’ is tuned by cross-validation. Evolving volatility is dealt with using a two-step ridge regression. I consider extensions that incorporate sparsity (the algorithm selects which parameters vary and which do not) and reduced-rank restrictions (variation is tied to a factor model). To demonstrate the usefulness of the approach, I use it to study the evolution of monetary policy in Canada using large time-varying local projections and TVP-VARs with demanding lag lengths. The applications require the estimation of up to 4600 TVPs, a task within the reach of the new method.}
}
@article{WOODSIDE2011153,
title = {Responding to the severe limitations of cross-sectional surveys: Commenting on Rong and Wilkinson’s perspectives},
journal = {Australasian Marketing Journal (AMJ)},
volume = {19},
number = {3},
pages = {153-156},
year = {2011},
note = {Special Section: Marketing and Public Policy},
issn = {1441-3582},
doi = {https://doi.org/10.1016/j.ausmj.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1441358211000231},
author = {Arch G. Woodside},
keywords = {Direct research, Executives, Folk theory-of-mind, fs/QCA.com, Sensemaking, Surveys, Thinking},
abstract = {While a meta-analysis is necessary to test the claim that the logic dominates the majority of studies, most studies by academic scholars on thinking and actions by executives appear to rely on cross-sectional surveys that use self-reports by executives via scaled (e.g., strongly disagree to strongly agree) instruments whereby one executive per firm completes the instrument and data are collected for 50–500 firms. Useable response rates in these studies are almost always below 30% of the distributions of the surveys. While these studies are sometimes worthwhile for learning how respondents assess concepts and relationships among concepts, Rong and Wilkinson’s perspective on the severe limits to the value of such studies rings true: such surveys reveal more about executives’ sensemaking processes than the actual processes. The limitations of using one-shot, one-person-per-firm, self-reports as valid indicators of causal relationships of actual processes are so severe that academics should do more than think twice before using such surveys as the main method for collecting data – if scholars seek to understand and describe actual processes additional methods are necessary for data collection. The relevant literature includes several gems of exceptionally high quality, validity, and usefulness in the study of actual processes; identifying these studies is a useful step toward reducing the reliance on one-shot self-report surveys.}
}
@article{GUPTA19971,
title = {Future Challenges for Fuzzy-Neural Computing Systems},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {25},
pages = {1-6},
year = {1997},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC'97), Kuala Lumpur, Malaysia, 22-25 September 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41292-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017412924},
author = {Madan M. Gupta},
keywords = {Neural Systems, Fuzzy Systems, Fuzzy Logic, Neural Fuzzy Computing},
abstract = {Recently, several significant advances have been made in two distinct theoretical areas. These theoretical advances have created an innovative field of theoretical and applied interest: fuzzy neural systems. Researchers have provided a theoretical basis in the field while industry has used this theoretical basis to create a new class of machines using the innovative technology of fuzzy neural networks. The theory of fuzzy logic provides a mathematical framework for capturing the uncertainties associated with human cognitive processes, such as thinking and reasoning. It also provides a mathematical morphology for emulating certain perceptual and linguistic attributes associated with human cognition. On the other hand, computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. The integration of these two fields, fuzzy logic and neural networks, has the potential for combining the benefits of these two fascinating fields into a single capsule. The intent of this paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks.}
}
@incollection{MURMAN2012323,
title = {15 - Innovation in aeronautics through Lean Engineering},
editor = {Trevor M. Young and Mike Hirst},
booktitle = {Innovation in Aeronautics},
publisher = {Woodhead Publishing},
pages = {323-360},
year = {2012},
isbn = {978-1-84569-550-7},
doi = {https://doi.org/10.1533/9780857096098.3.323},
url = {https://www.sciencedirect.com/science/article/pii/B978184569550750015X},
author = {E.M. Murman},
keywords = {Lean Engineering, Lean Product Development},
abstract = {Abstract:
The dynamics of innovation theory indicate that, for products as mature as aircraft, process innovation is an important contributor to product success and innovation. Many aerospace companies have adopted Lean Thinking as an enterprise-wide continuous improvement strategy. This chapter extends Lean Thinking to the engineering domain with a Lean Engineering framework based upon observational findings from a decade of research in the aerospace domain, published works on Toyota and Southwest Airlines, and practitioner input. Examples illustrate how the framework maybe be applied. Lean Engineering is not totally new to aerospace, and it continues to evolve. Future challenges are briefly summarized.}
}
@article{BUSBY20161029,
title = {Agent-based computational modelling of social risk responses},
journal = {European Journal of Operational Research},
volume = {251},
number = {3},
pages = {1029-1042},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722171501173X},
author = {J.S. Busby and B.S.S. Onggo and Y. Liu},
keywords = {OR in societal problem analysis, Multiagent systems, Risk management},
abstract = {A characteristic aspect of risks in a complex, modern society is the nature and degree of the public response – sometimes significantly at variance with objective assessments of risk. A large part of the risk management task involves anticipating, explaining and reacting to this response. One of the main approaches we have for analysing the emergent public response, the social amplification of risk framework, has been the subject of little modelling. The purpose of this paper is to explore how social risk amplification can be represented and simulated. The importance of heterogeneity among risk perceivers, and the role of their social networks in shaping risk perceptions, makes it natural to take an agent-based approach. We look in particular at how to model some central aspects of many risk events: the way actors come to observe other actors more than external events in forming their risk perceptions; the way in which behaviour both follows risk perception and shapes it; and the way risk communications are fashioned in the light of responses to previous communications. We show how such aspects can be represented by availability cascades, but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements, and the differentiation of private and public risk beliefs. Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically – such as endogenously-produced peaks in risk concern – can be explained by this model.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{SAVIN202110,
title = {Main topics in EIST during its first decade: A computational-linguistic analysis},
journal = {Environmental Innovation and Societal Transitions},
volume = {41},
pages = {10-17},
year = {2021},
note = {Celebrating a decade of EIST: What’s next for transition studies?},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S221042242100037X},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Machine learning, Topic modelling, Literature review},
abstract = {We analyse 465 articles published in EIST from June 2011 until June 2021 to identify topics addressed in the journal. We find eight main topics and assess how their shares changed over time as well as how many citations they received. The topics with the largest shares in all publications are “Theory of socio-technical transitions” and “Urban regimes and niches”. The two most cited topics, “Theory of socio-technical transitions” and “Geography and diffusion of eco-innovations”, showed a rising share over time, while the share of topic “Finance, investment and growth” declined. We further assess the geographical coverage of topics, through affiliations of the corresponding authors. The resulting map indicates dominant topics for the 34 countries that contributed to publications in EIST.}
}
@article{SUN201859,
title = {An ecosystemic framework for business sustainability},
journal = {Business Horizons},
volume = {61},
number = {1},
pages = {59-72},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317301271},
author = {Jiazhe Sun and Shunan Wu and Kaizhong Yang},
keywords = {Business sustainability, Systems theory, Ecosystemic theory, Complexity science, Adaptive management, Corporate sustainability},
abstract = {This article introduces an ecosystemic framework to foster innovation for business sustainability. We emphasize the idea of systemic thinking in which the business operates as a system similar to a living organism. In this framework, businesses impact the environment in which they operate in a fluid, dynamic, and interdependent way. This approach contrasts with the linear approach commonly used in business and other disciplines, which tries to explain what might cause an action or reaction but ignores any feedback effect between the subsequent action and its cause. This article offers practical solutions and guidance for business leaders to incorporate complexity science into creating sustainable businesses.}
}
@article{ASIF202532,
title = {Machine learning-driven catalyst design, synthesis and performance prediction for CO2 hydrogenation},
journal = {Journal of Industrial and Engineering Chemistry},
volume = {144},
pages = {32-47},
year = {2025},
issn = {1226-086X},
doi = {https://doi.org/10.1016/j.jiec.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S1226086X24006269},
author = {Muhammad Asif and Chengxi Yao and Zitu Zuo and Muhammad Bilal and Hassan Zeb and Seungjae Lee and Ziyang Wang and Taesung Kim},
keywords = {Heterogeneous catalysis, DFT calculation, Machine learning, 3D printing, CO hydrogenation},
abstract = {Atmospheric concentrations of CO2 must be lowered to mitigate climate change and rising global temperatures. CO2 utilization is the most promising approach for the sustainable reduction of CO2 emissions. Interdisciplinary research is gaining increasing attention due to its broader application potential and the promising results of combining various fields. Computational approaches in catalytic research could be cost-effective and environmentally friendly. Machine Learning (ML) and 3D printing technologies may soon be able to produce nanoscale raw materials to synthesize the catalyst for commercial-scale applications. In this review article, recent advances in catalyst synthesis using 3D printing technologies and ML-based catalytic reactions, particularly those in CO2 hydrogenation, are critically analyzed, with a focus on the function of ML model prediction. ML approaches with high prediction accuracies are discussed comprehensively. Based on the literature Gray-box models can provide useful insights by revealing the essential catalytic traits, factors, and circumstances that affect the results. They can also provide a practical solution by fusing the benefits of black-box algorithms, such as ensemble models and NNs, with feature importance analysis. Finally, suggestions and recommendations for the potential applications of ML in chemical science, especially in heterogeneous catalysis, are provided along with future research directions.}
}
@article{WANG2024109589,
title = {Cellular gradient algorithm for solving complex mechanical optimization design problems},
journal = {International Journal of Mechanical Sciences},
volume = {282},
pages = {109589},
year = {2024},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2024.109589},
url = {https://www.sciencedirect.com/science/article/pii/S0020740324006301},
author = {Rugui Wang and Xinpeng Li and Haibo Huang and Zhipeng Fan and Fuqiang Huang and Ningjuan Zhao},
keywords = {Mechanical optimization, Optimization algorithm, Discrete integrable problem, Cellular automaton, Gradient descent},
abstract = {In mechanical optimization design problems, there are often some non-continuous or non-differentiable objective functions. For these non-continuous and non-differentiable optimization objectives, it is often difficult for existing optimal design algorithms to find the desired optimal solutions. In this paper, we incorporate the idea of gradient descent into cellular automata and propose a Cellular Gradient (CG) method. First, we have given the basic rules and algorithmic framework of CG and designed three kinds of growth and extinction rules respectively. Then, the three evolutionary rules for cellular within a single cycle are analyzed separately for form and ordering. The best expressions for the cellular jealous neighbor rule and the solitary regeneration rule are given, and the most appropriate order in which the rules are run is selected. Finally, the solution results of the cellular gradient algorithm and other classical optimization design algorithms are compared with a multi-objective multi-parameter mechanical optimization design problem as an example. The computational results show that the cellular gradient algorithm has an advantage over other algorithms in solving global and dynamic mechanical optimal design problems. The novelty of CG is to provide a new way of thinking for solving optimization problems with global discontinuities.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{WELLEK1961715,
title = {The contribution of the perception-typological approaches to the typology of character, and the role of sensation, imagination, and thinking in the organizational concept of personality},
journal = {Acta Psychologica},
volume = {19},
pages = {715-723},
year = {1961},
issn = {0001-6918},
doi = {https://doi.org/10.1016/S0001-6918(61)80321-1},
url = {https://www.sciencedirect.com/science/article/pii/S0001691861803211},
author = {Albert Wellek}
}
@article{ANURADHA2022100429,
title = {A RNN based offloading scheme to reduce latency and preserve energy using RNNBOS},
journal = {Measurement: Sensors},
volume = {24},
pages = {100429},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422000630},
author = {C. Anuradha and M. Ponnavaikko},
keywords = {Computational offloading, Mobile edge computing, Deep neural network, Energy consumption and mobile cloud computing},
abstract = {Mobile cloud computing is currently evolving quickly in today's trend and it provides infinite number of applications to the people those who are using regularly.MCC means the mobile gadgets are strongly tied up with cloud technology to execute various application for attaining many tasks. Mobile devices contain different application according to its own capacity to hold each application. In which many applications are in need of connecting with cloud storage. A new proposed technique named RNNBOS (Recurrent Neural Network Based Offloading scheme) is used to compute calculations in terms of energy source of mobile device along with active conditions of network, Load computations, delay possibility of request from device and quantitative amount of data being transferred for this purpose. We have simulated the above technique using python tool and observed RNN based offloading scheme is good in execution of application using MCC.}
}
@article{LIU202257,
title = {Hierarchical neighborhood entropy based multi-granularity attribute reduction with application to gene prioritization},
journal = {International Journal of Approximate Reasoning},
volume = {148},
pages = {57-67},
year = {2022},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000809},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Gene selection, Granular computing, Multi-granularity attribute reduction, Neighborhood rough set, Trilevel thinking},
abstract = {As a prominent model of granular computing, neighborhood rough set provides clear granularity organization and expression in terms of inherent parameter (neighborhood radius). Such characteristic is widely captured in a plenitude of attribute reduction procedures, while igniting a tricky issue of tuning parameters. In this study, we therefore propose a parameter-free multi-granularity attribute reduction scheme. Fundamentally, our scheme applies three-way decision as thinking in threes. First, data-aware multi-granularity structure is automatically induced from self-contained distance space instead of manually edited or appointed granularities. Second, a novel multi-granularity feature evaluation criterion named hierarchical neighborhood entropy is defined to measure the feature significance. Finally, a sequential forward searching algorithm is designed to find the optimal reduct. With application to gene prioritization, our method performed on microarray data is experimentally demonstrated to be more effective and efficient in differentially expressed genes discovery as compared with other well-established attribute reduction algorithms.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{BOSL2025101480,
title = {Dynamical measures of developing neuroelectric fields in emerging consciousness},
journal = {Current Opinion in Behavioral Sciences},
volume = {61},
pages = {101480},
year = {2025},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101480},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001311},
author = {William J Bosl and Jenny R {Capua Shenkar}},
abstract = {Human consciousness emerges over time. From the moment of conception, a process of neurodevelopment and complexification begins, generating and supporting a neuroelectric field that can be quantified by computational methods from dynamical systems theory. In the early embryo, genetically driven cellular processes are mediated by endogenous electromagnetic fields and intrinsic electrical fields produced by migrating neurons. In the ambient cellular environment, these interactions influence each other, impacting neural migration. The emergence of Theory of Mind, often considered a hallmark of conscious awareness, is accompanied by increasing neural connectivity, neuroelectric field complexity, and more integrated information processing. Neurodegeneration in old age and the often-associated decline in conscious awareness correlate closely with changes in the dynamical complexity of the neuroelectric field. Monitoring trajectories of the neuroelectric field and its complexity changes through the lifespan presents a developmental perspective and empirical correlation for studying the emergence and decline of human consciousness.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{QIAN2024120487,
title = {E3WD: A three-way decision model based on ensemble learning},
journal = {Information Sciences},
volume = {667},
pages = {120487},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120487},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524004006},
author = {Jin Qian and Di Wang and Ying Yu and XiBei Yang and Shang Gao},
keywords = {Three-way decision, Ensemble strategy, Cluster ensemble, Membership degree, Critical threshold},
abstract = {Three-way decision model is an effective way to deal with complex decision problems. However, since the three-way decision models now proposed are all based on a single decision criterion, the decision results typically reflect only one preference of decision-makers. Thus, these models may also not effectively deal with complex decision-making problems. To solve the above problems, this paper proposes a new three-way decision model based on ensemble learning. Specifically, we first obtain different three-way decision results by employing different decision criteria. Then, we can acquire the core and candidate sets of the positive and negative regions through set operations. Next, we use the K-means algorithm to divide the candidate sets into three disjoint subsets based on similarities. After that, we adopt a hierarchical filtering method to select suitable objects from the candidate sets and add them to the core sets. Finally, we employ four three-way decision models with different decision criteria as examples to conduct experiments on eight datasets. Experimental results show that our proposed model can obtain higher classification accuracy and lower deferment rate than other traditional three-way decision models under most experimental conditions.}
}
@article{ALDAYA2024116708,
title = {Tachyons in “momentum-space” representation},
journal = {Nuclear Physics B},
volume = {1008},
pages = {116708},
year = {2024},
issn = {0550-3213},
doi = {https://doi.org/10.1016/j.nuclphysb.2024.116708},
url = {https://www.sciencedirect.com/science/article/pii/S0550321324002748},
author = {V. Aldaya and J. Guerrero and F.F. López-Ruiz},
abstract = {Obtaining the momentum space associated with tachyonic “particles” from the Poincaré group manifold proves to be rather intricate, departing very much from the ordinary dual to Minkowski space directly parametrized by space-time translations of the Poincaré group. In fact, although described by the constants of motion (Noether invariants) associated with space-time translations, they depend non-trivially on the parameters of the rotation subgroup. However, once the momentum space is parametrized by the Noether invariants, it behaves as that of ordinary particles. On the other hand, the evolution parameter is no longer the one associated with time translation, whose Noether invariant, Po, is now a basic one. Evolution takes place in a spatial direction. These facts not only make difficult the computation of the corresponding representation, but also force us to a sound revision of several traditional ingredients related to Cauchy hypersurface, scalar product and, of course, causality. After that, the theory becomes consistent and could shed new light on some special physical situations like inflation or traveling inside a black hole.}
}
@article{MATSUDA2009970,
title = {Multiple cognitive deficits in patients during the mild cognitive impairment stage of Alzheimer's disease: how are cognitive domains other than episodic memory impaired?},
journal = {International Psychogeriatrics},
volume = {21},
number = {5},
pages = {970-976},
year = {2009},
issn = {1041-6102},
doi = {https://doi.org/10.1017/S1041610209990330},
url = {https://www.sciencedirect.com/science/article/pii/S1041610224025894},
author = {Osamu Matsuda and Masahiko Saito},
keywords = {Alzheimer's disease, COGNISTAT, mild cognitive impairment},
abstract = {ABSTRACT
Background: Little is known about how cognitive domains other than episodic memory are affected during the mild cognitive impairment (MCI) stage of Alzheimer's disease (AD). We attempted to clarify this issue in this study. Methods: Fifty-seven Japanese subjects were divided into two groups: one comprising people in the MCI stage of AD (MCI group, n = 28) and the other of normal controls (NC group, n = 29). Cognitive functions were assessed using the Japanese version of the neurobehavioral cognitive status examination (J-COGNISTAT). Results: The MCI group performed significantly worse than the NC group on subtests that assessed orientation, confrontational naming, constructive ability, episodic memory, and abstract thinking. Three-quarters of the MCI group had deficits in memory and other non-mnemonic domains, particularly constructive ability and abstract thinking. However, within-subject comparisons showed that the MCI group performed significantly worse on the memory subtest compared to any other subtest. Conclusions: Besides episodic memory, multiple non-mnemonic cognitive domains, such as constructive ability and abstract thinking, are also impaired during the MCI stage of AD; however, these non-mnemonic deficits are smaller than episodic memory impairment.}
}
@article{TSUTAKAWA2020102972,
title = {Envisioning how the prototypic molecular machine TFIIH functions in transcription initiation and DNA repair},
journal = {DNA Repair},
volume = {96},
pages = {102972},
year = {2020},
issn = {1568-7864},
doi = {https://doi.org/10.1016/j.dnarep.2020.102972},
url = {https://www.sciencedirect.com/science/article/pii/S1568786420302214},
author = {Susan E. Tsutakawa and Chi-Lin Tsai and Chunli Yan and Amer Bralić and Walter J. Chazin and Samir M. Hamdan and Orlando D. Schärer and Ivaylo Ivanov and John A. Tainer},
keywords = {TFIIH, Helicase, Transcription initiation, Transcription-coupled repair, Nucleotide excision repair, XPB, XPD, Translocase, DNA damage, DNA repair},
abstract = {Critical for transcription initiation and bulky lesion DNA repair, TFIIH provides an exemplary system to connect molecular mechanisms to biological outcomes due to its strong genetic links to different specific human diseases. Recent advances in structural and computational biology provide a unique opportunity to re-examine biologically relevant molecular structures and develop possible mechanistic insights for the large dynamic TFIIH complex. TFIIH presents many puzzles involving how its two SF2 helicase family enzymes, XPB and XPD, function in transcription initiation and repair: how do they initiate transcription, detect and verify DNA damage, select the damaged strand for incision, coordinate repair with transcription and cell cycle through Cdk-activating-kinase (CAK) signaling, and result in very different specific human diseases associated with cancer, aging, and development from single missense mutations? By joining analyses of breakthrough cryo-electron microscopy (cryo-EM) structures and advanced computation with data from biochemistry and human genetics, we develop unified concepts and molecular level understanding for TFIIH functions with a focus on structural mechanisms. We provocatively consider that TFIIH may have first evolved from evolutionary pressure for TCR to resolve arrested transcription blocks to DNA replication and later added its key roles in transcription initiation and global DNA repair. We anticipate that this level of mechanistic information will have significant impact on thinking about TFIIH, laying a robust foundation suitable to develop new paradigms for DNA transcription initiation and repair along with insights into disease prevention, susceptibility, diagnosis and interventions.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{YANG2018182,
title = {A Geodesign Method of Human-Energy-Water Interactive Systems for Urban Infrastructure Design: 10KM2 Near-Zero District Project in Shanghai},
journal = {Engineering},
volume = {4},
number = {2},
pages = {182-189},
year = {2018},
note = {Sustainable Infrastructure},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918301978},
author = {Perry Pei-Ju Yang and Cheryl Shu-Fang Chi and Yihan Wu and Steven Jige Quan},
keywords = {Geodesign, Urban design, Urban infrastructure, Energy performance, Iterative process, Multi-objective optimization},
abstract = {The grand challenges of climate change demand a new paradigm of urban design that takes the performance of urban systems into account, such as energy and water efficiency. Traditional urban design methods focus on the form-making process and lack performance dimensions. Geodesign is an emerging approach that emphasizes the links between systems thinking, digital technology, and geographic context. This paper presents the research results of the first phase of a larger research collaboration and proposes an extended geodesign method for a district-scale urban design to integrate systems of renewable energy production, energy consumption, and storm water management, as well as a measurement of human experiences in cities. The method incorporates geographic information system (GIS), parametric modeling techniques, and multidisciplinary design optimization (MDO) tools that enable collaborative design decision-making. The method is tested and refined in a test case with the objective of designing a near-zero-energy urban district. Our final method has three characteristics. ① Integrated geodesign and parametric design: It uses a parametric design approach to generate focal-scale district prototypes by means of a custom procedural algorithm, and applies geodesign to evaluate the performances of design proposals. ② A focus on design flow: It elaborates how to define problems, what information is selected, and what criteria are used in making design decisions. ③ Multi-objective optimization: The test case produces indicators from performance modeling and derives principles through a multi-objective computational experiment to inform how the design can be improved. This paper concludes with issues and next steps in modeling urban design and infrastructure systems based on MDO tools.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{DIETRICH200722,
title = {Who’s afraid of a cognitive neuroscience of creativity?},
journal = {Methods},
volume = {42},
number = {1},
pages = {22-27},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306003100},
author = {Arne Dietrich},
keywords = {Consciousness, Insight, Prefrontal cortex, Right brain, Divergent thinking, Neuroimaging, Attention},
abstract = {This article has two goals. First, the ideas outlined here can be seen as a sustained and disciplined demolition project aimed at sanitizing our bad habits of thinking about creativity. Apart from the enormous amount of fluff out there, the study of creativity is, quite unfortunately, still dominated by a number of rather dated ideas that are either so simplistic that nothing good can possibly come out of them or, given what we know about the brain, factually mistaken. As cognitive neuroscience is making more serious contact with the knowledge base of creativity, we must, from the outset, clear the ground of these pernicious fossil traces from a bygone era. The best neuroimaging techniques help little if we don’t know what to look for. Second, as an antidote to these theoretical duds, the article offers fresh ideas on possible mechanisms of creativity. Given that they are grounded in current understanding of cognitive and neural processes, it is hoped that these ideas represent steps broadly pointing in the right direction. In the end, the fundamental question we must ask ourselves is what, exactly, are the mental processes—or their critical elements—that yield creative thoughts.}
}
@article{SOUZA2025112323,
title = {Techniques for eliciting IoT requirements: Sensorina Map and Mind IoT},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112323},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112323},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003674},
author = {Sabrina Souza and Eriky Rodrigues and Maria Meireles and Tanara Lauschner and Leandro Carvalho and José Carlos Maldonado and Tayana Conte},
keywords = {Requirements engineering, Design Thinking, Requirements’ elicitation techniques, Internet of Things},
abstract = {Context:
The Internet of Things (IoT) involves heterogeneous devices that interact and process data via the Internet. In the development of IoT systems, requirement elicitation is crucial. However, challenges such as heterogeneity, interoperability, scalability, and requirements volatility necessitate new approaches or adapting traditional techniques.
Objective:
In this context, this work proposes the Sensorina Map and IoT Mind as techniques adapted from the Empathy Map and Mind Map, respectively, to support requirement elicitation in IoT systems.
Method:
Two empirical studies were conducted in an academic environment to assess the feasibility of the techniques, then, a case study in industry environment.
Results:
The first study analyzed the ease of use and evaluated if it assisted software engineers in remembering the system requirements. The participants’ perceptions were collected through a Focus Group, refining the techniques. Subsequently, an observational study evaluated the techniques’ usefulness and ease of use. The results of the study demonstrated that the participants considered the methods feasible. The case study results revealed that the Sensorina Map is more suitable for advanced stages. At the same time, the Mind IoT suits better the initial phases, emphasizing the need for practical examples and adaptations to suit diverse user profiles.
Conclusion:
This work is expected to advance research in IoT systems and benefit professionals and researchers in this area.}
}
@article{MATHIS20245814,
title = {Decoding the brain: From neural representations to mechanistic models},
journal = {Cell},
volume = {187},
number = {21},
pages = {5814-5832},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424009802},
author = {Mackenzie Weygandt Mathis and Adriana {Perez Rotondo} and Edward F. Chang and Andreas S. Tolias and Alexander Mathis},
keywords = {decoding, encoding, deep learning, data-driven, normative models, BCIs, language},
abstract = {Summary
A central principle in neuroscience is that neurons within the brain act in concert to produce perception, cognition, and adaptive behavior. Neurons are organized into specialized brain areas, dedicated to different functions to varying extents, and their function relies on distributed circuits to continuously encode relevant environmental and body-state features, enabling other areas to decode (interpret) these representations for computing meaningful decisions and executing precise movements. Thus, the distributed brain can be thought of as a series of computations that act to encode and decode information. In this perspective, we detail important concepts of neural encoding and decoding and highlight the mathematical tools used to measure them, including deep learning methods. We provide case studies where decoding concepts enable foundational and translational science in motor, visual, and language processing.}
}
@article{NOVOTOTSKYVLASOV1995S114,
title = {PS-12-13 Event-related brain activity analysis by mean wave halfperiod duration computation method},
journal = {Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control},
volume = {97},
number = {4},
pages = {S114},
year = {1995},
issn = {0924-980X},
doi = {https://doi.org/10.1016/0924-980X(95)92838-D},
url = {https://www.sciencedirect.com/science/article/pii/0924980X9592838D},
author = {V.Y. Novototsky-Vlasov}
}
@article{WANG2024111842,
title = {Three-way decision based island harmony search algorithm for robust flow-shop scheduling with uncertain processing times depicted by big data},
journal = {Applied Soft Computing},
volume = {162},
pages = {111842},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111842},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006161},
author = {Bing Wang and Pengfei Zhang and Xiaozhi Wang and Quanke Pan},
keywords = {Robust flow-shop scheduling, Island harmony search, Big data, Three-way decision, Surrogate worst-case scenario},
abstract = {This paper discusses an uncertain two-machine permutation flow-shop scheduling problem (2PFSP) with total weighted tardiness and common due date. Uncertain processing times are described by a large set of discrete scenarios, which is a type of big data. The objective is to minimize the schedule performance under the worst-case scenario. Identifying the worst-case scenario for each evaluated schedule is quite time-consuming in the situation that the scenario set size is large so that the objective evaluation might be computationally expensive. To handle this difficulty, three-way decision is used to preprocess the large-size scenario set to get a reduced scenario set so that the concept of surrogate worst-case scenario is adopted. A hybrid harmony search algorithm of combining three-island framework and the scenario-based local search is developed to solve the discussed problem. Based on the single-scenario knowledge of 2PFSP, a problem-specific scenario-dependent neighborhood structure is constructed under the surrogate worst-case scenario. An extensive experiment was carried out. The computational results show that the application of surrogate worst-case scenario based on three-way decision is effective in reducing the time consuming while keeping schedule performance evaluation. Being compared to the worst-case scenario objective evaluation, for an example in the case of the middle bad-scenario ratio, the surrogate worst-case scenario objective evaluation made the solution algorithm save 12.95 % in average CPU time for all instances while the relative performance difference is only 1.809 % in average. Being compared to possible alternative algorithms derived from the state-of-the-art algorithms, the developed algorithm is advantageous for the addressed problems.}
}
@article{KINNEAR2024101190,
title = {Lecturers' use of questions in undergraduate mathematics lectures},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101190},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101190},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000671},
author = {George Kinnear and Gemma Hood and Eloise Lardet and Colette Sheard and Colin Foster},
keywords = {Funneling, Lecturing styles, Questioning, Student participation, University mathematics},
abstract = {Mathematics lecturers frequently ask questions in their lectures, and these questions presumably play an important role in students’ thinking about and learning of the lecture content. We replicated and developed a coding scheme used in previous research in the US to categorise lecturers’ questions in a sample of 136 lectures given by 24 lecturers at a research-intensive UK university. We found that the coding scheme could be applied reliably, and that factual questions were predominant (as in previous research). We explore differences in the lecturers’ use of questions – both between our UK sample and the previous US work, and between individual lecturers in our sample. We note the presence of strings of related successive questions from the lecturer, which we term ‘question chains’. We explore the nature of these, examine their prevalence, and seek to account for them in terms of the lecturers’ possible intentions.}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{ANDERSON1998214,
title = {Stereovision: beyond disparity computations},
journal = {Trends in Cognitive Sciences},
volume = {2},
number = {6},
pages = {214-222},
year = {1998},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(98)01180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661398011802},
author = {Barton L Anderson},
keywords = {sterovision, disparity, 3-D sterograms, perceptual grouping, occlusion},
abstract = {One of the most powerful sources of information about three-dimensional (3-D) structure is provided by stereovision (or stereopsis). For over a century, theoretical and empirical investigations into this ability have focused on the role of binocular disparity in generating percepts of 3-D structure. Recent work in image segmentation demonstrates that stereovision can cause large changes in perceptual organization that cannot be understood on the basis of binocular disparity alone. It is argued that these phenomena reveal the need for theoretical tools beyond those that have dominated the study of visual perception over the past three decades.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@article{CHEVRETTE20212024,
title = {The confluence of big data and evolutionary genome mining for the discovery of natural products},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2024-2040},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00013f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008789},
author = {Marc G. Chevrette and Athina Gavrilidou and Shrikant Mantri and Nelly Selem-Mojica and Nadine Ziemert and Francisco Barona-Gómez},
abstract = {ABSTRACT
This review covers literature between 2003–2021 The development and application of genome mining tools has given rise to ever-growing genetic and chemical databases and propelled natural products research into the modern age of Big Data. Likewise, an explosion of evolutionary studies has unveiled genetic patterns of natural products biosynthesis and function that support Darwin's theory of natural selection and other theories of adaptation and diversification. In this review, we aim to highlight how Big Data and evolutionary thinking converge in the study of natural products, and how this has led to an emerging sub-discipline of evolutionary genome mining of natural products. First, we outline general principles to best utilize Big Data in natural products research, addressing key considerations needed to provide evolutionary context. We then highlight successful examples where Big Data and evolutionary analyses have been combined to provide bioinformatic resources and tools for the discovery of novel natural products and their biosynthetic enzymes. Rather than an exhaustive list of evolution-driven discoveries, we highlight examples where Big Data and evolutionary thinking have been embraced for the evolutionary genome mining of natural products. After reviewing the nascent history of this sub-discipline, we discuss the challenges and opportunities of genomic and metabolomic tools with evolutionary foundations and/or implications and provide a future outlook for this emerging and exciting field of natural product research.}
}
@incollection{TONDEUR2024184,
title = {Chapter 6 - Batch control spike},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {184-239},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000314},
author = {Yves Tondeur},
keywords = {Compliance with PBMS/methods innovation rule & ISO 17025, Critical step, Erroneous beliefs & mental models, Function of the labeled standards, Out-of-control analytical system, Performance improvement, Sample fortification integrity, Systematic errors, Technology-in-use definition, Thinking method, Traditional QC samples effectiveness, Working relative response factors},
abstract = {When we are unaware of the causes for the observed deviations, we are more likely to react. When we react or are unaware of problems, we fail. Increasing the effectiveness of quality control samples starts with addressing what we do not know about them. So, this chapter first clarifies what we want, then describes what we have, and lastly what can be done to fill the gaps. The development, validation, and application of the batch control spike is a great illustration highlighting the importance the quality of the performance feedback and learning capacity the control samples are supposed to provide. When done correctly—while contextually questioning the relevance and appropriateness of current operating criteria, imposed limits and standards—the introduction of the batch control spike allows a process of critical errors identification, compensation, and progressive elimination to take place so that at the end, no significant systematic errors remain. This fact is demonstrated using z-scores from multiple international round-robin studies. In the context of achieving accuracy, the batch control spike examines the relationships between the standards used, when they are spiked, how they are spiked, and their purpose, that is, it renders the technology-in-use (isotope dilution) transparent and keeps it honest. It is also an excellent teaching tool. It is a quality learning sample helping the transformation of our methods into “thinking tools.”}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{STONE2022419,
title = {On second thoughts: changes of mind in decision-making},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {5},
pages = {419-431},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000407},
author = {Caleb Stone and Jason B. Mattingley and Dragan Rangelov},
keywords = {decision-making, change of mind, sequential sampling, metacognition},
abstract = {The ability to change initial decisions in the face of new or potentially conflicting information is fundamental to adaptive behavior. From perceptual tasks to multiple-choice tests, research has shown that changes of mind often improve task performance by correcting initial errors. Decision makers must, however, strike a balance between improvements that might arise from changes of mind and potential energetic, temporal, and psychological costs. In this review, we provide an overview of the change-of-mind literature, focusing on key behavioral findings, computational mechanisms, and neural correlates. We propose a conceptual framework that comprises two core decision dimensions – time and evidence source – which link changes of mind across decision contexts, as a first step toward an integrated psychological account of changes of mind.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{ZHAN2024121679,
title = {Conceptualizing future groundwater models through a ternary framework of multisource data, human expertise, and machine intelligence},
journal = {Water Research},
volume = {257},
pages = {121679},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121679},
url = {https://www.sciencedirect.com/science/article/pii/S0043135424005803},
author = {Chuanjun Zhan and Zhenxue Dai and Shangxian Yin and Kenneth C. Carroll and Mohamad Reza Soltanian},
keywords = {Groundwater model, Deep learning, Machine intelligence, Multisource data, Human expertise},
abstract = {Groundwater models are essential for understanding aquifer systems behavior and effective water resources spatio-temporal distributions, yet they are often hindered by challenges related to model assumptions, parametrization, uncertainty, and computational efficiency. Machine intelligence, especially deep learning, promises a paradigm shift in overcoming these challenges. A critical examination of existing machine-driven methods reveals the inherent limitations, particularly in terms of the interpretability and the ability to generalize findings. To overcome these challenges, we develop a ternary framework that synergizes the valuable insights from multisource data, human expertise, and machine intelligence. This framework capitalizes on the distinct strengths of each element: the value and relevance of multisource data, the innovative capacity of human expertise, and the analytical efficiency of machine intelligence. Our goal is to conceptualize sustainable water management practices and enhance our understanding and predictive capabilities of groundwater systems. Unlike approaches that rely solely on abundant data, our framework emphasizes the quality and strategic use of available data, combined with human intellect and advanced computing, to overcome current limitations and pave the way for more realistic groundwater simulations.}
}

@article{HUANG1993717,
title = {An investigation of gender differences in cognitive abilities among Chinese high school students},
journal = {Personality and Individual Differences},
volume = {15},
number = {6},
pages = {717-719},
year = {1993},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(93)90012-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699390012R},
author = {Jiafen Huang},
abstract = {This study investigated gender differences in 11 cognitive tests from a sample of grade 11 students in Shanghai, China. Research found that the girls outperformed boys significantly on Word Knowledge and Word Span tasks, and also on a Computational Speed and Accuracy test. Boys outperformed girls only on the Paper Folding test. Factor based scores showed that girls were superior to boys on memory, and verbal composites, whereas boys were superior to girls on the spatial composites. No gender differences were found on the Mathematical Thinking test and other reasoning tests. The research findings seemed to suggest that where the social conditions were more uniform the gender differences on visual-spatial and mathematical reasoning skills would be smaller.}
}
@article{HAMZI2023133853,
title = {Learning dynamical systems from data: A simple cross-validation perspective, part IV: Case with partial observations},
journal = {Physica D: Nonlinear Phenomena},
volume = {454},
pages = {133853},
year = {2023},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2023.133853},
url = {https://www.sciencedirect.com/science/article/pii/S0167278923002075},
author = {Boumediene Hamzi and Houman Owhadi and Yannis Kevrekidis},
keywords = {Learning dynamical systems, Kernel flows, Partial observations, Computational graph completion},
abstract = {A simple and interpretable way to learn a dynamical system from data is to interpolate its governing equations with a kernel. In particular, this strategy is highly efficient (both in terms of accuracy and complexity) when the kernel is data-adapted using Kernel Flows (KF) (Owhadi and Yoo, 2019), (which uses gradient-based optimization to learn a kernel based on the premise that a kernel is good if there is no significant loss in accuracy if half of the data is used for interpolation). In this work, we extend previous work on learning dynamical systems using Kernel Flows (Hamzi and Owhadi, 2021; Darcy et al. 2021; Lee et al. 2023; Darcy et al. 2023; Owhadi and Romit Maulik, 2021) to the case of learning vector-valued dynamical systems from time-series observations that are partial/incomplete in the state space. The method combines Kernel Flows with Computational Graph Completion.}
}
@article{NAVLAKHA201864,
title = {Network Design and the Brain},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {1},
pages = {64-78},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317302000},
author = {Saket Navlakha and Ziv Bar-Joseph and Alison L. Barth},
abstract = {Neural circuits have evolved to accommodate similar information processing challenges as those faced by engineered systems. Here, we compare neural versus engineering strategies for constructing networks. During circuit development, synapses are overproduced and then pruned back over time, whereas in engineered networks, connections are initially sparse and are then added over time. We provide a computational perspective on these two different approaches, including discussion of how and why they are used, insights that one can provide the other, and areas for future joint investigation. By thinking algorithmically about the goals, constraints, and optimization principles used by neural circuits, we can develop brain-derived strategies for enhancing network design, while also stimulating experimental hypotheses about circuit development and function.}
}
@incollection{WANG2001297,
title = {Computational Intelligence in Agile Manufacturing Engineering},
editor = {A. Gunasekaran},
booktitle = {Agile Manufacturing: The 21st Century Competitive Strategy},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {297-315},
year = {2001},
isbn = {978-0-08-043567-1},
doi = {https://doi.org/10.1016/B978-008043567-1/50016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080435671500164},
author = {Kesheng Wang}
}
@article{GEMMELL201720,
title = {Establishing the structures within populations of models},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {20-24},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300128},
author = {Philip M. Gemmell},
abstract = {As computational biology matures as a field, increasing attention is being paid to the relation of computational models to their target. One aspect of this is addressing how computational models can appropriately reproduce the variation seen in experimental data, with one solution being to use populations of models united by a common set of equations (the framework), with each individual member of the population (each model) possessing its own unique set of equation parameters. These model populations are then calibrated and validated against experimental data, and as a whole reproduce the experimentally observed variation. The primary focus of validation thus becomes the population, with the individual models' validation seemingly deriving from their membership of this population. The role of individual models within the population is not clear, with uncertainty regarding the relationship between individual models and the population they make up. This work examines the role of models within the population, how they relate to the population they make up, and how both can be said to be validated in this context.}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{HIPOLITO2023103510,
title = {Breaking boundaries: The Bayesian Brain Hypothesis for perception and prediction},
journal = {Consciousness and Cognition},
volume = {111},
pages = {103510},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023000478},
author = {Inês Hipólito and Michael Kirchhoff},
keywords = {Bayesian Brain Hypothesis, Modularity of the Mind, Cognitive processes, Informational boundaries},
abstract = {This special issue aims to provide a comprehensive overview of the current state of the Bayesian Brain Hypothesis and its standing across neuroscience, cognitive science and the philosophy of cognitive science. By gathering cutting-edge research from leading experts, this issue seeks to showcase the latest advancements in our understanding of the Bayesian brain, as well as its potential implications for future research in perception, cognition, and motor control. A special focus to achieve this aim is adopted in this special issue, as it seeks to explore the relation between two seemingly incompatible frameworks for the understanding of cognitive structure and function: the Bayesian Brain Hypothesis and the Modularity Theory of the Mind. In assessing the compatibility between these theories, the contributors to this special issue open up new pathways of thinking and advance our understanding of cognitive processes.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{NKONGOLO2022182,
title = {Using Deep Packet Inspection Data to Examine Subscribers on the Network},
journal = {Procedia Computer Science},
volume = {215},
pages = {182-191},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020920},
author = {Mike Nkongolo and Jacobus Phillipus {van Deventer} and Sydney Mambwe Kasongo},
keywords = {Deep packet inspection, machine learning, UGRansome, telecommunication, data science},
abstract = {This article proposes the creation of the deep packet inspection (DPI) dataset to study subscribers’ behavior on the network, applying ensemble learning to this dataset, and comparing it with the UGRansome dataset. The subscriber can be thought of as a person or a group of users using a network service or connectivity. The DPI features represent the subscriber network usage, and the ensemble learning approach is implemented on the DPI dataset to predict the subscriber's service category on the network. The classification and prediction problem addressed on the DPI dataset reached a precision of 100%. The paper predicts that the web and streaming categories with Netflix, Facebook, and YouTube services will be the most utilized in the next few years. This study will lead to a better understanding of the idiosyncratic behavior of active subscribers on the network, exposing novel network anomalies and facilitating the development of novel DPI systems.}
}
@article{BRYANSMITH2023105405,
title = {Real-time social media sentiment analysis for rapid impact assessment of floods},
journal = {Computers & Geosciences},
volume = {178},
pages = {105405},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001097},
author = {Lydia Bryan-Smith and Jake Godsall and Franky George and Kelly Egode and Nina Dethlefs and Dan Parsons},
keywords = {Social media, Sentiment analysis, Flooding, Artificial Intelligence},
abstract = {Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.}
}
@article{BINKOWSKA201435,
title = {Computational and experimental study of charge distribution in the α-disulfonyl carbanions},
journal = {Journal of Molecular Structure},
volume = {1062},
pages = {35-43},
year = {2014},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022286014000258},
author = {Iwona Binkowska and Jacek Koput and Arnold Jarczewski},
keywords = {Proton transfer, Carbon acids, Charge distribution,  computation},
abstract = {The electron densities of the disulfonyl carbanions were determined using experimental 13C chemical shifts. The 13C NMR spectra and electron densities for the disulfonyl, nitro, and cyano carbon acids were calculated at the MP2/cc-pVDZ level of theory. The calculated chemical shifts for disulfonyl carbanions show satisfying correlation with our own experimental data. The calculated π electron densities at the Cα atom correspond roughly to the “experimental” π electron densities estimated from the 13C chemical shifts. The natural charges at Cα in disulfonyl stabilized carbanions are significantly more negative than with other types of carbanions, partly because of the significant negative natural charge of the α carbon in parent carbon acids. The calculated increase of the negative charge caused by ionization is larger for sulfonyl carbon acids than for cyano- and nitroalkanes. The 13C chemical shifts δ of Cα in disulfonyl stabilized carbanions decrease with more negative calculated negative natural charge at Cα, with a slope of 220ppm/electron. The influence of phenyl ring para-substitution on the charge distribution in carbanions and relationship between the 13C chemical shifts and charge density have been discussed. It appears that the π electron density in these planar or nearly planar carbanions has a decisive impact on the chemical shifts.}
}
@article{DEALMEIDA2022478,
title = {Assisting in the choice to fill a vacancy to compose the PROANTAR team: Applying VFT and the CRITIC-GRA-3N methodology},
journal = {Procedia Computer Science},
volume = {214},
pages = {478-486},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019123},
author = {Isaque David Pereira {de Almeida} and Lucas Ramon dos Santos Hermogenes and Igor Pinheiro de Araújo Costa and Miguel Ângelo Lellis Moreira and Carlos Francisco Simões Gomes and Marcos {dos Santos} and David de Oliveira Costa and Ian José Agra Gomes},
keywords = {CRITIC-GRA-3N method, Brazilian Navy, COVID-19},
abstract = {Antarctica is the southernmost continent of our planet, and it has been verified as the coldest region on earth. The Brazilian Antarctic Program (PROANTAR) has as its main objective the promotion of high-quality scientific research in the Antarctic region, seeking to understand the events that occur there. PROANTAR, coordinated by the Navy Commander, has some sectors that are based in Brazil and others that are located in the Antarctic continent. The military that volunteers to occupy any vacancy that is allocated to that continent needs, besides passing through several pre-established criteria, to pass the selection process. The purpose of this article is to help the Naval Administration in the selection of volunteer officers to occupy a vacancy in the Antarctic continent. To obtain the alternatives, the officers that best fit the established vacancy, and the criteria to be evaluated, Value-Focused Thinking (VFT) was applied. Next, with all the necessary data, the CRITIC-GRA-3N method was used as a Multicriteria Decision Support (MDS) technique, the CRITIC-GRA-3N method, the CRITIC Importance Through Intercriteria Correlation (CRITIC) method to obtain the criteria weights and the Grey Relational Analysis (GRA) method, with three normalizations, to order the alternatives. At the end of the application of the methods, the article can generate five ordinations of the volunteer officers to occupy the vacancy offered in PROANTAR.}
}
@article{HABTEMARIAM1990653,
title = {Research in computational epidemiology},
journal = {Mathematical and Computer Modelling},
volume = {14},
pages = {653-658},
year = {1990},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(90)90263-M},
url = {https://www.sciencedirect.com/science/article/pii/089571779090263M},
author = {T. Habtemariam and D. Oryang and F. Gabreab and V. Robnett and G. Trammell},
abstract = {The emerging new area referred to as computational science or science done on a computer adds a third dimension to the traditional methods of theoretical and experimental approaches. Counterparts to computational science such as computational linguistice, computational engineering and others arc beginning to take roots. Naturally, new research paths and opportunities in computational epidemiology must also be explored. One of the major challenges in epidemiologic research is the issue of how to realistically and effectively handle complex bioepidemiologic dynamics involving interactions between humans or animals, etiological agents and the multiple array of environmental and socioeconomic determinants which affect these populations. To understand the behavior of such complex biological systems, it is useful to devise computer based simulation models. Computational epidemiologic approaches now provide alternative avenues to classical laboratory and/or field experimental methods. Systems which may be impractical because they are too large, or, not feasible because the cost is too prohibitive can now be simulated realistically. In the past obtaining solutions to biomathematical equations with any degree of complexity was impossible. However, the availability of powerful computers now makes the quantitative analysis of such systems feasible and indeed practical. With this in mind our research at Tuskegee University has focused on: a) Epidemiologic modelling and expert systems, and, b) Hypertext/hypermedia based epidemiologic knowledge management. The case studies for our research involve the bioepidemiologic dynamics of two complex host-parasite systems of trypanosoma and schistosoma. The ultimate goal is to develop resources and methodologies based on computational technology to advance epidemiologic research. The paper will address the methodological issues and findings as well as questions related to configuring an appropriate research workstation for computational epidemiology.}
}
@article{EDLA2015254,
title = {Is heart rate variability better than routine vital signs for prehospital identification of major hemorrhage?},
journal = {The American Journal of Emergency Medicine},
volume = {33},
number = {2},
pages = {254-261},
year = {2015},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2014.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S073567571400881X},
author = {Shwetha Edla and Andrew T. Reisner and Jianbo Liu and Victor A. Convertino and Robert Carter and Jaques Reifman},
abstract = {Objective
During initial assessment of trauma patients, metrics of heart rate variability (HRV) have been associated with high-risk clinical conditions. Yet, despite numerous studies, the potential of HRV to improve clinical outcomes remains unclear. Our objective was to evaluate whether HRV metrics provide additional diagnostic information, beyond routine vital signs, for making a specific clinical assessment: identification of hemorrhaging patients who receive packed red blood cell (PRBC) transfusion.
Methods
Adult prehospital trauma patients were analyzed retrospectively, excluding those who lacked a complete set of reliable vital signs and a clean electrocardiogram for computation of HRV metrics. We also excluded patients who did not survive to admission. The primary outcome was hemorrhagic injury plus different PRBC transfusion volumes. We performed multivariate regression analysis using HRV metrics and routine vital signs to test the hypothesis that HRV metrics could improve the diagnosis of hemorrhagic injury plus PRBC transfusion vs routine vital signs alone.
Results
As univariate predictors, HRV metrics in a data set of 402 subjects had comparable areas under receiver operating characteristic curves compared with routine vital signs. In multivariate regression models containing routine vital signs, HRV parameters were significant (P < .05) but yielded areas under receiver operating characteristic curves with minimal, nonsignificant improvements (+0.00 to +0.05).
Conclusions
A novel diagnostic test should improve diagnostic thinking and allow for better decision making in a significant fraction of cases. Our findings do not support that HRV metrics add value over routine vital signs in terms of prehospital identification of hemorrhaging patients who receive PRBC transfusion.}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{KNIGHT20061084,
title = {‘When I first came here, I thought medicine was black and white’: Making sense of medical students’ ways of knowing},
journal = {Social Science & Medicine},
volume = {63},
number = {4},
pages = {1084-1096},
year = {2006},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606000451},
author = {Lynn Valerie Knight and Karen Mattick},
keywords = {Medical training, Professional knowledge, Epistemology, Evidence-based medicine, United Kingdom},
abstract = {Personal beliefs about what knowledge is and how we understand, integrate and apply knowledge (known as personal epistemologies) are entrenched in the process of decision-making. Evidence-based medicine in all its forms brings with it the need for an ever more sophisticated appreciation of individual patients’ perspectives and ‘scientific’ perspectives within the clinical encounter. However, current theoretical perspectives on personal epistemology focus more on scientific ways of knowing where knowledge is abstracted and logical. We conducted semi-structured interviews to investigate medical students’ personal epistemological thinking towards the end of their second year of training at a new medical school in the South West of England. Whilst responses were varied, students appeared to express predominantly simplistic levels of epistemological thinking according to current developmental models of personal epistemology. However, the process of professional identity formation together with epistemological thinking brought together both scientific and experiential ways of knowing in a way that has largely been ignored by current theorists in the domain of personal epistemology.}
}
@article{SUN2025e01027,
title = {First-principles calculations of electronic and mechanical properties of magnesium indium intermetallic compounds},
journal = {Computational Condensed Matter},
volume = {43},
pages = {e01027},
year = {2025},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2025.e01027},
url = {https://www.sciencedirect.com/science/article/pii/S2352214325000267},
author = {Liang Sun and Yidan Huang and Kaifeng Zhao and Zuoming Chen and Xiongtao Shang and Wenzhen Xu and wenyan Zhai and Pengyue Han and Jin Jia and Jianhong Peng},
keywords = {Mg-In intermetallic compounds, First-principles calculations, Phonon spectra, Anisotropy, Mechanical properties, Electronic properties},
abstract = {In the search for innovative alternatives to aluminum-magnesium alloys, this study takes a unique approach by focusing on magnesium-indium binary alloys, with an emphasis on the intermetallic compounds Mg2In, MgIn3, Mg5In2, and Mg3In. With the help of cutting-edge first-principles computational techniques, the four compounds are comprehensively and thoroughly analyzed in terms of crystal structure, anisotropy, phonon spectra, electronic properties, and mechanical properties. The charge transfer phenomenon from magnesium to indium is found for the first time, and the s-orbital density of indium is at its peak in the Mg-In phase. In terms of mechanical properties, Mg2In, Mg5In2, and MgIn3 exhibit similar bulk moduli, while the shear modulus, Young's modulus, and hardness of MgIn3 are significantly lower than those of the other phases, emphasizing its unique deformability. Taking the results together, MgIn3 shows great potential for application in cutting-edge fields such as biomedical materials due to its compact size, corrosion resistance, low hardness, and high plasticity, which opens up a new way of thinking for the development of Mg-In alloy-based advanced materials.}
}
@article{FU2022107,
title = {Everyday Creativity is Associated with Increased Frontal Electroencephalography Alpha Activity During Creative Ideation},
journal = {Neuroscience},
volume = {503},
pages = {107-117},
year = {2022},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306452222004596},
author = {Lei Fu and Jia Zhao and Jiangzhou Sun and Yuchi Yan and Mujie Ma and Qunlin Chen and Jiang Qiu and Wenjing Yang},
keywords = {Everyday creativity, Alpha power, Alpha coherence, Creative ideation, Frontal cortex},
abstract = {Everyday creativity is the basic ability of human survival and penetrates every aspect of life. Nevertheless, the neural mechanisms underlying everyday creativity was largely unexplored. In this study, seventy-five participants completed the creative behaviour inventory, a tool for assessing creative behaviour in daily life. The participants also completed the alternate uses task (AUT) during an electroencephalography (EEG) assessment to evaluate creative thinking. Alpha power was used to quantify neural oscillations during the creative process, while alpha coherence was used to quantify information communication between frontal regions and other sites during creative ideation. Moreover, these two task-related quantitative measures were combined to investigate the relationship between individual differences in everyday creativity and EEG alpha activity during creative idea generation. Compared with the reference period, increased alpha power was observed in the frontal cortex of the right hemisphere and increased functional coupling was observed between frontal and parietal/temporal regions during the activation period. Interestingly, individual differences in everyday creativity were associated with distinct patterns of EEG alpha activity. Specifically, individuals with higher everyday creativity had increased alpha power in the frontal cortex, and increased changes in coherence in frontal-temporal regions of the right hemisphere while performing the AUT. It might indicate that individuals with higher everyday creativity had an enhanced ability to focus on internal information processing and control bottom-up stimuli, as well as better selection of novel semantic information when performing creative ideation tasks.}
}
@article{KARI2022102843,
title = {The Sabatier principle as a tool for discovery and engineering of industrial enzymes},
journal = {Current Opinion in Biotechnology},
volume = {78},
pages = {102843},
year = {2022},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2022.102843},
url = {https://www.sciencedirect.com/science/article/pii/S095816692200177X},
author = {Jeppe Kari and Kay Schaller and Gustavo A Molina and Kim Borch and Peter Westh},
abstract = {The recent breakthrough in all-atom, protein structure prediction opens new avenues for a range of computational approaches in enzyme design. These new approaches could become instrumental for the development of technical biocatalysts, and hence our transition toward more sustainable industries. Here, we discuss one approach, which is well-known within inorganic catalysis, but essentially unexploited in biotechnology. Specifically, we review examples of linear free-energy relationships (LFERs) for enzyme reactions and discuss how LFERs and the associated Sabatier Principle may be implemented in algorithms that estimate kinetic parameters and enzyme performance based on model structures.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{GENTILI2024150060,
title = {Living cells and biological mechanisms as prototypes for developing chemical artificial intelligence},
journal = {Biochemical and Biophysical Research Communications},
volume = {720},
pages = {150060},
year = {2024},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2024.150060},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X24005965},
author = {Pier Luigi Gentili and Pasquale Stano},
keywords = {Chemical AI, Synthetic cell, Chemical neural networks, Neuromorphic engineering, Molecular fuzzy sets, Molecular computing},
abstract = {Artificial Intelligence (AI) is having a revolutionary impact on our societies. It is helping humans in facing the global challenges of this century. Traditionally, AI is developed in software or through neuromorphic engineering in hardware. More recently, a brand-new strategy has been proposed. It is the so-called Chemical AI (CAI), which exploits molecular, supramolecular, and systems chemistry in wetware to mimic human intelligence. In this work, two promising approaches for boosting CAI are described. One regards designing and implementing neural surrogates that can communicate through optical or chemical signals and give rise to networks for computational purposes and to develop micro/nanorobotics. The other approach concerns “bottom-up synthetic cells” that can be exploited for applications in various scenarios, including future nano-medicine. Both topics are presented at a basic level, mainly to inform the broader audience of non-specialists, and so favour the rise of interest in these frontier subjects.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{HANNA2025100705,
title = {Future of Artificial Intelligence—Machine Learning Trends in Pathology and Medicine},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100705},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2025.100705},
url = {https://www.sciencedirect.com/science/article/pii/S0893395225000018},
author = {Matthew G. Hanna and Liron Pantanowitz and Rajesh Dash and James H. Harrison and Mustafa Deebajah and Joshua Pantanowitz and Hooman H. Rashidi},
keywords = {artificial intelligence, computational pathology, machine learning, operations},
abstract = {Artificial intelligence (AI) and machine learning (ML) are transforming the field of medicine. Health care organizations are now starting to establish management strategies for integrating such platforms (AI-ML toolsets) that leverage the computational power of advanced algorithms to analyze data and to provide better insights that ultimately translate to enhanced clinical decision-making and improved patient outcomes. Emerging AI-ML platforms and trends in pathology and medicine are reshaping the field by offering innovative solutions to enhance diagnostic accuracy, operational workflows, clinical decision support, and clinical outcomes. These tools are also increasingly valuable in pathology research in which they contribute to automated image analysis, biomarker discovery, drug development, clinical trials, and productive analytics. Other related trends include the adoption of ML operations for managing models in clinical settings, the application of multimodal and multiagent AI to utilize diverse data sources, expedited translational research, and virtualized education for training and simulation. As the final chapter of our AI educational series, this review article delves into the current adoption, future directions, and transformative potential of AI-ML platforms in pathology and medicine, discussing their applications, benefits, challenges, and future perspectives.}
}
@article{MANDAVE2023100276,
title = {Bio-inspired computing algorithms in dementia diagnosis – a application-oriented review},
journal = {Results in Control and Optimization},
volume = {12},
pages = {100276},
year = {2023},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666720723000784},
author = {Deepa D. Mandave and Lalit V. Patil},
keywords = {Dementia, Biomotivated algorithms, Image segmentation, Meta-heuristic, Alzheimer, Optimization, Feature selection},
abstract = {Dementia is a major neurocognitive disease which affects memory, thinking skills, attitudes, and social behavior, extremely causing disturbances in daily routine activities and social activities. Alzheimer is the most general form of dementia in the elderly. Recently, biomotivated techniques have become famous in the domain of healthcare and have obtained appreciable success. This review shows that these techniques are mostly utilized to resolve various problems such as image segmentation, feature selection, classification, and optimization in the detection of various disorders like cancer, anemia, Alzheimer, kidney and skin diseases. It is observed that the dementia diagnosis was performed using classical approaches which led to reduced performance (accuracy, precision). This performance parameter can be enhanced by using biomotivated techniques. This paper presents a comprehensive analysis of the different role of biomotivated metaheuristics in the domain of dementia diagnosis with a detailed analysis of published work. The results showed that a biomotivated technique plays an important role in dementia diagnosis.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{FURLAN2022163,
title = {The earth vibrates with analogies: The Dirac sea and the geology of the vacuum},
journal = {Studies in History and Philosophy of Science},
volume = {93},
pages = {163-174},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368122000590},
author = {Stefano Furlan and Rocco Gaudenzi},
keywords = {Analogies, Analogical thinking, Heuristics, History of quantum physics, Vacuum, Spontaneous symmetry breaking},
abstract = {The debate around analogy in modern physics that focuses on its role as a logical inference often correspondingly overlooks its historical dimension and the other equally important functions and aspects that are intertwined with this dimension. Inspired by a close investigation of the primary sources and archival material of a few historical actors, this paper lays out a framework on analogy-making which preserves as much as possible its historical complexity. While not losing sight of the logical role, our framework puts a special emphasis on the heuristic process, and aims at offering to the historian and philosopher of science as well as the physicist some tools to capture the subtle functions of analogical reasoning involved in such a process. After having traced it out theoretically, we make use of this framework to interpret the growth of the ideas of two remarkable physicists dealing with the multifaceted notion of vacuum in 20th century physics. We first consider the trajectory followed by John A. Wheeler, between the 1960s and 1970s, towards (in his own words) a “geology of the vacuum”; and then examine, starting from the hitherto neglected Japanese reception of the idea of Dirac sea in the early 1930s, the pathway that led Yoichiro Nambu to the discovery of spontaneous symmetry breaking.}
}
@article{BEECH2023105401,
title = {Consequences of phonological variation for algorithmic word segmentation},
journal = {Cognition},
volume = {235},
pages = {105401},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000355},
author = {Caroline Beech and Daniel Swingley},
keywords = {Language acquisition, Computational modeling, Word segmentation, Phonological variation},
abstract = {Over the first year, infants begin to learn the words of their language. Previous work suggests that certain statistical regularities in speech could help infants segment the speech stream into words, thereby forming a proto-lexicon that could support learning of the eventual vocabulary. However, computational models of word segmentation have typically been tested using language input that is much less variable than actual speech is. We show that using actual, transcribed pronunciations rather than dictionary pronunciations of the same speech leads to worse segmentation performance across models. We also find that phonologically variable input poses serious problems for lexicon building, because even correctly segmented word forms exhibit a complex, many-to-many relationship with speakers' intended words. Many phonologically distinct word forms were actually the same intended word, and many identical transcriptions came from different intended words. The fact that previous models appear to have substantially overestimated the utility of simple statistical heuristics suggests a need to consider the formation of the lexicon in infancy differently.}
}
@article{YANG2023414,
title = {A review of sequential three-way decision and multi-granularity learning},
journal = {International Journal of Approximate Reasoning},
volume = {152},
pages = {414-433},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2200192X},
author = {Xin Yang and Yanhua Li and Tianrui Li},
keywords = {Three-way decision, Granular computing, Sequential three-way decision, Three-way multi-granularity learning},
abstract = {The concept of three-way decision, interpreted and described as thinking, problem solving, and information processing in “threes”, has been widely studied and applied in machine learning and data engineering in recent years. In open-world environment, the connection and interaction of dynamic and uncertainty by multi-granularity learning gives more vitality to three-way decision. In this paper, we investigate and summarize the initial and development models of three-way decision. Then we revisit the historical line of sequential three-way decision from rough set to granular computing. Besides, we focus on exploring a unified framework of three-way multi-granularity learning with four crucial problems on mining uncertain region continually. Finally, we give some proposals on three-way decision associated with open-continual learning.}
}
@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}
@article{HILLERT2021103158,
title = {How did language evolve in the lineage of higher primates?},
journal = {Lingua},
volume = {264},
pages = {103158},
year = {2021},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S0024384121001303},
author = {Dieter Hillert},
keywords = {Broca’s area, Comparative studies, Homo erectus, Language capacity, Neural circuits, Prehistoric artefacts},
abstract = {Speech components emerged in the hominin lineage before the rise of modern human behavior and were already in place in monkey species. Evidence from genetics to archaeological records points to an accumulative increase of those computational properties required for modern language. At about 2.4 mya, the polytypical species Homo erectus sensu lato (s.l.) appeared with significant cortical growth indicated by neural migration factors and fossil skulls. The evidence suggests that early Homo erectus s.l. was equipped with a computational capacity for premodern language. The same species developed Acheulean toolmaking and showed signs of a symbolic and aesthetic mind at about half a mya. We conclude that the modern language capacity evolved at around 1 mya in the merging species late Homo erectus s.l. and pre-archaic Homo sapiens.}
}
@article{SCHACTER1999403,
title = {Computer-based performance assessments: a solution to the narrow measurement and reporting of problem-solving☆☆The findings and opinions expressed in this report do not reflect the position or policies of ISX, Advanced Research Projects Agency, the Department of the Navy, or the Department of Defense; nor do they reflect the positions or policies of the National Institute on Student Achievement, Curriculum, and Assessment, the Office of Educational Research and Improvement, or the US Department of Education.},
journal = {Computers in Human Behavior},
volume = {15},
number = {3},
pages = {403-418},
year = {1999},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(99)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563299000291},
author = {J. Schacter and H.E. Herl and G.K.W.K. Chung and R.A. Dennis and H.F. O'Neil},
keywords = {Assessment, Problem solving, Computers, Internet, Technology, Education},
abstract = {Although performance assessments test for higher order thinking and problem solving, they rarely report students' thinking process data back to teachers, students, or the public. Web-based database-backed performance assessments provide a viable means for concurrently reporting both performance and thinking process data. In the research conducted here, we report our findings from a study that assessed student problem solving using networked computers. Both performance and process data could be reported back to teachers and students such that they could diagnose and understand how they performed and what problem-solving processes contributed to or detracted from their performance.}
}
@article{DALLAT2019266,
title = {Risky systems versus risky people: To what extent do risk assessment methods consider the systems approach to accident causation? A review of the literature},
journal = {Safety Science},
volume = {119},
pages = {266-279},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517305295},
author = {Clare Dallat and Paul M. Salmon and Natassia Goode},
keywords = {Risk, Risk assessment, Risk assessment methods, Systems thinking},
abstract = {Accidents are now widely acknowledged to be a systems phenomenon. As part of a proactive approach to safety management, organisations use risk assessment methods to identify the hazards and associated risks that may lead to accidents. Although there is an extensive body of literature on the need for a systems thinking approach in accident analysis, little has been said regarding the theoretical underpinnings of risk assessment methods. The aim of this paper was to systematically review the risk assessment methods presented in the literature and evaluate the extent to which they are underpinned by a systems thinking approach. A total of 342 methods spanning a range of safety-critical domains were evaluated using Rasmussen’s tenets of accident causation. A key finding is that the majority of existing risk assessment methods are not consistent with Rasmussen’s model of accident causation (arguably the most popular model in safety science circles). Instead, the majority of risk assessment methods focus on risks at the so called sharp-end and largely view accidents as emerging from a linear, or chain-of-events process. This overlooks emergent risks at other levels of the system, including supervisory, managerial, regulatory and government levels. The findings therefore suggest that the majority of existing risk assessment methods may be inadequate for identifying hazards and analysing risks within complex sociotechnical systems. The implications for risk assessment practice are discussed.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{MOLINARO20231150,
title = {A goal-centric outlook on learning},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1150-1164},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002073},
author = {Gaia Molinaro and Anne G.E. Collins},
keywords = {goals, learning, decision-making, reinforcement learning, rewards, abstraction, motivation, computational modeling},
abstract = {Goals play a central role in human cognition. However, computational theories of learning and decision-making often take goals as given. Here, we review key empirical findings showing that goals shape the representations of inputs, responses, and outcomes, such that setting a goal crucially influences the central aspects of any learning process: states, actions, and rewards. We thus argue that studying goal selection is essential to advance our understanding of learning. By following existing literature in framing goal selection within a hierarchy of decision-making problems, we synthesize important findings on the principles underlying goal value attribution and exploration strategies. Ultimately, we propose that a goal-centric perspective will help develop more complete accounts of learning in both biological and artificial agents.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{WANG2022e09982,
title = {Applying the post-digital strategy of anexact architecture to non-standard design practices within the challenging construction contexts},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e09982},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09982},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012701},
author = {Sining Wang and Dandan Lin},
keywords = {Design practice strategy, Post-digital architecture, Parametric design, Developing region, Non-standard architecture},
abstract = {New architectural forms offered by digital design approaches often appear incompatible with the prescribed precision and control in construction, especially in developing regions where advanced implementation means are limited. In response, this paper suggests working with design practice indeterminacy. Named ‘anexact architecture’, the post-digital design practice strategy presents a convergent diagram of seeking the feasible design solution space. It relies on the procedural parametric modelling to constantly integrate computation and humanisation, so that a rigorous built outcome is capable of accommodating project-specific idiosyncrasies and constraints. The demonstrator projects are discussed based on the combination of the Participatory Action Research method and the idea of anexact architecture. This paper aims to illustrate the peculiarity of anexact architecture and its ideology of treating design delivery uncertainties as essentials rather than negatives when practicing in a volatile construction context.}
}
@article{SURYARAJ2024124407,
title = {Block based motion estimation model using CNN with representative point matching algorithm for object tracking in videos},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124407},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124407},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424012739},
author = {C.K. Suryaraj and M.R. Geetha},
keywords = {Motion Estimation, Object Tracking, CNN, RPM, SSIM, Video Sequence, Computation Time},
abstract = {Motion estimation is considered significant for tracking the movement of an object in video sequences, and it is widely used in various video processing applications. Traditionally, many researchers focus on pixel-based motion estimation for object tracking, but it experienced increased computation time and cost. To reduce computation time, the utilization of a block-based motion estimation approach for object tracking is a recent trend. The existing block-based approach faces difficulty in finding representative points within the intensity domain. Therefore, this current research merged the deep learning approach with a block-matching algorithm for achieving efficient object tracking. In this proposed work, initially, video sequences are collected from a benchmark video dataset. Then, the acquired video sequences are segmented into frames. From the segmented frames, current and previous frames are considered for motion estimation. Frames are sent for the data augmentation process in which the process of flipping, cropping, and rotation is carried out. Then, the augmented frames are sent into Convolutional Neural Network (CNN) for feature extraction. Representative Point Matching (RPM) is used to estimate the motion vector based on the extracted features. After estimating the motion vector, the similarity between two consecutive frames is found using Structural Similarity Index (SSIM) technique. Finally, based on the similarity score, the movement of an object in the video is tracked effectively. Simulation analysis of the proposed block-based motion estimation model is done by evaluating some performance metrics. RMSE, PSNR, Execution Time, SSIM, and accuracy obtained for the proposed model are 27.5, 26.5 db, 31 sec, 0.91, and 94 %. This analysis suggested that the proposed CNN-RPM motion estimation model performs better in tracking the movement of the object.}
}
@article{GREENSPAN1990490,
title = {A counterexample of the use of energy as a measure of computational accuracy},
journal = {Journal of Computational Physics},
volume = {91},
number = {2},
pages = {490-494},
year = {1990},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(90)90051-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999190900512},
author = {Donald Greenspan}
}
@article{BOVE20031040,
title = {Computational fluid dynamics in the evaluation of hemodynamic performance of cavopulmonary connections after the norwood procedure for hypoplastic left heart syndrome},
journal = {The Journal of Thoracic and Cardiovascular Surgery},
volume = {126},
number = {4},
pages = {1040-1047},
year = {2003},
issn = {0022-5223},
doi = {https://doi.org/10.1016/S0022-5223(03)00698-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022522303006986},
author = {Edward L. Bove and Marc R. {de Leval} and Francesco Migliavacca and Gualtiero Guadagni and Gabriele Dubini},
keywords = {17, 21},
abstract = {Objective
Computational fluid dynamics have been used to study the hemodynamic performance of surgical operations, resulting in improved design. Efficient designs with minimal energy losses are especially important for cavopulmonary connections. The purpose of this study was to compare hydraulic performance between the hemi-Fontan and bidirectional Glenn procedures, as well as the various types of completion Fontan operations.
Methods
Three-dimensional models were constructed of typical hemi-Fontan and bidirectional Glenn operations according to anatomic data derived from magnetic resonance scans, angiocardiograms, and echocardiograms. Boundary conditions were imposed, and fluid dynamics were calculated from a mathematic code. Power losses, flow distribution to each lung, and pressures were measured at three predetermined levels of pulmonary arteriolar resistance. Models of the lateral tunnel, total cavopulmonary connection, and extracardiac conduit completion Fontan operations were constructed, and power losses, total flow distribution, vena caval and pulmonary arterial pressures, and flow distribution of inferior vena caval return were calculated.
Results
The hemi-Fontan and bidirectional Glenn procedures performed nearly identically, with similar power losses and nearly equal flow distributions to each lung at all levels of pulmonary arteriolar resistance. However, the lateral tunnel Fontan procedure as performed after the hemi-Fontan operation had lower power losses (6.9 mW, pulmonary arteriolar resistance 3 units) than the total cavopulmonary connection (40.5 mW) or the extracardiac conduit (42.9 mW), although the inclusion of an enlargement patch toward the right in the total cavopulmonary connection was effective in reducing the difference (10.0 mW). Inferior vena caval flow to the right lung was 52% for the lateral tunnel, compared with 19%, 30%, 19%, and 15% for the total cavopulmonary connection, total cavopulmonary connection with right-sided enlargement patch, extracardiac conduit, and extracardiac conduit with a bevel to the left lung, respectively.
Conclusions
According to these methods, the hemi-Fontan and bidirectional Glenn procedures performed equally well, but important differences in energy losses and flow distribution were found after the completion Fontan procedures. The superior hydraulic performance of the lateral tunnel Fontan operation after the hemi-Fontan procedure relative to any other method may be due to closer to optimal caval offset achieved in the surgical reconstruction.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@article{SENVAR20161140,
title = {Hospital Site Selection via Hesitant Fuzzy TOPSIS},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1140-1145},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.656},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309296},
author = {Ozlem Senvar and Irem Otay and Eda Bolturk},
keywords = {Facility Layout, Location Selection, Multi criteria decision making (MCDM), TOPSIS, Hesitant fuzzy set (HFS)},
abstract = {This study handles the problem of establishing a well-organized and distributed network of a hospital that delivers its services to the target population. We propose a new multi criteria decision making (MCDM) process that integrates hesitant fuzzy sets (HFSs) to Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The MCDM process defined under uncertainties are perfectly defined by HFSs reflecting comprehensively hesitant thinking of decision makers. Our proposed methodology is implemented to select the optimum site for a new hospital in Istanbul.}
}
@article{BAUSO201776,
title = {Consensus via multi-population robust mean-field games},
journal = {Systems & Control Letters},
volume = {107},
pages = {76-83},
year = {2017},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167691117301287},
author = {D. Bauso},
keywords = {Synchronization, Consensus, Mean-field games},
abstract = {In less prescriptive environments where individuals are told ‘what to do’ but not ‘how to do’, synchronization can be a byproduct of strategic thinking, prediction, and local interactions. We prove this in the context of multi-population robust mean-field games. The model sheds light on a multi-scale phenomenon involving fast synchronization within the same population and slow inter-cluster oscillation between different populations.}
}
@article{BERNUS201583,
title = {Enterprise architecture: Twenty years of the GERAM framework},
journal = {Annual Reviews in Control},
volume = {39},
pages = {83-93},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000097},
author = {Peter Bernus and Ovidiu Noran and Arturo Molina},
abstract = {Apart from the 20-year anniversary in 2014 of the first publication of the GERAM (‘Generalised Enterprise Reference Architecture and Methodology’) Enterprise Architecture Framework, the timeliness of this paper lies in the new interest in the use of systems theory in enterprise architecture (EA), and consequently, ‘light-weight’ architecture frameworks (AFs). Thus, this paper is about the use of systems thinking and systems theory in EA and about how it is possible to reconcile and understand, based on a single overarching framework, the interplay of two major enterprise change endeavours: on one hand enterprise engineering (i.e. deliberate change) and on the other hand evolutionary, organic change. The paper also demonstrates how such change processes can be illustrated by employing systems thinking to construct dynamic business models; the evolution of these concepts is exemplified using past applications in networked enterprise building, and more recent proposals in environmental-, disaster- and healthcare management. Finally, the paper attempts to plot the way GERAM, as a framework to think about the creation and evolution of complex socio-technical systems, will continue to contribute to the society in the context of future challenges and emerging opportunities.}
}
@article{ZHANG2022104545,
title = {Watching a hands-on activity improves students’ understanding of randomness},
journal = {Computers & Education},
volume = {186},
pages = {104545},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001166},
author = {Icy (Yunyi) Zhang and Mary C. Tucker and James W. Stigler},
keywords = {Hands-on demonstration, Computer simulation, Statistics education, Multimedia learning, Online instruction, Instructional sequence, Embodied cognition},
abstract = {Introductory statistics students struggle to understand randomness as a data generating process, and especially its application to the practice of data analysis. Although modern computational techniques for data analysis such as simulation, randomization, and bootstrapping have the potential to make the idea of randomness more concrete, representing such random processes with R code is not as easy for students to understand as is something like a coin-flip, which is both concrete and embodied. In this study, in the context of multimedia learning, we designed and tested the efficacy of an instructional sequence that preceded computational simulations with embodied demonstrations. We investigated the role that embodied hands-on movement might play in facilitating students’ understanding of the shuffle function in R. Our findings showed that students who watched a video of hands shuffling data written on pieces of paper learned more from a subsequent live-coding demonstration of randomization using R than did students only introduced to the concept using R. Although others have found an advantage of students themselves engaging in hands-on activities, this study showed that merely watching someone else engage can benefit learning. Implications for online and remote instruction are discussed.}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{BELLA2023100509,
title = {Circular dichroism simulations of chiral buckybowls by means curvature analyses},
journal = {FlatChem},
volume = {40},
pages = {100509},
year = {2023},
issn = {2452-2627},
doi = {https://doi.org/10.1016/j.flatc.2023.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2452262723000417},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Buckybowl, Chirality, Curvature, TD-DFT, Circular dichroism},
abstract = {A detailed understanding and interpretation of chiral properties of molecular systems, especially in condensed phase, often requires computational models that allow their structural and electronic features to be connected to the observed experimental spectra. The present paper is focused on modelling the circular dichroism spectra of chiral buckybowls, combining topological aspects and the density functional theory. For the first time Ball Pivoting Algorithm was proposed to hook up the chemical topology to the DFT through the surface reconstruction. Particularly, the gaussian curvature of a constructed probe set of corannulene and sumanene derivatives was used as discriminant parameter to benchmark a list of 10 functionals (B3LYP, B97D, M06-2X, HSEH1PBE, wB97XD, CAM-B3LYP, LC-wPBE, TPSSTPSS, mPW1PW91 and APFD). The latter provide to be noticeably accurate to reproduce the curvature effect of the considered molecules. A TD-DFT/BOMD mixed approach provided a comprehensive overview of the spectral chiral pattern prediction trends when multiple DFT functionals are scanned. The preliminary topological analysis efforts were then recompensed with the very precise computed CD spectra, again APFD confirmed as the leader functional, this time for TD-DFT vertical transition calculations. Therefore, we strongly recommend the use of the of dispersion embedded APFD functional coupled with the 6–311++G(2d,2p) basis set for the computation of the functionalized chiral buckybowls ECD spectra. © 2017 Elsevier Inc. All rights reserved.}
}
@article{VIGNAPIANO201999,
title = {Disorganization and cognitive impairment in schizophrenia: New insights from electrophysiological findings},
journal = {International Journal of Psychophysiology},
volume = {145},
pages = {99-108},
year = {2019},
note = {The Neurophysiology of Schizophrenia: A Critical Update},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S016787601830984X},
author = {Annarita Vignapiano and Thomas Koenig and Armida Mucci and Giulia M. Giordano and Antonella Amodio and Mario Altamura and Antonello Bellomo and Roberto Brugnoli and Giulio Corrivetti and Giorgio {Di Lorenzo} and Paolo Girardi and Palmiero Monteleone and Cinzia Niolu and Silvana Galderisi and Mario Maj},
keywords = {Disorganization dimension, Difficulty in abstract thinking, Neurocognitive domains, Alpha rhythm, Spectral power, Topographic analysis},
abstract = {In subjects with schizophrenia (SCZ), the disorganization dimension is a strong predictor of real-life functioning. “Conceptual disorganization” (P2), “Difficulty in abstract thinking” (N5) and “Poor attention” (G11) are core features of the disorganization factor, evaluated using the Positive and Negative Syndrome Scale. The heterogeneity of this dimension and its overlap with neurocognitive deficits are still debated. Within the multicenter study of the Italian Network for Research on Psychoses, we investigated electrophysiological and neurocognitive correlates of disorganization and its component items to assess the heterogeneity of this dimension and its possible overlap with neurocognitive deficits. Resting state EEG was recorded in 145 stabilized SCZ and 69 matched healthy controls (HC). Spectral amplitude was averaged in ten frequency bands. Neurocognitive domains were assessed by MATRICS Consensus Cognitive Battery (MCCB). RAndomization Graphical User software explored band spectral amplitude differences between groups and correlations with disorganization and MCCB scores in SCZ. Correlations between disorganization and MCCB scores were also investigated. Compared to HC, SCZ showed increased delta, theta, and beta 1 and decreased alpha 2 activity. A negative correlation between alpha 1 and disorganization was observed in SCZ. At the item level, only “N5” showed the same correlation. MCCB neurocognitive composite score was associated with disorganization, “P2” and “N5”. Our findings suggest only a partial overlap between disorganization and neurocognitive impairment. The association of alpha 1 with the “N5” item suggests that some aspects of disorganization could be underpinned by the impairment of basic neurobiological functions that are only partially evaluated using MCCB.}
}
@article{CALEFFI2024110672,
title = {Distributed quantum computing: A survey},
journal = {Computer Networks},
volume = {254},
pages = {110672},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110672},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624005048},
author = {Marcello Caleffi and Michele Amoretti and Davide Ferrari and Jessica Illiano and Antonio Manzalini and Angela Sara Cacciapuoti},
keywords = {Quantum internet, Quantum networks, Quantum communications, Quantum computing, Quantum computation, Distributed quantum computing, Quantum algorithms, Quantum compiler, Quantum compiling, Simulator},
abstract = {Nowadays, quantum computing has reached the engineering phase, with fully-functional quantum processors integrating hundreds of noisy qubits. Yet – to fully unveil the potential of quantum computing out of the labs into the business reality – the challenge ahead is to substantially scale the qubit number, reaching orders of magnitude exceeding thousands of fault-tolerant qubits. To this aim, the distributed quantum computing paradigm is recognized as the key solution for scaling the number of qubits. Indeed, accordingly to such a paradigm, multiple small-to-moderate-scale quantum processors communicate and cooperate for executing computational tasks exceeding the computational power of single processing devices. The aim of this survey is to provide the reader with an overview about the main challenges and open problems arising with distributed quantum computing from a computer and communications engineering perspective. Furthermore, this survey provides an easy access and guide towards the relevant literature and the prominent results in the field.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{TRAGER20241555,
title = {The human touch: Utilizing AlphaFold 3 to analyze structures of endogenous metabolons},
journal = {Structure},
volume = {32},
number = {10},
pages = {1555-1562},
year = {2024},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0969212624003356},
author = {Toni K. Träger and Christian Tüting and Panagiotis L. Kastritis},
abstract = {Summary
Computational structural biology aims to accurately predict biomolecular complexes with AlphaFold 3 spearheading the field. However, challenges loom for structural analysis, especially when complex assemblies such as the pyruvate dehydrogenase complex (PDHc), which catalyzes the link reaction in cellular respiration, are studied. PDHc subcomplexes are challenging to predict, particularly interactions involving weaker, lower-affinity subcomplexes. Supervised modeling, i.e., integrative structural biology, will continue to play a role in fine-tuning this type of prediction (e.g., removing clashes, rebuilding loops/disordered regions, and redocking interfaces). 3D analysis of endogenous metabolic complexes continues to require, in addition to AI, precise and multi-faceted interrogation methods.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{HALLOWELL2023100240,
title = {Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100240},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100240},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000240},
author = {Nina Hallowell and Shirlene Badger and Francis McKay and Angeliki Kerasidou and Christoffer Nellåker},
keywords = {Computational phenotyping, Rare disease, Diagnosis, AI, Qualitative interviews},
abstract = {Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic.}
}
@article{HEGG201856,
title = {Preservice teacher proficiency with transformations-based congruence proofs after a college proof-based geometry class},
journal = {The Journal of Mathematical Behavior},
volume = {51},
pages = {56-70},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301153},
author = {Meredith Hegg and Dimitri Papadopoulos and Brian Katz and Timothy Fukawa-Connelly},
keywords = {Mathematics teacher education, Transformational geometry, Proof},
abstract = {This report explores pre-service teachers’ proficiency with concepts of transformational geometry at the end of a semester-long advanced geometry course. In the course, the instructor incorporated transformational geometry content, including congruence proofs, in an attempt to prepare the pre-service teachers to teach high school geometry in alignment with the Common Core State Standards for Mathematics. At the conclusion of the course, students expressed a preference for using traditional triangle congruence criteria (SAS, ASA, SSS, and AAS) over using transformations to complete proofs, but were nevertheless generally successful in completing proofs using transformations. Similarly, while the students often described thinking of transformations in terms of analytic forms, they were successfully able to prove triangle congruences in synthetic contexts. Finally, some evidence indicates that students may have motion or process conceptions of transformations, but not map or object conceptions, but this evidence is not conclusive.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@article{IONESCU2014275,
title = {Embodied Cognition: Challenges for Psychology and Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {128},
pages = {275-280},
year = {2014},
note = {International Conference: EDUCATION AND PSYCHOLOGY CHALLENGES - TEACHERS FOR THE KNOWLEDGE SOCIETY – 2nd EDITION EPC – TKS 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.03.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814022472},
author = {Thea Ionescu and Dermina Vasc},
keywords = {embodiment, cognition, sensory-motor processes, action, education.},
abstract = {Embodied cognition considers that human cognition is fundamentally grounded in sensory-motor processes and in our body's morphology and internal states. In this paper, we discuss some of the features of this post-cognitivist approach and the challenges that follow for psychology and education. These challenges point to the need to reconsider cognition and the way we pursue education today. If we want to have an efficient educational system we have to look at fundamental research in cognitive science to have an accurate description of what cognition is. Only then can we design optimal educational settings for the development of thinking.}
}
@article{GUO2024324,
title = {Optimization of robot manipulator configuration calibration by using Zhang neural network for repetitive motion},
journal = {Applied Mathematical Modelling},
volume = {134},
pages = {324-348},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X24002853},
author = {Pengfei Guo and Yunong Zhang and Shuai Li and Ning Tan},
keywords = {Temporally dependent quadratic programming, Filtered reciprocal-kind Zhang neural network, Lyapunov stability, Robot manipulator configuration calibration},
abstract = {High precision and low complexity control algorithm plays an important role in the developing of the end-effector instrumentation of different robot manipulators. In order to reduce the kinetic energy and the high-speed drift phenomenon of the repetitive motion tracking task, the robot manipulator needs to calibrate its configuration. In this paper, we formulate the configuration calibration of the robot manipulator for the repetitive motion task as a future quadratic programming optimization problem constrained with equality constraints, which is also regarded as a fundamental problem in artificial intelligence and modern control engineering. Zhang neural network, which is a canonical method, can be adopted to deal with the continuous form of the future optimization problem, named as temporally dependent quadratic programming problem with equality constraints. In order to overcome the issue of temporally dependent inverse computing, a novel Zhang neural network model and its uncertain disturbance tolerant model, which are termed as filtered reciprocal-kind Zhang neural network model and uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network model, respectively, are proposed by integrating the energy-type cost function and Zhang neural network design formula for solving the temporally dependent quadratic programming problem with equality constraints in this paper. Based on the Euler discrete formula and the models, the discrete filtered reciprocal-kind Zhang neural network and the discrete uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network algorithms are proposed for solving the future quadratic programming problem with equality constraints and the robot manipulator configuration calibration problem of repetitive motion. The convergence properties of the reciprocal-kind Zhang neural network model and its corresponding uncertain disturbance tolerant model are obtained by Lyapunov stability theory of nonlinear system and its corresponding perturbed system, while the convergence property of the filtered reciprocal-kind Zhang neural network model is analyzed by the limit thinking. For the repetitive motion task, three experiments for solving the configuration calibration problem of PUMA560, Kinova Jaco2, and Franka Emika Panda robot manipulators are performed to illustrate the effectiveness, robustness and superiority of our proposed discrete filtered reciprocal-kind Zhang neural network algorithms.}
}
@article{FINGER2025101535,
title = {When kids juggle it all: Biliteracy instruction and the development of discourse connectedness in L1 and L2 writing},
journal = {Cognitive Development},
volume = {73},
pages = {101535},
year = {2025},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2024.101535},
url = {https://www.sciencedirect.com/science/article/pii/S0885201424001205},
author = {Ingrid Finger and Cristiane Ely Lemke and Larissa da Silva Cury and Natália Bezerra Mota and Janaina Weissheimer},
keywords = {Bilingual writing development, Discourse connectedness, Graph analysis, Narrative writing},
abstract = {The present longitudinal study explored how bilingual educational contexts shape children's cognitive and linguistic development. Its main goal was to investigate the development of discourse connectedness (measured by long-range connectedness - LSC) in written narratives in Portuguese (L1) and English (L2) by 78 children of a bilingual school in Brazil within a year span (from 2021 to 2022). Participants created a narrative in their L1 or L2 based on a sequence of five images, which were analyzed with the computational tool SpeechGraphs (Mota et al., 2014). Connectedness scores were expected to vary as a function of Language (L1, L2) and of Year of data collection (Time 1, Time 2), favoring, respectively, the L1 and Time 2. The results confirmed our hypotheses, with long-range recurrence (LSC) scores in the L1 narratives higher than in the L2 at both times of data collection. In addition, the longitudinal analysis revealed higher connectedness scores for narratives written in Time 2 in both languages. Overall, our findings indicate that the children's performance in terms of connectedness progressed in a parallel way in the two languages during the school years, with an expected advantage for the narratives written in their dominant language. In addition, they highlight the potential of using SpeechGraphs - a cost-effective, non-invasive computational tool - to analyze children's use of two prestige languages in a particular bilingual educational context.}
}
@article{RUTTEN20211,
title = {50 Years of Russian Literature: Mapping, Mixing, and Queering Slavic Literary Studies},
journal = {Russian Literature},
volume = {125-126},
pages = {1-8},
year = {2021},
note = {50 Years of Russian Literature & Teffi’s Theatrical & Cinematic Work},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000673},
author = {Ellen Rutten},
keywords = {Russian Literature, Editorial, Transdisciplinarity, Slavic Literary Studies, Transnational Academic Communication},
abstract = {Russian Literature turned fifty this year. In this editorial contribution, editor-in-chief Ellen Rutten reflects on the journal’s past, its current profile, and future editorial plans. As Rutten argues, Russian Literature has three distinguishing features. First, the journal has always generously invited other disciplines on board – and its transdisciplinary inclusivity has increased in recent years – while maintaining a steady gaze on Slavic literary studies. Second, the journal acts as a transnational and transcontinental scholarly contact zone – a status that cannot be isolated from our choice to publish both Anglophone and Russophone analyses. And third, Russian Literature brings together a range of scholarly voices and genres that is unusually broad for a scholarly periodical, through a strategy of active editorial outreach to young talents and leading experts in the field. Rutten concludes with a few words on upcoming volumes and plans, including new archival publications and volumes-in-the-making inspired by recent shifts in thinking about geopolitics, gender, and health and environment.}
}
@article{KOSCHINSKY2013172,
title = {The case for spatial analysis in evaluation to reduce health inequities},
journal = {Evaluation and Program Planning},
volume = {36},
number = {1},
pages = {172-176},
year = {2013},
note = {Special Section: Rethinking Evaluation of Health Equity Initiatives},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0149718912000237},
author = {Julia Koschinsky},
keywords = {Spatial analysis, Spatial perspective, Program evaluation, Evaluation, Health inequities, Realist evaluation, Randomized control trials (RCTs)},
abstract = {The article begins by giving an overview of spatial thinking concepts that are relevant to evaluation. The article relates the spatial perspective to both a realist evaluation and a randomized control trial perspective in evaluation to demonstrate the benefits of a spatialized program and evaluation perspective. The article mainly suggests that the adoption of a spatial perspective can add new insights to the theory and practice of evaluation in ways that helps evaluation move closer to reducing health inequities.}
}
@article{XIA2023100730,
title = {Understanding common human driving semantics for autonomous vehicles},
journal = {Patterns},
volume = {4},
number = {7},
pages = {100730},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100730},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000703},
author = {Yingji Xia and Maosi Geng and Yong Chen and Sudan Sun and Chenlei Liao and Zheng Zhu and Zhihui Li and Washington Yotto Ochieng and Panagiotis Angeloudis and Mireille Elhajj and Lei Zhang and Zhenyu Zeng and Bing Zhang and Ziyou Gao and Xiqun (Michael) Chen},
keywords = {human-machine interaction, neuroscience, hierarchical understanding abstraction, electroencephalography, neural-informed model, driving behavior perception, driving semantics, autonomous vehicle},
abstract = {Summary
Autonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomous transport systems is complete. The critical challenge of improving mutual understanding between both vehicle types cannot be addressed only by feeding extensive driving data into data-driven models but by enabling autonomous vehicles to understand and apply common driving behaviors analogous to human drivers. Therefore, we designed and conducted two electroencephalography experiments for comparing the cerebral activities of human linguistics and driving understanding. The results showed that driving activates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic understanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understand common human driving behavior in a brain-inspired manner. This study highlights the pathway of fusing neuroscience into complex human behavior understanding tasks and provides a computational neural model to understand human driving behaviors, which will enable autonomous vehicles to perceive and think like human drivers.}
}
@incollection{FROEHLICH2023685,
title = {Mixed methods and social network analysis},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {685-692},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110590},
author = {Dominik E. Froehlich},
keywords = {Data collection, Education research, Ethics, Mixed methods social network analysis, Mixed methods, Relational methods, Research design, Social network analysis, Structure},
abstract = {In this chapter, we discuss the application of mixed methods thinking to social network analysis, a methodological approach that focuses on social relationships and structures. For that purpose, we first define mixed methods and social network analysis and their intersection, which we call Mixed Methods Social Network Analysis (MMSNA). We then summarize the historical developments in social network analysis, which also explain the reason for the increasing application of MMSNA in educational research. The majority of the chapter then focuses on how MMSNA is applied in educational research and what the main topics of the current academic debates are.}
}
@article{THOMPSON2011107,
title = {Intuition, reason, and metacognition},
journal = {Cognitive Psychology},
volume = {63},
number = {3},
pages = {107-140},
year = {2011},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010028511000454},
author = {Valerie A. Thompson and Jamie A. {Prowse Turner} and Gordon Pennycook},
keywords = {Metacognition, Reasoning, Dual Process Theories, Intuition, Analytic thinking, Retrospective confidence},
abstract = {Dual Process Theories (DPT) of reasoning posit that judgments are mediated by both fast, automatic processes and more deliberate, analytic ones. A critical, but unanswered question concerns the issue of monitoring and control: When do reasoners rely on the first, intuitive output and when do they engage more effortful thinking? We hypothesised that initial, intuitive answers are accompanied by a metacognitive experience, called the Feeling of Rightness (FOR), which can signal when additional analysis is needed. In separate experiments, reasoners completed one of four tasks: conditional reasoning (N=60), a three-term variant of conditional reasoning (N=48), problems used to measure base rate neglect (N=128), or a syllogistic reasoning task (N=64). For each task, participants were instructed to provide an initial, intuitive response to the problem along with an assessment of the rightness of that answer (FOR). They were then allowed as much time as needed to reconsider their initial answer and provide a final answer. In each experiment, we observed a robust relationship between the FOR and two measures of analytic thinking: low FOR was associated with longer rethinking times and an increased probability of answer change. In turn, FOR judgments were consistently predicted by the fluency with which the initial answer was produced, providing a link to the wider literature on metamemory. These data support a model in which a metacognitive judgment about a first, initial model determines the extent of analytic engagement.}
}
@article{KIRIMTAY2025111785,
title = {Tau and MAP6 establish labile and stable domains on microtubules},
journal = {iScience},
volume = {28},
number = {3},
pages = {111785},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111785},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225000446},
author = {Koray Kirimtay and Wenqiang Huang and Xiaohuan Sun and Liang Qiang and Dong V. Wang and Calvin T. Sprouse and Erin M. Craig and Peter W. Baas},
keywords = {Cell Biology, Cellular neuroscience},
abstract = {Summary
We previously documented that individual microtubules in the axons of cultured juvenile rodent neurons consist of a labile domain and a stable domain and that experimental depletion of tau results in selective shortening and partial stabilization of the labile domain. After first confirming these findings in adult axons, we sought to understand the mechanism that accounts for the formation and maintenance of these microtubule domains. We found that fluorescent tau and MAP6 ectopically expressed in RFL-6 fibroblasts predominantly segregate on different microtubules or different domains on the same microtubule, with the tau-rich ones becoming more labile than in control cells and the MAP6-rich ones being more stable than in control cells. These and other experimental findings, which we studied further using computational modeling with tunable parameters, indicate that these two MAPs do not merely bind to pre-existing stable and labile domains but actually create stable and labile domains on microtubules.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{NAGLE2019100684,
title = {Using APOS theory as a framework for considering slope understanding},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100684},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318301469},
author = {Courtney Nagle and Rafael Martínez-Planell and Deborah Moore-Russo},
keywords = {APOS, Slope, APOS levels between stages, Precalculus, Rate of change, Totality},
abstract = {In this paper a framework for slope is proposed using APOS (Action-Process-Object-Schema) Theory and conceptualizations of slope previously identified in research. The proposed APOS-slope framework allows for discussion of students’ cognitive development in relation to different conceptualizations of slope. As such, it may be adopted as a means to advance future research or as a way to plan instruction. In particular, the framework uses specific examples to consider interrelations between the ways of thinking about slope that have been reported to provide additional insight on how individuals understand this concept. The proposed framework contributes to the field by bringing together a number of past studies related to slope and providing a common ground under which these works might be interpreted.}
}
@article{SENAPATI202449,
title = {Oxymoron: An Automatic Detection from the Corpus},
journal = {Procedia Computer Science},
volume = {244},
pages = {49-56},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029788},
author = {Apurbalal Senapati},
keywords = {Oxymoron, Antonymy, Corpus, Computational linguistic, Natural Language Processing, Bengali language},
abstract = {An oxymoron is a linguistic phenomenon in which a pair of opposite or antonymous words are combined to convey a new meaning. Sometimes, it is used to express figurative, irony, or rhetoric within the text. This issue has received relatively less attention in the realms of linguistics and computational disciplines. Oxymorons play a significant role in various language-processing applications. This study represents a pioneering effort in the exploration of oxymorons in the Bengali language. A corpus-based study of oxymoron is a fundamental issue that has not been explored so far. A system has been proposed for the automated recognition of oxymorons from a given corpus. Frequency analysis, semantic similarity, and an antonym dictionary have been employed to discern oxymorons within the corpus. The system achieved promising results when tested on a Bengali corpus, and found 308 distinct oxymorons. A corpus-based descriptive statistics is measured in two different corpora. The most common oxymorons are ranked based on their frequency. Their notable presence underscores the importance of the Bengali language. This study aimed to explore fundamental questions concerning oxymorons, such as the automated detection of oxymorons within a corpus, descriptive statistics regarding oxymorons across languages, and the process of their construction and creation. Additionally, efforts were made to extract oxymorons from large language models using zero-shot prompts, but the results were not as promising compared to our proposed system.}
}
@article{CADART2025113107,
title = {An optimal penalty method for the joint stiffening in beam models of additively manufactured lattice structures},
journal = {International Journal of Solids and Structures},
volume = {306},
pages = {113107},
year = {2025},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2024.113107},
url = {https://www.sciencedirect.com/science/article/pii/S0020768324004669},
author = {T. Cadart and T. Hirschler and S. Bahi and S. Roth and F. Demoly and N. Lebaal},
keywords = {Lattice structure, Beam formulation, Penalty method, Joint stiffening, Optimization, Additive manufacturing, Material jetting},
abstract = {Additive manufacturing is revolutionizing structural design, with lattice structures becoming increasingly prominent due to their superior mechanical properties. However, simulating these structures quickly and accurately using the finite element method (FEM) remains challenging. Recent research has highlighted beam element simulation within FEM as a more efficient alternative to traditional solid FE simulations, achieving similar accuracy with reduced computational resources. However, a significant challenge is managing the lack of rigidity at nodes and the prevalence of low aspect ratio beams. While various methodologies have been proposed to address these issues, there is still a gap in the comprehensive evaluation of their limitations. An optimal node penalization methodology is required to expand the limited range of accurately represented lattice behavior. A preliminary study investigates lattice geometries through comparative analysis of solid and beam FE simulations. Built on this, we developed a methodology suitable to linear, dynamics and nonlinear beam FE simulations, contributing to enhanced computational speed and accuracy. Several lattice structures were printed using material jetting and quasi-static compressive tests were conducted to validate the methodology’s accuracy. The numerical results reveal a good accuracy between the proposed beam FE methodology and the experimental data, offering a better alternative to conventional FEM for energy absorption in terms of computing time.}
}
@article{LU2022100056,
title = {Nonlinear EEG signatures of mind wandering during breath focus meditation},
journal = {Current Research in Neurobiology},
volume = {3},
pages = {100056},
year = {2022},
issn = {2665-945X},
doi = {https://doi.org/10.1016/j.crneur.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2665945X22000298},
author = {Yiqing Lu and Julio Rodriguez-Larios},
keywords = {EEG, Mind wandering, Meditation, Complexity, Nonlinear analysis},
abstract = {In meditation practices that involve focused attention to a specific object, novice practitioners often experience moments of distraction (i.e., mind wandering). Previous studies have investigated the neural correlates of mind wandering during meditation practice through Electroencephalography (EEG) using linear metrics (e.g., oscillatory power). However, their results are not fully consistent. Since the brain is known to be a chaotic/nonlinear system, it is possible that linear metrics cannot fully capture complex dynamics present in the EEG signal. In this study, we assess whether nonlinear EEG signatures can be used to characterize mind wandering during breath focus meditation in novice practitioners. For that purpose, we adopted an experience sampling paradigm in which 25 participants were iteratively interrupted during meditation practice to report whether they were focusing on the breath or thinking about something else. We compared the complexity of EEG signals during mind wandering and breath focus states using three different algorithms: Higuchi's fractal dimension (HFD), Lempel-Ziv complexity (LZC), and Sample entropy (SampEn). Our results showed that EEG complexity was generally reduced during mind wandering relative to breath focus states. We conclude that EEG complexity metrics are appropriate to disentangle mind wandering from breath focus states in novice meditation practitioners, and therefore, they could be used in future EEG neurofeedback protocols to facilitate meditation practice.}
}
@article{KORN2023102578,
title = {Navigating large chemical spaces in early-phase drug discovery},
journal = {Current Opinion in Structural Biology},
volume = {80},
pages = {102578},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X23000520},
author = {Malte Korn and Christiane Ehrt and Fiorella Ruggiu and Marcus Gastreich and Matthias Rarey},
abstract = {The size of actionable chemical spaces is surging, owing to a variety of novel techniques, both computational and experimental. As a consequence, novel molecular matter is now at our fingertips that cannot and should not be neglected in early-phase drug discovery. Huge, combinatorial, make-on-demand chemical spaces with high probability of synthetic success rise exponentially in content, generative machine learning models go hand in hand with synthesis prediction, and DNA-encoded libraries offer new ways of hit structure discovery. These technologies enable to search for new chemical matter in a much broader and deeper manner with less effort and fewer financial resources. These transformational developments require new cheminformatics approaches to make huge chemical spaces searchable and analyzable with low resources, and with as little energy consumption as possible. Substantial progress has been made in the past years with respect to computation as well as organic synthesis. First examples of bioactive compounds resulting from the successful use of these novel technologies demonstrate their power to contribute to tomorrow's drug discovery programs. This article gives a compact overview of the state-of-the-art.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{NISSEL2024105856,
title = {Why wearing a yellow hat is impossible: Chinese and U.S. children's possibility judgments},
journal = {Cognition},
volume = {251},
pages = {105856},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105856},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001422},
