author = {Pauline Olivier and Tara Butler and Pascal Guehl and Jean-Luc Coll and Renaud Chabrier and Pooran Memari and Marie-Paule Cani},
keywords = {Sketch-based modeling, Interactive geometric modeling, Sketch-based animation, Narration},
abstract = {Having simple methods of illustration is essential to scientific thinking. To complement the abstract sketches regularly used in cell biology, we propose DynBioSketch, an easy-to-use digital modeling and animation tool, enabling biologists to resort to less simplified representations when necessary without having to call professional artists. DynBioSketch is an interactive sketching system dedicated to the design and communication of biological phenomena at the cellular scale that can be illustrated in a few minutes of animation. Our model integrates 3D modeling, pattern-based design of 3D shape distributions, and sketch-based animation. These elements can be combined to create complex scenarios such as the infection phenomenon on which we focus, allowing a narrative design adapted to communication between researchers or educational applications in biology. Our results, along with a user study conducted with biology researchers, highlight the potential of DynBioSketch in enabling the direct design of dynamic visual summaries that convey relevant information, as shown in our infection case study. By bridging the gap between abstract representations used by experts and more illustrative depictions, DynBioSketch opens a new avenue for communicating biological concepts.}
}
@article{CHONG2016257,
title = {A generalized cognitive hierarchy model of games},
journal = {Games and Economic Behavior},
volume = {99},
pages = {257-274},
year = {2016},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0899825616300847},
author = {Juin-Kuan Chong and Teck-Hua Ho and Colin Camerer},
keywords = {Cognitive hierarchy, Level- model, Level- model, Generalized cognitive hierarchy, Non-equilibrium structural models, Behavioral game theory},
abstract = {Subjects in simple games frequently exhibit non-equilibrium behaviors. Cognitive hierarchy (CH) and level k (LK) are two prevailing structural models that capture such behaviors well. This paper proposes a generalized CH (GCH) model that nests a variant of the LK model, called LM. GCH differs from CH in two ways. First, each lower level's actual frequency is exponentially weighted with α to form level-k's belief on relative proportions; α captures stereotype bias. CH assumes no stereotype bias (α=1) and LM assumes extreme bias (α=∞). Second, GCH replaces random choice with minimum aversion for level 0. Level 0s are more likely to choose strategies that never yield the minimum payoff for any of the opponent's strategies. GCH captures behaviors better than CH and LK in fifty-five n×m games from four datasets. Robustness tests using three new games further validate GCHs descriptive strength over CH and LK.}
}
@article{KAZEMI2002203,
title = {Exploring test performance in mathematics: the questions children’s answers raise},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {203-224},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00118-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001189},
author = {Elham Kazemi},
keywords = {Children’s thinking, Mathematical performance, Interpreting problems, Testing},
abstract = {This article investigates children’s mathematical performance on test items, specifically multiple-choice questions. Using interviews with 90 fourth-graders, it reveals why particular kinds of items are more or less difficult for students. By using multiple-choice questions and juxtaposing them with similar open-ended problems, the findings underscore the costs of not attending to children’s thinking in designing and interpreting problems. The data from this study suggest that when answering multiple-choice questions, students’ attention is drawn to the choices themselves. They do not necessarily think through the problem first and thus make their choices based on (often incorrect) generalizations they have made about problem-solving. Whether students answered a multiple-choice question or a similar open-ended problem first impacted both their performance and their reasoning. Moreover, children draw on their life experiences when the context of the problem is salient, thus ignoring important parameters of the stated problem. Implications for investigating children’s thinking, instruction, and test design are discussed.}
}
@article{LENG20242963,
title = {An Improved YOLOv8-Based Method for Real-Time Detection of Harmful Tea Leaves in Complex Backgrounds},
journal = {Phyton-International Journal of Experimental Botany},
volume = {93},
number = {11},
pages = {2963-2981},
year = {2024},
issn = {0031-9457},
doi = {https://doi.org/10.32604/phyton.2024.057166},
url = {https://www.sciencedirect.com/science/article/pii/S0031945724001643},
author = {Xin Leng and Jiakai Chen and Jianping Huang and Lei Zhang and Zongxuan Li},
keywords = {Harmful tea leaves, YOLO-DBD, Focal-CIoU Loss, dynamic head, Bi-Level Routing Attention},
abstract = {Tea, a globally cultivated crop renowned for its unique flavor profile and health-promoting properties, ranks among the most favored functional beverages worldwide. However, diseases severely jeopardize the production and quality of tea leaves, leading to significant economic losses. While early and accurate identification coupled with the removal of infected leaves can mitigate widespread infection, manual leaves removal remains time-consuming and expensive. Utilizing robots for pruning can significantly enhance efficiency and reduce costs. However, the accuracy of object detection directly impacts the overall efficiency of pruning robots. In complex tea plantation environments, complex image backgrounds, the overlapping and occlusion of leaves, as well as small and densely harmful leaves can all introduce interference factors. Existing algorithms perform poorly in detecting small and densely packed targets. To address these challenges, this paper collected a dataset of 1108 images of harmful tea leaves and proposed the YOLO-DBD model. The model excels in efficiently identifying harmful tea leaves with various poses in complex backgrounds, providing crucial guidance for the posture and obstacle avoidance of a robotic arm during the pruning process. The improvements proposed in this study encompass the Cross Stage Partial with Deformable Convolutional Networks v2 (C2f-DCN) module, Bi-Level Routing Attention (BRA), Dynamic Head (DyHead), and Focal Complete Intersection over Union (Focal-CIoU) Loss function, enhancing the model’s feature extraction, computation allocation, and perception capabilities. Compared to the baseline model YOLOv8s, mean Average Precision at IoU 0.5 (mAP0.5) increased by 6%, and Floating Point Operations Per second (FLOPs) decreased by 3.3 G.}
}
@article{WANG2023120782,
title = {A closed-loop analysis approach for ensuring stormwater source control design solution to achieve the intended goals},
journal = {Water Research},
volume = {247},
pages = {120782},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120782},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423012228},
author = {Sheng Wang and Lidan Feng and Yezi Yuan},
keywords = {Sponge city, Stormwater source control, Closed-loop analysis, Bioretention, Runoff frequency spectrum},
abstract = {Stormwater source controls have been adopted worldwide to address hydrological and environmental impairments caused by the spread of impervious surfaces in cities. Current design method in China uses 30-year daily rainfall records to generate relationship of rainfall volume capture ratio (αg) and daily design storm, and then uses design storm to propose design solution. However, source control performance differs from rain to rain, and hence the design solution's actual effect may deviate from αg. Borrowing closed-loop feedback concept from business domain, this study proposes closed-loop analysis (CLA) which uses design solution's 30-year simulated result as data feedback to check design solution's effectiveness and then make improvements if necessary. It consists of four methods: 1) hourly design storm statistical method, for addressing the weakness of current daily design storm; 2) design solution model credibility examination method, for guaranteeing credibility of 30-year simulated results for CLA; 3) appropriate design storms determination method for source control without underdrain; 4) additional design parameters optimization method for source control with underdrain. Taking Xiamen city for example, case study results shows that design solution's 30-year simulated results were consistent/comparable with sizing calculation formula that was used to propose design solution, and therefore they were credible for CLA. Appropriate design storms ensured design solutions without underdrain to achieve the intended αg±3 %. Optimal design parameters combinations ensured design solutions with underdrain to achieve αg but also restore natural runoff events with pre- and post-development runoff frequency spectra similarity being 0.670–0.691. Based on stormwater mathematical model, CLA can drive source control design computation to a new methodological stage.}
}
@incollection{OXMAN2001269,
title = {Chapter 12 - The Mind in Design: A Conceptual Framework for Cognition in Design Education},
editor = {Charles M. Eastman and W. Michael McCracken and Wendy C. Newstetter},
booktitle = {Design Knowing and Learning: Cognition in Design Education},
publisher = {Elsevier Science},
address = {Oxford},
pages = {269-295},
year = {2001},
isbn = {978-0-08-043868-9},
doi = {https://doi.org/10.1016/B978-008043868-9/50012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080438689500127},
author = {Rivka Oxman},
abstract = {Publisher Summary
This chapter considers the role of cognitive content of design and design thinking as a basis for developing an educational approach. Various design researchers discussed cognitive approaches in design, and the role of knowledge and representations as a cognitive design-thinking tool. Most of these studies are related directly to design and design thinking rather than to the learning task in design learning and design education. Irrespective of the specific design domain, traditional educational models in design education are based upon the replication of professional-task performance. The measure of learning is generally equated with the evaluation of the product of designing rather than on what might be considered a learning increment. The cognitive properties of design learning have never been the subject of design education. As a consequence, there presently exists a lack of educational theories of learning that function as an underpinning of design education. It is now possible to demonstrate that the derivation of design knowledge through constructive processes, in itself, provides a medium for design learning. This chapter suggests that special design learning environments must be developed to enhance and supplement formal education and foster personal development in design learning.}
}
@article{DEAN2020482,
title = {Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death},
journal = {Journal of Affective Disorders},
volume = {266},
pages = {482-491},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0165032719322554},
author = {Hannah J. Dean and Ryan L. Boyd},
keywords = {Edgar Allan Poe, LIWC, Depression, Suicide, Digital humanities},
abstract = {Background
To help shed light on the peculiar circumstances surrounding the death of the famed macabre and mystery writer, poet, editor, and literary critic, we explored the potential role of depression in the life and death of Edgar Allan Poe via his written language.
Method
Using computerized language analysis, we analyzed works from Poe's corpora of personal letters (N = 309), poems (N = 49), and short stories (N = 63), and investigated whether a pattern of linguistic cues consistent with depression and suicidal cognition were discernible throughout the writer's life, particularly in his final years. Building on past work, language scores were collapsed into a composite depression metric for each text. Data from each work type was subsequently compiled and graphed into a single plot by year, with scores exceeding the 95th percentile (p < 0.05) considered statistically significant and treated as potential depressive episodes.
Results
Significant, consistent patterns of depression were not found and do not support suicide as a cause of death. However, linguistic evidence was found suggesting the presence of several potential depressive episodes over the course of Poe's life – these episodes were the most pronounced during years of Poe's greatest success, as well as those following the death of his late wife.
Limitations
Given the sampling method, it is not possible to establish direct causality; results should be considered informed but tentative.
Conclusion
This investigation demonstrates the utility of language analysis for capturing disruptive/maladaptive emotional responses to life events.}
}

@article{SHYJA2023100465,
title = {Link quality and energy efficient optimal simplified cluster based routing scheme to enhance lifetime for wireless body area networks},
journal = {Nano Communication Networks},
volume = {37},
pages = {100465},
year = {2023},
issn = {1878-7789},
doi = {https://doi.org/10.1016/j.nancom.2023.100465},
url = {https://www.sciencedirect.com/science/article/pii/S1878778923000315},
author = {V. Irine Shyja and G. Ranganathan and V. Bindhu},
keywords = {WBAN, Clustering, Cluster head, Multipath routing scheme, Link quality},
abstract = {Monitoring of patient’s health in the medical industry can be enabled using wireless body area networks (WBANs), which are already used for various purposes, including assisting in human safety. It is imperative to use better power management strategies since the body sensors are small and the battery cannot hold a charge for a long time. Due to the vast amounts of information generated by medical sensors, resource-constrained networks face a significant challenge when guaranteeing the specified quality of service (QoS). Moreover, the WBAN regularly meets the primary hassle of QoS degradation because of congestion WBAN structure can easily compromise heterogeneous and complex networks. Either inappropriate data collection or using energy effectively to transmit medical data without the expense of travel and length has become an important one. To address this issue, the present research work ‘Link Quality and Energy Efficient Optimal Clustering-Multipath (LEOC-MP)’ scheme tries to explore an answer. The main goals of the LEOC-MP (Optimal Link Quality and Energy Efficient Optimal Clustering-Multipath) system are to guarantee node-to-node link quality, lengthen network life, and compute high-performing cluster heads to guarantee reliable multi path data transfer. This work was executed in three phases. First, an optimal simplified clustering technique for data collection from body sensors using an improved pelican optimization (ICO) algorithm is introduced. Next, multiple design constraints for node rank computation, energy efficiency, link quality, path loss, distance, and delay are used. Besides, an Auto-Regressive Probabilistic Neural Network (AR-PNN) is introduced to optimize those design constraints and compute the cluster head (CH) of each cluster. Multipath firing is then performed using a moderated puffer-fish optimization (MPO) algorithm that finds the closest optimal and shortest node to transmit optimal drug data. The work is simulated using an NS-3 environment, and the results are obtained. The outcome of this work is analyzed with existing methodologies, and the results prove that the present work consistently outperforms the existing methodologies.}
}
@article{SCHAEFER2024108796,
title = {GPT-4 as a biomedical simulator},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108796},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008813},
author = {Moritz Schaefer and Stephan Reichl and Rob {ter Horst} and Adele M. Nicolas and Thomas Krausgruber and Francesco Piras and Peter Stepper and Christoph Bock and Matthias Samwald},
keywords = {Biomedical simulation, Large language models, GPT-4, Computational biology, Artificial intelligence},
abstract = {Background
Computational simulation of biological processes can be a valuable tool for accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks. This study provides proof-of-concept for the use of GPT-4 as a versatile simulator of biological systems.
Methods
We introduce SimulateGPT, a proof-of-concept for knowledge-driven simulation across levels of biological organization through structured prompting of GPT-4. We benchmarked our approach against direct GPT-4 inference in blinded qualitative evaluations by domain experts in four scenarios and in two quantitative scenarios with experimental ground truth. The qualitative scenarios included mouse experiments with known outcomes and treatment decision support in sepsis. The quantitative scenarios included prediction of gene essentiality in cancer cells and progression-free survival in cancer patients.
Results
In qualitative experiments, biomedical scientists rated SimulateGPT's predictions favorably over direct GPT-4 inference. In quantitative experiments, SimulateGPT substantially improved classification accuracy for predicting the essentiality of individual genes and increased correlation coefficients and precision in the regression task of predicting progression-free survival.
Conclusion
This proof-of-concept study suggests that LLMs may enable a new class of biomedical simulators. Such text-based simulations appear well suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulations, but for which extensive knowledge is available as written text. Finally, we propose several directions for further development of LLM-based biomedical simulators, including augmentation through web search retrieval, integrated mathematical modeling, and fine-tuning on experimental data.}
}
@article{BURRIS1992175,
title = {Discriminator varieties and symbolic computation},
journal = {Journal of Symbolic Computation},
volume = {13},
number = {2},
pages = {175-207},
year = {1992},
issn = {0747-7171},
doi = {https://doi.org/10.1016/S0747-7171(08)80089-2},
url = {https://www.sciencedirect.com/science/article/pii/S0747717108800892},
author = {Stanley Burris},
abstract = {We look at two aspects of discriminator varieties which could be of considerable interest in symbolic computation:1.discriminator varieties are unitary (i.e., there is always a most general unifier of two unifiable terms), and2.every mathematical problem can be routinely cast in the form†p1 ≈ q1, …, pk ≈ qk implies the equation x ≈ y. Item (l) offers possibilities for implementations in computational logic, and (2) shows that Birkhoff's five rules of inference for equational logic are all one needs to prove theorems in mathematics.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@incollection{LUND202435,
title = {3 - Choice Awareness strategies},
editor = {Henrik Lund},
booktitle = {Renewable Energy Systems (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {35-50},
year = {2024},
isbn = {978-0-443-14137-9},
doi = {https://doi.org/10.1016/B978-0-443-14137-9.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141379000036},
author = {Henrik Lund},
keywords = {Alternatives assessment, Choice Awareness, Feasibility studies, Institutional barriers, Market barriers, Path dependency, Public regulation, Radical technological change, Renewable energy systems, Technical alternatives},
abstract = {This chapter introduces strategies to raise the awareness of how to implement smart energy systems in fully decarbonized societies using the Choice Awareness theory. Choice Awareness is based on the understanding that existing institutional perceptions and organizational interests will often seek to eliminate certain choices from the political decision-making process, when the introduction of radical technological change is discussed. The counterstrategy is to raise public awareness that alternatives do exist and that it is possible to make a choice. Counterstrategies may involve the design of technical alternatives, feasibility studies based on institutional economic thinking, and the design of public regulation measures seen in the light of conflicting interests as well as changes in the democratic decision-making infrastructure. Each of the strategies is elaborated on in this chapter.}
}
@article{ZHANG2010S238,
title = {Biomimetic Construction of Category Mental Imagery Based on Recognition Mechanism of Visual Cortex of Human Brain},
journal = {Journal of Bionic Engineering},
volume = {7},
pages = {S238-S244},
year = {2010},
issn = {1672-6529},
doi = {https://doi.org/10.1016/S1672-6529(09)60241-9},
url = {https://www.sciencedirect.com/science/article/pii/S1672652909602419},
author = {Xianghe Zhang and Dan Wang and Luquan Ren and Pingping Liu},
keywords = {multi-dimensional space biomimetic informatics, artificial intelligence, cognitive science, mental imagery, visual cortex, object recognition},
abstract = {The principle of homology-continuity in Multi-Dimensional Biomimetic Informatics Space is applied to construct the identifying mechanism of category of deep representation of mental imagery. The model of each cerebral region involved in recognizing is established respectively and a feedforward method for establishing category mental imagery is proposed. First, the model of feature acquisition is developed based on Hubel-Wiesel model, and Gaussian function is used to simulate the simple cell receptive field to satisfy the specific function of visual cortex. Second, multiple input aggregation operation is employed to simulate the feature output of complex cells to get the invariance representation in feature space. Then, imagery basis is extracted by unsupervised learning algorithm based on the primary feature and category mental imagery is obtained by building Radial Basis Function (RBF) network. Finally, the system model is tested by training set and test set composed of real images. Experimental results show that the proposed method can establish valid deep representation of these samples, based on which the biomimetic construction of category mental imagery can be achieved. This method provides a new idea for solving imagery problem and studying imagery thinking.}
}
@article{GILBERT2018278,
title = {Decoding intentions of self and others from fMRI activity patterns},
journal = {NeuroImage},
volume = {172},
pages = {278-290},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.12.090},
url = {https://www.sciencedirect.com/science/article/pii/S105381191731114X},
author = {Sam J. Gilbert and Hoki Fung},
keywords = {Intentions, MVPA, fMRI},
abstract = {Previous studies using multi-voxel pattern analysis have decoded the content of participants' delayed intentions from patterns of fMRI data. Here we investigate whether this technique can be used to decode not only participants' own intentions, but also their representation of the intentions held by other people. In other words: if Sam is thinking about Hoki, can we decode the content of Hoki's intention by scanning Sam's brain? We additionally distinguished two components of intentions: action-plans versus goals, and included novel control analyses that allowed us to distinguish intending an outcome from simply expecting it to occur or simulating its consequences. Regions of frontal, parietal, and occipital cortex contained patterns from which it was possible to decode intentions of both self and other. Furthermore, crossclasification between self and other was possible, suggesting overlap between the two. Control analyses suggested that these results reflected visuo-spatial processes by which intentions were generated in our paradigm, rather than anything special about intentions per se. There was no evidence for any representation of intentions as mental states distinct from visuospatial processes involved in generating their content and/or simulating their outcomes. These findings suggest that the brain activity patterns decoded in intention-decoding fMRI studies may reflect domain-general processes rather than being intention-specific.}
}
@article{LAW2023112555,
title = {Frontopolar cortex represents complex features and decision value during choice between environments},
journal = {Cell Reports},
volume = {42},
number = {6},
pages = {112555},
year = {2023},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2023.112555},
url = {https://www.sciencedirect.com/science/article/pii/S2211124723005661},
author = {Chun-Kit Law and Nils Kolling and Chetwyn C.H. Chan and Bolton K.H. Chau},
keywords = {frontopolar cortex, ventromedial prefrontal cortex, decision making, environment choice, convolutional neural network, CNN},
abstract = {Summary
Important decisions often involve choosing between complex environments that define future item encounters. Despite its importance for adaptive behavior and distinct computational challenges, decision-making research primarily focuses on item choice, ignoring environment choice altogether. Here we contrast previously studied item choice in ventromedial prefrontal cortex with lateral frontopolar cortex (FPl) linked to environment choice. Furthermore, we propose a mechanism for how FPl decomposes and represents complex environments during decision making. Specifically, we trained a choice-optimized, brain-naive convolutional neural network (CNN) and compared predicted CNN activation with actual FPl activity. We showed that the high-dimensional FPl activity decomposes environment features to represent the complexity of an environment to make such choice possible. Moreover, FPl functionally connects with posterior cingulate cortex for guiding environment choice. Further probing FPl’s computation revealed a parallel processing mechanism in extracting multiple environment features.}
}
@article{NAWAZ2024102806,
title = {Grappling with a sea change: Tensions in expert imaginaries of marine carbon dioxide removal},
journal = {Global Environmental Change},
volume = {85},
pages = {102806},
year = {2024},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2024.102806},
url = {https://www.sciencedirect.com/science/article/pii/S0959378024000104},
author = {Sara Nawaz and Javier Lezaun},
abstract = {While research on marine carbon dioxide removal (mCDR) expands apace, significant unknowns persist regarding the risks and benefits of individual mCDR options. This paper analyses the assumptions and expectations that animate expert understandings of mCDR, with a focus on issues that are central to the responsible governance of this emerging field of climate action. Drawing upon interviews with experts involved in mCDR research projects both academic and entrepreneurial, we highlight four thematic tensions that orient their thinking but are often unstated or left implicit in scientific and technical assessments: (1) the relevance of ‘naturalness’ as a criterion of evaluation for mCDR approaches; (2) the perceived need to accelerate research and development activities via alternative paradigms of evidence-building; (3) a framing of mCDR as a form of waste management that will, in turn, generate new (and currently poorly understood) forms of environmental pollutants; and (4) a commitment to inclusive governance mixed with difficulty in identifying specific stakeholders or constituencies in mCDR interventions. Although expert consensus on these four issues is unlikely, we suggest ways of ensuring that consideration of these themes enriches debate on the responsible development of novel mCDR capabilities.}
}
@article{SCHUMNY1993319,
title = {Nanosystems - molecular machinery, manufacturing, and computation: by K. Eric Drexler. John Wiley & Sons, Chichester, England, 1992. ISBN 0-471-57518-6. 556 pages. Illustrated, Appendices, Glossary with detailed explanations, 337 references, extended Index.},
journal = {Computer Standards & Interfaces},
volume = {15},
number = {2},
pages = {319-320},
year = {1993},
note = {Object-Oriented Reference Models},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(93)90019-N},
url = {https://www.sciencedirect.com/science/article/pii/092054899390019N},
author = {Harald Schumny}
}
@article{MEY1988743,
title = {Computation and the soul},
journal = {Journal of Pragmatics},
volume = {12},
number = {5},
pages = {743-789},
year = {1988},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(88)90056-2},
url = {https://www.sciencedirect.com/science/article/pii/0378216688900562},
author = {Jacob L. Mey and Mary Talbot},
abstract = {This article is both a review of Sperber and Wilson's Relevance and a broader critical discussion of linguistic pragmatics, from which Relevance has arisen. Four separate sections focus on Communication, Assumptions, the Metaphor of the Black Box and the Principle of Relevance itself. The authors conclude that the reductionist model of the human mind as an information-processing device, developed by Sperber and Wilson, is irredeemably asocial and therefore relevant to neither communication nor cognition.}
}
@article{VIDENOVIK2024100616,
title = {Game-based learning approach in computer science in primary education: A systematic review},
journal = {Entertainment Computing},
volume = {48},
pages = {100616},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100616},
url = {https://www.sciencedirect.com/science/article/pii/S187595212300071X},
author = {Maja Videnovik and Ana {Madevska Bogdanova} and Vladimir Trajkovik},
keywords = {Educational game, Game-based learning, Computer science, Primary education},
abstract = {This paper reviews the current situation concerning the implementation of game-based learning in computer science in primary education, providing insight into current trends, identifying strengths and potential research topics. Articles published in four databases from 2017 to 2021 are included in the analysis and an in-depth analysis of 32 articles is done. Different types of games, implemented in various educational contexts, are presented in these articles. Most of them describe implemented methodology, game-based environment or are evaluating the effectiveness of the created game or the approach. The possibility of implementing a game-based approach while learning other computer science topics or measuring the effectiveness of learning by designing a game as a pedagogical strategy are some areas for future research.}
}
@article{GAULD2024116255,
title = {Exploring the interplay of clinical reasoning and artificial intelligence in psychiatry: Current insights and future directions},
journal = {Psychiatry Research},
volume = {342},
pages = {116255},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116255},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124005407},
author = {Christophe Gauld and Vincent P. Martin and Hugo Bottemanne and Pierre Fourneret and Jean-Arthur Micoulaud-Franchi and Guillaume Dumas},
keywords = {Artificial intelligence, Statistical prediction, Psychiatry, Computational sciences, Explainability, Deep learning},
abstract = {For many years, it has been widely accepted in the psychiatric field that clinical practice cannot be reduced to finely tuned statistical prediction systems utilizing diverse clinical data. Clinicians are recognized for their unique and irreplaceable roles. In this brief historical overview, viewed through the lens of artificial intelligence (AI), we propose that comprehending the reasoning behind AI can enhance our understanding of clinical reasoning. Our objective is to systematically identify the factors that shape clinical reasoning in medicine, based on six factors that were historically considered beyond the reach of statistical methods: open-endedness, unanalyzed stimulus-equivalences, empty cells, theory mediation, insufficient time, and highly configured functions. Nevertheless, a pertinent consideration in the age of AI is whether these once-considered insurmountable specific factors of clinicians are now subject to scrutiny or not. Through example in AI, we demonstrate that a deeper understanding of these factors not only sheds light on clinical decision-making and its heuristic processes but also underscores the significance of collaboration between AI experts and healthcare professionals. This comparison between AI and clinical reasoning contributes to a better grasp of the current challenges AI faces in the realm of clinical medicine.}
}
@article{KEMPF2023103747,
title = {Point pattern and spatial analyses using archaeological and environmental data – A case study from the Neolithic Carpathian Basin},
journal = {Journal of Archaeological Science: Reports},
volume = {47},
pages = {103747},
year = {2023},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2022.103747},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X22004102},
author = {Michael Kempf and Gerrit Günther},
keywords = {Environmental archaeology, Quantitative archaeology, Computational methods, Spatial analysis, R, Point pattern analysis (PPA)},
abstract = {Computational methods recently gained momentum in archaeological science, particularly affecting large site distribution samples and environmental explanatory parameters. However, quantitative and environmental archaeology are still considered to be limited to a small number of experts and thus less ready to use in general research. Here, we present a case study that integrates computational methods and environmental data into archaeological spatial analyses using Point Pattern Analysis (PPA). We introduce a basic approach to model, visualise, and interpret archaeological site distributions as functions of explanatory covariates in a regional setting of the Neolithic period in the Carpathian Basin. The integration of environmental and socio-cultural variables in a multicomponent analysis allows to distinguish site location parameters and preferences across different chronological periods. Using the code to this article and open-access spatial data, the workflow can be adapted to different regional contexts and chronological periods, making it particularly suitable for spatial pattern comparison.}
}
@article{ERDMANN202242,
title = {A generative framework for the study of delusions},
journal = {Schizophrenia Research},
volume = {245},
pages = {42-49},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420306277},
author = {Tore Erdmann and Christoph Mathys},
keywords = {Delusion, Dirichlet process, Hierarchical predictive coding, Auxiliary hypothesis, Epistemic trust},
abstract = {Despite the ubiquity of delusional information processing in psychopathology and everyday life, formal characterizations of such inferences are lacking. In this article, we propose a generative framework that entails a computational mechanism which, when implemented in a virtual agent and given new information, generates belief updates (i.e., inferences about the hidden causes of the information) that resemble those seen in individuals with delusions. We introduce a particular form of Dirichlet process mixture model with a sampling-based Bayesian inference algorithm. This procedure, depending on the setting of a single parameter, preferentially generates highly precise (i.e. over-fitting) explanations, which are compartmentalized and thus can co-exist despite being inconsistent with each other. Especially in ambiguous situations, this can provide the seed for delusional ideation. Further, we show by simulation how the excessive generation of such over-precise explanations leads to new information being integrated in a way that does not lead to a revision of established beliefs. In all configurations, whether delusional or not, the inference generated by our algorithm corresponds to Bayesian inference. Furthermore, the algorithm is fully compatible with hierarchical predictive coding. By virtue of these properties, the proposed model provides a basis for the empirical study and a step toward the characterization of the aberrant inferential processes underlying delusions.}
}
@article{SAMSONOVICH2022824,
title = {Key Advanced Research Initiative: A Manifesto for the New-Generation Artificial Intelligence},
journal = {Procedia Computer Science},
volume = {213},
pages = {824-831},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.140},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922018397},
author = {Alexei V. Samsonovich and Sergey A. Shumsky and Valery E. Karpov and Artemy A. Kotov and Anton G. Kolonin},
keywords = {AGI, humanlike AI, strong AI, virtual evolution, machine learning, evolutionary computation, artificial creativity, anthropocentric AI},
abstract = {The goal here is to identify key directions for the future advanced research initiatives in Artificial Intelligence (AI) and beyond. The following areas are identified as having particular importance: (1) socially emotional, ethical, and moral AI, (2) self-developing and self-sustainable AI, and (3) human-analogous AI, inspired by the human psychology. As a result, a general concept is formulated with the intent to clarify and unify the currently popular slogans, including Artificial General Intelligence (AGI), Strong AI, Human-Level or Humanlike AI (HLAI), Brain-Inspired or Biologically Inspired Cognitive Architectures (BICA), and more. The key idea of the proposed concept is that future AI must open a new angle of view and new perspectives to humans, thereby enriching and transforming the society, helping it to solve its problems and taking the civilization to a new level. While being created by humans, for humans, and fully compatible with humans at the social level, it will not be “a human in silicon”, but rather an “alien”: intelligent, friendly, and welcome. Its principles will combine preprogrammed basic functions and its own natural ontogeny in a virtual social environment. Forms of implementation will range from virtual entities to wearable electronics and autonomous robots. The expected impact on the society will be immense and crucial for its survival.}
}
@article{WORTMANN2017173,
title = {Differentiating parametric design: Digital workflows in contemporary architecture and construction},
journal = {Design Studies},
volume = {52},
pages = {173-197},
year = {2017},
note = {Parametric Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300352},
author = {Thomas Wortmann and Bige Tunçer},
keywords = {parametric design, design automation, architectural design, software design, parametric master model},
abstract = {This paper examines Parametric Design (PD) in contemporary architectural practice. It considers three case studies: The Future of Us pavilion, the Louvre Abu Dhabi and the Morpheus Hotel. The case studies illustrate how, compared to non-parametrically and older parametrically designed projects, PD is employed to generate, document and fabricate designs with a greater level of detail and differentiation, often at the level of individual building components. We argue that such differentiation cannot be achieved with conventional Building Information Modelling and without customizing existing software. We compare the case studies' PD approaches (objected-oriented programming, functional programming, visual programming and distributed visual programming) and decomposition, algorithms and data structures as crucial factors for the practical viability of complex parametric models and as key aspects of PD thinking.}
}
@article{CHRISTENSEN2016125,
title = {Towards a formal assessment of design literacy: Analyzing K-12 students' stance towards inquiry},
journal = {Design Studies},
volume = {46},
pages = {125-151},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1530140X},
author = {Kasper Skov Christensen and Mikkel Hjorth and Ole Sejer Iversen and Paulo Blikstein},
keywords = {design education, design research, reflective practices, evaluation},
abstract = {We present a tool for quantitative assessment of K-12 students' stance towards inquiry as an important part of students' development of design literacy. On a basis of design thinking literature, we position designerly stance towards inquiry as a prerequisite for engaging with wicked problems. The Design Literacy (DeL) assessment tool contains design of a qualitative survey question, a coding scheme for assessing aspects of a designerly stance towards inquiry, and a description of how, we have validated the results through a large-scale survey administration in K-12 education. Our DeL tool is meant to provide educators, leaders, and policy makers with strong arguments for introducing design literacy in K-12 schools, which, we posit, function within in an age of measurement.}
}
@article{TABACHNECKSCHIJF1997305,
title = {CaMeRa: A computational model of multiple representations},
journal = {Cognitive Science},
volume = {21},
number = {3},
pages = {305-350},
year = {1997},
note = {Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(99)80026-3},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800263},
author = {Hermina J.M. Tabachneck-Schijf and Anthony M. Leonardo and Herbert A. Simon},
abstract = {This research aims to clarify, by constructing and testing a computer simulation, the use of multiple representations in problem solving, focusing on their role in visual reasoning. The model is motivated by extensive experimental evidence in the literature for the features it incorporates, but this article focuses on the system's structure. We illustrate the model's behavior by simulating the cognitive and perceptual processes of an economics expert as he teaches some well-learned economics principles while drawing a graph on a blackboard. Data in the experimental literature and concurrent verbal protocols were used to guide construction of a linked production system and parallel network, CaMeRa (Computation with Multiple Representations), that employs a “Mind's Eye” representation for pictorial information, consisting of a bitmap and associated node-link structures. Propositional list structures are used to represent verbal information and reasoning. Small individual pieces from the different representations are linked on a sequential and temporary basis to form a reasoning and inferencing chain, using visually encoded information recalled to the Mind's Eye from long-term memory and from cues recognized on an external display. CaMeRa, like the expert, uses the diagrammatic and verbal representations to complement one another, thus exploiting the unique advantages of each.}
}
@incollection{KIRWAN202047,
title = {Chapter 3 - Strategies, planning, and design},
editor = {Christopher Kirwan and Fu Zhiyong},
booktitle = {Smart Cities and Artificial Intelligence},
publisher = {Elsevier},
pages = {47-67},
year = {2020},
isbn = {978-0-12-817024-3},
doi = {https://doi.org/10.1016/B978-0-12-817024-3.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170243000039},
author = {Christopher Kirwan and Fu Zhiyong},
keywords = {Citizen Engagement, Co-design, Data Visualization, Design Thinking, Generative Design, Information Architecture, Living Lab, Metadesign, Real-time Data, Simulation, Transdisciplinary Methods, User Experience},
abstract = {Traditional planning and design methodologies can now be augmented by new innovative tools and processes enabled by AI and smart technologies. These facilitate a more open-ended, multi-dimensional approach that incorporates diverse stakeholders to shape the potential of a collective intelligent operating system — one that best reflects the inherent nature of each unique city and urban condition. The design of smart cities must incorporate and adapt a combination of universal standards and localized policies through global civil society organizations and public-private-people partnerships established to serve the user and citizen as participants of the living city. New methods including co-design, co-creation, citizen participation and user experience (UX) feedback foster inclusive cities. Living labs and innovation hubs provide opportunities and spaces to prototype such initiatives. Transdisciplinary approaches are needed more than ever to expand our scope of inclusion to all life forms, including the rights of animals and nature as stakeholders. By applying a new combination of human and AI-enabled methods such as design thinking, machine learning and generative design, cities can now augment and improve their current state seamlessly, integrating technologies and management as an autopoietic smart city operating system.}
}
@incollection{ZHANG2021440,
title = {The Use and Value of Geographic Information Systems in Transportation Modeling},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {440-447},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10364-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717103641},
author = {Ming Zhang},
keywords = {Accessibility modeling, Big-data, Four-step models, Geographic information systems (GIS), Land use-transportation integrated (LUTI) models, New mobility, Relational database, Topological data structure, Traffic simulation, Transportation modeling, Visualization},
abstract = {Geographic information systems (GIS) and transportation modeling have been developing in their respective fields separately. However, the geographic focus of GIS and the geographic nature of transportation make the use of GIS in transportation modeling both a logical choice and a motivation for modeling enhancement. This chapter first explains how the topological data structure underlying the design of vector GIS fits well the needs of data handling for transportation modeling. It then discusses the use and value of GIS in transportation modeling in three areas: (1) GIS for the four-step transportation models and for specific transportation modeling interests such as accessibility modeling and land use-transportation integrated (LUTI) modeling; (2) GIS visualization capabilities for presenting transportation modeling output and for facilitating visual thinking and knowledge construction; and (3) the potential of and challenges to GIS in transportation modeling amid rapid development of big-data technologies and new mobility.}
}
@article{FESTA2024,
title = {Incidence of circular refurbishment measures on indoor air quality and comfort conditions in two real buildings: Experimental and numerical analysis},
journal = {Energy and Built Environment},
year = {2024},
issn = {2666-1233},
doi = {https://doi.org/10.1016/j.enbenv.2024.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666123324000394},
author = {Valentino Festa and Silvia Ruggiero and Sara Riccardi and Margarita- Niki Assimakopoulos and Dimitra Papadaki},
keywords = {Energy building refurbishment, Experimental campaign, Indoor air quality, Thermal comfort, Computational fluid dynamics analysis},
abstract = {The application of Circular Economy to construction sector is a key to attain carbon neutrality, since it is responsible of 40 % of natural resource consumption. In this frame the importance of an efficient building refurbishment process throughout recycled material and renewable energy is fundamental. From an overview about building refurbishment emerges the need to investigate aspects related to Indoor Environmental Quality and the comparison between in-field measurements with output of dynamic simulation models. The present study aims to fill these two gaps by means an energy renovation of two real buildings in Greece. The work develops within the European project “Drive 0″, born to promote deep environmentally friendly retrofitting by means of circular renovation concepts. The methodological approach involves on-site monitoring of a series of parameters describing the energy, microclimate environmental and air quality, before and after the energy requalification. In addition, a numerical model developed in Building Energy Simulation program is calibrated and a Computational Fluid Dynamics is developed. From the in-field measurements emerges that, on one hand, the refurbishment of heating system shows a great improvement of indoor thermal conditions, with Total Volatile Organic Compounds concentration that sometimes exceed 3.0 mg/m3; on the other hand an integrated thermal insulation reduces infiltrations and changes the envelope behaviour, with a global energy saving of 30 % during winter and autumn periods. Another result of the study shows that a numerical model developed in Building Energy Simulation program and calibrated on energy consumption can greatly fit the local thermal comfort distribution of the occupant zone and predict the indoor air quality, if it outputs are used as input data in a Computational Fluid Dynamics study. These results can be beneficial to decision makers and designers for evaluating emitters positioning, opening design and mechanical ventilation strategies, aimed at reducing energy costs.}
}
@article{ZHANG2024101412,
title = {Predicting the Mathematics Literacy of Resilient Students from High‐performing Economies: A Machine Learning Approach},
journal = {Studies in Educational Evaluation},
volume = {83},
pages = {101412},
year = {2024},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2024.101412},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X24000919},
author = {Yimei Zhang and Maria Cutumisu},
keywords = {Academic resilience, Machine learning, Mathematics literacy, Cultural differences},
abstract = {Mathematics is a crucial yet challenging subject for all students. Therefore, it is important to understand the role of academic resilience in mathematics, which enables students to overcome academic challenges. This study applied two machine learning algorithms, Lasso Regression (LR) and Random Forest (RF), to predict the mathematics literacy of resilient students from high-performing economies across cultures in PISA 2022. The findings indicated both RF and LR performed better in Western cultures than in Eastern cultures. Furthermore, in Eastern cultures, mathematics self-efficacy for 21st-century skills played an important role in predicting resilient students’ mathematics literacy, followed by self-efficacy towards mathematics, and mathematics anxiety. In Western cultures, self-efficacy towards mathematics was the predominant predictor, followed by mathematics self-efficacy for 21st-century skills. Theoretically, this study identifies key factors in predicting resilient students’ mathematics literacy across cultures. Methodologically, it is the first to apply ML in exploring resilient students’ mathematics literacy. Practically, it guides educators interested in developing interventions to improve resilient students’ mathematics literacy.}
}
@article{KOLIBIUS2025,
title = {On the origin of memory neurons in the human hippocampus},
journal = {Trends in Cognitive Sciences},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2025.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364661325000312},
author = {Luca D. Kolibius and Sheena A. Josselyn and Simon Hanslmayr},
keywords = {episodic memory, hippocampus, concept neurons, Indexing Theory, single neurons, conjunctive coding, memory trace, engram},
abstract = {The hippocampus is essential for episodic memory, yet its coding mechanism remains debated. In humans, two main theories have been proposed: one suggests that concept neurons represent specific elements of an episode, while another posits a conjunctive code, where index neurons code the entire episode. Here, we integrate new findings of index neurons in humans and other animals with the concept-specific memory framework, proposing that concept neurons evolve from index neurons through overlapping memories. This process is supported by engram literature, which posits that neurons are allocated to a memory trace based on excitability and that reactivation induces excitability. By integrating these insights, we connect two historically disparate fields of neuroscience: engram research and human single neuron episodic memory research.}
}
@article{BRADLEY2016277,
title = {Pilot Testing the Debriefing for Meaningful Learning Evaluation Scale},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {7},
pages = {277-280},
year = {2016},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1876139916000104},
author = {Cynthia Sherraden Bradley and Kristina Thomas Dreifuerst},
keywords = {DML, debriefing, effective briefing, debriefing evaluation, measurement},
abstract = {Background
Debriefing for Meaningful Learning (DML), an evidence-based debriefing method, promotes thinking like a nurse through reflective learning. Despite widespread adoption of DML, little is known about how well it is implemented. To assess the effectiveness of DML implementation, an evaluative rubric was developed and tested.
Sample
Three debriefers who had been trained to use DML at least 1 year previously, submitted five recorded debriefings each for evaluation.
Methods
Three raters who were experts in DML scored each of the 15 recorded debriefing session using DML Evaluation Scale (DMLES). Observable behaviors were scored with binary options. These raters also assessed the items in the DMLES for content validity.
Results
Cronbach's alpha, intraclass correlation coefficients, and Content Validity Index scores were calculated to determine reliability and validity.
Conclusion
Use of DMLES could support quality improvement, teacher preparation, and faculty development. Future testing is warranted to investigate the relationship between DML implementation and clinical reasoning.}
}
@article{PATON200263,
title = {Process, structure and context in relation to integrative biology},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {63-72},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00176-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001769},
author = {Ray Paton},
keywords = {Ecology, Proteins, Category theory, Modelling, Function, Liver},
abstract = {This paper seeks to provide some integrative tools of thought regarding biological function related to ideas of process, structure, and context. The incorporation of linguistic and mathematical thinking is discussed within the context of managing thinking about natural systems as described by Robert Rosen. Examples from ecology, protein networks, and liver function are introduced to illustrate key ideas. It is hoped that these tools of thought, and the further work needed to mobilise such ideas, will continue to address a number of issues raised and pursued by Michael Conrad, such as the seed-germination model and vertical information processing.}
}
@article{FAYEZ2023105905,
title = {Moringa extract reverses pilocarpine-induced hippocampal sclerosis in rats with temporal lobe epilepsy},
journal = {Journal of Functional Foods},
volume = {111},
pages = {105905},
year = {2023},
issn = {1756-4646},
doi = {https://doi.org/10.1016/j.jff.2023.105905},
url = {https://www.sciencedirect.com/science/article/pii/S1756464623005054},
author = {Shaimaa Fayez and Nourhan {Hisham Shady} and Iten M. Fawzy and Sherif A. Maher and Entesar {Ali saber} and Mahmoud Elrehany and Alaa M. Alqahtani and Esam S. Allehyani and Ahmed M. Shawky and Usama {Ramadan Abdelmohsen} and Nada M. Mostafa},
keywords = {, Moringinine A, Computational studies, Epilepsy},
abstract = {The horseradish tree “Moringa oleifera” is the most nutritious terrestrial plant around the globe. Although native to India, its fast growth and drought resistance ability enabled the plant to be cultivated worldwide. In the current study, we report on the isolation of a new phenolic methyl ester namely moringinine A (1) along with four other known compounds viz. caffeic acid (2), ferulic acid (3), 4-hydroxybenzonitrile (4), and 4-hydroxyphenyl acetic acid (5) from Moringa seeds. The later compound was first to be isolated from family Moringaceae. Compounds identification was guided by interplay of NMR and HR-ESI-MS analysis. Anti-epileptic studies conducted in vivo showed that the extract attenuates convulsions by suppressing stress–induced pro-inflammatory markers TNF-α, IL-1β, IL-6, and IFN-ɣ whereas upregulating the anti-inflammatory markers TGF-β and IL-10 in the hippocampal tissues of epileptic rats. The isolated compounds were subjected to computational studies through docking on lactate dehydrogenase A(LDH) and interleukin-6 (IL-6), where all showed binding modes and interaction energies comparable to those of the reference drug diazepam. ADME investigation revealed good pharmacokinetic and drug-likeness properties. These results show that Moringa oleifera seeds could potentially be used as adjuvant in the management of epilepsy.}
}
@article{WOZNIAK2023489,
title = {BiLSTM deep neural network model for imbalanced medical data of IoT systems},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {489-499},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004095},
author = {Marcin Woźniak and Michał Wieczorek and Jakub Siłka},
keywords = {Medical informatics, Deep learning, Multi-optimization learning, BiLSTM, IoT},
abstract = {Health informatics is one of the most developed field in recent time. Computational Intelligence is among the most influential factors that may help to improve patient oriented and secure decision support model. In this article we present a model of IoT system, which combines BiLSTM deep learning with Decision Tree model and data balancing strategy used to help in automated diagnosis support. Presented solution include experimental series of data preprocessing using well established balancing algorithms with custom parameters and modifications in order to best prepare the data for the network training. Such algorithms are ADASYN, SMOTE-Tomek, etc. The system helps to evaluate questionnaires and securely exchange documents between patient and corresponding medical team. From the level of system patient and doctors are able to see automated diagnosis provided by deep learning model. The model gives an important advance to help patients faster. Results show that proposed BiLSTM deep learning with decision tree mode detects diseases from questionnaires with accuracy above 96%, precision above 88% and recall above 96% which proves efficiency of our proposed model.}
}
@article{DELIMA2024107089,
title = {Integrating artificial intelligence and wing geometric morphometry to automate mosquito classification},
journal = {Acta Tropica},
volume = {249},
pages = {107089},
year = {2024},
issn = {0001-706X},
doi = {https://doi.org/10.1016/j.actatropica.2023.107089},
url = {https://www.sciencedirect.com/science/article/pii/S0001706X23002760},
author = {Vinicio Rodrigues {de Lima} and Mauro César Cafundó {de Morais} and Karin Kirchgatter},
keywords = {Mosquito-borne diseases, Species identification, Integrative approach},
abstract = {Mosquitoes (Diptera: Culicidae) comprise over 3500 global species, primarily in tropical regions, where the females act as disease vectors. Thus, identifying medically significant species is vital. In this context, Wing Geometric Morphometry (WGM) emerges as a precise and accessible method, excelling in species differentiation through mathematical approaches. Computational technologies and Artificial Intelligence (AI) promise to overcome WGM challenges, supporting mosquito identification. AI explores computers' thinking capacity, originating in the 1950s. Machine Learning (ML) arose in the 1980s as a subfield of AI, and deep Learning (DL) characterizes ML's subcategory, featuring hierarchical data processing layers. DL relies on data volume and layer adjustments. Over the past decade, AI demonstrated potential in mosquito identification. Various studies employed optical sensors, and Convolutional Neural Networks (CNNs) for mosquito identification, achieving average accuracy rates between 84 % and 93 %. Furthermore, larval Aedes identification reached accuracy rates of 92 % to 94 % using CNNs. DL models such as ResNet50 and VGG16 achieved up to 95 % accuracy in mosquito identification. Applying CNNs to georeference mosquito photos showed promising results. AI algorithms automated landmark detection in various insects' wings with repeatability rates exceeding 90 %. Companies have developed wing landmark detection algorithms, marking significant advancements in the field. In this review, we discuss how AI and WGM are being combined to identify mosquito species, offering benefits in monitoring and controlling mosquito populations.}
}
@article{FORREST19901,
title = {Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks: Introduction to the proceedings of the ninth annual CNLS conference},
journal = {Physica D: Nonlinear Phenomena},
volume = {42},
number = {1},
pages = {1-11},
year = {1990},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899090063U},
author = {Stephanie Forrest}
}
@incollection{OGRADY2020135,
title = {Cyber Security},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {135-141},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10532-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105323},
author = {Nathaniel O'Grady and Andrew C. Dwyer},
keywords = {Computing, Cybersecurity, Cyberspace, Digital, Infrastructure, Networks, Privacy, Software, Surveillance},
abstract = {As computation has become increasingly integrated into everyday life, critical infrastructure, state defense, and cybersecurity has become a new, crucial area of inquiry for geographers. This is due to the fast-changing, new securities that are being formed and enabled through, by and because of the growing role of computation. Geographers have studied cybersecurity as collectively constituted through a complex mixture of technologies, materialities, cultures, knowledges. In so doing, they have probed a range of phenomena crucial to cybersecurity; from technical processes such as encryption, malware infection, and threat detection, to the social arrangements and negotiations between various organizations and states, the implications of surveillance and big data on privacy, and how threats affect various infrastructure that support ways of life across the globe. Nevertheless, geographers do not simply consider cybersecurity as a mode of security imposed “online” or through digital technologies. Rather, in its practice, geographers have demonstrated how cybersecurity involves and invokes socio-political complications around criminality, protection, inequalities, privacy, surveillance, private enterprise, and the role of the state in the life of citizens.}
}
@article{PIQUEIRA2016271,
title = {A comparison of LMC and SDL complexity measures on binomial distributions},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {271-275},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008882},
author = {José Roberto C. Piqueira},
keywords = {Binomial, Complexity, Information, Measure, Probability},
abstract = {The concept of complexity has been widely discussed in the last forty years, with a lot of thinking contributions coming from all areas of the human knowledge, including Philosophy, Linguistics, History, Biology, Physics, Chemistry and many others, with mathematicians trying to give a rigorous view of it. In this sense, thermodynamics meets information theory and, by using the entropy definition, López-Ruiz, Mancini and Calbet proposed a definition for complexity that is referred as LMC measure. Shiner, Davison and Landsberg, by slightly changing the LMC definition, proposed the SDL measure and the both, LMC and SDL, are satisfactory to measure complexity for a lot of problems. Here, SDL and LMC measures are applied to the case of a binomial probability distribution, trying to clarify how the length of the data set implies complexity and how the success probability of the repeated trials determines how complex the whole set is.}
}
@article{SAMPSON20052095,
title = {Comments on: “Pore network simulation of fluid inbibition into paper during coating: II. Characterization of paper's morphology and computation of its effective permeability tensor” by Ghassemzadeh and Sahimi [Chemical Engineering Science 59(2004) 2265–2280]},
journal = {Chemical Engineering Science},
volume = {60},
number = {7},
pages = {2095},
year = {2005},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2004.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009250904009327},
author = {W.W. Sampson and C.T.J. Dodson}
}
@article{BORSTLER2023111592,
title = {Investigating acceptance behavior in software engineering—Theoretical perspectives},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111592},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111592},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002680},
author = {Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen},
keywords = {Acceptance behavior, Dual process theory, Technology acceptance, Theory, TAM, UTAUT, TPB},
abstract = {Background:
Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology.
Objective:
To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development.
Method:
We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking).
Results:
We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior.
Conclusions:
Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories.}
}
@article{LEMOEL2020110,
title = {Towards a multi-level understanding in insect navigation},
journal = {Current Opinion in Insect Science},
volume = {42},
pages = {110-117},
year = {2020},
note = {Neuroscience * Biomechanics of Insect Flight and Bio-inspired engineering},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214574520301310},
author = {Florent {Le Moël} and Antoine Wystrach},
abstract = {To understand the brain is to understand behaviour. However, understanding behaviour itself requires consideration of sensory information, body movements and the animal’s ecology. Therefore, understanding the link between neurons and behaviour is a multi-level problem, which can be achieved when considering Marr’s three levels of understanding: behaviour, computation, and neural implementation. Rather than establishing direct links between neurons and behaviour, the matter boils down to understanding two transitions: the link between neurons and brain computation on one hand, and the link between brain computations and behaviour on the other hand. The field of insect navigation illustrates well the power of such two-sided endeavour. We provide here examples revealing that each transition requires its own approach with its own intrinsic difficulties, and show how modelling can help us reach the desired multi-level understanding.}
}
@article{SHRYANE2020112806,
title = {Is cognitive behavioural therapy effective for individuals experiencing thought disorder?},
journal = {Psychiatry Research},
volume = {285},
pages = {112806},
year = {2020},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2020.112806},
url = {https://www.sciencedirect.com/science/article/pii/S0165178119302793},
author = {Nick Shryane and Richard Drake and Anthony P. Morrison and Jasper Palmier-Claus},
keywords = {Psychosis, Cognitive behavioural therapy, Thought disorder, Randomized Controlled Trial},
abstract = {Various clinical guidelines recommend cognitive behavioural therapy (CBT) to treat psychosis without reference to patients’ thought disorder. However, there is a risk that disorganized thinking hampers CBT. We tested the prediction that thought disorder would interfere with the effectiveness of CBT for hallucinations and delusions, compared to treatment as usual and supportive counselling, in secondary data from two large, single blind randomised controlled trials. We fitted latent growth curve models separately for the development of frequency and distress of symptoms. CBT was significantly more successful than counselling in reducing delusional frequency in the short term and hallucinatory distress at any point, even in those with relatively high thought disorder. We found little evidence that clinicians should restrict CBT in this subgroup of patients. Nevertheless, the findings highlight the importance of effective initial treatment of thought disorder in maximising the benefit of CBT for psychosis, particularly for reducing distress from hallucinations.}
}
@article{TALL1999223,
title = {What Is the Object of the Encapsulation of a Process?},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {223-241},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00029-2},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000292},
author = {David Tall and Michael Thomas and Gary Davis and Eddie Gray and Adrian Simpson},
abstract = {Several theories have been proposed to describe the transition from process to object in mathematical thinking. Yet, what is the nature of this “object” produced by the “encapsulation” of a process? Here, we outline the development of some of the theories (including Piaget, Dienes, Davis, Greeno, Dubinsky, Sfard, Gray, and Tall) and consider the nature of the mental objects (apparently) produced through encapsulation and their role in the wider development of mathematical thinking. Does the same developmental route occur in geometry as in arithmetic and algebra? Is the same development used in axiomatic mathematics? What is the role played by imagery?}
}
@article{HUANG200870,
title = {Investigating the cognitive behavior of generating idea sketches through neural network systems},
journal = {Design Studies},
volume = {29},
number = {1},
pages = {70-92},
year = {2008},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X07000750},
author = {Yinghsiu Huang},
keywords = {drawings, computer supported design, visual reasoning, neural network},
abstract = {Design can be regarded as a seeing–moving–seeing process, where designers repeatedly see and generate ideas that are based on what they have done. The crucial point of design thinking is how designers recognize ambiguous shapes from sketches and then transfer them into different shapes. This study attempts to conduct cognitive experiments to elucidate the sketching process and to simulate two types of sketching behavior used by neural network systems. When exhibiting the first type of sketching behavior, designers are able to transform their original sketches to satisfy requirements. Simulating this type of visual cognitive behavior by neural networks could help computers modify shapes to meet design requirements, as human designers do. When demonstrating the second type of sketching behavior, designers are able to see an ambiguous shape as different complete shapes so as to associate divergent design ideas. Another set of neural networks investigated in this study could also associate different shapes by adjusting the TSL and produce different idea sketches from the same shape.}
}
@article{BUTLER2021170,
title = {Expert performance and crowd wisdom: Evidence from English Premier League predictions},
journal = {European Journal of Operational Research},
volume = {288},
number = {1},
pages = {170-182},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030480X},
author = {David Butler and Robert Butler and John Eakins},
keywords = {OR in sports, Prediction, Experts},
abstract = {This paper analyses the forecasting accuracy of experts vis-à-vis laypeople over three seasons of English Premier League matches. We find that former professional football players have superior forecasting ability when compared to laypeople. The results give partial support to the view that a crowd forecast offers the greatest precision. Pundits generate a positive return while both the crowd and laypeople generate losses. As the prediction of multiple score outcomes represents a computationally difficult task, both groups display forecasting biases including a preference toward specific score forecasts. The results are relevant for those concerned with gambling behaviour if the forecasting strategies adopted here generalise to match betting markets.}
}
@article{ARORA1990131,
title = {Computational design optimization: A review and future directions},
journal = {Structural Safety},
volume = {7},
number = {2},
pages = {131-148},
year = {1990},
issn = {0167-4730},
doi = {https://doi.org/10.1016/0167-4730(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016747309090063U},
author = {Jasbir S. Arora},
keywords = {optimization methods, nonlinear problems, review, computational aspects, engineering design},
abstract = {A mathematical model for design optimization of engineering systems is defined. Computational algorithms to treat the model are reviewed and their features are discussed. The attributes of a good algorithm are given. Sequential quadratic programming algorithms that generate and use the approximate Hessian of the Lagrange function to calculate the search direction are the most recent methods. They are the most reliable methods among the available ones. Several other computational aspects, such as robust implementation of algorithms, use of a knowledge base, interactive use of optimization, and use of a database and database management system, are discussed. Recent developments in the field and future directions are presented.}
}
@article{LIU2022189,
title = {Granular cabin: An efficient solution to neighborhood learning in big data},
journal = {Information Sciences},
volume = {583},
pages = {189-201},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011543},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Xin Yang and Dun Liu and Pengfei Zhang and Jie Wang},
keywords = {Computational efficiency, Granular computing, Neighborhood learning, Neighborhood rough set},
abstract = {Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data.}
}
@article{FATH2005485,
title = {Elucidating public perceptions of environmental behavior: a case study of Lake Lanier},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {485-498},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000611},
author = {Brian D. Fath and M.B. Beck},
keywords = {Cultural theory, Integrated environmental assessment, Stakeholder participation},
abstract = {Participation of stakeholders in stewardship of the aquatic environment, including participation from members of the general public, has become much more widespread than was the case a decade or so ago. With this shift, from a former predominantly technocratic stance to something of a democratic stance on the style of management, it becomes important to elucidate public perceptions of environmental behavior. The paper examines this issue: from a rather specific perspective, where the role of time is significant; with a specific purpose in mind—for defining illustrative stakeholder aspirations for the future, whose plausibility is to be assessed against a computational model of lake behavior; and for a specific case study, Lake Lanier in the Chattahoochee watershed of Georgia, USA. Perturbations and variation in the behavior of the aquatic environment span many time frames, from the very short-term response associated with storms, infrastructure failure, transient pollution events, and so on, to the much longer-term, for instance, the biogeochemical ‘ageing’ of a lake over many decades and more. Our analysis is devoted to data from a survey of stakeholder imagination and perceptions of how the future state of Lake Lanier may evolve in the relatively short term (2–5 years) and in the long term, defined as 25+ years (the span of a generation). Overall, stakeholders are pessimistic and fear that things will be worse in the longer term. Guided largely by thinking on the perspectives of the social solidarities of Cultural Theory, extraction and analysis of sub-samples of the survey responses show that this outlook over the two frames of time is persistent, irrespective of what are, in principle, rather different ‘global’ attitudes towards the man-environment relationship. Of interest inter alia to the foresight generating procedure, by which the ‘reachability’ of stakeholder-derived futures for the lake is to be assessed using a computational model of the relevant parts of the science base, is the question of whether the same small number of priorities for further research on lake behavior is robust in the face of the rich variety of aspirations for the future inevitable in a democratic community of stakeholders.}
}
@article{CORDA2021100834,
title = {The secret of planets’ perihelion between Newton and Einstein},
journal = {Physics of the Dark Universe},
volume = {32},
pages = {100834},
year = {2021},
issn = {2212-6864},
doi = {https://doi.org/10.1016/j.dark.2021.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2212686421000650},
author = {Christian Corda},
abstract = {Three different approaches show that, contrary to a longstanding conviction older than 160 years, the advance of Mercury’s perihelion can be achieved in Newtonian gravity with a very high precision by correctly analyzing the situation without neglecting Mercury’s mass. General relativity remains more precise than Newtonian physics, but Newtonian framework is more powerful than researchers and astronomers were thinking till now, at least for the case of Mercury. The Newtonian formula of the advance of planets’ perihelion breaks down for the other planets. The predicted Newtonian result is indeed too large for Venus and Earth. Therefore, it is also shown that corrections due to gravitational and rotational time dilation, in an intermediate framework which analyzes gravity between Newton and Einstein, solve the problem. By adding such corrections, a result consistent with the one of general relativity is indeed obtained. Thus, the most important results of this paper are two: (i) It is not correct that Newtonian theory cannot predict the anomalous rate of precession of the perihelion of planets’ orbit. The real problem is instead that a pure Newtonian prediction is too large. (ii) Perihelion’s precession can be achieved with the same precision of general relativity by extending Newtonian gravity through the inclusion of gravitational and rotational time dilation effects. This second result is in agreement with a couple of recent and interesting papers of Hansen, Hartong and Obers. Differently from such papers, in the present work the importance of rotational time dilation is also highlighted. Finally, it is important to stress that a better understanding of gravitational effects in an intermediate framework between Newtonian theory and general relativity, which is one of the goals of this paper, could, in principle, be crucial for a subsequent better understanding of the famous Dark Matter and Dark Energy problems.}
}
@article{CHEN201217,
title = {Data-Brain driven systematic human brain data analysis: A case study in numerical inductive reasoning centric investigation},
journal = {Cognitive Systems Research},
volume = {15-16},
pages = {17-32},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S138904171100012X},
author = {Jianhui Chen and Ning Zhong and Peipeng Liang},
keywords = {Data-Brain, Systematic human brain data analysis, Provenance, Brain Informatics},
abstract = {As a crucial step in understanding human intelligence, Brain Informatics (BI) focuses on thinking centric investigations of human cognitive functions with respect to multiple activated brain areas and neurobiological processes for a given task. Although it has been recognized that systematic human brain data analysis is an important issue of BI methodology, the existing expert-driven multi-aspect data analysis excessively depends on individual capabilities and cannot be widely adopted in BI community. In this paper, we propose a Data-Brain driven approach for systematic brain data analysis, which is implemented by using the Data-Brain, Data-Brain based BI provenances and Global Learning Scheme for BI. Furthermore, a human numerical inductive reasoning centric investigation is described to demonstrate significance and usefulness of the proposed approach. Such a Data-Brain driven approach reduces the dependency on individual capabilities and provides a practical way for realizing the systematic human brain data analysis of BI methodology.}
}
@article{MURRAY2019928,
title = {Center Finding in E. coli and the Role of Mathematical Modeling: Past, Present and Future},
journal = {Journal of Molecular Biology},
volume = {431},
number = {5},
pages = {928-938},
year = {2019},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022283619300269},
author = {Seán M. Murray and Martin Howard},
keywords = {bacterial cell division positioning, plasmid segregation, MinCDE system, ParABS system, mathematical modeling},
abstract = {We review the key role played by mathematical modeling in elucidating two center-finding patterning systems in Escherichia coli: midcell division positioning by the MinCDE system and DNA partitioning by the ParABS system. We focus particularly on how, despite much experimental effort, these systems were simply too complex to unravel by experiments alone, and instead required key injections of quantitative, mathematical thinking. We conclude the review by analyzing the frequency of modeling approaches in microbiology over time. We find that while such methods are increasing in popularity, they are still probably heavily under-utilized for optimal progress on complex biological questions.}
}
@incollection{POULSEN201543,
title = {Chapter 3 - Better Concurrency and SIMD on HBM},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {43-67},
year = {2015},
isbn = {978-0-12-802118-7},
doi = {https://doi.org/10.1016/B978-0-12-802118-7.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021187000030},
author = {Jacob Weismann Poulsen and Per Berg and Karthik Raman},
keywords = {HIROMB-BOOS-Model, Danish Meteorological Institute, Operational ocean models, Parallelization, Validation, Verification, OpenMP, MPI, Intel® VTune™ Amplifier, Performance Monitoring Unit, Vectorization, Nesting, Scaling, Cache layout},
abstract = {This chapter describes some work that is being performed at the Danish Meteorological Institute for optimization of a 3D ocean circulation model code with roots back to the 1990s and which is known as the HIROMB-BOOS-Model. The optimization of this large code is instructive agreeing with the authors’ strong belief that the best performance only comes with a focus on architecting for it starting with appropriate data structures. The thinking process and techniques used in this chapter have wide applicability: focus on data locality and then apply threading and vectorization techniques. This way of thinking should be on the mind of every programmer working to design a high-performance application.}
}
@article{GORDON2018273,
title = {Healthier Choices in School Cafeterias: A Systematic Review of Cafeteria Interventions},
journal = {The Journal of Pediatrics},
volume = {203},
pages = {273-279.e2},
year = {2018},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0022347618309363},
author = {Katelyn Gordon and Linda Dynan and Robert Siegel},
keywords = {school cafeteria, behavioral economics, childhood obesity, food selection},
abstract = {Objective
To describe school cafeteria interventions in terms of a behavioral economics scheme and to assess which system is more likely to be effective in improving food selection or consumption.
Study design
With this systematic review, we categorize cafeteria interventions using the behavioral economics theory of Kahneman into system 1 (fast and intuitive thinking) and system 2 (slow and cognitively demanding) or mixed (having elements of system 1 and system 2). Pertinent studies were identified from review of the literature of interventions performed in school and cafeteria settings in children grades K-12 within the past 5 years (2012-2017) at time of search.
Results
In all, 48 of 978 studies met inclusion criteria. By defining success as a 30% improvement in a desired outcome or statistically significant reduction in body mass index, 89% of system 1, 67% of mixed (had both system 1 and 2 elements), and only 33% of system 2 interventions were successful.
Conclusions
This review found successful system 1 type school cafeteria interventions to be more common than system 2 type interventions and system 2 type interventions are less effective than system 1.}
}
@article{SCHULTZ2010174,
title = {Models and methods in motion: Declining the dogma dance},
journal = {Futures},
volume = {42},
number = {2},
pages = {174-176},
year = {2010},
note = {Epistemological pluralism in futures studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001736},
author = {Wendy Schultz},
abstract = {I take a communicative pragmatist and realist approach to futures studies. This implies a sensitivity to understanding what the audience can absorb and using futures methods effectively to create spaces for new futures. While Wilber's work affords us with new insights to engage with methodology, is not the only path. Indeed, it is intellectual bigotry to demand that everyone master the tools one personally deems most appropriate. Critical conversations about futures must remain open, where post-modernist and integral thinking widen our horizons, they are welcomed, where they straitjacket our thoughts, they are not.}
}
@article{WHEELER2020192,
title = {Ideology and predictive processing: coordination, bias, and polarization in socially constrained error minimization},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {192-198},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620300632},
author = {Nathan E Wheeler and Suraiya Allidina and Elizabeth U Long and Stephen P Schneider and Ingrid J Haas and William A Cunningham},
abstract = {Recent models of cognition suggest that the brain may implement predictive processing, in which top-down expectations constrain incoming sensory data. In this perspective, expectations are updated (error minimization) only if sensory data sufficiently deviate from these expectations (prediction error). Although originally applied to perception, predictive processing is thought to generally characterize cognitive architecture, including the social cognitive processes involved in ideological thinking. Scaling up these simple computational principles to the social sphere outlines a path by which group members may adopt shared ideologies and beliefs to predict behavior and cooperate with each other. Because ideological judgments are of specific interest to others in our political groups, we may increasingly regulate each other’s thinking, sharing the process of error minimization. In this paper, we outline how this process of shared error minimization may lead to shared ideologies and beliefs that allow group members to predict and cooperate with each other, and how, as a consequence, political polarization and extremism may result.}
}
@article{PACE2023105433,
title = {Exploring future research and innovation directions for a sustainable blue economy},
journal = {Marine Policy},
volume = {148},
pages = {105433},
year = {2023},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2022.105433},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X22004808},
author = {Lisa A. Pace and Ozcan Saritas and Alan Deidun},
keywords = {Foresight, Blue economy, Interdisciplinary science, Marine science, Sustainable development, Stakeholder participation},
abstract = {The blue economy integrates commercial, research and innovation activities across diverse industrial sectors. Achieving a sustainable blue economy requires unlocking the potential of science and innovation to develop innovative ocean sustainability solutions. This study explores the role of foresight in co-creating alternative, preferred futures for a sustainable blue economy looking towards 2030 and in establishing an interdisciplinary dialogue about research and innovation opportunities to achieve these futures. To this end, a foresight exercise is conducted with marine scientists and researchers in 6 countries in Europe. The exercise is designed in three stages: scanning, scenario-building and strategic orientation, and uses a combination of foresight methods to encourage creative thinking and exploration. The scenarios developed in the study describe alternative future worlds built on the establishment of self-sustaining communities and engaged societies; the diffusion of digitalisation and growth of blue biotechnologies; booming ecosystem services and open and collaborative research infrastructures that impact different sectors of the blue economy. A portfolio of research and innovation areas is developed that aims to inspire new research directions in four domains: (i) integrated ocean management tools; (ii) closed loop, circular polyculture systems; (iii) co-creation of innovation and transdisciplinary research; and (iv) open access and collaborative databases supporting ecosystem services. The study highlights the role of foresight in bridging across disciplinary perspectives and industry sectors. Foresight can be used to complement Decision-Support Systems and other quantitative approaches for research agenda-setting and for decision-making on policies addressing sustainability in the marine sciences. The process contributes to futures skills-building at institutional level and helps establish a futures mindset for strategic planning.}
}
@article{MCGILL2021113697,
title = {Evaluation of public health interventions from a complex systems perspective: A research methods review},
journal = {Social Science & Medicine},
volume = {272},
pages = {113697},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113697},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000290},
author = {Elizabeth McGill and Vanessa Er and Tarra Penney and Matt Egan and Martin White and Petra Meier and Margaret Whitehead and Karen Lock and Rachel {Anderson de Cuevas} and Richard Smith and Natalie Savona and Harry Rutter and Dalya Marks and Frank {de Vocht} and Steven Cummins and Jennie Popay and Mark Petticrew},
keywords = {Systems thinking, Complexity science, Evaluation methodologies, Public health, Practice},
abstract = {Introduction
Applying a complex systems perspective to public health evaluation may increase the relevance and strength of evidence to improve health and reduce health inequalities. In this review of methods, we aimed to: (i) classify and describe different complex systems methods in evaluation applied to public health; and (ii) examine the kinds of evaluative evidence generated by these different methods.
Methods
We adapted critical review methods to identify evaluations of public health interventions that used systems methods. We conducted expert consultation, searched electronic databases (Scopus, MEDLINE, Web of Science), and followed citations of relevant systematic reviews. Evaluations were included if they self-identified as using systems- or complexity-informed methods and if they evaluated existing or hypothetical public health interventions. Case studies were selected to illustrate different types of complex systems evaluation.
Findings
Seventy-four unique studies met our inclusion criteria. A framework was developed to map the included studies onto different stages of the evaluation process, which parallels the planning, delivery, assessment, and further delivery phases of the interventions they seek to inform; these stages include: 1) theorising; 2) prediction (simulation); 3) process evaluation; 4) impact evaluation; and 5) further prediction (simulation). Within this framework, we broadly categorised methodological approaches as mapping, modelling, network analysis and ‘system framing’ (the application of a complex systems perspective to a range of study designs). Studies frequently applied more than one type of systems method.
Conclusions
A range of complex systems methods can be utilised, adapted, or combined to produce different types of evaluative evidence. Further methodological innovation in systems evaluation may generate stronger evidence to improve health and reduce health inequalities in our complex world.}
}
@article{MAHMUD20233933,
title = {Detection of Different Stages of Alzheimer’s Disease Using CNN Classifier},
journal = {Computers, Materials and Continua},
volume = {76},
number = {3},
pages = {3933-3948},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.039020},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823000450},
author = {S M Hasan Mahmud and Md Mamun Ali and Mohammad Fahim Shahriar and Fahad Ahmed Al-Zahrani and Kawsar Ahmed and Dip Nandi and Francis M. Bui},
keywords = {Alzheimer’s disease, early detection, convolutional neural network, data augmentation, random oversampling, machine learning},
abstract = {Alzheimer’s disease (AD) is a neurodevelopmental impairment that results in a person’s behavior, thinking, and memory loss. The most common symptoms of AD are losing memory and early aging. In addition to these, there are several serious impacts of AD. However, the impact of AD can be mitigated by early-stage detection though it cannot be cured permanently. Early-stage detection is the most challenging task for controlling and mitigating the impact of AD. The study proposes a predictive model to detect AD in the initial phase based on machine learning and a deep learning approach to address the issue. To build a predictive model, open-source data was collected where five stages of images of AD were available as Cognitive Normal (CN), Early Mild Cognitive Impairment (EMCI), Mild Cognitive Impairment (MCI), Late Mild Cognitive Impairment (LMCI), and AD. Every stage of AD is considered as a class, and then the dataset was divided into three parts binary class, three class, and five class. In this research, we applied different preprocessing steps with augmentation techniques to efficiently identify AD. It integrates a random oversampling technique to handle the imbalance problem from target classes, mitigating the model overfitting and biases. Then three machine learning classifiers, such as random forest (RF), K-Nearest neighbor (KNN), and support vector machine (SVM), and two deep learning methods, such as convolutional neuronal network (CNN) and artificial neural network (ANN) were applied on these datasets. After analyzing the performance of the used models and the datasets, it is found that CNN with binary class outperformed 88.20% accuracy. The result of the study indicates that the model is highly potential to detect AD in the initial phase.}
}
@article{KLIGER20217,
title = {Dynamic Archeology or Distant Reading: Literary Study Between Two Formalisms},
journal = {Russian Literature},
volume = {122-123},
pages = {7-28},
year = {2021},
note = {Digital Humanities and Russian and East European Studies},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000429},
author = {Ilya Kliger},
keywords = {Computational Literary Studies, Distant Reading, Literary Form, Russian Formalism, OPOIAZ},
abstract = {Scholars working within computational literary studies often invoke Russian Formalism as a methodologically like-minded school of thought and a repository of useful insights, which can at last be tested with the help of recently developed digital techniques. Yet the two formalisms diverge starkly when it comes to three of their most fundamental categories of analysis: first, in their respective conceptions of literary form itself; next, in their notions of history and of what it means to tell the history of form; and finally, in the ways in which they construe the relationship between literature and society as a whole, or, in other words, in their corresponding sociologies of literary form. This paper, then, is a contribution to creating the conditions for the possibility of a genuine exchange between the two formalisms here at issue by focusing, first and foremost, on what divides them.}
}
@article{CAI2024118870,
title = {An efficient Meta-VSW method for ship behaviors recognition and application},
journal = {Ocean Engineering},
volume = {311},
pages = {118870},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118870},
url = {https://www.sciencedirect.com/science/article/pii/S002980182402208X},
author = {Zhiyuan Cai and Qidong Fan and Lecheng Li and Long Yu and Congbo Li},
keywords = {Ship behavior recognition, Unsupervised algorithm, Massive unknown data, Meta-trajectory, Operational efficiency, Fuel consumption},
abstract = {Ship behaviors refer to the operational process such as sailing, entering into port/departure, etc., which indicate by their position, speed, and so on. The collected big data normally have been treated by unsupervised Machine Learning methods. However, the process is time consuming and lacks consideration of time continuity. From the unknown data to recognize and recur the ship behaviors is still a complex problem. Hence, this study proposes a universal Meta-trajectory Variable Sliding Window (Meta-VSW) method to provide an efficient and high-fidelity solution. In this method, the ship data were connected into the smallest units by the meta-trajectory coding, and combines with variable sliding windows to achieve fast, continuous and accurate recognition of ship behaviors. Taking an inland-water ship and a marine transport ship as examples, the validity of the method was fulfilled and compared with two commonly used algorithms, Affinity Propagation (AP) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN). It has the fastest computational speed and can effectively classify the behaviors of massive unknown data from different ships. And it has good performance in capturing behavior boundaries, with the recognition accuracy up to 0.9. Then, the method was applied to analyze the operational effectively and fuel consumption.}
}
@article{COWLEY2019104025,
title = {Wide coding: Tetris, Morse and, perhaps, language},
journal = {Biosystems},
volume = {185},
pages = {104025},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104025},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719301820},
author = {S J Cowley},
keywords = {Organic codes, Distributed language, Adaptors, Wide cognition, Reading, Languaging},
abstract = {Code biology uses protein synthesis to pursue how living systems fabricate themselves. Weight falls on intermediary systems or adaptors that enable translated DNA to function within a cellular apparatus. Specifically, code intermediaries bridge between independent worlds (e.g. those of RNAs and proteins) to grant functional lee-way to the resulting products. Using this Organic Code (OC) model, the paper draws parallels with how people use artificial codes. As illustrated by Tetris and Morse, human players/signallers manage code functionality by using bodies as (or like) adaptors. They act as coding intermediaries who use lee-way alongside “a small set of arbitrary rules selected from a potentially unlimited number in order to ensure a specific correspondence between two independent worlds” (Barbieri, 2015). As with deep learning, networked bodily systems mesh inputs from a coded past with current inputs. Received models reduce ‘use’ of codes to a run-time or program like process. They overlook how molecular memory is extended by living apparatuses that link codes with functioning adaptors. In applying the OC model to humans, the paper connects Turing’s (1937) view of thinking to Wilson’s (2004) appeal to wide cognition. The approach opens up a new view of Kirsh and Maglio’s (1994) seminal studies on Tetris. As players use an interface that actualizes a code or program, their goal-directed (i.e. ‘pragmatic’) actions co-occur with adaptor-like ‘filling in’ (i.e. ‘epistemic’ moves). In terms of the OC model, flexible functions derive from, not actions, but epistemic dynamics that arise in the human-interface-computer system. Second, I pursue how a Morse radio operator uses dibs and dabs that enable the workings of an artificial code. While using knowledge (‘the rules’) to resemiotize by tapping on a transmission key, bodily dynamics are controlled by adaptor-like resources. Finally, turning to language, I sketch how the model applies to writing and reading. Like Morse operators, writers resemiotize a code-like domain of alphabets, spelling-systems etc. by acting as (or like) bodily adaptors. Further, in attending to a text-interface (symbolizations), a reader relies on filling-in that is (or feels) epistemic. Given that humans enact or mimic adaptor functions, it is likely that the OC model also applies to multi-modal language.}
}
@article{FUJISHIRO2025103006,
title = {Chromatin domains in the cell: Phase separation and condensation},
journal = {Current Opinion in Structural Biology},
volume = {91},
pages = {103006},
year = {2025},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2025.103006},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X25000247},
author = {Shin Fujishiro and Masaki Sasai and Kazuhiro Maeshima},
abstract = {Negatively charged genomic DNA wraps around positively charged core histone octamers to form nucleosomes, which, along with proteins and RNAs, self-organize into chromatin within the nucleus. In eukaryotic cells, chromatin forms loops that collapse into chromatin domains and serve as functional units of the genome. Chromatin domains vary in physical properties based on gene activity and are assembled into A (euchromatin) and B (heterochromatin) compartments. Since various factors—such as chromatin-binding proteins, histone modifications, transcriptional states, depletion attraction, and cations—can significantly impact chromatin organization, the formation processes of these hierarchical structures remain unclear. No single imaging, genomics, or modeling method can provide a complete picture of the process. Beautiful models can sometimes fool our thinking. In this short review, we critically discuss the formation mechanisms of the chromatin domain in the cell from a physical point of view, including phase separation and condensation.}
}
@incollection{KAMESWARI202581,
title = {Chapter 4 - Future trends and research challenges in digital twins},
editor = {Sailesh Iyer and Anand Nayyar and Anand Paul and Mohd Naved},
booktitle = {Digital Twins for Smart Cities and Villages},
publisher = {Elsevier},
pages = {81-101},
year = {2025},
isbn = {978-0-443-28884-5},
doi = {https://doi.org/10.1016/B978-0-443-28884-5.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044328884500004X},
author = {Y. Lalitha Kameswari and B. {Omkar Lakshmi Jagan} and Thayyaba Khatoon Mohammed and Shady H.E. {Abdel Aleem}},
keywords = {Artificial intelligence, Digital twins, Integration, Internet of Things, Machine learning, Urban planning},
abstract = {With applications ranging from manufacturing and healthcare to urban planning and energy, digital twin technology has become a ground-breaking idea. In order to provide light on the potential breakthroughs and obstacles that lie ahead, this study examines future trends and research challenges in the field of digital twins. Several significant themes are anticipated to influence the development of digital twins as they continue to change. Digital twins will be better able to learn from complex situations in real time because to the combination of artificial intelligence (AI) and machine learning (ML). Digital twins will be given the ability to foresee, improve, and react to dynamic changes as a result of this AI-driven evolution, which will ultimately result in more effective and resilient systems. Another important development is the idea of federated digital twins, in which various interconnected digital twin instances work together to represent a larger, interconnected system. By integrating the strengths of several digital twins, this method makes it easier to model and analyze very complex and interconnected systems, such as smart cities or multimodal transportation networks. It is also projected that digital twins would spread into the Internet of Things (IoT) space. A closer connection between the real and virtual worlds will be made possible by the seamless integration of sensors, actuators, and data streams with digital twin platforms. This pattern will open the door to fresh perspectives and opportunities for improvement. To fully realize the potential of digital twins, a number of scientific challenges must be overcome. As the integration of real-time data from physical systems raises worries about unauthorized access and potential vulnerabilities, data privacy and security continue to be of the utmost importance. Additionally, sophisticated methods for data assimilation, model validation, and uncertainty quantification are needed to create accurate and trustworthy digital twin models. Another urgent issue is interoperability. The creation of standardized interfaces and protocols is essential to facilitate seamless integration and data sharing as digital twins proliferate across many sectors and domains. Furthermore, novel approaches to distributed computing and high-performance simulation are necessary to meet the scalability and computational requirements of large-scale digital twin ecosystems. In-depth analysis of these trends and problems is provided in this chapter, along with suggestions for future research areas and solutions. The field of digital twins is set for a paradigm-shifting impact on how we build, function, and interact with the physical world by tackling these issues and exploiting new trends.}
}
@article{ZALL2024101285,
title = {Towards emotion-aware intelligent agents by utilizing knowledge graphs of experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101285},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101285},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000792},
author = {Raziyeh Zall and Mohammad Reza Kangavari},
keywords = {Affective computing, Intelligent agent, Cognitive architecture, Appraisal theory, Emotional mental state},
abstract = {Because of the increasing presence of intelligent agents in various aspects of human social life, social skills play a vital role in ensuring these systems exhibit acceptable and realistic behavior in social communication. The importance of emotional intelligence in social capabilities is noteworthy, so incorporating emotions into the behaviors of intelligent agents is essential. Therefore, some computational models of emotions have been presented to develop intelligent agents that exhibit emotional human-like behaviors. However, most current computational models of emotions neglect the dynamic learning of the affective meaning of events based on agents’ experiences. Such models evaluate the events in the environment according to emotional aspects without considering the context of the situations. Also, these models capture the emotional states of agents by using predefined rules determined according to psychological theories. Therefore, they disregard the data-driven methods that can obtain the relationships between appraisal variables and emotions based on natural human data with fewer assumptions on the nature of such relationships. To address these issues, we proposed a novel and unified affective-cognitive framework (EIAEC) to facilitate the development of emotion-aware intelligent agents. EIAEC uses appraisal theories to acquire the emotional states of the agent in various situations. This paper contains four main contributions: 1- We have designed an efficient episodic memory that uses events and their conditional contexts to store and retrieve knowledge and experiences. This memory facilitates emotional expressions and decision-making adapted to the situations of the agent. 2- A novel method has been proposed that learns context-dependent affective values associated with events by using the agent’s experiences in various contexts. Subsequently, we acquired appraisal variables using the elements and related meta-data in episodic memory. 3- We have proposed a new data-driven method that maps appraisal variables to emotional states. 4- Moreover, a method has been developed to update the activation values regarding actions by using the emotional states of the agent. This method models the influence of emotions on the agent’s decision-making. Finally, we simulate a driving scenarios in our proposed framework to manifest the generated emotions in different situations and conditions. Moreover, we show how the proposed method learns the affective meaning of events and actions used in appraisal computing.}
}
@incollection{MAGGIONI2010255,
title = {Knowledge Domains and Domain Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {255-264},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00483-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004838},
author = {L. Maggioni and P.A. Alexander},
keywords = {Discipline, Domain, History, Knowledge, Learning, Mathematics, Reading, Science, Writing},
abstract = {The roots of current disciplines and domains of study reach well back in history. An exploration of their development shows that these areas of knowledge have not only reflected cultural changes, but have also influenced societies, especially through formal educational systems. Besides being characterized by their focus on a particular part of the world, disciplines are also distinguished by a specific way of thinking about their respective domains of study. Psychological research has identified several features of these pathways to knowledge (e.g., reading, writing, history, mathematics, and science) that generally define the landscape of academic practice.}
}
@article{OMRAN2022114806,
title = {Valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment: Approaching green chemistry and circular economy principles},
journal = {Journal of Environmental Management},
volume = {311},
pages = {114806},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.114806},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722003796},
author = {Basma A. Omran and Kwang-Hyun Baek},
keywords = {Green synthesis, Zero-cost, Nanomaterials, Wastewater treatment, Sustainability},
abstract = {Water pollution is one of the most critical issues worldwide and is a priority in all scientific agendas. Green nanotechnology presents a plethora of promising avenues for wastewater treatment. This review discusses the current trends in the valorization of zero-cost, biodegradable, and readily available agro-industrial biowaste to produce green bio-nanocatalysts and bio-nanosorbents for wastewater treatment. The promising roles of green bio-nanocatalysts and bio-nanosorbents in removing organic and inorganic water contaminants are discussed. The potent antimicrobial activity of bio-derived nanodisinfectants against water-borne pathogenic microbes is reviewed. The bioactive molecules involved in the chelation and tailoring of green synthesized nanomaterials are highlighted along with the mechanisms involved. Furthermore, this review emphasizes how the valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment adheres to the fundamental principles of green chemistry, circular economy, nexus thinking, and zero-waste manufacturing. The potential economic, environmental, and health impacts of valorizing agro-industrial biowaste to green nanomaterials are highlighted. The challenges and future outlooks for the management of agro-industrial biowaste and safe application of green nanomaterials for wastewater treatment are summarized.}
}
@article{MANZOLLI2022112211,
title = {A review of electric bus vehicles research topics – Methods and trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {159},
pages = {112211},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112211},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122001344},
author = {Jônatas Augusto Manzolli and João Pedro Trovão and Carlos Henggeler Antunes},
keywords = {Electric bus, Electric mobility, Research gaps, Sustainability, Fleet operation, Energy management},
abstract = {The transportation sector accounts for a significant share of greenhouse gas emissions. Hence, the electrification of this sector is a crucial contributor to the mitigation of global warming. Recent studies suggest that electric vehicles will be economically paired with internal combustion engine vehicles in the near future. However, relying on private vehicle decarbonization only cannot deliver comprehensive space management efficiency solutions in urban environments. Therefore, it is essential to invest in the technological development and deployment of electric buses for public transportation, directly enhancing the quality of life in large cities. From this perspective, this review examines a wide range of scientific literature on electric bus research using science mapping methods and content analysis to support critical thinking unveiling the main research streams, methods, and gaps of the field. The analysis indicates that future research on electric buses will be mainly devoted to sustainability (encompassing economic, environmental and quality of service dimensions), energy management strategies, and fleet operation.}
}
@article{KAKOOEE20241466,
title = {Impact of Pavlovian Approach Bias on Bidirectional Planning in Spatial Navigation Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1466-1478},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.593},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026449},
author = {Reza Kakooee and Mohammad TH Beheshti and Mehdi Keramati},
keywords = {Reinforcement Learning, Computational Modeling, Bidirectional Planning, Decision-Making, Pavlovian Bias},
abstract = {Bidirectional planning refers to a form of goal-directed decision-making process that combines forward and backward planning. Forward planning expands decision trees from the current state towards simulated futures, while backward planning starts the tree expansion from specific goal points in the opposite direction. Previous research has highlighted the impact of Pavlovian approach bias on behavior, showing that animals move towards appetitive outcomes regardless of the appropriateness of such behavior for achieving those outcomes. However, it remains unexplored whether the Pavlovian approach influences behavior by biasing backward planning. This research introduces a spatial navigation task to investigate the involvement of backward planning in humans’ action-selection process and to determine whether the Pavlovian approach biases behavior through backward planning. The results reveal the behavioral signature of backward planning in humans and show that Pavlovian approach bias can influence both forward and backward planning, leading to decisions that are not necessarily instrumentally more efficient. Additionally, we developed a bidirectional planning algorithm based on reinforcement learning to simulate the participants’ decisions. The simulation results suggest that the observed behavioral patterns can be parsimoniously explained by assuming that the Pavlovian approach bias acts as a pruning mechanism when expanding decision trees in both forward and backward directions.}
}
@article{KUGURAKOVA2016217,
title = {Neurobiological Plausibility as Part of Criteria for Highly Realistic Cognitive Architectures},
journal = {Procedia Computer Science},
volume = {88},
pages = {217-223},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.428},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916316842},
author = {Vlada Kugurakova and Maxim Talanov and Denis Ivanov},
keywords = {Lövheim cube, cognitive architectures, neurobiological realism},
abstract = {In this paper we analyze neurobiologically inspired approaches to implement emotions in computational systems. We propose the criteria for realistic cognitive architectures and analyze current architectures using aforementioned criteria. The analysis indicated several interesting architectures H-CogAff, BICA that inspired us to start the development of our own based on biological realistic approaches.}
}
@article{BENTON2023105626,
title = {Associative learning or Bayesian inference? Revisiting backwards blocking reasoning in adults},
journal = {Cognition},
volume = {241},
pages = {105626},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105626},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723002603},
author = {Deon T. Benton and David H. Rakison},
keywords = {Causal reasoning, Causal mechanisms, Computational models, Analytical models, Associative learning, Bayesian inference},
abstract = {Causal reasoning is a fundamental cognitive ability that enables humans to learn about the complex interactions in the world around them. However, the cognitive mechanisms that underpin causal reasoning are not well understood. For instance, there is debate over whether Bayesian inference or associative learning best captures causal reasoning in human adults. The two experiments and computational models reported here were designed to examine whether adults engage in one form of causal inference called backwards blocking reasoning, whether the presence of potential distractors affects performance, and how adults' ratings align with the predictions of different computational models. The results revealed that adults engaged in backwards blocking reasoning regardless of whether distractor objects are present and that their causal judgements supported the predictions of a Bayesian model but not the predictions of two different associative learning models. Implications of these results are discussed.}
}
@article{RUTER2000519,
title = {Analysis, finite element computation and error estimation in transversely isotropic nearly incompressible finite elasticity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {5},
pages = {519-541},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(99)00286-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045782599002868},
author = {Marcus Rüter and Erwin Stein},
abstract = {In this paper we present constitutive models for nearly incompressible, transversely isotropic materials in finite hyperelasticity, particularly for reinforced rubber-like materials, which are of essential engineering interest. The theory is developed using a convected curvilinear coordinate system based on a mixed two-field displacement–pressure energy functional. Furthermore, an a posteriori error estimator without multiplicative constants is derived for non-linear anisotropic problems, which measures the discretization error in the first Piola–Kirchhoff stresses in the L2-norm by solving local Neumann problems with equilibrated tractions. Illustrative numerical examples demonstrate the anisotropic material behaviour of reinforced materials and the efficiency of using adaptive finite element methods.}
}
@article{MCCOWN201233,
title = {Farmers use intuition to reinvent analytic decision support for managing seasonal climatic variability},
journal = {Agricultural Systems},
volume = {106},
number = {1},
pages = {33-45},
year = {2012},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X11001557},
author = {R.L. McCown and P.S. Carberry and N.P. Dalgliesh and M.A. Foale and Z. Hochman},
keywords = {Decision support, Simulation, Information system, Cognitive system, Intuition, Climatic risk},
abstract = {The FARMSCAPE Information System emerged in a long-running research program aimed at making simulation models useful to Australian farmers in managing climatic variability. This paper is about how well it has worked. This is reported in relation to two standards: (1) the value to thinking and action expressed by farmers and their consultants, (2) correspondence with theory about learning and judgement in uncertain external environments. The former utilises recorded narrative interviews with participants over many years. The latter uses a cognitive framework drawn from theory of judgment and decision making featuring the relationship between intuition and analysis (McCown, 2011). The cognitive theory framework makes sense of several evaluation surprises. The first was high enthusiasm by largely-intuitive farmers for an analytic approach to soil water in conjunction with a newly-appreciated “bucket” metaphor for water balance. The second surprise was the virtual absence of soil water measurement 10years later. This had been replaced by various intuitive estimates, calibrated to maintain a heuristic relationship with regard to the “bucket” as a resource. Farmers and their advisers were facilitated in using simulation for thought experiments and planning under climatic uncertainty. Benchmarking enabled problem solving in documented conditions. Scenario analysis using historical climate records supported thought experiments by providing probability distributions that were valued for shaping expectations as a “history of the future”. In retrospective evaluation interviews, researchers were surprised to find that yield forecasting and tactical decision making, anticipated to be analyses that were both site- and season-specific forecasts, had served farmers as “management gaming” simulations to aid formulating action rules for such conditions, thus reducing the need for an on-going decision-aiding service. Equipped with their soil monitoring techniques and with their heuristic rules, farmers still reserved a place for simulation “when you’ve got a planting situation out of the ordinary.”}
}
@article{DENHAM2022105526,
title = {Visualization and modeling of forest fire propagation in Patagonia},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105526},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105526},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002262},
author = {Mónica M. Denham and Sigfrido Waidelich and Karina Laneri},
keywords = {Simulation, Modeling, Forest fire behavior, High-performance computing, GPGPU},
abstract = {Fire propagation is a big concern all over the world. Visualization is a valuable tool to test possible different scenarios for fire spread, specially for designing strategies for fire control, mitigation and management. We present a parallel High-Performance Computing (HPC) forest fire simulator with an interactive and intuitive user interface that offers several functionalities to the user. The visualization interface allows to choose the propagation model of preference, the scenario of interest, as well as numerous simulation features including firebreaks and ignition points. We show some of the outputs for two different mathematical models for fire spreading. The simulator was developed with an open source philosophy in the framework of Faster Than Real Time (FTRT) applications thinking on its possible use in the field during a forest fire propagation. It can be run in Linux (Ubuntu) and Windows Operating Systems and for portability purposes the simulator was also implemented on a NVIDIA Jetson Nano.}
}
@article{ZMIGROD202034,
title = {The role of cognitive rigidity in political ideologies: theory, evidence, and future directions},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {34-39},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619301147},
author = {Leor Zmigrod},
abstract = {A contentious debate in political psychology has centred on the role of cognitive rigidity in shaping individuals’ political ideologies and worldviews. Early theories in the 1950s posited that strict ideological doctrines may tend to attract individuals with dispositions towards mental rigidity. This question has persisted: Does psychological rigidity foster a tendency towards ideological extremism? This review evaluates the empirical landscape with respect to the rigidity-of-the-extreme and the rigidity-of-the-right hypotheses and offers conceptual and methodological recommendations for future research avenues. The evidence suggests that cognitive rigidity is linked to ideological extremism, partisanship, and dogmatism across political and non-political ideologies. Advances in the measurement of ideological extremity and cognitive rigidity will facilitate further elucidation regarding how exactly the two hypotheses may be reconciled and why they have been historically placed in a potentially false competition. This synthesis suggests that a scientifically rigorous understanding of the cognitive roots of ideological thinking may be essential for developing effective antidotes to intolerance and intergroup hostility.}
}
@article{LI2025102888,
title = {From assistance to reliance: Development and validation of the large language model dependence scale},
journal = {International Journal of Information Management},
volume = {83},
pages = {102888},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2025.102888},
url = {https://www.sciencedirect.com/science/article/pii/S0268401225000209},
author = {Zewei Li and Zheng Zhang and Mingwei Wang and Qi Wu},
keywords = {Factor model of LLMs dependence, Large language models, Functional dependence, Existential dependence, Alleviating interventions},
abstract = {With the rapid advancement of large language models (LLMs), the phenomenon of LLMs dependence has emerged and garnered significant attention. However, previous scales have been insufficient to measure the extent of individuals' dependence on LLMs. The current study aims to utilize the human-computer trust model and addiction theory to develop and validate the LLMs dependence scale (LDS) and to report its psychometric properties. An exploratory structural investigation of LLMs dependence was conducted with a sample of 421 LLMs users (Sample 1), which included items analysis, exploratory factor analysis, and network analysis. Additionally, a formal test was performed with a separate sample of 1030 LLMs users (Sample 2), with the data undergoing structural validation through confirmatory factor analysis, validity testing, and reliability testing. To mitigate the potential negative impacts of LLMs dependence, we employed the NodeIdentifyR algorithm for computational simulation interventions to identify critical intervention nodes within the LLMs dependence network. The results indicated that the LDS (18 items) exhibited a bifactor structure of functional dependence and existential dependence. The confirmatory factor model demonstrated a good fit and the LDS also showed good criterion-related validity. Subsequent simulated results of alleviating interventions suggested that users' existential dependence was primarily driven by their dependence on LLMs to handle tedious and boring tasks, while functional dependence was more influenced by users' belief in the omnipotence of LLMs. In summary, the factor structure of the LDS is clear, and its reliability and validity indices meet psychometric standards, making it an effective tool for measuring LLMs dependence.}
}
@article{ZHANG2022101060,
title = {The neural encoding of productive phonological alternation in speech production: Evidence from Mandarin Tone 3 sandhi},
journal = {Journal of Neurolinguistics},
volume = {62},
pages = {101060},
year = {2022},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2022.101060},
url = {https://www.sciencedirect.com/science/article/pii/S0911604422000045},
author = {Jie Zhang and Caicai Zhang and Stephen Politzer-Ahles and Ziyi Pan and Xunan Huang and Chang Wang and Gang Peng and Yuyu Zeng},
keywords = {Tone sandhi, Mandarin Chinese, Speech production, Event-related potentials, Phonological alternation, Word frequency},
abstract = {The understanding of alternation is a key goal in phonological research. But little is known about how phonological alternations are implemented in speech production. The current study tested the hypothesis that the production of words that undergo a highly productive alternation, Mandarin Tone 3 sandhi, is supported by a computation mechanism, which predicts that this alternation is subserved by neural activity in a time-window associated with post-lexical phonological and phonetic encoding regardless of word frequency. ERPs were recorded while participants sub-vocally produced high- and low-frequency disyllabic words that do or do not require sandhi. Sandhi words elicited more positive ERPs than non-sandhi words over left anterior channels around 336–520 ms after participants saw the cue instructing them to initiate sub-vocal production, but this effect was not significantly modulated by word frequency. These findings are consistent with predictions of the computation mechanism and have implications for current psycholinguistic models of speech production. (150 words)}
}
@article{BULLEY20203457,
title = {Children Devise and Selectively Use Tools to Offload Cognition},
journal = {Current Biology},
volume = {30},
number = {17},
pages = {3457-3464.e3},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0960982220308514},
author = {Adam Bulley and Thomas McCarthy and Sam J. Gilbert and Thomas Suddendorf and Jonathan Redshaw},
keywords = {cognitive artifacts, cognitive offloading, cognitive development, extended mind, metacognition},
abstract = {Summary
From maps sketched in sand to supercomputing software, humans ubiquitously enhance cognitive performance by creating and using artifacts that bear mental load [1, 2, 3, 4, 5]. This extension of information processing into the environment has taken center stage in debates about the nature of cognition in humans and other animals [6, 7, 8, 9]. How does the human mind acquire such strategies? In two experiments, we investigated the developmental origins of cognitive offloading in 150 children aged between 4 and 11 years. We created a memory task in which children were required to recall the location of hidden targets. In one experiment, participants were provided with a pre-specified cognitive offloading opportunity: an option to mark the target locations with tokens during the hiding period. Even 4-year-old children quickly adopted this external strategy and, in line with a metacognitive account, children across ages offloaded more often when the task was more difficult. In a second experiment, we provided children with the means to devise their own cognitive offloading strategy. Very few younger children spontaneously devised a solution, but by ages 10 and 11, nearly all did so. In a follow-up test phase, a simple prompt greatly increased the rate at which the younger children devised an offloading strategy. These findings suggest that sensitivity to the difficulties of thinking arises early in development and improves throughout the early school years, with children learning to modify the world around them to compensate for their cognitive limits.}
}
@article{WANG2022269,
title = {Intelligent Attack Analysis for IRS Communications with Incomplete Information},
journal = {Procedia Computer Science},
volume = {202},
pages = {269-276},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005695},
author = {Han Wang and Tianlin Zhu and Dapeng Li and Rui Jiang and Xiaoming Wang and Youyun Xu},
keywords = {IRS system, monitoring, attacking tactic, greedy, robust attack},
abstract = {Intelligent Reflection Surface (IRS) will be widely used in future communication system construction to reduce construction costs and improve coverage. However, IRS systems are generally equipped with controllers to receive wireless signal instructions, this increases the vulnerability of future communications. In this paper, we present an attack tactic to provide a way of thinking for the future defense deployment. At the beginning, the hacker cannot know the whole communication system, then it continuously attacks the IRS system, monitor the communication system, and sequentially learns new information about the system in each attacking round in order to attack more effectively in the next round. A two-layer optimal mathematical model is presented to describe the BS and the hacker’s decision. And the two-layer optimization which is difficult to solve is transformed into a single layer linear optimization by using equivalent transformation and dual transformation. A series of mathematical experiments are used to test different scenarios applicable to different monitoring style, and verify that the tactic proposed in this paper can effectively interfere with the system.}
}
@article{SAFARZYNSKA20121011,
title = {Evolutionary theorizing and modeling of sustainability transitions},
journal = {Research Policy},
volume = {41},
number = {6},
pages = {1011-1024},
year = {2012},
note = {Special Section on Sustainability Transitions},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2011.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0048733312000595},
author = {Karolina Safarzyńska and Koen Frenken and Jeroen C.J.M. {van den Bergh}},
keywords = {Coevolution, Evolutionary economics, Group selection, Lock-in, Niche, Regime, Social learning, Transition, Transition management},
abstract = {This paper argues that evolutionary thinking and modeling can contribute to the emerging research on sustainability transitions and their management. Evolutionary theory provides a range of concepts and mechanisms that are useful in making existing theorizing about transitions more precise and complete. In particular, we will discuss how the multi-level, multi-phase, co-evolutionary, and social learning dynamics underlying transitions can be addressed in evolutionary models. In addition, evolutionary theorizing offers suggestions for extending current theoretical frameworks of transitions. Group selection provides a good example. We review the small set of formal evolutionary models of sustainability transitions, and show that existing formal evolutionary models of technological, social and institutional change can provide useful inputs to transition research and management.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{KOKIS200226,
title = {Heuristic and analytic processing: Age trends and associations with cognitive ability and cognitive styles},
journal = {Journal of Experimental Child Psychology},
volume = {83},
number = {1},
pages = {26-52},
year = {2002},
issn = {0022-0965},
doi = {https://doi.org/10.1016/S0022-0965(02)00121-2},
url = {https://www.sciencedirect.com/science/article/pii/S0022096502001212},
author = {Judite V. Kokis and Robyn Macpherson and Maggie E. Toplak and Richard F. West and Keith E. Stanovich},
abstract = {Developmental and individual differences in the tendency to favor analytic responses over heuristic responses were examined in children of two different ages (10- and 11-year-olds versus 13-year-olds), and of widely varying cognitive ability. Three tasks were examined that all required analytic processing to override heuristic processing: inductive reasoning, deductive reasoning under conditions of belief bias, and probabilistic reasoning. Significant increases in analytic responding with development were observed on the first two tasks. Cognitive ability was associated with analytic responding on all three tasks. Cognitive style measures such as actively open-minded thinking and need for cognition explained variance in analytic responding on the tasks after variance shared with cognitive ability had been controlled. The implications for dual-process theories of cognition and cognitive development are discussed.}
}
@article{BALMER2024105411,
title = {Design Space Exploration and Explanation via Conditional Variational Autoencoders in Meta-Model-Based Conceptual Design of Pedestrian Bridges},
journal = {Automation in Construction},
volume = {163},
pages = {105411},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105411},
url = {https://www.sciencedirect.com/science/article/pii/S092658052400147X},
author = {Vera Balmer and Sophia V. Kuhn and Rafael Bischof and Luis Salamanca and Walter Kaufmann and Fernando Perez-Cruz and Michael A. Kraus},
keywords = {Computational design, Design space exploration, Generative AI, Conditional Variational Autoencoder, Explainable AI, Pedestrian bridge},
abstract = {Today, engineers rely on conventional iterative (often manual) techniques for conceptual design. Emerging parametric models facilitate design space exploration based on quantifiable performance metrics, yet remain time-consuming and computationally expensive, leaving room for improvement. This paper provides a design exploration and explanation framework to augment the designer via a Conditional Variational Autoencoder (CVAE), which serves as a forward performance predictor as well as an inverse design generator conditioned on a set of performance requests. Hence, the CVAE overcomes the limitations of traditional iterative techniques by learning a differentiable mapping for a highly nonlinear design space, thus enabling sensitivity analysis. These methods allow for informing designers about (i) relations of the model between features and performances and (ii) structural improvements under user-defined objectives. The framework is tested on a case-study and proves its potential to serve as a future co-pilot for conceptual design studies of diverse civil structures and beyond.}
}
@article{WANG2024111131,
title = {Three-way clustering: Foundations, survey and challenges},
journal = {Applied Soft Computing},
volume = {151},
pages = {111131},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.111131},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623011493},
author = {Pingxin Wang and Xibei Yang and Weiping Ding and Jianming Zhan and Yiyu Yao},
keywords = {Cluster analysis, Two-way clustering, Three-way decision, Three-way clustering},
abstract = {Clustering, as an unsupervised data mining technique, allows us to classify similar objects into the same cluster according to certain criteria. It helps us identify patterns between objects, reveal the associations between objects, and discover hidden structures. Traditional two-way clustering (2W clustering) algorithms represent one cluster by one set and only two types of relationships are considered between a sample and a cluster, namely, belonging to and not belonging to. Two-way decision is not always feasible especially in situations that are characterized by uncertainty and lack of information. Guided by the principle of three-way decision (3WD) as thinking in threes, three-way clustering (3W clustering) addresses the information uncertainty problem using core and the fringe regions to character a cluster. The universe is split into three sections by these two sets, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging-to. Compared with 2W clustering methods, 3W clustering incorporates the fringe region to describe the uncertain relationship between objects and clusters, which provides more information about the clustering structure. This survey points out the historical developments of three-way clustering and makes an overview of the achievements in the field of three-way clustering. In addition, to reap a clearer grasp of the development and research significance of three-way clustering, we divide the existing three-way clustering approaches into two categories and present the bibliometric analysis of related approaches. Finally, we point out some challenges and future research topics in three-way clustering. It is hoped that this review can serve as a reference and provide convenience for scholars and practitioners in the field of three-way clustering.}
}
@article{ZHANG2017123,
title = {Collective decision optimization algorithm: A new heuristic optimization method},
journal = {Neurocomputing},
volume = {221},
pages = {123-137},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216311183},
author = {Qingyang Zhang and Ronggui Wang and Juan Yang and Kai Ding and Yongfu Li and Jiangen Hu},
keywords = {Collective decision optimization algorithm, Artificial neural networks, Meta-heuristic, Decision-making},
abstract = {Recently, inspired by nature, diversiform successful and effective optimization methods have been proposed for solving many complex and challenging applications in different domains. This paper proposes a new meta-heuristic technique, collective decision optimization algorithm (CDOA), for training artificial neural networks. It simulates the social behavior of human based on their decision-making characteristics including experience-based phase, others'-based phase, group thinking-based phase, leader-based phase and innovation-based phase. Different corresponding operators are designed in the methodology. Experimental results carried out on a comprehensive set of benchmark functions and two nonlinear function approximation examples demonstrate that CDOA is competitive with respect to other state-of-art optimization algorithms.}
}
@article{CARVALHO2016169,
title = {Origins and evolution of enactive cognitive science: Toward an enactive cognitive architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {169-178},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000535},
author = {Leonardo Lana de Carvalho and Denis James Pereira and Sophia Andrade Coelho},
keywords = {Cognitive science, Enaction, Complex systems, Cognitive architecture},
abstract = {This paper presents a historical perspective on the origin of the enactive approach to cognitive science, starting chronologically from cybernetics, with the aim of clarifying its main concepts, such as enaction, autopoiesis, structural coupling and natural drift; thus showing their influences in computational approaches and models of cognitive architecture. Works of renowned authors, as well as some of their main commentators, were addressed to report the development of enactive approach. We indicate that the enactive approach transcends its original context within biology, and at a second moment within connectionism, changes the understanding of the relationships so far established between the body and the environment, and the ideas of conceptual relationships between the mind and the body. The influence on computational theories is of great importance, leading to new artificial intelligence systems as well as the proposition of complex, autopoietic and alive machines. Finally, the article stresses the importance of the enactive approach in the design of agents, understanding that previous approaches have very different cognitive architectures and that a prototypical model of enactive cognitive architecture is one of the largest challenges today.}
}
@article{JAHANIFAR2024103132,
title = {Mitosis detection, fast and slow: Robust and efficient detection of mitotic figures},
journal = {Medical Image Analysis},
volume = {94},
pages = {103132},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103132},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524000574},
author = {Mostafa Jahanifar and Adam Shephard and Neda Zamanitajeddin and Simon Graham and Shan E. Ahmed Raza and Fayyaz Minhas and Nasir Rajpoot},
keywords = {Mitosis, Detection, Segmentation, Breast cancer, MIDOG, TUPAC, Computational pathology, Deep learning},
abstract = {Counting of mitotic figures is a fundamental step in grading and prognostication of several cancers. However, manual mitosis counting is tedious and time-consuming. In addition, variation in the appearance of mitotic figures causes a high degree of discordance among pathologists. With advances in deep learning models, several automatic mitosis detection algorithms have been proposed but they are sensitive to domain shift often seen in histology images. We propose a robust and efficient two-stage mitosis detection framework, which comprises mitosis candidate segmentation (Detecting Fast) and candidate refinement (Detecting Slow) stages. The proposed candidate segmentation model, termed EUNet, is fast and accurate due to its architectural design. EUNet can precisely segment candidates at a lower resolution to considerably speed up candidate detection. Candidates are then refined using a deeper classifier network, EfficientNet-B7, in the second stage. We make sure both stages are robust against domain shift by incorporating domain generalization methods. We demonstrate state-of-the-art performance and generalizability of the proposed model on the three largest publicly available mitosis datasets, winning the two mitosis domain generalization challenge contests (MIDOG21 and MIDOG22). Finally, we showcase the utility of the proposed algorithm by processing the TCGA breast cancer cohort (1,124 whole-slide images) to generate and release a repository of more than 620K potential mitotic figures (not exhaustively validated).}
}
@article{GALLISTEL199243,
title = {Preverbal and verbal counting and computation},
journal = {Cognition},
volume = {44},
number = {1},
pages = {43-74},
year = {1992},
note = {Numerical Cognition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(92)90050-R},
url = {https://www.sciencedirect.com/science/article/pii/001002779290050R},
author = {C.R. Gallistel and Rochel Gelman},
abstract = {We describe the preverbal system of counting and arithmetic reasoning revealed by experiments on numerical representations in animals. In this system, numerosities are represented by magnitudes, which are rapidly by inaccurately generated by the Meck and Church (1983) preverbal counting mechanism. We suggest the following. (1) The preverbal counting mechanisms is the source of the implicit principles that guide the acquisition of verbal counting. (2) The preverbal system of arithmetic computation provides the framework for the assimilation of the verbal system. (3) Learning to count involves, in part, learning a mapping from the preverbal numerical magnitudes to the verbal and written number symbols and the inverse mappings from these symbols to the preverbal magnitudes. (4) Subitizings is the use of the preverbal counting process and the mapping from the resulting magnitudes to number words in order to generate rapidly the number words for small numerosities. (5) The retrieval of the number facts, which plays a central role in verbal computation, is mediated via the inverse mappings from verbal and written numbers to the preverbal magnitudes and the use of these magnitudes to find the appropriate cells in tabular arrangements of the answers. (6) This model of the fact retrieval process accounts for the salient features of the reaction time differences and error patterns revealed by expriments on mental arithmetic. (7) The application of verbal and written computational algorithms goes on in parallel with, and is to some extent guided by, preverbal computations, both in the child and in the adult.}
}
@article{MEHRYAR2024109812,
title = {AI and climate resilience governance},
journal = {iScience},
volume = {27},
number = {6},
pages = {109812},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109812},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224010344},
author = {Sara Mehryar and Vahid Yazdanpanah and Jeffrey Tong},
keywords = {Natural sciences, Earth sciences, Environmental science, Environmental policy, Social sciences},
abstract = {Summary
While artificial intelligence (AI) offers promising solutions to address climate change impacts, it also raises many application limitations and challenges. A risk governance perspective is used to analyze the role of AI in supporting decision-making for climate adaptation, spanning risk assessment, policy analysis, and implementation. This comprehensive review combines expert insights and systematic literature review. The study’s findings indicate a large emphasis on applying AI to climate “risk assessments,” particularly regarding hazard and exposure assessment, but a lack of innovative approaches and tools to evaluate resilience and vulnerability as well as prioritization and implementation process, all of which involve subjective, qualitative, and context-specific elements. Additionally, the study points out challenges such as difficulty of simulating complex long-term changes, and evolving policies and human behavior, reliance on data quality and computational resources, and the need for improved interpretability of results as areas requiring further development.}
}
@article{VANCOUVER20081,
title = {Integrating self-regulation theories of work motivation into a dynamic process theory},
journal = {Human Resource Management Review},
volume = {18},
number = {1},
pages = {1-18},
year = {2008},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053482208000028},
author = {Jeffrey B. Vancouver},
keywords = {Self-regulation, Control theory, Goals, Computational modeling, Dynamic processes},
abstract = {Instead of merely combining theories of self-regulation, the current paper articulates a dynamic process theory of the underlying cognitive subsystems that explain relationships among long-used constructs like goals, expectancies, and valence. Formal elements of the theory are presented in an attempt to encourage the building of computational models of human actors, thinkers, and learners in organizational contexts. Discussion focuses on the application of these models for understanding the dynamics of individuals interacting in their organizations.}
}
@article{ALDRIDGE1967155,
title = {A wave analogue as a guide to ultrasonic thinking},
journal = {Ultrasonics},
volume = {5},
number = {3},
pages = {155-162},
year = {1967},
issn = {0041-624X},
doi = {https://doi.org/10.1016/S0041-624X(67)80060-X},
url = {https://www.sciencedirect.com/science/article/pii/S0041624X6780060X},
author = {E.E. Aldridge}
}
@incollection{BRAME201915,
title = {Chapter 2 - Course Design: Making Choices About Constructing Your Course},
editor = {Cynthia J. Brame},
booktitle = {Science Teaching Essentials},
publisher = {Academic Press},
pages = {15-28},
year = {2019},
isbn = {978-0-12-814702-3},
doi = {https://doi.org/10.1016/B978-0-12-814702-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147023000020},
author = {Cynthia J. Brame},
keywords = {Undergraduate, science, education, course design, learning goals, learning objectives, guiding questions, formative assessment},
abstract = {Designing or redesigning a course can be a creative and rewarding effort, but it is always a challenge. Science is characterized by continuous change and an ever-growing (and already large!) body of knowledge, and our courses often seek to help students understand the core knowledge, experimental tools, and ways of thinking in a field. It’s a big task. Further, a course may play a particular role in the curriculum, serving as a prerequisite, a capstone, or the course in which students learn a particular skill. How do you pick on what to focus, and how do you organize your course to help your students be able to transfer their knowledge to a new setting? How can you design the course to help your students build a conceptual framework that can expand and grow as their understanding grows? This chapter describes six principles to guide your course design and provides suggestions for more detailed resources.}
}
@incollection{KLATT200719,
title = {Perspectives for process systems engineering – a personal view from academia and industry},
editor = {Valentin Pleşu and Paul Şerban Agachi},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {24},
pages = {19-32},
year = {2007},
booktitle = {17th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(07)80027-7},
url = {https://www.sciencedirect.com/science/article/pii/S1570794607800277},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, critical assessment, emerging fields, modeling, design, optimization, control, operations, numerical algorithms, software.},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Modeling, simulation and optimization technologies have been developed to a mature state. These technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. Systems thinking has been established in industrial practice largely through powerful commercial process simulation software and through mandatory courses in most chemical engineering programs. This contribution reflects on the past, present and future of PSE. Special emphasis will be on the perspectives of this field from an academic and industrial point of view.}
}
@incollection{WHITTEN201953,
title = {Chapter 4 - Guided Cognition Effects in Learning Mathematics},
editor = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
booktitle = {Guided Cognition for Learning},
publisher = {Academic Press},
pages = {53-108},
year = {2019},
isbn = {978-0-12-817538-5},
doi = {https://doi.org/10.1016/B978-0-12-817538-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128175385000043},
author = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
keywords = {Advance organizers, Consolidators, Effective homework, Efficient homework, Guided Cognition design, Homework, Long-term or long-lasting learning, Middle school mathematics learning},
abstract = {This chapter reports 11 experiments that were done to determine whether Guided Cognition-designed homework facilitates learning middle school mathematics, and if so, to determine how it helps and what is learned. Experiments were performed in two middle schools and included 8th graders in two experiments and 7th graders in nine experiments. Mathematics topics ranged from fractions to integers to geometry. As in the literature experiments, students were in their normal school environment following their regular curriculum and were unaware that their learning was being observed. Guided Cognition design was found to be effective for learning mathematics. Working story problems that were enriched with cognitive events such as role play, divergent thinking, visualizing and illustrating, and relating to prior experience raised scores on unexpected quizzes by about a letter grade. Another unexpected quiz found that the improvements in problem-solving performance persisted for 6months. Guided Cognition homework was also found to be efficient in that students who worked eight problems and then performed four cognitive events performed as well on unexpected quizzes as students who worked 24 problems in the same time interval. Another pair of experiments determined that modest gains could be made from merely reading completed examples of cognitive events, but that these gains were not long-lasting. Performing the cognitive events was found to be most effective for long-term performance. Another experiment found that experiencing cognitive events after working some mathematics problems can help consolidate knowledge of how to work such problems.}
}
@article{BRANDT20051578,
title = {Mental spaces and cognitive semantics: A critical comment},
journal = {Journal of Pragmatics},
volume = {37},
number = {10},
pages = {1578-1594},
year = {2005},
note = {Conceptual Blending Theory},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2004.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378216605000603},
author = {Per Aage Brandt},
keywords = {Mental Space Theory, Truth conditions, Spinoza, Semantic domains, Mental architecture, Material anchors},
abstract = {The article criticizes the negative influence of modern analytic, anti-semantic and anti-phenomenological thinking on cognitive semantics, and the errors or weaknesses of analysis it induces in current Mental Space Theory (MST). It also shows how a less inhibited theory of meaning, mental spaces and blending could develop more useful analyses of empirical occurrences, such as the artifacts called ‘material anchors’ and works of art — here exemplified by a painting by Matisse.}
}
@article{FEIZIZADEH2024103764,
title = {Spatiotemporal mapping of urban trade and shopping patterns: A geospatial big data approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {128},
pages = {103764},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.103764},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224001183},
author = {Bakhtiar Feizizadeh and Davoud Omarzadeh and Thomas Blaschke},
keywords = {Shopping pattern mapping, GIS, Geospatial big data, Data-driven approaches, Spatially explicit, GIScience, Novel methodology},
abstract = {The economic viability of an urban area in terms of trade and shopping significantly impacts its residents’ quality of life and is crucial for any sustainable development initiative. Geographic information systems (GIS) are well established, but the use of GIS technology within finance and trade analysis is still in its infancy. In this article, we highlighted the potential of GIS technology and big data analytics and demonstrated the importance of thinking in spatial terms for analysing patterns within the trade and finance industries. We studied spatiotemporal trade and shopping patterns in the city of Tabriz using data generated by customer purchase transactions obtained from 5200 stores, shopping, business and service centres. We employed time series transaction data collected from the points of sale in stores, shopping, service and business centres located in different areas of the city. We applied four well known geospatial big data driven approaches including machine learning nearest neighbour, kernel density estimation, space–time pattern mining and spatiotemporal coupling tele-coupling for detecting and mapping of spatial trade hotspot patterns. The results of this study indicated the potential of GIScience methods for the explicit spatial mapping of trade and shopping patterns. The results revealed that the city centre, particularly the Bazaar of Tabriz, acts as the city’s heart of trade, and we identify additional major business hotspots. Furthermore, the results allow for studying the impacts of unbalanced urban development in Tabriz, where the wealthy suburbs with high quality of life, such as Valiasr and Elguli, host the major shopping hotspots. The spatial patterns obtained enable local stakeholders, decision makers and authorities to develop strategic plans for urban sustainable development in Tabriz. The geospatial big data approach used can stimulate novel and progressive research. Results of this study demonstrate methodological advancements in GIScience by ’spatializing’ individual purchase data and therefor proposing an explicit geospatial big data analysis approach.}
}
@article{REIS2024103184,
title = {Machine learning methods in physical therapy: A scoping review of applications in clinical context},
journal = {Musculoskeletal Science and Practice},
volume = {74},
pages = {103184},
year = {2024},
issn = {2468-7812},
doi = {https://doi.org/10.1016/j.msksp.2024.103184},
url = {https://www.sciencedirect.com/science/article/pii/S2468781224002790},
author = {Felipe J.J. Reis and Matheus Bartholazzi Lugão de Carvalho and Gabriela de Assis Neves and Leandro Calazans Nogueira and Ney Meziat-Filho},
keywords = {Artificial intelligence, Computational intelligence, Machine intelligence, Computer reason, Physical therapy modalities},
abstract = {Background
Machine learning (ML) efficiently processes large datasets, showing promise in enhancing clinical practice within physical therapy.
Objective
The aim of this scoping review is to provide an overview of studies using ML approaches in clinical settings of physical therapy.
Data sources
A scoping review was performed in PubMed, EMBASE, PEDro, Cochrane, Web of Science, and Scopus.
Selection criteria
We included studies utilizing ML methods. ML was defined as the utilization of computational systems to encode patterns and relationships, enabling predictions or classifications with minimal human interference.
Data extraction and data synthesis
Data were extracted regarding methods, data types, performance metrics, and model availability.
Results
Forty-two studies were included. The majority were published after 2020 (n = 25). Fourteen studies (33.3%) were in the musculoskeletal physical therapy field, nine (21.4%) in neurological, and eight (19%) in sports physical therapy. We identified 44 different ML models, with random forest being the most used. Three studies reported on model availability. We identified several clinical applications for ML-based tools, including diagnosis (n = 14), prognosis (n = 7), treatment outcomes prediction (n = 7), clinical decision support (n = 5), movement analysis (n = 4), patient monitoring (n = 3), and personalized care plan (n = 2).
Limitation
Model performance metrics, costs, model interpretability, and explainability were not reported.
Conclusion
This scope review mapped the emerging landscape of machine learning applications in physical therapy. Despite the growing interest, the field still lacks high-quality studies on validation, model availability, and acceptability to advance from research to clinical practice.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{XUE2024108224,
title = {Interaction dynamics of social support expressions predict future support-seeking behaviors in online support groups},
journal = {Computers in Human Behavior},
volume = {156},
pages = {108224},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108224},
url = {https://www.sciencedirect.com/science/article/pii/S074756322400092X},
author = {Haoning Xue and Wang Liao and Jingwen Zhang},
keywords = {Compensation, Computational methods, Interaction dynamics, Online support groups, Reciprocity, Support-seeking},
abstract = {Maintaining the sustainability of online support groups (OSGs) presents a significant challenge. Integrating the literature on interaction dynamics and supportive communication, this study investigated how interaction dynamics in supportive communication foster long-term support-seeking behaviors that are crucial to sustaining continuous support exchanges in OSGs. Using a large-scale dataset of 48,868 posts and 468,243 comments over ten years from an OSG, this study examined how reciprocity and compensation of emotional and informational support, signaled by emotional expressions and analytical expressions, predicted a poster's future support-seeking behaviors in the OSG. Results showed that a poster's future support-seeking behaviors were positively associated with receiving (a) reciprocity of analytical expressions and (b) compensation of negative emotional expressions with positive emotional expressions in their past posts. However, reciprocity of negative emotional expressions was negatively associated with a poster's future support-seeking behaviors. This study emphasizes social support as an ongoing interactive process and its importance in motivating support-seeking behaviors and fostering a thriving OSG.}
}
@article{MOAVENI2018452,
title = {Modified Hankel Interaction Index Array for Input-Output Pairing with Improved Characteristics},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {452-457},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.342},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320251},
author = {Bijan Moaveni and Wolfgang Birk},
keywords = {Control configuration selection, Interaction measure, Hankel Interaction Index Array, System Gramians},
abstract = {In this study, a modified version of Hankel Interaction Index Array (HIIA) for control configuration selection is presented which can overcome some of its shortcomings, like e.g. scaling dependency, or not relating to closed loop system properties. Inspired by the relative gain array approach, the HIIA is reformulated in the relative gain thinking by considering the effect of closing loops. The ratio of the Hankel norm of the subsystems in closed and open loop are used to state a modified version of HIIA, which has improved characteristics compared to the original HIIA. Properties of the modified HIIA are discussed and benchmarked with established methods on three example cases.}
}
@article{TONNANG2022100964,
title = {Advances in data-collection tools and analytics for crop pest and disease management},
journal = {Current Opinion in Insect Science},
volume = {54},
pages = {100964},
year = {2022},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2022.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2214574522000992},
author = {Henri EZ Tonnang and Daisy Salifu and Bester T Mudereri and Joel Tanui and Andrew Espira and Thomas Dubois and Elfatih M Abdel-Rahman},
abstract = {Innovative methods in data collection and analytics for pest and disease management are advancing together with computational efficiency. Tools, such as the open-data kit, research electronic data capture, fall armyworm monitoring, and early warning- system application and remote sensing have aided the efficiency of all types of data collection, including text, location, images, audio, video, and others. Concurrently, data analytics have also evolved with the application of artificial intelligence and machine learning (ML) for early warning and decision-support systems. ML has repeatedly been used for the detection, diagnosis, modeling, and prediction of crop pests and diseases. This paper thus highlights the innovations, implications, and future progression of these technologies for sustainability.}
}
@article{PHONAPICHAT20143169,
title = {An Analysis of Elementary School Students’ Difficulties in Mathematical Problem Solving},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {3169-3174},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.728},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814007459},
author = {Prathana Phonapichat and Suwimon Wongwanich and Siridej Sujiva},
keywords = {Mathematical problem solving, mathematical difficulties, mathematical skills, elementary school students},
abstract = {The main purpose of mathematics teaching is to enable students to solve problems in daily life. Unfortunately, according to the latest national test results, most students lack mathematical problem solving skills. This proves to be one of the reasons why overall achievement in mathematics is considered quite low. It also reflects that students have difficulties in comprehending mathematical problems affecting the process of problem-solving. Therefore, in order to allow teachers to establish a proper teaching plan suitable for students’ learning process, this research aims to analyze the difficulties in mathematical problem solving among elementary school students. Samples are divided into two groups, elementary school students and mathematics teachers. Data collection was conducted by structured interview, documentary analysis, and survey tests. Data analysis was conducted by descriptive statistics, and content analysis. The results suggest that there are several difficulties in problem solving, namely 1) Students have difficulties in understanding the keywords appearing in problems, thus cannot interpret them in mathematical sentences. 2) Students are unable to figure out what to assume and what information from the problem is necessary to solving it, 3) Whenever students do not understand the problem, they tend to guess the answer without any thinking process, 4) Students are impatient and do not like to read mathematical problems, and 5) Students do not like to read long problems. Therefore, the results found in this research will lead to the creation and the development of mathematical problem solving diagnostic tests for teachers, in order to improve students’ mathematical problem solving skills.}
}
@article{CHERRIER2023104497,
title = {Household heterogeneity in macroeconomic models: A historical perspective},
journal = {European Economic Review},
volume = {158},
pages = {104497},
year = {2023},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2023.104497},
url = {https://www.sciencedirect.com/science/article/pii/S0014292123001265},
author = {Beatrice Cherrier and Pedro Garcia Duarte and Aurélien Saïdi},
keywords = {History of macroeconomics, Heterogeneous agents, Bewley models, Permanent income hypothesis, Aggregation, Equity premium puzzle, Precautionary savings},
abstract = {In this paper, we trace the rise of heterogeneous household models in mainstream macroeconomics from the turn of the 1980s to the early 2000s, when these models evolved into an identifiable and consistent literature. We show that different communities across the US and Europe considered heterogeneous agents for various reasons and developed models that differed in their theoretical and empirical strategies. Minnesota economists primarily focused on incorporating stochastic heterogeneity into general equilibrium models. Other researchers refined growth models or tried to find alternatives to the permanent income hypothesis, leading them to explore more structural heterogeneity. We also document the computational challenges that some of these communities faced, how they gradually became aware of each other's work, and how they faced criticisms from macro- and microeconomists, many of them trained in European countries and dissatisfied with the theoretical and empirical aggregation strategies underlying these models.}
}
@article{FU2013729,
title = {Expert representation of design repository space: A comparison to and validation of algorithmic output},
journal = {Design Studies},
volume = {34},
number = {6},
pages = {729-762},
year = {2013},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000495},
author = {Katherine Fu and Joel Chan and Christian Schunn and Jonathan Cagan and Kenneth Kotovsky},
keywords = {computer supported design, design by analogy, design methods, engineering design},
abstract = {Development of design-by-analogy tools is a promising design innovation research avenue. Previously, a method for computationally structuring patent databases as a basis for an automated design-by-analogy tool was introduced. To demonstrate its strengths and weaknesses, a computationally-generated structure is compared to four expert designers' mental models of the domain. Results indicate that, compared to experts, the computationally-generated structure is sensible in clustering of patents and organization of clusters. The computationally-generated structure represents a space in which experts can find common ground/consensus – making it promising to be intuitive/accessible to broad cohorts of designers. The computational method offers a resource-efficient way of usefully conceptualizing the space that is sensible to expert designers, while maintaining an element of unexpected representation of the space.}
}
@article{SIGALA2018151,
title = {New technologies in tourism: From multi-disciplinary to anti-disciplinary advances and trajectories},
journal = {Tourism Management Perspectives},
volume = {25},
pages = {151-155},
year = {2018},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211973617301435},
author = {Marianna Sigala},
abstract = {Technologies transform tourism management and marketing from a static and utilitarian sense (whereby managers and tourists use technologies as tools) to a transformative conceptualization whereby tourism markets and actors both shape and are shaped by technology. This paper unravels the transformative power of technologies on: the tourism actors and resources (both the traditional but also new actors, i.e. the technology agents); the ways actors interact to (co-)create but also (co-)destruct tourism value; and the context in which tourism actors interact from a linear supply chain tourism ‘industry’ to a complex socio-technical smart tourism ecosystem. To study such complex phenomena and transformations, the paper emphasises that research should not only adopt a multi-disciplinary approach, but it also needs to follow an anti-disciplinary thinking whereby new knowledge and constructs do not simply fall within existing paradigms, disciplinary silos and mindsets once developed by studying the ‘pure’ humans and their behaviours.}
}
@article{WILSON1997575,
title = {Computation and controversy: Value conflicts and social choices: R. KLING (Ed.) 2nd ed. Academic Press, New York (1996). xxiv + 961 pp., ISBN 0-12-415040-3},
journal = {Information Processing & Management},
volume = {33},
number = {4},
pages = {575-577},
year = {1997},
issn = {0306-4573},
doi = {https://doi.org/10.1016/S0306-4573(97)82727-6},
url = {https://www.sciencedirect.com/science/article/pii/S0306457397827276},
author = {Tom Wilson}
}
@incollection{KAKKAR2025215,
title = {Chapter 11 - A neuroinspired journey: Tracing the evolution and objectives of neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {215-238},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000092},
author = {Mohit Kumar Kakkar},
keywords = {Neuromorphic computing, Von Neumann Model, Neural network architecture, Artificial neural networks, Neuromorphic framework},
abstract = {A subfield of computing known as “neuromorphic computing” (NC) develops cutting-edge and effective computing platforms by drawing inspiration from the anatomy and physiology of the human brain. The potential for energy savings offered by NC is one of its primary benefits. Neuromorphic systems aim to replicate the brain's remarkable energy efficiency by utilizing specialized hardware designs and algorithms that optimize computation and minimize data movement to reduce power consumption. This work presents a thorough investigation of the development and goals of neuromorphic frameworks, taking motivation from the complicated functions of the human mind. It investigates the motives and driving forces behind the pursuit of NC, such as the search for highly parallel and energy-efficient computing architectures. In addition, this work has discussed the thematic areas and benchmarks for progress in NC as well. This chapter serves as a guide through the neuroinspired journey of neuromorphic systems, revealing insight into their past, present, and future.}
}

@article{ARORA2022108615,
title = {Music as a blend of spirituality, culture, and mind mollifying drug},
journal = {Applied Acoustics},
volume = {189},
pages = {108615},
year = {2022},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2021.108615},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X2100709X},
author = {Shefali Arora and Abhinav Tyagi},
keywords = {Music, Science, Emotions, Society, Health, COVID-19 etc},
abstract = {Science inspires music more often than human imagination. Music is an integral part of all societies, including animal ones. It is behaving like an instrument for digesting information. It has been proven to help in healing the body, mind, and culture. Music can maintain and regulate emotion. It is a common thread among large social groups and is used as a tool to navigate through life. Real science and real music require a steady thinking process. It is a way of finding compatibility within a society as well as developing a link with other societies. Music plays a developmental role in a person’s identity, cultural worldview and permeates through life. In nutshell “Science explains Music and Music makes us The Human”}
}
@article{XU2021104922,
title = {Brain decoding in multiple languages: Can cross-language brain decoding work?},
journal = {Brain and Language},
volume = {215},
pages = {104922},
year = {2021},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2021.104922},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X2100016X},
author = {Min Xu and Duo Li and Ping Li},
keywords = {Cross-language brain decoding, Neural representation, Multivariate pattern analysis, Computational modeling, Multilingualism},
abstract = {The approach of cross-language brain decoding is to use models of brain decoding from one language to decode stimuli of another language. It has the potential to provide new insights into how our brain represents multiple languages. While it is possible to decode semantic information across different languages from neuroimaging data, the approach’s overall success remains to be tested and depends on a number of factors such as cross-language similarity, age of acquisition/proficiency levels, and depth of language processing. We expect to see continued progress in this domain, from a traditional focus on words and concrete concepts toward the use of naturalistic experimental tasks involving higher-level language processing (e.g., discourse processing). The approach can also be applied to understand how cross-modal, cross-cultural, and other nonlinguistic factors may influence neural representations of different languages. This article provides an overview of cross-language brain decoding with suggestions for future research directions.}
}
@article{YANG20163,
title = {Modeling Urban Design with Energy Performance},
journal = {Energy Procedia},
volume = {88},
pages = {3-8},
year = {2016},
note = {CUE 2015 - Applied Energy Symposium and Summit 2015: Low carbon cities and urban energy systems},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1876610216300662},
author = {Perry Pei-Ju Yang and Jinyue Yan},
keywords = {Urban design computational model, Energy process model, Urban energy system, Urban design, Energy performance},
abstract = {Traditional urban design methods focus on the form-making process and lack performance dimensions such as energy efficiency. There are inherent differences between Urban Design as a model of decision-making for choosing form alternatives and Energy System Modeling as a model of evaluating and assessing system functions. To design a high energy performance city, the gap between the two models must be bridged. We propose a research design that combines the Urban Design Computational Model (UDCM) and the Optimization Model of Energy Process (OMEP) to demonstrate how an urban design computation can be integrated with an energy performance process and system. An evidence-based case study of community-level near zero energy districts will be needed for future work.}
}
@article{THOMPSON2024100094,
title = {Alzheimer’s disease and the mathematical mind},
journal = {Brain Multiphysics},
volume = {6},
pages = {100094},
year = {2024},
issn = {2666-5220},
doi = {https://doi.org/10.1016/j.brain.2024.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2666522024000054},
author = {Travis B. Thompson and Bradley Z. Vigil and Robert S. Young},
keywords = {Alzheimer’s disease, Mathematical modeling, Scientific computing},
abstract = {Throughout the 19th and 20th centuries, aided by advances in medical imaging, discoveries in physiology and medicine have added nearly 25 years to the average life expectancy. This resounding success brings with it a need to understand a broad range of age-related health conditions, such as dementia. Today, mathematics, neuroimaging and scientific computing are being combined with fresh insights, from animal models, to study the brain and to better understand the etiology and progression of Alzheimer’s disease, the most common cause of age-related dementia in humans. In this manuscript, we offer a brief primer to the reader interested in engaging with the exciting field of mathematical modeling and scientific computing to advance the study of the brain and, in particular, human AD research. Statement of Significance Modeling Alzheimer’s disease is a highly interdisciplinary field and finding an effective starting point can be a considerable challenge. To address this challenge, this manuscript briefly highlights some central components of AD related protein pathology, useful classes of mathematical models for brain and AD research and effective computational resources for the practical prospective practitioner.}
}
@article{CHEN2024e26409,
title = {Physiological records-based situation awareness evaluation under aviation context: A comparative analysis},
journal = {Heliyon},
volume = {10},
number = {5},
pages = {e26409},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e26409},
url = {https://www.sciencedirect.com/science/article/pii/S240584402402440X},
author = {Jun Chen and Anqi Chen and Bingkun Jiang and Xinyu Zhang},
keywords = {Situation awareness, Electroencephalogram, Brain electrical activity mapping, Convolutional neural network, Multi-class classification, Aviation decision-making},
abstract = {Situational Awareness (SA) assessment is of paramount importance in various domains, with particular significance in the military for safe aviation decision-making. It involves encompassing perception, comprehension, and projection levels in human beings. Accurate evaluation of SA statuses across these three levels is crucial for mitigating human false-positive and false-negative rates in monitoring complex scenarios in the aviation context. This study proposes a comprehensive comparative analysis by involving two types of physiological records: electroencephalogram (EEG) signals and brain electrical activity mapping (BEAM) images. These two modalities are leveraged to automate precise SA evaluation using both conventional machine learning and advanced deep learning techniques. Benchmarking experiments reveal that the BEAM-based deep learning models attain state-of-the-art performance scores of 0.955 for both SA perception and comprehension levels, respectively. Conversely, the EEG signals-based manual feature extraction, selection, and classification approach achieved a superior accuracy of 0.929 for the projection level of SA. These findings collectively highlight the potential of deploying diverse physiological records as valuable computational tools for enhancing SA evaluation throughout aviation decision-making safety.}
}
@article{WOLFENGAGEN2016359,
title = {Migration of the Individuals},
journal = {Procedia Computer Science},
volume = {88},
pages = {359-364},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.449},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317057},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey V. Kosikov and Irina A. Parfenova and Mikhail Yu. Ermak and Vasiliy D. Petrov and Ilya A. Nikulin and Victor A. Kholodov},
keywords = {data model, computational model, conceptual modeling, stage-by-stage cognition model, variable domains, Big Data, Thick Data},
abstract = {The individuals are modeled by the elements of variable domains. The primitive frame to detect the individual migration from domain to domain is proposed. The supporting computational model is based on a separation of individuals into actual, possible and virtual ones. As was shown, this leads to an adoption of the stage-by-stage cognition model with a pair of evolvents to capture dynamics of the domains – the 2-dimensions model. The first evolvent reflects the generation of the individuals in a domain, the beginning of and canceling out their existence in a domain. The second evolvent reflects the shifts in properties of the individuals. As awaited this unified data model will have the applications to a wide range of models in computer science and Information Technologies.}
}
@article{SAYALI2023614,
title = {The costs and benefits of psychedelics on cognition and mood},
journal = {Neuron},
volume = {111},
number = {5},
pages = {614-630},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2022.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0896627322011527},
author = {Ceyda Sayalı and Frederick S. Barrett},
keywords = {psychedelics, cognitive control, meta-control, creativity, cognitive flexibility, cognitive stability, dopamine, serotonin, dose-dependency, baseline dependency},
abstract = {Summary
Anecdotal evidence has indicated that psychedelic substances may acutely enhance creative task performance, although empirical support for this claim is mixed at best. Clinical research has shown that psychedelics might have enduring effects on mood and well-being. However, there is no neurocognitive framework that ties acute changes in cognition to long-term effects in mood. In this review, we operationalize creativity within an emerging cognitive control framework and assess the current empirical evidence of the effects of psychedelics on creativity. Next, we leverage insights about the mechanisms and computations by which other psychoactive drugs act to enhance versus impair cognition, in particular to those that act on catecholamines, the neurophysiological consequences of which are relatively well understood. Finally, we use the same framework to link the suggested psychedelic-induced improvements in creativity with enduring psychedelic-induced improvements in mood.}
}
@incollection{STAPLETON2014127,
title = {8.07 - Administrative Evil and Patient Health: A Critique of the Impact of Manufacturing Systems on Health Care},
editor = {Saleem Hashmi and Gilmar Ferreira Batalha and Chester J. {Van Tyne} and Bekir Yilbas},
booktitle = {Comprehensive Materials Processing},
publisher = {Elsevier},
address = {Oxford},
pages = {127-150},
year = {2014},
isbn = {978-0-08-096533-8},
doi = {https://doi.org/10.1016/B978-0-08-096532-1.00813-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008096532100813X},
author = {L. Stapleton},
keywords = {AMAT, Health, Manufacturing},
abstract = {Manufacturing systems principles underpin enterprise information systems. Nowadays these principles, and the systems that accompany them, are widely applied across various sectors, including health services management systems. The question arises: To what extent are these principles appropriate for health care management applications? This chapter explores the question from a human-centered systems perspective by examining the rationalities and assumptions that underpin manufacturing systems and applying these ideas to health care contexts. Human-centered systems have a long theoretical tradition within the automation and control community stretching back at least into the 1970s. It is a particularly strong theme in manufacturing systems research. As automation and control systems are increasingly important outside the factory, many researchers are revisiting core concepts within this tradition. One particularly important sector is health care, which, in recent years, has implemented a range of AMAT (automation and machine-assisted thinking)-type solutions not the least of which are enterprise resource planning systems (ERPs). These implementations have been accompanied by highly publicized systems failures. Ethical problems have also arisen. The chapter exposes an ‘administrative evil’ that relegates the patient to the status of a subassembly, a component in an ever-more complex health care production line. Humans are dehumanized in the rationality of our health care administrative systems. The chapter concludes that health care systems projects should adopt a human-centered approach that draws on research in manufacturing, automation, and control engineering as well as other disciplines.}
}
@article{LIU2022102936,
title = {A review of spatially-explicit GeoAI applications in Urban Geography},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102936},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001339},
author = {Pengyuan Liu and Filip Biljecki},
keywords = {Urban studies, Deep learning, Socio-economics, Location encoder, Graph neural network},
abstract = {Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.}
}
@article{GRIFFIN1977127,
title = {On the application of boundary conditions to time dependent computations for quasi one-dimensional fluid flows},
journal = {Computers & Fluids},
volume = {5},
number = {3},
pages = {127-137},
year = {1977},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(77)90019-6},
url = {https://www.sciencedirect.com/science/article/pii/0045793077900196},
author = {Michael D. Griffin and Anderson {John D.}},
abstract = {A study is made of the influence of boundary and initial conditions on time-dependent finite-difference solutions of quasi-one-dimensional duct flows. Several questions are addressed: (1) Under what conditions will a time-dependent solution converge to a steady-state supersonic flow, (2) Under what conditions will it converge to subsonic flow and (3) What conditions are necessary to insure a particular unique solution for subsonic flows. The results provide an orientation, or way of thinking, about the role of such conditions in time-dependent solutions of steady-state flows. The results also show that supersonic solutions are readily obtained by holding only pressure and temperature fixed at the duct inlet, and allowing velocity to float. However, subsonic solutions require pressure, temperature and velocity to be fixed at both the duct inlet and exit. If no conditions are held fixed at the exit, the results always converge to the supersonic solution, even if the fixed inlet mass flow is less than critical. In such a case, the program appears to generate additional mass flow between the inlet and throat, sufficient to choke the flow. These results also have some impact on two- and three-dimensional time-dependent solutions where subsonic flow is present on some or all portions of the flow boundaries.}
}
@article{WANG20221,
title = {The past and future of mapping the biomarkers of psychosis},
journal = {Current Opinion in Behavioral Sciences},
volume = {43},
pages = {1-5},
year = {2022},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621001261},
author = {Ling-Ling Wang and Simon SY Lui and Raymond CK Chan},
abstract = {Biomarker research has investigated the neurobiological basis for individual differences in liability to psychosis. However, few biomarkers for psychosis have been consistently found to be useful. This paper reviews several previous approaches to identify putative biomarkers of liability to psychosis, and then highlights the lessons that we have learned. We argue that limiting our research to clinical patients at the extreme of the psychosis continuum would likely obscure our knowledge as to how a minority of the general population would develop psychosis. Research on putative neurobiological origins of inter-individual differences in personality traits may be useful in mapping the biomarkers of psychosis. To identify biomarkers applicable to the general population, we advocate the transdiagnostic and psychosis continuum approach. We also advocate the use of multivariate analyses and computational modelling to tackle complex multi-modal datasets. More research should be conducted to study intra-individual variations over different ranges of timescale.}
}
@article{HUSSAIN2025109490,
title = {A neural network integrated mathematical model to analyze the impact of nutritional status on cognitive development of child},
journal = {Computers in Biology and Medicine},
volume = {185},
pages = {109490},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109490},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524015750},
author = {Zakir Hussain and Malaya Dutta Borah},
keywords = {Cognitive development, Cognition, Nutritional status, Neural network, Mathematical model},
abstract = {Cognitive development is a crucial developmental aspect of children. It is a concise field of study in psychology and neuroscience that focuses on various developmental aspects of the brain. Among all other factors, nutritional status is believed to play a very important role in cognitive development. The purpose of this work is to analyze the impacts of different nutritional status levels on the child’s cognitive development. This work designs a model that uses a neural network and differential equations. The neural network is applied on a dataset called “Child Birth Weight Dataset” available at IEEE Dataport ( http://dx.doi.org/10.21227/dvd4-3232) for finding the nutritional status of a child. The different levels of nutritional status, such as low-nutritional status, normal-nutritional status, and over-nutritional status are integrated with the formulated differential equations. The model is computationally simulated considering four different sets of parameter values that represent four different perspectives such as ‘only positive’, ‘only negative’, ‘mix and unequal weight’, and ‘mix and equal weight’ of the influencing factors. The experimental results show that normal-nutritional status is the best nutritional status for cognitive development. However, the best cognitive development happens when all other influencing factors like environmental effects, socioeconomic status, heredity, learning opportunities, and use of experiences are given equal importance. The results also depict that the low- and over-nutritional status cannot restrict cognitive development for a long time. After a certain period, the development gets triggered and it happens. It may be slow and not up to the mark of the development under normal-nutritional status, but it happens. Simply it can be said that nutritional status alone does not have control over the cognitive development of a child. Along with nutritional status, other influencing factors are important too.}
}
@article{BRANASGARZA2012254,
title = {Cognitive effort in the Beauty Contest Game},
journal = {Journal of Economic Behavior & Organization},
volume = {83},
number = {2},
pages = {254-260},
year = {2012},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2012.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167268112001278},
author = {Pablo Brañas-Garza and Teresa García-Muñoz and Roberto Hernán González},
keywords = {Beauty Contest Game, Raven, Cognitive Reflection Test},
abstract = {This paper analyzes cognitive effort in 6 different oneshot p-beauty games. We use both Raven and Cognitive Reflection tests to identify subjects’ abilities. We find that the Raven test does not provide any insight on Beauty Contest Game playing but CRT does: subjects with higher scores on this test are more prone to play dominant strategies. The results are confirmed when levels of reasoning instead of entries in the BCG are used.}
}
@article{DUAN2024101258,
title = {Concept cognition for knowledge graphs: Mining multi-granularity decision rule},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101258},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101258},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000524},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Qun Liu and Qin Jiang and Huamin Zhu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Decision rule},
abstract = {As part of cognitive intelligence, concept cognition for knowledge graphs aims to clearly grasp the typical characteristics of the things referred to by the concept, which can provide prior knowledge for machine understanding and thinking. Different from concept learning and formal concept analysis that learn new concepts from data and the general decision rule that comes from an independent decision table, this paper cognizes an existing concept by decision rules that come from multiple granularities. Specifically, 1) concept cognition for knowledge graphs is realized from the perspective of mining multi-granularity decision rule. 2) Decision tables corresponding to four granularities form a multi-granularity decision table group, and then the result from coarser granularity can guide and help obtaining the result from finer granularity. 3) We propose a framework for mining multi-granularity decision rules, which involves going from a multi-granularity decision table group to the frequent maximal attribute patterns to the decision rules to the credible decision rules. Finally, we verified effectiveness of dividing positive and negative data, monotonicity of attribute patterns in a multi-granularity decision table group, and downward monotonicity of credibility, and observed the impact of the parameter min_cov and min_conf on execution times.}
}
@article{KNIGHT20158,
title = {Making grammars: From computing with shapes to computing with things},
journal = {Design Studies},
volume = {41},
pages = {8-28},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000605},
author = {Terry Knight and George Stiny},
keywords = {computational model(s), design theory, perception, reflective practice, shape grammar},
abstract = {Recent interest in making and materiality spans from the humanities and social sciences to engineering, science, and design. Here, we consider making through the lens of a unique computational theory of design: shape grammars. We propose a computational theory of making based on the improvisational, perception and action approach of shape grammars and the shape algebras that support them. We modify algebras for the materials (basic elements) of shapes to define algebras for the materials of objects, or things. Then we adapt shape grammars for computing shapes to making grammars for computing things. We give examples of making grammars and their algebras. We conclude by reframing designing and making in light of our computational theory of making.}
}
@incollection{TIN1994299,
title = {Baby-Sit: Towards a situation-theoretic computational environment},
editor = {Carlos Martín-Vide},
series = {North-Holland Linguistic Series: Linguistic Variations},
publisher = {Elsevier},
volume = {56},
pages = {299-308},
year = {1994},
booktitle = {Current Issues in Mathematical Linguistics},
issn = {0078-1592},
doi = {https://doi.org/10.1016/B978-0-444-81693-1.50034-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444816931500348},
author = {Erkan Tin and Varol Akman},
abstract = {Publisher Summary
This chapter provides an overview of a situation-theoretic computational environment. Situation theory was first formulated in detail by Jon Barwise and John Perry in 1983. Various versions of the theory have been applied to a number of linguistic issues, resulting in what is commonly known as situation semantics. Individuals, properties, relations, spatio-temporal locations, and situations are the basic constructs of situation theory. The world is viewed as a collection of objects, sets of objects, properties, and relations. Infons are discrete items of information and situations are first-class objects that describe parts of the real world. Information flow is made possible by a network of abstract links among high-order uniformities, viz. situation types. The chapter presents a computational approach to situation theory and its associated environment (called BABY-SIT). The environment is dubbed BABY-SIT because it is believed that presently it includes far too many provisional, make-shift design decisions. The chapter highlights that compared to existing approaches, BABY-SIT enhances the basic features of situation theory.}
}
@article{HORWICH201323622,
title = {Chaperonin-mediated Protein Folding},
journal = {Journal of Biological Chemistry},
volume = {288},
number = {33},
pages = {23622-23632},
year = {2013},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.X113.497321},
url = {https://www.sciencedirect.com/science/article/pii/S0021925820452524},
author = {Arthur L. Horwich},
keywords = {Protein Folding, Chaperone Chaperonin, Molecular Chaperone, Yeast, Protein Misfolding, Polypeptide},
abstract = {We have been studying chaperonins these past twenty years through an initial discovery of an action in protein folding, analysis of structure, and elucidation of mechanism. Some of the highlights of these studies were presented recently upon sharing the honor of the 2013 Herbert Tabor Award with my early collaborator, Ulrich Hartl, at the annual meeting of the American Society for Biochemistry and Molecular Biology in Boston. Here, some of the major findings are recounted, particularly recognizing my collaborators, describing how I met them and how our great times together propelled our thinking and experiments.}
}
@article{ALI2024100170,
title = {A conceptual IoT framework based on Anova-F feature selection for chronic kidney disease detection using deep learning approach},
journal = {Intelligence-Based Medicine},
volume = {10},
pages = {100170},
year = {2024},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2024.100170},
url = {https://www.sciencedirect.com/science/article/pii/S2666521224000371},
author = {Md Morshed Ali and Md Saiful Islam and Mohammed Nasir Uddin and Md. Ashraf Uddin},
keywords = {IoT Framework, Machine learning, Deep learning, Feature selection techniques, ANOVA F-test, Healthcare technology, Medical diagnosis, Kidney disease prediction, Classification},
abstract = {Chronic kidney disease (CKD) is becoming an increasingly significant health issue, especially in low-income countries where access to affordable treatment is limited. Additionally, CKD is associated with various dietary factors, including liver failure, diabetes, anemia, nerve damage, inflammation, peroxidation, obesity, and other related conditions. Therefore, early prediction of CKD is important to progress the functionality of the kidney. In recent times, IoT has been widely used in a diversity of healthcare sectors through the incorporation of monitoring devices such as digital sensors and medical devices for patient monitoring from remote places. To overcome the problem, this research proposed a conceptual architecture for CKD detection. The sensor layer of the architecture includes IoT devices to collect data and the proposed classifier, MLP (Multi-Layer Perceptron), utilizes the Anova-F feature selection technique to effectively detect CKD (Chronic Kidney Disease). In addition to MLP, four other classifiers including ANN (Artificial Neural Network), Simple RNN (Recurrent Neural Network), GRU (Gated Recurrent Unit), and SVM (Support Vector Machine), are employed for comparative analysis of accuracy. Furthermore, three additional feature selection techniques, namely Chi-squared, SFFS (Sequential Floating Forward Selection), and SBFS (Sequential Backward Floating Selection), are utilized to evaluate their impact on the accuracy of CKD detection. Our proposed method outperforms all other approaches with a remarkable accuracy of 99 % while maintaining efficient computational time. This advancement is crucial in developing a highly accurate machine capable of predicting CKD in remote areas with ease.}
}
@article{YANG2022107728,
title = {Mixed data-driven sequential three-way decision via subjective–objective dynamic fusion},
journal = {Knowledge-Based Systems},
volume = {237},
pages = {107728},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009692},
author = {Xin Yang and Yang Chen and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way decision, Sequential three-way decision, Mixed data, subjective–objective, Dynamic fusion},
abstract = {In the context of granular computing, sequential three-way decision is a useful tool to triadic thinking, triadic computing and triadic processing from coarser to finer under multilevel and multiview granularity space. In this paper, we mainly explore a novel framework of sequential three-way decision for the fusion of mixed data from the subjective and objective dynamic perspectives. The former focuses on the decision maker’s dynamic behavior without considering the time-evolving data, and the latter emphasizes on dealing with dynamic mixed data over time by multi-stage decision-making. We firstly utilize four T-norm operators and kernel-based similarity relations to integrate different types of dynamic data. Then the subjective and objective models of sequential three-way decision are investigated based on decision thresholds, attribute importance and cost reduction. Finally, the comparative experiments are reported to verify that our proposed models can achieve the lower decision cost and the acceptable accuracy.}
}
@article{KISELEV2022e09664,
title = {Predicting verbal reasoning from virtual community membership in a sample of Russian young adults},
journal = {Heliyon},
volume = {8},
number = {6},
pages = {e09664},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09664},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022009525},
author = {Pavel Kiselev and Valeriya Matsuta and Artem Feshchenko and Irina Bogdanovskaya and Boris Kiselev},
keywords = {Verbal reasoning, Social networking site, Virtual community, Machine learning},
abstract = {Predicting personality traits from social networking site profiles can help to assess individual differences in verbal reasoning without using long questionnaires. Inspired by earlier studies, which investigated whether abstract-thinking ability are predictable by social networking sites data, we used supervised machine learning to predict verbal-reasoning ability based on a proposed set of features extracted from virtual community membership. A large sample (N = 3,646) of Russian young adults aged 18–22 years approved access to the data from their social networking accounts and completed an online test on verbal reasoning. We experimented with binary classification machine-learning models for verbal-reasoning prediction. Prediction performance was tested on isolated control subsamples for men and women. The results of prediction on AUC-ROC metrics for control subsamples over 0.7 indicated reasonably good performance on predicting verbal-reasoning level. We also investigated the contribution of virtual community's genres to verbal reasoning level prediction for male and female participants. Theoretical interpretations of results stemming from both Vygotsky's sociocultural theory and behavioural genomics are discussed, including the implication that virtual communities make up a non-shared environment that can cause variance in verbal reasoning. We intend to conduct studies to explore the implications of the results further.}
}
@article{ROSSI2011820,
title = {MAPIT: a pedagogical-relational ITS},
journal = {Procedia Computer Science},
volume = {3},
pages = {820-826},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005107},
author = {Pier Giuseppe Rossi and Simone Carletti},
keywords = {Teachers’ thinking, Intelligent tutoring system, Multi agent system, Learning management system, Tracking data, Chat-bot},
abstract = {The majority of Intelligent Tutoring System architectures are focused on supporting learners through content retrieval or in one or more given subject matters; examples of this can be found in Baghera [1], MyClass, Andes [2], Gramy, Advanced Geometry Tutor [7]. The implementation of such architectures are time-consuming and are generally not interoperable with other domains [3]. The presented research describes the experimentation of a Open Source, LMS enhanced with elements of AI aiming at supporting online teachers’ and tutors’ work by using a KB specific to relational and pedagogical aspects, not connected to a specific subject matter. Such implementation needs to be provided of an authoring tool easily and readily usable by tutors and teachers of different subjects and with medium level IT training. Starting point of our investigation has been a preliminary analysis of machine-mediated, human-human interactions (MM-HHI) and communications by using the Teachers’ thinking approach [4], [5], [6]. We considered messages exchanged between teachers/tutors and online students in three post-graduate, online courses running at the University of Macerata during 2008–2010 by the Faculty of Education. The study showed that about 30% of messages concerned structured information that could be straightforwardly retrieved by an artificial agent; almost all remaining messages were instead deeply bound to student’s learning path or required a significant input by the teacher/tutor, while the residual part of messages could — to some extents — be delegated to an intelligent agent having access to students’ tracking data in order to display visual information to users or trigger alarms to tutors. The investigation carried out prompted us for the deployment of an Open Source chat-bot system that would retrieve information already coded into the courses or originated by students through the analysis of their activity logs; the chat-bot agent uses this structured information in order to answer students’ most common questions hence relieving teachers and tutors from doing this repetitive task. The system is being implemented on a OLAT ver. 6.3 LMS loosely coupled to a JADE-based Multi Agent System in charge of processing user tracking data and running the ALICE chat-bot integrated with the platform messaging system.}
}
@article{RULE2020900,
title = {The Child as Hacker},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {900-915},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301741},
author = {Joshua S. Rule and Joshua B. Tenenbaum and Steven T. Piantadosi},
keywords = {Learning and cognitive development, Language of thought, Hacking, Computational modeling, Program induction},
abstract = {The scope of human learning and development poses a radical challenge for cognitive science. We propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that children’s learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.}
}
@article{WOLFENGAGEN2024101185,
title = {Semantic configuration model with natural transformations},
journal = {Cognitive Systems Research},
volume = {83},
pages = {101185},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101185},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001195},
author = {Viacheslav Wolfengagen and Larisa Ismailova and Sergey Kosikov and Igor Slieptsov and Sebastian Dohrn and Alexander Marenkov and Vladislav Zaytsev},
keywords = {Information process, Configuration, Morphing, Cognitive preference, Semantic net, Functor, Natural transformation},
abstract = {In the present work, efforts have been made to create a configuration-based approach to knowledge extraction. The notion of granularity is developed, which allows fine-tuning the expressive possibilities of the semantic network. As known, the central issues for knowledge-based systems are what’s-in-a-node and what’s-in-a-link. As shown, the answer can be obtained from the functor-as-object representation. Then the nodes are functors, and the main links are natural transformations. Such a model is applicable to represent morphing, and the object is considered as a process, which is in a harmony with current ideas on computing. It is possible to represent information channels that carry out the transformations of processes. The possibility of generating displaced concepts and the generation of families of their morphs is shown, the evolvent of stages of knowledge and properties of the process serve as parameters.}
}
@article{FALOMIR201931,
title = {Special issue on problem-solving, creativity and spatial reasoning},
journal = {Cognitive Systems Research},
volume = {58},
pages = {31-34},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S138904171930261X},
author = {Zoe Falomir and Ana-Maria Olteţeanu},
keywords = {Cognitive systems, Problem-solving, Creativity, Computational creativity, Spatial reasoning, Cognition, General artificial intelligence, Spatial cognition},
abstract = {Problem-solving, creativity and spatial reasoning are high level abilities of cognitive systems with high potential for synergies. However, they have been treated separately by different fields. This special issue presents research work on these topics, aiming to observe their interrelations in order to create theoretical approaches, methodologies and computational tools to advance work on creativity and spatial problem-solving in cognitive systems.}
}
@incollection{KAMPIS1991345,
title = {Chapter Seven - SELF-REPRODUCTION AND COMPUTATION},
editor = {GEORGE KAMPIS},
booktitle = {Self-Modifying Systems in Biology and Cognitive Science},
publisher = {Pergamon},
address = {Amsterdam},
pages = {345-403},
year = {1991},
volume = {6},
series = {IFSR International Series on Systems Science and Engineering},
isbn = {978-0-08-036979-2},
doi = {https://doi.org/10.1016/B978-0-08-036979-2.50012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080369792500124},
author = {GEORGE KAMPIS}
}
@article{CAMARGO20181116,
title = {A method for integrated process simulation in the mining industry},
journal = {European Journal of Operational Research},
volume = {264},
number = {3},
pages = {1116-1129},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717306409},
author = {Luis Felipe Riehs Camargo and Luis Henrique Rodrigues and Daniel Pacheco Lacerda and Fabio Sartori Piran},
keywords = {O.R. in natural resources, Production, Simulation, Systems thinking},
abstract = {This paper proposes a method of Integrated Process Simulation (MIPS), which considers the dynamic, stochastic and systemic characteristics of mining operations to support investment decisions in this industry. This MIPS supports development of a Decision Support System (DSS) that considers product quality, process productivity and production costs. A case study is described that used the MIPS to make better investment decisions. The MIPS has proven, in practice, to be effective in several applications; for example, in defining the maintenance policy for critical equipment in an iron ore concentration plant; the process for removing impurities and simulating the company's budget to evaluate the viability of different business plans.}
}
@article{XIE2024,
title = {Wide human-like neural network incorporating driving styles for human-like driving intention analysis},
journal = {Journal of Intelligent Transportation Systems},
year = {2024},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2024.2425304},
url = {https://www.sciencedirect.com/science/article/pii/S1547245024000471},
author = {Jiming Xie and Yan Zhang and Yaqin Qin and Ke Li and Shuai Dong and Siyu Liu and Yulan Xia},
keywords = {autonomous vehicle, decision making, driving intention, human driving vehicle, width human-like neural network},
abstract = {Enhancing the synergy between autonomous and human-driven vehicles at the societal level requires understanding drivers’ behaviors and cognitive patterns, as well as conducting human-like driving intention analysis. To achieve this goal, this study designs a novel framework for analyzing human-like driving intention. Firstly, a spectral clustering method is employed to characterize driving styles. Secondly, a misclassification cost matrix is tailored to different driving needs. Finally, inspired by the complex neural networks found in the human brain, we develop a specialized lightweight neural network, termed the Width Human-like Neural Network (WNN), aimed at realizing personalized cognition and facilitating human-like decision-making in driving intention. Experimental studies and validation based on natural driving trajectory data from Kunming, China, demonstrate that the method accurately infers internal implicit driving intention from external explicit and observable driving behaviors, achieving a prediction accuracy of 99.8%. This framework strategically allocates limited computational resources to critical areas for autonomous vehicles and exemplifies best practices for improving neural network performance in driving intention analysis tasks.}
}
@article{FARHAT199435,
title = {Simulation of compressible viscous flows on a variety of MPPs: computational algorithms for unstructured dynamic meshes and performance results},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {35-60},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00075-1},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000751},
author = {Charbel Farhat and Stéphane Lanteri},
abstract = {We report here on our effort in simulating unsteady viscous flows on the iPSC-860, the CM-5 and the KSR-1 MPPs (Massively Parallel Processors), using a Monotonic Upwind Scheme for Conservation Laws finite volume/finite element method on fully unstructured fixed and moving grids. We advocate mesh partitioning with message passing as a portable paradigm for parallel processing. We present and discuss several performance results obtained on all three MPP systems in terms of interprocessor communication costs, I/O, scability and sheer performance.}
}
@article{BYLANDER1994165,
title = {The computational complexity of propositional STRIPS planning},
journal = {Artificial Intelligence},
volume = {69},
number = {1},
pages = {165-204},
year = {1994},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370294900817},
author = {Tom Bylander},
abstract = {I present several computational complexity results for propositional STRIPS planning, i.e., STRIPS planning restricted to ground formulas. Different planning problems can be defined by restricting the type of formulas, placing limits on the number of pre-and postconditions, by restricting negation in pre- and postconditions, and by requiring optimal plans. For these types of restrictions, I show when planning is tractable (polynomial) and intractable (NP-hard). In general, it is PSPACE-complete to determine if a given planning instance has any solutions. Extremely severe restrictions on both the operators and the formulas are required to guarantee polynomial time or even NP-completeness. For example, when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. When definite Horn ground formulas are permitted, determining plan existence is PSPACE-complete even if operators are limited to zero preconditions and one postcondition. One of the interesting tractable problems is if each operator is restricted to positive preconditions and one postcondition (only ground literals). The blocks-world problem, slightly modified, is a subproblem of this restricted planning problem. These results in combination with previous analyses are not encouraging for domain-independent planning.}
}
@article{BORDY201329,
title = {Radiotherapy out-of-field dosimetry: Experimental and computational results for photons in a water tank},
journal = {Radiation Measurements},
volume = {57},
pages = {29-34},
year = {2013},
note = {Proceedings of the Workshop: Dosimetry for Second Cancer Risk Estimation EURADOS Annual Meeting Vienna 2012},
issn = {1350-4487},
doi = {https://doi.org/10.1016/j.radmeas.2013.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1350448713002710},
author = {J.M. Bordy and I. Bessieres and E. d'Agostino and C. Domingo and F. d'Errico and A. {di Fulvio} and Ž. Knežević and S. Miljanić and P. Olko and A. Ostrowsky and B. Poumarede and S. Sorel and L. Stolarczyk and D. Vermesse},
keywords = {Radiotherapy, Photon dosimetry, Out of field doses, Scatter radiation, Leakage},
abstract = {The first objective of this work was to check and select a set of four kinds of passive photon, dosimeters (two thermo-luminescence dosimeter (TLD) types, one radiophotoluminescence (RPL) dosimeter and one optically stimulated luminescence (OSL) dosimeter) together with a common measurement protocol. Dosimeters were calibrated in a reference clinical linear acccelerator beam in a water tank at a reference facility at the Laboratoire National Henri Becquerel (CEA LIST/LNE LNHB, Saclay. Radiation qualities of 6, 12 and 20 MV were used with standard calibration conditions described in IAEA TRS 398 and non-standard conditions. Profile and depth dose ion chamber measurements were also made to provide reference values. Measurements were made in a water tank into which pipes could be inserted which held dosimeters in pre-determined and reproducible positions. The water tank was built to enable investigation of doses up to 60 cm from the beam axis. A first set of experiments was carried out with the beam passing through the tank. From this first experiment, penumbra and out-of-field dose profiles including water and collimator scatter and leakage were found over three orders of magnitude. Two further sets of experiments using the same experimental arrangement with the beam outside the tank, to avoid water scatter, were designed to measure collimator scatter and leakage by closing the jaws of the collimator. Depending on the energy, typical leakage and collimator scatter represents 10–40% and 30–50% of the total out-of-field doses respectively. It was concluded that all dosimeters can be used for out-of-field photon dosimetry. All show good uniformity, good reproducibility, and can be used down to low doses expected at distances remote from the subsequent radiotherapy target volume.}
}
@incollection{KOCH2009125,
title = {Consciousness: Theoretical and Computational Neuroscience},
editor = {Larry R. Squire},
booktitle = {Encyclopedia of Neuroscience},
publisher = {Academic Press},
address = {Oxford},
pages = {125-130},
year = {2009},
isbn = {978-0-08-045046-9},
doi = {https://doi.org/10.1016/B978-008045046-9.00407-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080450469004071},
author = {C. Koch and G. Tononi},
abstract = {Consciousness is a puzzling, state-dependent property of certain types of complex, adaptive, and highly interconnected systems. The best example is a healthy and attentive human brain. If the brain is anesthetized, consciousness ceases. Small lesions in the midbrain and thalamus of patients can lead to a complete loss of consciousness, while destruction of circumscribed parts of the cerebral cortex of patients can eliminate very specific aspects of consciousness, such as the ability to be aware of motion or to recognize objects (e.g., faces), without a concomitant loss of consciousness in general. Given the similarity in brain structure and behavior, biologists commonly assume that at least some animals, in particular nonhuman primates, share certain aspects of consciousness with humans. Brain scientists are exploiting a number of empirical approaches that shed light on the neural basis of consciousness. At present, it is not known to what extent artificial systems, such as computers, robots, or the World Wide Web as a whole, are or can become ‘conscious.’ What is needed is a theory of consciousness that explains in quantitative terms what types of systems, with what architecture, can possess conscious states.}
}
@article{MAGRI2023105535,
title = {Scene context is predictive of unconstrained object similarity judgments},
journal = {Cognition},
volume = {239},
pages = {105535},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105535},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001695},
author = {Caterina Magri and Eric Elmoznino and Michael F. Bonner},
keywords = {Contextual associations, Objects, Scenes, Similarity, Convolutional neural networks, Natural image statistics},
abstract = {What makes objects alike in the human mind? Computational approaches for characterizing object similarity have largely focused on the visual forms of objects or their linguistic associations. However, intuitive notions of object similarity may depend heavily on contextual reasoning—that is, objects may be grouped together in the mind if they occur in the context of similar scenes or events. Using large-scale analyses of natural scene statistics and human behavior, we found that a computational model of the associations between objects and their scene contexts is strongly predictive of how humans spontaneously group objects by similarity. Specifically, we learned contextual prototypes for a diverse set of object categories by taking the average response of a convolutional neural network (CNN) to the scene contexts in which the objects typically occurred. In behavioral experiments, we found that contextual prototypes were strongly predictive of human similarity judgments for a large set of objects and rivaled the performance of models based on CNN representations of the objects themselves or word embeddings for their names. Together, our findings reveal the remarkable degree to which the natural statistics of context predict commonsense notions of object similarity.}
}
@article{DARICI2024123327,
title = {How will I break AI? Post-Luddism in the AI age: Fuzzy MCDM synergy},
journal = {Technological Forecasting and Social Change},
volume = {202},
pages = {123327},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123327},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524001239},
author = {Sefer Darıcı and Muhammad Riaz and Gülay Demir and Zekiye Tamer Gencer and Dragan Pamucar},
keywords = {AI, Communication, Post-Luddism, Fuzzy set, DEMATEL, LMAW},
abstract = {This study proposes a fuzzy multi-criteria model to assess the risk of unemployment among professionals in the communication sector in Turkey, prompted by the rapid development and evolution of artificial intelligence (AI) technologies. The method integrates Fuzzy The Decision Making Trial and Evaluation Laboratory (F-DEMATEL) and Fuzzy Logarithm Methodology of Additive Weights (F-LMAW) procedures. Data were collected from 20 experts representing professions such as public relations, advertising, journalism, and design through a 12-question survey. In the analysis, the F-DEMATEL procedure was initially employed to determine attitudes towards AI technologies, followed by the application of the F-LMAW procedure to assess the magnitude of AI's impact on occupational groups. Findings reveal a nuanced stance: while professionals acknowledge the necessity of AI for their work, they are unwilling to accept unemployment due to more advanced AI. This newly identified structure, termed Post-Luddism, highlights concerns over technological unemployment, particularly pronounced in professions like journalism where job prospects are limited and creative thinking is paramount. In other communication fields, the intensive use of technology mitigates fears of AI harm. However, even in journalism, there exists a propensity to perceive AI as detrimental. These insights shed light on communication professionals' apprehensions and attitudes towards AI's effects. Policymakers and stakeholders can leverage this understanding to formulate strategic measures, considering the divergent perspectives among professional groups regarding AI, towards mitigating potential unemployment risks and fostering AI-adaptive strategies.}
}
@article{CHEN2024132899,
title = {A novel offshore wind power prediction model based on TCN-DANet-sparse transformer and considering spatio-temporal coupling in multiple wind farms},
journal = {Energy},
volume = {308},
pages = {132899},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132899},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224026732},
author = {Juntao Chen and Xueying Fu and Lingli Zhang and Haoye Shen and Jibo Wu},
keywords = {Dual attention network, Temporal convolutional network, Offshore wind power prediction, Sparse transformer, Spatio-temporal coupling},
abstract = {Offshore wind power capacity is growing, leading to larger clustered farms. Accurately predicting offshore wind power capacity is crucial for power system stability; however, current studies often overlook neighbouring installations. To address this, this study presents the Temporal Convolutional Network-Dual Attention Network-Sparse Transformer (TCN-DANet-Sparse Transformer) model, which considers the spatiotemporal coupling of multiple wind farms. Before detailing our model, we review the existing prediction methods, noting their limitations in capturing interconnected adjacent wind farms. Our model integrates spatial information from nearby farms to enhance prediction reliability. Through Pearson Correlation Coefficient analysis, we explore the temporal and spatial coupling features. Using overlapping sliding windows, we partition farms into subsequences, processed with TCN-DANet for efficient spatio-temporal feature extraction. These features are then input into the Sparse Transformer to improve the computational efficiency. Validated using a dataset from Kächele et al., our model outperforms the baseline on the London Wind Farm. In spring, for Case 1, the mean square error (MSE) of the main model decreased by 43.19 % compared to that of the TCN-DANet-transformer model. Similarly, for Case 2, the MSE of the main model is reduced by 41.69 %.}
}
@incollection{DAS2025193,
title = {Chapter 15 - Systems pharmacology – principles, methods and applications},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {193-206},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000158},
author = {Arpan Jyoti Das and Habeeb {Shaik Mohideen}},
keywords = {CADD, Computational biology, Molecular network, Omics networking, Systems biology, Systems pharmacology},
abstract = {Socializing has become much easier with the advent of scintillating discoveries and technological advancements in computer science, information technology applications, and of course, the indispensable Internet. Historical and unimaginable success has been achieved by breaking horizons in almost all the fields, but biology. Biological phenomena are so difficult to understand because of the complex cross-talks between different entities such as DNA, RNA, proteins, lipids, carbohydrates, and their relevant downstream postprocessing modifications. The dance and interplay involving these moieties as a function of a drug is called as Systems Pharmacology. The multiscale modeling and simulation approach of systems pharmacology employ computational models to simulate drug effects at various levels, from molecular interactions to organ-level responses, allowing for a more comprehensive understanding of drug action. In this chapter, we will delve into the historical landscapes, methods and principles, tools, applications, frameworks, and benefits of systems pharmacology that will give a beginner a comprehensive understanding of the field.}
}
@article{PEZZULO2013270,
title = {Action simulation in the human brain: Twelve questions},
journal = {New Ideas in Psychology},
volume = {31},
number = {3},
pages = {270-290},
year = {2013},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2013.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X13000263},
author = {Giovanni Pezzulo and Matteo Candidi and Haris Dindo and Laura Barca},
keywords = {Action simulation, Internal model, Forward model, Motor control, Action understanding},
abstract = {Although the idea of action simulation is nowadays popular in cognitive science, neuroscience and robotics, many aspects of the simulative processes remain unclear from empirical, computational, and neural perspectives. In the first part of the article, we provide a critical review and assessment of action simulation theories advanced so far in the wider literature of embodied and motor cognition. We focus our analysis on twelve key questions, and discuss them in the context of human and (occasionally) primate studies. In the second part of the article, we describe an integrative neuro-computational account of action simulation, which links the neural substrate (as revealed in neuroimaging studies of action simulation) to the components of a computational architecture that includes internal modeling, action monitoring and inhibition mechanisms.}
}
@article{MUTHUSAMY2025112916,
title = {High-precision malware detection in android apps using quantum explainable hierarchical interaction network},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112916},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112916},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015508},
author = {Ramnath Muthusamy and Yesubai Rubavathi Charles},
keywords = {Recommender system, Android applications, Real or fake app detection, Privacy, Feature interaction, Quantum superposition and entanglement},
abstract = {The exponential growth of Android applications has increased the prevalence of fraudulent and malicious apps, posing significant risks to user security and privacy. Existing detection methodologies often struggle with poor interpretability, scalability, and computational complexity, limiting their effectiveness. To address these challenges, this study introduces the Quantum Explainable Hierarchical Interaction Network (QEHIN), a novel framework designed to detect real and fake Android applications with superior accuracy and interpretability. QEHIN incorporates quantum computing principles such as superposition and entanglement to model high-order feature interactions effectively. Its innovative architecture includes a Quantum Embedding Layer for transforming input features into quantum states, a Quantum Hierarchical Interaction Network (QHIN) for capturing complex dependencies, a Quantum Deep Neural Network (QDNN) for enhanced feature processing, and a Quantum Cross-Hierarchical Unit (QCHU) to ensure seamless integration across hierarchical levels. This design achieves precise, transparent, and scalable detection of malicious applications, addressing the shortcomings of traditional methods. Evaluation on the Google Play Store Reviews, MobileRec, and Android-App-Recommendation datasets demonstrates the novelty and effectiveness of QEHIN. It achieves an accuracy of 98.86 %, precision of 98.78 %, recall of 98.82 %, and a kappa score of 98.54 %, significantly outperforming existing approaches.}
}
@article{JOHNSON1989319,
title = {Exploiting parallelism in computational science},
journal = {Future Generation Computer Systems},
volume = {5},
number = {2},
pages = {319-337},
year = {1989},
note = {Grand Challenges to Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(89)90050-2},
url = {https://www.sciencedirect.com/science/article/pii/0167739X89900502},
author = {Gary M. Johnson},
abstract = {The full exploitation of numerical simulation as an independent approach to the solution of engineering and scientific problems requires computing capability far exceeding that which is presently available. In this paper, the computing requirements posed by challenging problems in several disciplines are examined and contrasted with contemporary supercomputer resources. Of the means available to help fill the gap between the demands of computational science and the performance level of present-generation supercomputer systems, parallel processing appears to have the greatest potential for near-term success. Parallel computer architectures are reviewed and categorized according to processing units, memory, and interconnection scheme. Philosophies of parallel processing are discussed. They are distinguished by the number and size of the parallel tasks which they employ. Scientific problems are examined for parallelism inherent at the physical level. Typical algorithms and their mappings onto parallel architectures are discussed. Computational examples are presented to document the performance of scientific applications on present-generation parallel processors. Projections are made concerning software developments and machine architectures.}
}
@incollection{MILLER2020181,
title = {Chapter eight - Data science and the exposome},
editor = {Gary W. Miller},
booktitle = {The Exposome (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {181-209},
year = {2020},
isbn = {978-0-12-814079-6},
doi = {https://doi.org/10.1016/B978-0-12-814079-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128140796000080},
author = {Gary W. Miller},
keywords = {Bioinformatics, systems biology, models, computational biology, machine learning, Bayesian methods, artificial intelligence, causal inference},
abstract = {Data science is focused on extracting meaningful value from complex datasets. Exposome-related data are certainly complex with information coming from a myriad of sources. The huge amounts of data must be organized in some manner that allows appropriate interpretations and associations to be drawn. It is unlikely that unsupervised approaches will allow for causal associations to be made, but with proper study design and appropriate statistical and computational models, it should be possible to derive meaningful connections between complex exposures and specific health outcomes. The complex types of data will undoubtedly require sophistical mathematical approaches, including bioinformatics, computational, machine learning, and systems biology-based techniques. This chapter reviews some of the possible strategies that can be used to keep track of the diverse and massive datasets that will result from exposome research.}
}
@article{BESOLD201597,
title = {Towards integrated neural–symbolic systems for human-level AI: Two research programs helping to bridge the gaps},
journal = {Biologically Inspired Cognitive Architectures},
volume = {14},
pages = {97-110},
year = {2015},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000468},
author = {Tarek R. Besold and Kai-Uwe Kühnberger},
keywords = {Research program, Neural–symbolic integration, Complexity theory, Cognitive architectures, Agent architectures},
abstract = {After a human-level AI-oriented overview of the status quo in neural–symbolic integration, two research programs aiming at overcoming long-standing challenges in the field are suggested to the community: The first program targets a better understanding of foundational differences and relationships on the level of computational complexity between symbolic and subsymbolic computation and representation, potentially providing explanations for the empirical differences between the paradigms in application scenarios and a foothold for subsequent attempts at overcoming these. The second program suggests a new approach and computational architecture for the cognitively-inspired anchoring of an agent’s learning, knowledge formation, and higher reasoning abilities in real-world interactions through a closed neural–symbolic acting/sensing–processing–reasoning cycle, potentially providing new foundations for future agent architectures, multi-agent systems, robotics, and cognitive systems and facilitating a deeper understanding of the development and interaction in human-technological settings.}
}
@article{PITTAPANTAZI2007301,
title = {Secondary school students’ levels of understanding in computing exponents},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {4},
pages = {301-311},
year = {2007},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000508},
author = {Demetra Pitta-Pantazi and Constantinos Christou and Theodossios Zachariades},
keywords = {Exponents, Prototype, Conceptual change},
abstract = {The aim of this study is to describe and analyze students’ levels of understanding of exponents within the context of procedural and conceptual learning via the conceptual change and prototypes’ theory. The study was conducted with 202 secondary school students with the use of a questionnaire and semi-structured interviews. The results suggest that three levels of understanding can be identified. At the first level students’ interpretation of exponents is based upon exponents that symbolize natural numbers. At Level 2, students’ knowledge acquisition process is a process of enrichment of the existing conceptual structures. Students at this level are able to compute exponents with negative numbers by extending the application of prototype examples. Finally, at Level 3 students not only extend the prototype examples but also reorganize their thinking in order to compute and compare exponents with roots, a concept which is quite different from the concept of exponents with natural numbers.}
}
@article{HERNANDEZRAMIREZ2024414,
title = {The Future End of Design Work: A Critical Overview of Managerialism, Generative AI, and the Nature of Knowledge Work, and Why Craft Remains Relevant},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {414-440},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000960},
author = {Rodrigo Hernández-Ramírez and João Batalheiro Ferreira},
keywords = {creativity, design work, generative artificial intelligence (GenAI), knowledge work, managerialism},
abstract = {This article examines the transformation of design work under the influence of managerialism and the rise of Generative Artificial Intelligence (GenAI). Drawing on John Maynard Keynes’s projections of technological unemployment and the evolving nature of work, it argues that despite advancements in automation, work has not diminished but rather devalued. Design, understood as a type of knowledge work, faces an apparent existential crisis. GenAI grows adept at mimicking the output of creative processes. The article explores how the fear of the end of design work fueled by the rise of GenAI is rooted in a misunderstanding of design work. This misunderstanding is driven by managerialism—an ideology that prioritizes efficiency and quantifiable outcomes over the intrinsic value of work. Managerialism seeks to instrumentalize and automate design, turning it into a controllable procedure to generate quantifiable creative outputs. The article argues why design work cannot be turned into a procedure and automated using GenAI. Advocates of these systems claim they enhance productivity and open new opportunities. However, evidence so far shows that flawed GenAI models produce disappointing outcomes while operating at a significant environmental cost. The article concludes by arguing for a robust theory of design—one that acknowledges the unique ontological and epistemic boundaries of design work and underscores why design cannot be reduced to a procedural output.}
}
@article{GHAVAM2021128776,
title = {The life cycle environmental impacts of a novel sustainable ammonia production process from food waste and brown water},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128776},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128776},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621029747},
author = {Seyedehhoma Ghavam and Caroline M. Taylor and Peter Styring},
keywords = {Green ammonia, Waste utilization, Carbon capture and sequestration, Carbon capture and utilization, Greenhouse gas emissions, Life cycle assessment},
abstract = {To replace existing high impact ammonia production technologies, a new sustainability-driven waste-based technology producing green ammonia with and without urea was devised using life cycle thinking and sustainable design principles, targeting efficiency, carbon emissions, water, and power use competitiveness. We have used life cycle assessment to determine whether cradle-to-gate, multiple configurations of the core waste-based processes integrating several carbon capture/utilization options can compete environmentally with other available ammonia technologies. Our waste-to-ammonia processes reduce potential impacts from abiotic depletion, human toxicity, and greenhouse gas (GHG) emissions relative to fossil-based and renewable technologies. Among the assessed technologies, coupling dark fermentation with anaerobic digestion and capturing CO2 for sequestration or later use is most efficient for GHGs, water, and energy, consuming 27% less energy and reducing GHGs by 98% compared to conventional ammonia. Water use is 38% lower than water electrolysis and GHGs are 94% below municipal waste incineration routes per kg NH3. Additionally, displacing conventional, high impact urea by integrating urea production from process CO2 decreases life cycle environmental impacts significantly despite increased energy demand. On a fertilizer-N basis, the ammonia + urea configuration without dark fermentation performs best on all categories included. Methane and ammonia leakage cause nearly all life cycle impacts, indicating that failing to prevent leakage undermines the effectiveness of new technologies such as these. Our results show that a green ammonia/ammonia + urea process family as designed here can reduce waste and prevent the release of additional CO2 from ammonia production while avoiding fossil-based alternatives and decreasing emissions from biogenic waste sources.}
}
@article{WEI2021189,
title = {Multi-core-, multi-thread-based optimization algorithm for large-scale traveling salesman problem},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {1},
pages = {189-197},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2020.06.055},
url = {https://www.sciencedirect.com/science/article/pii/S1110016820303227},
author = {Xin Wei and Liang Ma and Huizhen Zhang and Yong Liu},
keywords = {Multi-core, Multi-thread, Traveling Salesman Problem, Optimization Algorithm},
abstract = {With the rapid development of general hardware technology, microcomputers with multi-core CPUs have been widely applied in commercial services and household usage in the last ten years. Multi-core chips could, theoretically, lead to much better performance and computational efficiency than single-core chips. But so far, they have not shown general advantages for users, other than for operating systems and some specialized software. It is not easy to transform traditional single-core-based algorithms into multi-core-, multi-thread-based algorithms that can greatly improve efficiency, because of difficulties in computation and scheduling of hardware kernels, and because some programming languages cannot support multi-core, multi-thread programming. Therefore, a kind of multi-core-, multi-thread-based fast algorithm was designed and coded with Delphi language for the medium- and large-scale traveling salesman problem instances from TSPLIB, which can fully speed up the searching process without loss of quality. Experimental results show that the algorithm proposed can, under the given hardware limitations, take full advantage of multi-core chips and effectively balance the conflict between increasing problem size and computational efficiency and thus acquire satisfactory solutions.}
}
@article{LIEVENS2021128,
title = {A service design perspective on the stakeholder engagement journey during B2B innovation: Challenges and future research agenda},
journal = {Industrial Marketing Management},
volume = {95},
pages = {128-141},
year = {2021},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2021.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850121000791},
author = {Annouk Lievens and Vera Blažević},
keywords = {Stakeholder engagement, B2B innovation process, Stakeholder engagement journey, Innovation networks, Service design},
abstract = {Innovation in business-to-business (B2B) contexts deals with highly dynamic, complex, and heterogeneous constellations of stakeholders with a diversity of goals, motives, and capabilities that further challenge successful management of B2B innovation processes and outcomes. Complex challenges, such as sustainability and digitization trends, push these B2B firms to embrace new innovation methods that help them manage disruptive change. Service design thinking has emerged as an innovation management practice emphasizing a human-centered innovation process of user interactions, creativity, and learning mindsets. In this article, we aim to evaluate the challenges and develop a research agenda on how service design can effectively enable stakeholders' engagement during the B2B innovation process. We argue that to advance service design opportunities for stakeholder engagement, we need to address the unique complexities and challenges of stakeholder engagement during innovation from a systemic and dynamic process perspective. From a systemic perspective, we zoom in on the building blocks of stakeholder engagement and address multi-level stakeholder engagement platforms (i.e., innovation networks). From a dynamic process perspective, we treat stakeholder engagement as an emerging process and zoom in on the temporal and relational connections and hybrid orchestration to allow for both structural and emerging stakeholder engagement during innovation. We develop a stakeholder engagement journey in which we integrate service and innovation stages and propose how service design activities can support and facilitate the aforementioned challenges and complexities. Finally, we identify concrete research questions and, accordingly, develop a research agenda for future research on stakeholder engagement in B2B innovation trajectories.}
}
@article{CRILLY2021333,
title = {The Evolution of “Co-evolution” (Part II): The Biological Analogy, Different Kinds of Co-evolution, and Proposals for Conceptual Expansion},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {7},
number = {3},
pages = {333-355},
year = {2021},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2021.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405872621000927},
author = {Nathan Crilly},
keywords = {Design process, Design thinking, Creativity, Biological analogy, Interdisciplinarity},
abstract = {Descriptions of problem-solution “co-evolution” either explicitly or implicitly draw an analogy between processes of design and processes of biological evolution. Analogies of this kind are common in research because of their potential to assist in explanation and discovery. However, reviewing the design literature reveals that the discussion of design co-evolution has become disconnected from the biological analogy on which it is founded, and from which other disciplines draw. Here, I explore the function of the co-evolution analogy, provide an illustrative example from biology, and explore the varieties of co-evolution to which design might be compared. By doing so, I propose two possible directions for expanding the design co-evolution concept: (i) examining what co-evolves in addition to, or instead of, problems and solutions, and (ii) examining the different levels at which co-evolution occurs. Both of these proposals are illustrated with a variant of the design co-evolution diagram.}
}
@article{TAMILVENDAN2024469,
title = {Parametric optimization in drilling of sisal–glass reinforced epoxy composites using Taguchi grey relational analysis method},
journal = {Transactions of the Canadian Society for Mechanical Engineering},
volume = {48},
number = {3},
pages = {469-476},
year = {2024},
issn = {0315-8977},
doi = {https://doi.org/10.1139/tcsme-2024-0018},
url = {https://www.sciencedirect.com/science/article/pii/S0315897724000570},
author = {D. Tamilvendan and A.R. Ravikumar and R. Thirumalai},
keywords = {Taguchi grey relational analysis, drlling, glass fiber, sisal fiber, composite},
abstract = {This research work intends to study the effect of hybridization of glass and sisal fiber, stacking sequence and tensile properties of the composite. The sisal-glass fiber hybrid composites laminates are prepared using reinforced plain woven sisal fabric (unidirectional) and plainwoven glass fabric. In this research study, 27 experiments are conducted as per L27 orthogonal array. Five process parameters are selected and three responses are considered in this work. The drilling of the composite specimen is considered and the drilling process parameters such as speed, feed rate, drill diameter, material thickness, and drill point angle are selected. The responses considered in this work are delamination factor, thrust force, and torque. Taguchi analysis is performed and the response table for means for the responses is determined, and the most influencing parameter in the drilling of the composite specimen is analyzed. The grey relational coefficients are computed and followed with the computation of the grey relational grade. The grey relational grades are calculated for determining the highest contributing parameter in the drilling of the sisal fiber and glass fiber reinforced hybrid composite specimen. The optimum drilling process parameters are ranked and the ranks presented represent the sequence of run resulting in optimum solutions.}
}
@article{HOIFODT2015,
title = {Predictors of Response to Web-Based Cognitive Behavioral Therapy With High-Intensity Face-to-Face Therapist Guidance for Depression: A Bayesian Analysis},
journal = {Journal of Medical Internet Research},
volume = {17},
number = {9},
year = {2015},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.4351},
url = {https://www.sciencedirect.com/science/article/pii/S1438887115002137},
author = {Ragnhild Sørensen Høifødt and Matthias Mittner and Kjersti Lillevoll and Susanne Kvam Katla and Nils Kolstrup and Martin Eisemann and Oddgeir Friborg and Knut Waterloo},
keywords = {treatment outcome, computer-assisted therapy, cognitive behavior therapy, depression, primary health care, Bayesian analysis},
abstract = {Background
Several studies have demonstrated the effect of guided Internet-based cognitive behavioral therapy (ICBT) for depression. However, ICBT is not suitable for all depressed patients and there is a considerable level of nonresponse. Research on predictors and moderators of outcome in ICBT is inconclusive.
Objective
This paper explored predictors of response to an intervention combining the Web-based program MoodGYM and face-to-face therapist guidance in a sample of primary care patients with mild to moderate depressive symptoms.
Methods
Participants (N=106) aged between 18 and 65 years were recruited from primary care and randomly allocated to a treatment condition or to a delayed treatment condition. The intervention included the Norwegian version of the MoodGYM program, face-to-face guidance from a psychologist, and reminder emails. In this paper, data from the treatment phase of the 2 groups was merged to increase the sample size (n=82). Outcome was improvement in depressive symptoms during treatment as assessed with the Beck Depression Inventory-II (BDI-II). Predictors included demographic variables, severity variables (eg, number of depressive episodes and pretreatment depression and anxiety severity), cognitive variables (eg, dysfunctional thinking), module completion, and treatment expectancy and motivation. Using Bayesian analysis, predictors of response were explored with a latent-class approach and by analyzing whether predictors affected the slope of response.
Results
A 2-class model distinguished well between responders (74%, 61/82) and nonresponders (26%, 21/82). Our results indicate that having had more depressive episodes, being married or cohabiting, and scoring higher on a measure of life satisfaction had high odds for positively affecting the probability of response. Higher levels of dysfunctional thinking had high odds for a negative effect on the probability of responding. Prediction of the slope of response yielded largely similar results. Bayes factors indicated substantial evidence that being married or cohabiting predicted a more positive treatment response. The effects of life satisfaction and number of depressive episodes were more uncertain. There was substantial evidence that several variables were unrelated to treatment response, including gender, age, and pretreatment symptoms of depression and anxiety.
Conclusions
Treatment response to ICBT with face-to-face guidance may be comparable across varying levels of depressive severity and irrespective of the presence and severity of comorbid anxiety. Being married or cohabiting, reporting higher life satisfaction, and having had more depressive episodes may predict a more favorable response, whereas higher levels of dysfunctional thinking may be a predictor of poorer response. More studies exploring predictors and moderators of Internet-based treatments are needed to inform for whom this treatment is most effective.
Trial Registration
Australian New Zealand Clinical Trials Registry number: ACTRN12610000257066; https://www.anzctr.org.au/trial_view.aspx?id=335255 (Archived by WebCite at http://www.webcitation.org/6GR48iZH4).}
}
@article{RANGANATHAN201958,
title = {Emotion Mining in Social Media Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {58-66},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.160},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313389},
author = {Jaishree Ranganathan and Angelina Tzacheva},
keywords = {Data Mining, Emotion, Microblog, Sentiment Analysis, Twitter},
abstract = {Emotions are known to influence the perception of human beings along with their memory, thinking and imagination. Human perception is important in today’s world in a wide range of factors including but not limited to business, education, art, and music. Microblogging and Social networking sites like Twitter, Facebook are challenging sources of information that allow people to share their feelings and thoughts on a daily basis. In this paper we propose an approach to automatically detect emotions on Twitter messages that explores characteristics of the tweets and the writer’s emotion using Support Vector Machine LibLinear model and achieve 98% accuracy. Emotion mining gained attraction in the field of computer science due to the vast variety of systems that can be developed and promising applications like remote health care system, customer care services, smart phones that react based on users’s emotion, vehicles that sense emotion of the driver. These emotions help understand the current state of user. In order to perform suitable actions or provide suggestions on how user’s can enhance their feeling for a better healthy life-style we use actionable recommendations. In this work we extract action rules with respect to the user emotions that help provide suggestions for user’s.}
}
@article{XIAO1996292,
title = {On the effects of ampoule tilting during vertical Bridgman growth: three-dimensional computations via a massively parallel, finite element method},
journal = {Journal of Crystal Growth},
volume = {167},
number = {1},
pages = {292-304},
year = {1996},
issn = {0022-0248},
doi = {https://doi.org/10.1016/0022-0248(96)00231-X},
url = {https://www.sciencedirect.com/science/article/pii/002202489600231X},
author = {Qiang Xiao and Satheesh Kuppurao and Andrew Yeckel and Jeffrey J. Derby},
abstract = {Three-dimensional convection and asymmetric radial segregation, caused by ampoule tilting during the vertical Bridgman growth, are analyzed using a novel, massively parallel, finite element model. The growth of cadmium telluride with a dilute dopant is considered and found to be surprisingly sensitive to the amount of tilt - as little as one degree of misalignment of the ampoule axis from the gravitational vector produces a significant three-dimensional flow and a concomitant skewing of the dopant distribution along the surface of the growing solid. This indicates the need for precise ampoule axis alignment to ensure process reproducibility. Analysis of the dopant distribution along the solid-liquid interface of the tilted system reveals a surface region more uniform in dopant concentration than any corresponding region of the interface of the perfectly aligned system. For systems in which low radial segregation is very important, growth with an intentional axis offset may be beneficial.}
}
@article{CORPONI2021,
title = {Frontal lobes dysfunction across clinical clusters of acute schizophrenia},
journal = {Revista de Psiquiatría y Salud Mental},
year = {2021},
issn = {1888-9891},
doi = {https://doi.org/10.1016/j.rpsm.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1888989121001324},
author = {Filippo Corponi and Yana Zorkina and Daniel Stahl and Andrea Murru and Eduard Vieta and Alessandro Serretti and Аnna Morozova and Alexander Reznik and Georgiy Kostyuk and Vladimir Pavlovich Chekhonin},
keywords = {Schizophrenia, Frontal lobe, Precision medicine, Cluster analysis, Machine learning},
abstract = {Introduction
Schizophrenia is a clinical construct comprising manifold phenotypes underlying heterogeneous biological underpinnings. The Positive and Negative Syndrome Scale (PANSS) represents the standard tool in the clinical characterization of patients affected by schizophrenia, allowing to detect different clinical profiles within the disorder. Frontal lobes are a key area of brain dysfunction in schizophrenia. We investigated whether different clinical profiles in acute schizophrenia show differences in frontal lobes dysfunction or not.
Methods
We defined PANSS-derived principal components in a sample of 516 acute patients. These components were used as clustering variables in a finite-mixture model. Frontal lobe impairment, measured with the frontal assessment battery (FAB) score, was adjusted for disease duration and compared across the clinical clusters with ANCOVA. A supervised-learning approach was then implemented to reveal most informative PANSS items.
Results
A three-cluster solution emerged: a first profile with high-moderate expression for the positive and excitability/hostility component; a second profile scoring high on depression/anxiety and low on other components; a third profile, comprising the majority of the study population (74%), with a heavy affection on the negative-disorganization dimensions. After controlling for disease duration, frontal lobe impairment significantly differed across the aforementioned clusters, with the third cluster being the most affected. Two PANSS items presented the highest predictive value for FAB total score.
Conclusions
Among negative and disorganization symptoms, “difficulty in abstract thinking” and “lack of spontaneity/flow in conversation” are specifically mapped to higher levels of frontal lobes dysfunction, hinting at similar features with other neurological disorders involving frontal lobes.}
}
@article{WANG2020256,
title = {Fine-grained neural decoding with distributed word representations},
journal = {Information Sciences},
volume = {507},
pages = {256-272},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519307820},
author = {Shaonan Wang and Jiajun Zhang and Haiyan Wang and Nan Lin and Chengqing Zong},
keywords = {Neural decoding, fMRI word decoding, Word class, Stimuli paradigm, Word embedding models, Informative voxels},
abstract = {fMRI word decoding refers to decode what the human brain is thinking by interpreting functional Magnetic Resonance Imaging (fMRI) scans from people watching or listening to words, representing a sort of mind-reading technology. Existing works decoding words from imaging data have been largely limited to concrete nouns from a relatively small number of semantic categories. Moreover, such studies use different word-stimulus presentation paradigms and different computational models, lacking a comprehensive understanding of the influence of different factors on fMRI word decoding. In this paper, we present a large-scale evaluation of eight word embedding models and their combinations for decoding fine-grained fMRI data associated with three classes of words recorded from three stimulus-presentation paradigms. Specifically, we investigate the following research questions: (1) How does the brain-image decoder perform on different classes of words? (2) How does the brain-image decoder perform in different stimulus-presentation paradigms? (3) How well does each word embedding model allow us to decode neural activation patterns in the human brain? Furthermore, we analyze the most informative voxels associated with different classes of words, stimulus-presentation paradigms and word embedding models to explore their neural basis. The results have shown the following: (1) Different word classes can be decoded most effectively with different word embedding models. Concrete nouns and verbs are more easily distinguished than abstract nouns and verbs. (2) Among the three stimulus-presentation paradigms (picture, sentence and word clouds), the picture paradigm achieves the highest decoding accuracy, followed by the sentence paradigm. (3) Among the eight word embedding models, the model that encodes visual information obtains the best performance, followed by models that encode textual and contextual information. (4) Compared to concrete nouns, which activate mostly vision-related brain regions, abstract nouns activate broader brain regions such as the visual, language and default-mode networks. Moreover, both the picture paradigm and the model that encodes visual information have stronger associations with vision-related brain regions than other paradigms and word embedding models, respectively.}
}
@incollection{ZIEGLERRODRIGUEZ2025169,
title = {Chapter 6 - Life cycle assessment of constructed wetlands: measuring their contribution to sustainable development},
editor = {Asheesh Kumar Yadav and Jan Vymazal and Yaqian Zhao and Pratiksha Srivastava},
booktitle = {Emerging Developments in Constructed Wetlands},
publisher = {Elsevier},
pages = {169-193},
year = {2025},
isbn = {978-0-443-14078-5},
doi = {https://doi.org/10.1016/B978-0-443-14078-5.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443140785000064},
author = {Kurt Ziegler-Rodriguez and Marianna Garfí},
keywords = {Sustainable development, life cycle assessment, constructed wetland, biological waste treatment, water management},
abstract = {Life cycle thinking has led to the development of a series of methodologies that evaluate the sustainability of any process, product, or activity, considering the three aspects of sustainable development: the environmental, economic, and social pillars. These methodologies called the (Environmental) Life Cycle Assessment, the Social Life Cycle Assessment and the Life Cycle Costing, have the peculiarity to consider the whole life cycle of a product or process, from the extraction of raw materials to their end of life. At the same time, sustainable development has led to the strengthening of disciplines and novel technologies such as circular bioeconomy, industrial ecology, and nature-based solutions. In this context, constructed wetlands have been gaining popularity since they are a low-cost alternative for urban and industrial wastewater treatment in small communities. The performed life cycle assessments of these technologies have shown that, regardless of the model, configuration, or type of waste treated, they have low environmental impacts compared with conventional solutions (e.g., activated sludge system) due to low energy requirements, no chemicals consumption, and avoidance of off-site management and transportation practices. In terms of costs, constructed wetlands can drastically reduce the costs associated with wastewater treatment and management. However, more efforts should be made in order to define the social benefits of this technology (e.g., local employment generation, landscape improvement) and the quality of the recovered resources (e.g., treated water, fertilizer).}
}
@article{PISTIKOPOULOS2021107252,
title = {Process systems engineering – The generation next?},
journal = {Computers & Chemical Engineering},
volume = {147},
pages = {107252},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107252},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421000302},
author = {E N Pistikopoulos and Ana Barbosa-Povoa and Jay H Lee and Ruth Misener and Alexander Mitsos and G V Reklaitis and V Venkatasubramanian and Fengqi You and Rafiqul Gani},
keywords = {Process systems engineering, Synthesis-design, Optimization, Control, Modelling, Supply chain},
abstract = {Process Systems Engineering (PSE) is the scientific discipline of integrating scales and components describing the behavior of a physicochemical system, via mathematical modelling, data analytics, design, optimization and control. PSE provides the ‘glue’ within scientific chemical engineering, and offers a scientific basis and computational tools towards addressing contemporary and future challenges such as in energy, environment, the ‘industry of tomorrow’ and sustainability. This perspective article offers a guide towards the next generation of PSE developments by looking at its history, core competencies, current status and ongoing trends.}
}
@article{AVEN201633,
title = {On the use of conservatism in risk assessments},
journal = {Reliability Engineering & System Safety},
volume = {146},
pages = {33-38},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015002938},
author = {Terje Aven},
keywords = {Conservatism, Risk assessments, Knowledge},
abstract = {It is common to use conservatism in risk assessments, replacing uncertain quantities with values that lead to a higher level of risk. It is argued that the approach represents a practical method for dealing with uncertainties and lack of knowledge in risk assessment. If the computed probabilities meet the pre-defined criteria with the conservative quantities, there is strong support for the “real risk” to meet these criteria. In this paper we look more closely into this practice, the main aims being to clarify what it actually means and what the implications are, as well as providing some recommendations. The paper concludes that conservatism should be avoided in risk assessments – “best judgements” should be the ruling thinking, to allow for meaningful comparisons of options. By incorporating sensitivity analyses and strength of knowledge judgements for the background knowledge on which the assigned probabilities are based, the robustness of the conclusions can be more adequately assessed.}
}
@article{BERNABEI2023100172,
title = {Students’ use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100172},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100172},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000516},
author = {Margherita Bernabei and Silvia Colabianchi and Andrea Falegnami and Francesco Costantino},
keywords = {LLM, ChatGPT, Higher education, Essay generation},
abstract = {The accessibility of advanced Artificial Intelligence-based tools, like ChatGPT, has made Large Language Models (LLMs) readily available to students. These LLMs can generate original written content to assist students in their academic assessments. With the rapid adoption of LLMs, exemplified by the popularity of OpenAI's ChatGPT, there is a growing need to explore their application in education. Few studies examine students' use of LLMs as learning tools. This paper focuses on the application of ChatGPT in engineering higher education through an in-depth case study. It investigates whether engineering students can generate high-quality university essays with LLMs assistance, whether existing LLMs identification systems can detect essays produced with LLMs, and how students perceive the usefulness and acceptance of LLMs in learning. The research adopts a deductive/inductive approach, combining conceptualization and empirical evidence analysis. The study involves mechanical and management engineering students, who compose essays using LLMs. The essay assessment showed good results, but some recommendations emerged for teachers and students. Thirteen LLMs detectors were tested without achieving satisfactory results, suggesting to avoid LLMs ban. In addition, students were administered a questionnaire based on constructs and items that follow the technology acceptance models available in the literature. The results contribute to qualitative evidence by highlighting possible future research and educational practices.}
}
@article{GAN2021101212,
title = {Translating novel HPC techniques into efficient geoscience solutions},
journal = {Journal of Computational Science},
volume = {52},
pages = {101212},
year = {2021},
note = {Case Studies in Translational Computer Science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101212},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320305135},
author = {Lin Gan and Haohuan Fu and Guangwen Yang},
keywords = {Computational geoscience application, Numerical simulation, High performance computing, Translational computer science},
abstract = {Computational geoscience is an established field for better understanding and protecting our planet. It covers a wide range of different fields that are closely related to Earth systems. As a popular research area that largely relies on high performance computing, the efficient translation of novel techniques from computer science to practical geoscience solutions has emerged as an important and challenging problem. Based on a series of efforts in conducting interdisciplinary research in computer science and geoscience, this paper summarizes the measures we have taken and the lessons we have learned to successfully translate selected computational laboratory innovations into practical solutions.}
}
@article{HAREL201758,
title = {Field-based hypotheses on advancing standards for mathematical practice},
journal = {The Journal of Mathematical Behavior},
volume = {46},
pages = {58-68},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300457},
author = {Guershon Harel},
keywords = {Common Core State Standards in Mathematics (CCSSM), Standards for mathematical practice},
abstract = {The Common Core State Standards in Mathematics (CCSSM, 2010) are organized around two types of standards: the standards for mathematical content and standards for mathematical practice. The central goal of this paper is to present cognitive and instructional analyses of standards for mathematical practice through a discussion of field-based activities with inservice secondary mathematics teachers and students. A potential value of the study is that it provides researchers with specific field-based hypotheses on advancing standards for mathematical practice.}
}
@article{WANG2021102528,
title = {Automatic diagnosis of ECG disease based on intelligent simulation modeling},
journal = {Biomedical Signal Processing and Control},
volume = {67},
pages = {102528},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102528},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421001257},
author = {Xu Wang and Runchuan Li and Shuhong Wang and Shengya Shen and Wenzhi Zhang and Bing Zhou and Zongmin Wang},
keywords = {Intelligent simulation modeling, Rule, ECG diseases, Diagnosis},
abstract = {In order to better assist doctors in diagnosing cardiovascular diseases, a set of end-to-end automatic diagnosis algorithms for ECG diseases based on intelligent simulation modeling are proposed. Firstly, wavelet transform and threshold method are used to denoise the ECG signal and locate the waveform in this paper. Secondly, waveform features are extracted. Finally, the rule method is used to convert the doctors’ thinking of diagnosing the disease into a description of the ECG characteristics of the disease to diagnose the ECG disease, and the algorithm is verified on the public database CCDD and the private data all-in-one machine data. The results show that this method is not inferior to the deep learning method. Now 11 types of diseases and 10 types of rhythm can be diagnosed.}
}
@article{TEIXEIRADUARTE2022112513,
title = {Review on layout optimization strategies of offshore parks for wave energy converters},
journal = {Renewable and Sustainable Energy Reviews},
volume = {163},
pages = {112513},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112513},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122004178},
author = {Felipe Teixeira-Duarte and Daniel Clemente and Gianmaria Giannini and Paulo Rosa-Santos and Francisco Taveira-Pinto},
keywords = {Computational intelligence techniques, Wave energy, Renewable energy, Layout optimization, Offshore parks, Energy parks, WEC arrays},
abstract = {Layout optimization of wave energy offshore parks is a challenging task, as it encompasses various design objectives and constraints attributed to the complex hydrodynamic interactions. The wave energy converter (WEC) park performance is affected by local environment and device characteristics. To solve this challenge, advanced numerical algorithms, including artificial intelligence, have been applied to a wide range of case studies. Nevertheless, this process remains incomplete, which keeps it as a pertinent research topic in the field of WEC development. The present paper provides an overview of the current state and research trends of offshore WEC park layout optimization. To analyze the state-of-the-art, the paper targets the last decades’ research on this topic, summarizing the studies, addressing the optimization objective and the employed methods and separating them according to the corresponding technique. The review showed that the results strongly depend on the methodologies applied. Furthermore, a preferential use of computational intelligence techniques has been observed in recent years.}
}
@article{SHIBATA2021436,
title = {Sensitivity – Local index to control chaoticity or gradient globally –},
journal = {Neural Networks},
volume = {143},
pages = {436-451},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002471},
author = {Katsunari Shibata and Takuya Ejima and Yuki Tokumaru and Toshitaka Matsuki},
keywords = {Sensitivity, Sensitivity adjustment learning (SAL), Edge of chaos, Recurrent neural network (RNN), Deep feedforward neural network (DFNN), Vanishing gradient problem},
abstract = {Here, we introduce a fully local index named “sensitivity” for each neuron to control chaoticity or gradient globally in a neural network (NN). We also propose a learning method to adjust it named “sensitivity adjustment learning (SAL)”. The index is the gradient magnitude of its output with respect to its inputs. By adjusting its time average to 1.0 in each neuron, information transmission in the neuron changes to be moderate without shrinking or expanding for both forward and backward computations. That results in moderate information transmission through a layer of neurons when the weights and inputs are random. Therefore, SAL can control the chaoticity of the network dynamics in a recurrent NN (RNN). It can also solve the vanishing gradient problem in error backpropagation (BP) learning in a deep feedforward NN or an RNN. We demonstrate that when applying SAL to an RNN with small and random initial weights, log-sensitivity, which is the logarithm of RMS (root mean square) sensitivity over all the neurons, is equivalent to the maximum Lyapunov exponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP through time) to avoid the vanishing gradient problem in a 300-layer NN or an RNN that learns a problem with a lag of 300 steps between the first input and the output. Compared with manually fine-tuning the spectral radius of the weight matrix before learning, SAL’s continuous nonlinear learning nature prevents loss of sensitivities during learning, resulting in a significant improvement in learning performance.}
}
@incollection{ROCAVERT202065,
title = {Arts Bias},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {65-68},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23612-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245236122},
author = {Carla Rocavert},
keywords = {Algorithm, Arts, Bias, Creativity, Capitalism, Elite, Neoliberalism, Permanence, Technology, Utility},
abstract = {This entry posits that current debates around ‘arts bias’ are indicative of evolving definitions of creativity. It discusses themes of utility and permanence to illuminate tensions between historical conceptions of artistic creativity and newer fields, especially those which are driving the global economy toward an increasingly technologically-oriented paradigm under neoliberal capitalism. The arrival of computational creativity and the practice of applying algorithmic data technologies to artmaking are discussed.}
}
@article{CHRISTENSEN2025102467,
title = {perms: Likelihood-free estimation of marginal likelihoods for binary response data in Python and R},
journal = {Journal of Computational Science},
volume = {84},
pages = {102467},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2024.102467},
url = {https://www.sciencedirect.com/science/article/pii/S1877750324002606},
author = {Dennis Christensen and Per August Jarval Moen},
keywords = {Binary classification, Bioassay, Marginal likelihood estimation, Permutation counting},
abstract = {In Bayesian statistics, the marginal likelihood (ML) is the key ingredient needed for model comparison and model averaging. Unfortunately, estimating MLs accurately is notoriously difficult, especially for models where posterior simulation is not possible. Recently, the idea of permutation counting was introduced, which provides an estimator which can accurately estimate MLs of models for exchangeable binary responses. Such data arise in a multitude of statistical problems, including binary classification, bioassay and sensitivity testing. Permutation counting is entirely likelihood-free and works for any model from which a random sample can be generated, including nonparametric models. Here we present perms, a package implementing permutation counting. Following optimisation efforts, perms is computationally efficient and can handle large data problems. It is available as both an R package and a Python library. A broad gallery of examples illustrating its usage is provided, which includes both standard parametric binary classification and novel applications of nonparametric models, such as changepoint analysis. We also cover the details of the implementation of perms and illustrate its computational speed via a simple simulation study.}
}
@article{ROBINSON2021,
title = {Development of the Organonitrogen Biodegradation Database: Teaching Bioinformatics and Collaborative Skills to Undergraduates during a Pandemic},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {1},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v22i1.2351},
url = {https://www.sciencedirect.com/science/article/pii/S1935787721000745},
author = {Serina L. Robinson and Troy Biernath and Caleb Rosenthal and Dean Young and Lawrence P. Wackett and Betsy M. Martinez-Vaz},
abstract = {Physical distancing and inaccessibility to laboratory facilities created an opportunity to transition undergraduate research experiences to remote, digital platforms, adding another level of pedagogy to their training. Basic bioinformatics skills together with critical analysis of scientific literature are essential for addressing research questions in modern biology.
ABSTRACT
Physical distancing and inaccessibility to laboratory facilities created an opportunity to transition undergraduate research experiences to remote, digital platforms, adding another level of pedagogy to their training. Basic bioinformatics skills together with critical analysis of scientific literature are essential for addressing research questions in modern biology. The work presented here describes a fully online, collaborative research experience created to allow undergraduate students to learn those skills. The research experience was focused on the development and implementation of the Organonitrogen Biodegradation Database (ONDB, z.umn.edu/ondb). The ONDB was developed to catalog information about the cost, chemical properties, and biodegradation potential of commonly used organonitrogen compounds. A cross-institutional team of undergraduate researchers worked in collaboration with two faculty members and a postdoctoral fellow to develop the database. Students carried out extensive online literature searches and used a biodegradation prediction website to research and represent the microbial catabolism of different organonitrogen compounds. Participants employed computational tools such as R, Shiny, and flexdashboard to construct the database pages and interactive web interface for the ONDB. Worksheets and forms were created to encourage other students and researchers to gather information about organonitrogen compounds and expand the database. Student progress was evaluated through biweekly project meetings, presentations, and a final reflection. The ONDB undergraduate research experience provided a platform for students to learn bioinformatics skills while simultaneously developing a teaching and research tool for others.}
}
@article{EBEL2024104612,
title = {Cooperative object transportation with differential-drive mobile robots: Control and experimentation},
journal = {Robotics and Autonomous Systems},
volume = {173},
pages = {104612},
year = {2024},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104612},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023002518},
author = {Henrik Ebel and Mario Rosenfelder and Peter Eberhard},
keywords = {Cooperative manipulation, Non-prehensile manipulation, Robotic networks, Distributed optimization, Non-holonomic robots, Hardware validation},
abstract = {Non-prehensile cooperative object transportation is a challenging model problem for distributed control and organization methods but also has practical applications. Therefore, it is widely studied in distributed robotics research. This paper describes and evaluates a novel transportation scheme for differential-drive mobile robots that is, to the authors’ best knowledge, the most versatile scheme of its kind successfully evaluated with real-world hardware. The proposed scheme can conceptually deal with any number of robots and arbitrary polygonal objects, including non-convex ones, without having to retune, retrain, or reconfigure any of the control parameters between different scenarios. This is achieved by splitting the task into a formation control and a formation finding task, both of which are tackled with model-based approaches using distributed optimization. Formation control and formation finding are complicated by the robots’ non-holonomic kinematic constraints. Therefore, a tailored distributed model predictive controller is used for formation control. Finding formations relies on a multibody-dynamics representation of the robots-object system to properly account for contact and non-holonomic constraints. Due to these measures, the transportation scheme achieves a very satisfactory performance and dexterity in real-world hardware experiments utilizing network communication and distributed computation.}
}
@article{KWON2019109608,
title = {Towards codification of thunderstorm/downburst using gust front factor: Model-based and data-driven perspectives},
journal = {Engineering Structures},
volume = {199},
pages = {109608},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109608},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619306315},
author = {Dae Kun Kwon and Ahsan Kareem},
keywords = {Wind loads, Nonstationary process, Gust front, Gust front factor, Downburst, Thunderstorm, Codes and standards},
abstract = {Winds associated with gust fronts originating from a thunderstorm/downburst exhibit rapid changes during a short time period which may be accompanied by changes in direction. For several decades, a number of studies have been focused on identifying the characteristics of such nonstationary gust front winds in a variety of manners such as experimental/numerical methods and full-scale measurements. Yet, beginning the dialogue on any guidelines for design practice has thus far not evolved, in part due to a limited consensus on such characteristics among studies in conjunction with paucity of available data needed for vetting and corroborating, which is further impacted by the presence of nonstationarity. In an effort to establishing a new design procedure for this type of wind load effect on structures, the gust front factor (GFF) framework has been proposed by authors that encapsulates both the kinematic and dynamic features of gust front induced wind effects on structures, which distinguish themselves from those experienced in conventional boundary layer flows. This study revisits the gust front factor framework seeking to take the next step toward a possible initial framework for codification of gust front winds from model-based and data-driven perspectives. A modular and extensible web-enabled framework to estimate gust front related wind load effects is envisaged to rationally and holistically quantify design loads. This would promote design practice to enhance disaster resilience of the built environment. In this context, a closed-form expression concerning nonstationary fluctuations for a case of a long pulse duration is derived to facilitate rapid evaluation of nonstationary turbulence effects. A preliminary uncertainty analysis is also carried out to assess the influence of uncertainties associated with the load effects of gust front winds and the reliability of GFF. In addition, a comparison of the model-based gust front factor with a recently introduced thunderstorm response spectrum technique to assess their relative performance is carried out. In view of the lessons learned from the history of the gust loading factor on codes and standards, a possible living codification concept through a learning and updating invoking the emerging “Design Thinking” approach is discussed.}
}
@article{ROOTESMURDY2024100987,
title = {Cortical similarities in psychiatric and mood disorders identified in federated VBM analysis via COINSTAC},
journal = {Patterns},
volume = {5},
number = {7},
pages = {100987},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.100987},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924001028},
author = {Kelly Rootes-Murdy and Sandeep Panta and Ross Kelly and Javier Romero and Yann Quidé and Murray J. Cairns and Carmel Loughland and Vaughan J. Carr and Stanley V. Catts and Assen Jablensky and Melissa J. Green and Frans Henskens and Dylan Kiltschewskij and Patricia T. Michie and Bryan Mowry and Christos Pantelis and Paul E. Rasser and William R. Reay and Ulrich Schall and Rodney J. Scott and Oliver J. Watkeys and Gloria Roberts and Philip B. Mitchell and Janice M. Fullerton and Bronwyn J. Overs and Masataka Kikuchi and Ryota Hashimoto and Junya Matsumoto and Masaki Fukunaga and Perminder S. Sachdev and Henry Brodaty and Wei Wen and Jiyang Jiang and Negar Fani and Timothy D. Ely and Adriana Lorio and Jennifer S. Stevens and Kerry Ressler and Tanja Jovanovic and Sanne J.H. {van Rooij} and Lydia M. Federmann and Christiane Jockwitz and Alexander Teumer and Andreas J. Forstner and Svenja Caspers and Sven Cichon and Sergey M. Plis and Anand D. Sarwate and Vince D. Calhoun},
keywords = {transdiagnostic, federated analysis, COINSTAC, psychiatric disorders, regression, mood disorders, decentralized, gray matter, PTSD, mild cognitive impairment},
abstract = {Summary
Structural neuroimaging studies have identified a combination of shared and disorder-specific patterns of gray matter (GM) deficits across psychiatric disorders. Pooling large data allows for examination of a possible common neuroanatomical basis that may identify a certain vulnerability for mental illness. Large-scale collaborative research is already facilitated by data repositories, institutionally supported databases, and data archives. However, these data-sharing methodologies can suffer from significant barriers. Federated approaches augment these approaches by enabling access or more sophisticated, shareable and scaled-up analyses of large-scale data. We examined GM alterations using Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation, an open-source, decentralized analysis application. Through federated analysis of eight sites, we identified significant overlap in the GM patterns (n = 4,102) of individuals with schizophrenia, major depressive disorder, and autism spectrum disorder. These results show cortical and subcortical regions that may indicate a shared vulnerability to psychiatric disorders.}
}
@article{KERREN2025,
title = {Exploring the role of dimensionality transformation in episodic memory},
journal = {Trends in Cognitive Sciences},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2025.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S136466132500021X},
author = {Casper Kerrén and Daniel Reznik and Christian F. Doeller and Benjamin J. Griffiths},
keywords = {episodic memory, dimensionality reduction and dimensionality expansion, neural oscillations, corticohippocampal connectivity, neural representations},
abstract = {Episodic memory must accomplish two adversarial goals: encoding and storing a multitude of experiences without exceeding the finite neuronal structure of the brain, and recalling memories in vivid detail. Dimensionality reduction and expansion (‘dimensionality transformation’) enable the brain to meet these demands. Reduction compresses sensory input into simplified, storable codes, while expansion reconstructs vivid details. Although these processes are essential to memory, their neural mechanisms for episodic memory remain unclear. Drawing on recent insights from cognitive psychology, systems neuroscience, and neuroanatomy, we propose two accounts of how dimensionality transformation occurs in the brain: structurally (via corticohippocampal pathways) and functionally (through neural oscillations). By examining cross-species evidence, we highlight neural mechanisms that may support episodic memory and identify crucial questions for future research.}
}
@article{NOVIANTRI2023446,
title = {Unsteady State Temperature Distribution Inside House Based on Slope Roof},
journal = {Procedia Computer Science},
volume = {227},
pages = {446-453},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.545},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301712X},
author = {Viska Noviantri and Agus Diemas Prayoga and Denny Pratama},
keywords = {unsteady state heat equation, ghost point, finite difference method, slope roof},
abstract = {The house is a building that serves as a place to gather with family and also to get comfort. The house is designed so the occupants can feel aesthetically and functionally comfortable. It will be interesting to discuss the temperature inside the house as one of the comfort factors. The study aims to analyze the temperature conditions inside the house, which are influenced by the slope of the roof. A two-dimensional unsteady state heat equation represents this temperature since the temperature distribution satisfies the heat transfer concept and changes over time. This governing equation will be approximated by the forward time center space scheme as one finite difference method and completed by the quadratic ghost point method since the house domain is an irregular shape. Van Neumann criteria are applied here to analyze the stability of the computational approach for this numerical scheme. Furthermore, these schemes are implemented in MATLAB application to quantitatively and visually display temperature dynamics. Some simulations completed these approximations to see temperature variations over time. The results show that the bigger slope causes the average temperature to be cooler. In other words, the temperature in the house will be more comfortable when the roof slope gets bigger.}
}
@article{VALLVERDU20146,
title = {What are Simulations? An Epistemological Approach},
journal = {Procedia Technology},
volume = {13},
pages = {6-15},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000139},
author = {Jordi Vallverdú},
keywords = {model, computer, simulation, epistemology, representation},
abstract = {Contemporary sciences use a wide and diverse range of computational simulations, including in the areas of aeronautics, chemistry, bioinformatics, social sciences, AI, the physics of elementary particles and most other scientific fields. A simulation is a mathematical model that describes or creates computationally a system process. Simulations are our best cognitive representation of complex reality, that is, our deepest conception of what reality is. In this paper we defend that a simulation is equivalent epistemologically and ontologically with all other types of cognitive models of elements of reality. Therefore, simulations cannot be considered secondary nor weak instruments to approach to the reality analysis.}
}
@article{XIE2024e34960,
title = {Enhanced nonlinear active noise control: A novel approach using brain storm optimization algorithm},
journal = {Heliyon},
volume = {10},
number = {15},
pages = {e34960},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e34960},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024109917},
author = {Jiangchun Xie and Jianmin Ma},
keywords = {Active noise control, Brain storm optimization (BSO) algorithm, Filtered-x least mean squares (FxLMS) algorithm, Nonlinear noise reduction extended Kalman Filter (EKF), Noise reduction performance, Multi-frequency noise},
abstract = {Active Noise Control (ANC) systems play a crucial role in reducing unwanted noise in various settings. Traditional ANC methods, like the Filtered-x Least Mean Squares (FxLMS) algorithm, are effective in linear noise scenarios. However, they often struggle with more nonlinear and complex noise patterns. This paper introduces a novel approach using the brain storm optimization (BSO) algorithm in nonlinear ANC systems, which represents a significant departure from conventional techniques. The BSO algorithm, inspired by human brainstorming processes, excels in addressing the complexities of nonlinear noise by incorporating principles, such as delayed evaluation, free imagination, quantity and quality, and comprehensive improvement. By combining the BSO algorithm with an Extended Kalman Filter (EKF), a new ANC system is proposed that can adapt to a wide range of noise types with improved speed and accuracy. Experimental results showcase the superior performance of the BSO algorithm, achieving an impressive noise reduction of up to 48 dB (dB) in a 500Hz sinusoidal noise scenario, with a convergence time as fast as 0.01 s, outperforming the FxLMS algorithm by a significant margin. Moreover, in complex environments with multi-frequency and random noise, the BSO algorithm consistently demonstrates better noise reduction and quicker convergence, reducing noise levels by up to 27 dB within 0.001 s. The innovative use of the BSO algorithm in ANC systems not only enhances noise reduction capabilities, especially for nonlinear and complex noise signals, but also improves convergence times, paving the way for future advancements in ANC technologies.}
}
@article{NOURANI2015891,
title = {Predictive Control, Competitive Model Business Planning, and Innovation ERP},
journal = {Procedia Computer Science},
volume = {65},
pages = {891-900},
year = {2015},
note = {International Conference on Communications, management, and Information technology (ICCMIT'2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915028781},
author = {Cyrus F. Nourani and Codrina Lauth},
keywords = {Competitive Models, Innovation Management, ERP. Multiplayer Games, Game Trees Computing, Predictive Modeling, Planning, Competitive Models, Dynamic Programming},
abstract = {New optimality principles are put forth based on competitive model business planning. A Generalized MinMax local optimum dynamic programming algorithm is presented and applied to business model computing where predictive techniques can determine local optima. Based on a systems model an enterprise is not viewed as the sum of its component elements, but the product of their interactions. The paper starts with introducing a systems approach to business modeling. A competitive business modeling technique, based on the author's planning techniques is applied. Systemic decisions are based on common organizational goals, and as such business planning and resource assignments should strive to satisfy higher organizational goals. It is critical to understand how different decisions affect and influence one another. Here, a business planning example is presented where systems thinking technique, using Causal Loops, are applied to complex management decisions. Predictive modeling specifics are briefed. A preliminary optimal game modeling technique is presented in brief with applications to innovation and R&D management. Conducting gap and risk analysis can assist with this process. Example application areas to e-commerce with management simulation models are examined.}
}
@article{RITCHIE2012649,
title = {Styles for philosophers of science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {43},
number = {4},
pages = {649-656},
year = {2012},
note = {Part Special Issue: Styles of Thinking},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112000490},
author = {Jack Ritchie},
keywords = {Ian Hacking, Styles of Thinking, Realism, Self-authentication},
abstract = {In this paper I discuss the bearing of Hacking’s ideas about Scientific Styles on traditional debates in the philosophy of science concerning rationality and realism. I argue that a kind of deflationary position with regard to realism debates is a natural consequence of Hacking’s claim that styles are self-authenticating. I then go on to argue, using an example of van Fraassen’s, that Hacking should allow a methodological role for realism debates and hence they are not idle, as he has claimed, although their resolution may not be important.}
}
@incollection{WILLIAMS2020341,
title = {Chapter 17 - Begin with the human: Designing for safety and trustworthiness in cyber-physical systems},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {341-357},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00017-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000171},
author = {Elizabeth T. Williams and Ehsan Nabavi and Genevieve Bell and Caitlin M. Bentley and Katherine A. Daniell and Noel Derwort and Zac Hatfield-Dodds and Kobi Leins and Amy K. McLennan},
keywords = {Trust, Safety, Autonomy, Agency, Assurance, Metrics, Interfaces, Human-machine interaction, Cyber-physical systems},
abstract = {Control systems are designed and built to manage and regulate the behavior of other systems. The use of artificial intelligence (AI) in control systems has simultaneously created new opportunities and new challenges in how we create, manage, and govern cyber-physical systems. In this paper, we discuss the challenge of defining and developing a model for contemplating how these systems will potentially learn, evolve, and act without human intervention. We present an analytical framework for thinking about trust and safety in these systems—both key factors for shared context in human-machine teams—and demonstrate its application using an example from history.}
}
@article{LI2024120889,
title = {Hierarchical fuzzy inference based on Bandler-Kohout subproduct},
journal = {Information Sciences},
volume = {677},
pages = {120889},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120889},
url = {https://www.sciencedirect.com/science/article/pii/S002002552400803X},
author = {Dechao Li and Zhisong Liu and Qiannan Guo},
keywords = {Fuzzy implication, Fuzzy inference, Bandler-Kohout subproduct, Hierarchical system, T-norm},
abstract = {Fuzzy inference with the Bandler-Kohout subproduct (BKS) has been successfully applied in many fields such as fuzzy control, artificial intelligence, image processing, data mining, decision-making, prediction, classification and so on. However, one has to face with the rule explosion in these applications. To deal with this problem, hierarchical fuzzy systems with the compositional rule of inference (CRI) method have been constructed by a series of low-dimensional sub fuzzy systems. And it has been proved that hierarchical fuzzy inference method can efficiently restrain the explosion of fuzzy rules. Therefore, in order to increase the computational efficiency of the fuzzy inference based on the BKS when multi-input-single-output (MISO) fuzzy rules are involved, this paper mainly constructs two hierarchical fuzzy inference methods based on the BKS in which the if-then rules are respectively interpreted by fuzzy implications and ML-implications. Moreover, the validity of the two BKS hierarchical fuzzy inferences is studied with the GMP rules. Finally, two examples are employed to illustrate the computational efficiency of our proposed BKS hierarchical inference methods.}
}
@incollection{WARE20081,
title = {Chapter 1 - Visual Queries},
editor = {Colin Ware},
booktitle = {Visual Thinking},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-22},
year = {2008},
isbn = {978-0-12-370896-0},
doi = {https://doi.org/10.1016/B978-0-12-370896-0.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708960000019},
author = {Colin Ware},
abstract = {Publisher Summary
This book about graphic design provides a channel for clear communication that supports visual thinking and acts as an interface to the vast information resources of the modern world. Visual thinking is a process that has the allocation of attention as its very essence. Attention, however, is multifaceted. Making an eye movement is an act of attending. Eye movements are executed to satisfy the need for information and can be thought of as a sequence of visual queries on the visual world. The idea of the visual query is shorthand for what one does when obtaining information either from the world at large or from some kind of information display. Understanding what visual queries are easily executed is a critical skill for the designer. The special skill of designers is not so much skill with drawing or graphic design software, although these are undoubtedly useful, but the talent to analyze a design in terms of its ability to support the visual queries of others. One reason why design is difficult is that the designer already has the knowledge expressed in the design and has seen it develop from inception and therefore cannot see it with fresh eyes. The solution is to be analytic and this is where this book is intended to have value. Effective design should start with a visual task analysis, determine the set of visual queries to be supported by a design, and then use color, form, and space to efficiently serve those queries.}
}
@article{CHANDRA2018306,
title = {New narratives of development work? Making sense of social entrepreneurs’ development narratives across time and economies},
journal = {World Development},
volume = {107},
pages = {306-326},
year = {2018},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2018.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X18300780},
author = {Yanto Chandra},
keywords = {Development narrative, Development, Social enterprise, Social entrepreneur, Computational linguistics},
abstract = {This article views social entrepreneurship as a relatively new model for achieving sustainable development. It also identifies development narratives that social entrepreneurs (SEs) construct to represent and promote their work as an important research gap in development studies. Drawing on the development and narratology literature, and employing computational linguistics (CL) techniques, this article compares the development narratives of 1076 Ashoka SEs across two periods (2009–2013 and 1994–1998) and two economies (developing and developed). CL analyses reveal important themes that characterize the identity, framing and orientations of development SEs across time and economies. The findings demonstrate how SE development narratives i) tend to be more pragmatic and solution-centric, and contain less political ideology than conventional development narratives, ii) combine extant development ideas and models but reframe them in new ways to address contemporary, complex development challenges, and iii) reflect a ‘bottom-up’ approach that encourages local ownership and collaborations with various social and economic sectors to achieve development goals. Overall, this study identifies the increasing importance of SEs in the development industry and reveals new aspects of SEs—their latent political framing, collective-utilitarian identities, and topical areas—that require further research via development narratives.}
}
@article{WANG2025100834,
title = {Toward bridging the gap between machine intelligence and machine wisdom: Dilemmas and conjectures},
journal = {The Innovation},
pages = {100834},
year = {2025},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2025.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2666675825000372},
author = {Rui Wang and Shixuan Liu and Changjun Fan and Guozheng Li and Jincai Huang and Zhong Liu and Gang Zhou},
abstract = {In recent years, artificial intelligence (AI) has achieved tremendous development, akin to a significant leap, similar to progressing from 1 to 100. However, a significant gap still exists between current machine intelligence and human wisdom: machine intelligence is constrained to post hoc inference based on existing data, lacking the ability for genuine exploratory innovation and possessing no prospective reasoning inherent to human wisdom. Drawing inspiration from human wisdom, this article presents conjectures for overcoming the four dilemmas faced by machine intelligence: neglect of silicon-based cognition, lack of artistry, pitfall of perfectionism, and obsession with uniformity. These conjectures aim to propel machine intelligence toward machine wisdom, achieving a great leap from 1 to i.}
}
@article{AIROLDI2024101864,
title = {The nested relationality of perceived legitimacy: Mapping taste hierarchies with granular digital traces},
journal = {Poetics},
volume = {102},
pages = {101864},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101864},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000032},
author = {Massimo Airoldi},
keywords = {Taste, Cultural hierarchies, Music classification, Youtube, Digital traces},
abstract = {The article has a double purpose. On the one hand, it contributes to theories of cultural legitimacy and classification. Based on data about consumers’ music evaluations, it shows that taste hierarchies are configured as nested and relational classificatory systems. Nested, because rank systems of symbolic value are collectively recognized, reproduced, and negotiated by consumers not only at the level of genres, but also at lower, nested levels – e.g., sub-genre, artist, single artwork; relational, because the value attributed to music by consumers is ordinarily assessed and constructed through analogies and comparisons, and partly depends on the classifier's relative position in the social space. On the other hand, this paper makes a key methodological contribution: by analyzing large amounts of YouTube data through computational methods and in combination with survey data, it illustrates how the granularity of digital traces can advance sociological research on cultural categories, meaning structures and symbolic imaginaries.}
}
@article{YANG2025105265,
title = {Harmony in diversity: Digital literacy research in a multidisciplinary landscape},
journal = {Computers & Education},
volume = {230},
pages = {105265},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105265},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000338},
author = {Feng Yang and Ruiyang Yao and Yunyue Ren and Luxuan Guo},
keywords = {Information literacy, Interdisciplinary projects, Applications in subject areas, Bibliometrics},
abstract = {The advent of the digital era has significantly heightened interest in digital literacy across multidisciplinary backgrounds and has endowed these fields with interdisciplinary and integrative characteristics. In this study, we employed VOSviewer and Bibliometrix for bibliometric and descriptive analyses of digital literacy, and we analyzed 3005 records from the Social Science Citation Index and Science Citation Index. We constructed keyword co-occurrence time networks across five distinct research areas and supplemented them with keyword co-occurrence frequencies to examine similarities and differences between research themes from diverse disciplinary perspectives. The findings of this study indicate that although various fields recognize the significance of digital literacy, different fields prioritize different aspects. As the main field of research, Education & Educational Research focus primarily on the pedagogical practices of cultivating digital literacy, whereas Communication emphasizes the cultivation of digital literacy to address challenges in information dissemination. Information Science & Library Science typically view libraries as central to digital literacy. Moreover, Computer Science research emphasizes the leveraging of technology, whereas Psychology explores the connection between digital literacy and cognitive processes. Analyzing the differences between different disciplines and drawing new ideas from them is of great significance for Education & Educational Research regarding how to deepen digital literacy education content, construct digital literacy education contexts, integrate digital literacy education resources, narrow the digital divide, and promote educational equity in the future.}
}
@incollection{VARGHESE202275,
title = {Chapter 4 - Principles in action},
editor = {George Varghese and Jun Xu},
booktitle = {Network Algorithmics (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {75-107},
year = {2022},
isbn = {978-0-12-809927-8},
doi = {https://doi.org/10.1016/B978-0-12-809927-8.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128099278000099},
author = {George Varghese and Jun Xu},
keywords = {Buffer validation, Dijkstra's algorithm, virtual circuit, transport protocols},
abstract = {Part 2 of the book begins a detailed look at specific network bottlenecks such as data copying and control transfer. While the principles are used in these later chapters, the focus of these later chapters is on the specific bottleneck being examined. Given that network algorithmics is as much a way of thinking as it is a set of techniques, it seems useful to round out Part 1 by seeing the principles in action on small, self contained, but nontrivial network problems. Thus this chapter provides examples of applying the principles in solving specific networking problems. The examples are drawn from real problems, and some of the solutions are used in real products. Unlike subsequent chapters, this chapter is not a collection of new material followed by a set of exercises. Instead, this chapter can be thought of as an extended set of exercises. In Section 4.1 to Section 4.15, 15 problems are motivated and described. Each problem is followed by a hint that suggests specific principles, which is then followed by a solution sketch. There are also a few exercises after each solution. In classes and seminars on the topic of this chapter, the audience enjoyed inventing solutions by themselves (after a few hints were provided), rather than directly seeing the final solutions.}
}
@article{LIN2022104649,
title = {Towards a cross-level understanding of Bayesian inference in the brain},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {137},
pages = {104649},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104649},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422001385},
author = {Chin-Hsuan Sophie Lin and Marta I. Garrido},
keywords = {Probabilistic inference, Bayesian decision theory, Uncertainty, Sampling, Variational approximation, Neural codes, Marr’s level of analysis},
abstract = {Perception emerges from unconscious probabilistic inference, which guides behaviour in our ubiquitously uncertain environment. Bayesian decision theory is a prominent computational model that describes how people make rational decisions using noisy and ambiguous sensory observations. However, critical questions have been raised about the validity of the Bayesian framework in explaining the mental process of inference. Firstly, some natural behaviours deviate from Bayesian optimum. Secondly, the neural mechanisms that support Bayesian computations in the brain are yet to be understood. Taking Marr’s cross level approach, we review the recent progress made in addressing these challenges. We first review studies that combined behavioural paradigms and modelling approaches to explain both optimal and suboptimal behaviours. Next, we evaluate the theoretical advances and the current evidence for ecologically feasible algorithms and neural implementations in the brain, which may enable probabilistic inference. We argue that this cross-level approach is necessary for the worthwhile pursuit to uncover mechanistic accounts of human behaviour.}
}
@incollection{GARDNER2024153,
title = {Chapter 7 - Smart design for cultural heritage},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {153-174},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000057},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Heritage futures, Interaction, Physical computing, Smart cultural heritage, Smart heritage, Smart city, Urban technology},
abstract = {This chapter explores the evolving relationship between cultural heritage and the smart city. The role of smart technologies in a cultural heritage context is often assumed to involve the integration of sensor-based technologies and computational systems to autonomously monitor and manage sites. This chapter expands the definition of smart heritage to include urban technology projects that use sensor-based technologies and physical computing to realize situated and embodied interaction experiences that encourage citizens and visitors to share and co-create cultural heritage experiences with each other. It discusses existing and speculative urban technology projects that combine spatial design and physical computing affordances to create cultural heritage experiences that can be simultaneously attuned to both the past and the future.}
}
@article{THIEDE201536,
title = {Can teachers accurately predict student performance?},
journal = {Teaching and Teacher Education},
volume = {49},
pages = {36-44},
year = {2015},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2015.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X1500013X},
author = {Keith W. Thiede and Jonathan L. Brendefur and Richard D. Osguthorpe and Michele B. Carney and Amanda Bremner and Sam Strother and Steven Oswalt and Jennifer L. Snow and John Sutton and Dan Jesse},
keywords = {Teacher judgment, Judgment accuracy, Mathematics achievement},
abstract = {In two studies, we examined the effect of professional development to improve mathematics instruction on the accuracy of teachers' monitoring of student learning. Study 1 was conducted with 36 teachers participating in three years of professional development. Judgment accuracy was influenced by the fidelity with which what was learned in the professional development. Study 2 was conducted with 64 teachers from 8 schools, which were randomly assigned to receive professional development or serve as a control. Judgment accuracy was greater for teachers receiving professional development than for teachers who did not and teachers were better to predict students' computational skills.}
}
@article{MACLEOD2019101201,
title = {Mesoscopic modeling as a cognitive strategy for handling complex biological systems},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {78},
pages = {101201},
year = {2019},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.101201},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618300839},
author = {Miles MacLeod and Nancy J. Nersessian},
keywords = {Mesoscopic modeling, Middle-out strategy, Systems biology, Model building, Mental modeling, Distributed cognition, Bounded rationality},
abstract = {In this paper we aim to give an analysis and cognitive rationalization of a common practice or strategy of modeling in systems biology known as a middle-out modeling strategy. The strategy in the cases we look at is facilitated through the construction of what can be called mesoscopic models. Many models built in computational systems biology are mesoscopic (midsize) in scale. Such models lack the sufficient fidelity to serve as robust predictors of the behaviors of complex biological systems, one of the signature goals of the field. This puts some pressure on the field to provide reasons for why and how these practices are warranted despite not meeting the stated goals of the field. Using the results of ethnographic study of problem-solving practices in systems biology, we aim to examine the middle-out strategy and mesoscopic modeling in detail and to show that these practices are rational responses to complex problem solving tasks on cognitive grounds in particular. However making this claim requires us to update the standard notion of bounded rationality to take account of how human cognition is coupled to computation in these contexts. Our account fleshes out the idea that has been raised by some philosophers on the “hybrid” nature of computational modeling and simulation. What we call “coupling” both extends modelers’ capacities to handle complex systems, but also produces various cognitive and computational constraints which need to be taken into account in any computational problem solving strategy seeking to maintain insight and control over the models produced.}
}
@article{GAO2023106199,
title = {Letter to the Editor on a shallow water wave equation in Results Phys. 43, 106048 (2022) and its generalization},
journal = {Results in Physics},
volume = {44},
pages = {106199},
year = {2023},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2022.106199},
url = {https://www.sciencedirect.com/science/article/pii/S2211379722008208},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Generalized shallow water wave equation, Similarity reductions, Symbolic computation},
abstract = {Results Phys. 43, 106048 (2022) has amusingly retrieved some solitonic and other analytic solutions for a shallow water wave equation presented there. In this Letter, we suggest that such an equation be moreover studied in line with Results Phys. 43, 106048 (2022). Employing symbolic computation, for a generalization of that equation, with respect to the displacement and velocity of the water, we construct a family of the similarity reductions, to a known ordinary differential equation. Our results depend on the gravitational force and wave height.}
}
@article{HE2023112111,
title = {Predicting thermodynamic stability of magnesium alloys in machine learning},
journal = {Computational Materials Science},
volume = {223},
pages = {112111},
year = {2023},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112111},
url = {https://www.sciencedirect.com/science/article/pii/S0927025623001052},
author = {Xi He and Jinde Liu and Chen Yang and Gang Jiang},
keywords = {Machine learning, DFT, Thermodynamic stability, Magnesium alloy},
abstract = {Density functional theory (DFT) have been widely used to screen thermodynamically stable material; however, its high computational cost limits its use. In this paper, we explore the use of DFT data from high-throughput calculations to create faster machine learning (ML) models that can be used to screen thermodynamically stable magnesium alloy materials. Our methods work by utilizing the kernel ridge regression (KRR) algorithm, as well as Deep Potential Molecular Dynamics (DeePMD) to train ML models for predicting the formation energy of magnesium alloys. The accuracy, stability, and generalization ability of the ML models created under both methods are evaluated in detail. Meanwhile, we have conducted in-depth comparative analysis of the two methods, which concluded that the accuracy of DeePMD model performs better and time efficiency of KRR model has more advantages. The results show that the best performing DeePMD model and KRR model achieve the RMSE of 0.43 meV/atom and 6.80 meV/atom, indicating that our methods provide a reliable idea for obtaining the formation energy of magnesium alloys.}
}
@incollection{GOMILA201219,
title = {3 - The Relevance of Language for Thought: A Continuum of Possibilities},
editor = {Antoni Gomila},
booktitle = {Verbal Minds},
publisher = {Elsevier},
address = {London},
pages = {19-33},
year = {2012},
isbn = {978-0-12-385200-7},
doi = {https://doi.org/10.1016/B978-0-12-385200-7.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852007000035},
author = {Antoni Gomila},
keywords = {Cognitive restructuring, linguistic relativism, Whorf, Vygotsky, thinking for speaking, modular interphase, social scaffolding, categorical effect, perceptual similarity},
abstract = {Publisher Summary
This chapter directs the influence and relevance of language on thoughts. Though there has been no outlined domain on how exactly language effects cognitive architecture, the chapter critically studies five most relevant positions that have attracted defenders, critics since twentieth century to contemporary proposals. It discuses relativism, cognitive restructuring, thinking for speaking, language as modular interface, and language as social scaffolding. Linguistic relativism finds its roots in Romanticism as a reaction to the supremacist attitudes of the “Enlightment thinkers,” who were in the business of establishing hierarchies of languages. Cognitive system, being linguistic, acquires a supplementary system of cognitive representation and processing, which transforms the basic capabilities of system and gives rise to new possibilities. Since language is an interface between the modules it attempts to concede to some cognitive impact without challenging the general cognitive architecture of modules of thought as a successful representational vehicle. Lastly, human minds are socially and culturally constituted minds and therefore linguistic symbols (like other kinds of symbols and other social tools in general) allow the individual to externally discharge cognitive processes through language.}
}
@article{BENARIE1992291,
title = {Air pollution modeling: P. Zannetti, Computational Mechanics Publications, Southampton, U.K. 1990, 444 pp. Price: £59.00},
journal = {Science of The Total Environment},
volume = {119},
pages = {291},
year = {1992},
issn = {0048-9697},
doi = {https://doi.org/10.1016/0048-9697(92)90273-U},
url = {https://www.sciencedirect.com/science/article/pii/004896979290273U},
author = {Michel Benarie}
}
@article{BLACUTT2025586,
title = {Bias toward escape responding during reinforcement learning among those with suicidal ideation},
journal = {Journal of Psychiatric Research},
volume = {181},
pages = {586-595},
year = {2025},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2024.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0022395624007222},
author = {Miguel Blacutt and Caitlin M. O'Loughlin and Brooke A. Ammerman},
keywords = {Suicide, Avoidance, Computational, Impulsivity, Drift diffusion},
abstract = {Self-injury and suicide can be characterized by reward system dysfunction and self-reports of active efforts to escape unpleasant emotional states. Therefore, individuals with histories of suicidal ideation (SI) should exhibit a preference for active escape from unpleasant states, which exceed the effects of impulsive behavior under distress and lack of premeditation. Participants made active (Go) or passive (No-Go) choices in response to stimuli to escape or avoid an unpleasant state in a behavioral task. A drift-diffusion reinforcement learning model was used to estimate latent biases for active escape and avoidance in people with and without SI history. Bayesian logistic regression was used to examine the relationship between escape and avoid bias with SI. Escape bias predicted SI history, whereas avoidance bias did not. Escape bias remained a significant predictor of SI when controlling for negative urgency and lack of premeditation. Those with histories of SI demonstrate a decision-making bias favoring escape from aversive states. This bias remains significant after adjusting for facets of impulsivity linked to hasty decisions under distress and lack of premeditation. A heightened escape response may help clinicians to identify SI risk and develop targeted treatments to attenuate the escape bias.}
}
@article{MOTANIETO2023103965,
title = {The Mexican Carbon Capture and Storage Platform: Construction of a boundary object for bridging the gaps between contexts, actors, and disciplines},
journal = {International Journal of Greenhouse Gas Control},
volume = {129},
pages = {103965},
year = {2023},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2023.103965},
url = {https://www.sciencedirect.com/science/article/pii/S1750583623001354},
author = {J. Mota-Nieto and J.A. Fernández-Reyes and P.M. García-Meneses},
keywords = {CCS/CCUS, Communication platform, Mexico, Boundary objects, Stakeholders},
abstract = {Carbon Capture and Storage (CCS) is a technology identified as a potential solution to mitigate climate change by reducing carbon emissions from large-scale emitters. If CCS is expected to be adopted globally, transparent and reliable data and information must be readily attainable to all stakeholders to support the technology choice and decision-making process. The implementation of CCS requires effective communication and collaboration strategies. Still, materials and communication platforms to inform stakeholders about the potential and contribution of CCS are predominantly accessible in English since ongoing projects are mainly located in English-speaking countries. The Mexican Carbon Capture and Storage platform (MeCCS) was developed as a digital sharing and learning space for national stakeholders to obtain and expand their knowledge about CCS technology in Spanish. It was constructed as a boundary object (BO) to bridge different communities and disciplines, facilitating communication, understanding, and cooperation. The platform includes diverse elements that combine science and art to produce dissemination materials for different audiences to help build critical thinking and inform them about CCS technology. The platform confirmed its capacity to transfer and translate knowledge one year after its launch. It also served to connect different audiences in Mexico and globally and identify further areas of research and CCS-related efforts.}
}
@article{CORTESE2024108397,
title = {Applications of genome-scale metabolic models to the study of human diseases: A systematic review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {256},
pages = {108397},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108397},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724003900},
author = {Nicola Cortese and Anna Procopio and Alessio Merola and Paolo Zaffino and Carlo Cosentino},
keywords = {Genome-scale metabolic networks, Constraint-based modeling, Systems biology, Simulation, Systematic literature review},
abstract = {Background and Objectives:
Genome-scale metabolic networks (GEMs) represent a valuable modeling and computational tool in the broad field of systems biology. Their ability to integrate constraints and high-throughput biological data enables the study of intricate metabolic aspects and processes of different cell types and conditions. The past decade has witnessed an increasing number and variety of applications of GEMs for the study of human diseases, along with a huge effort aimed at the reconstruction, integration and analysis of a high number of organisms. This paper presents a systematic review of the scientific literature, to pursue several important questions about the application of constraint-based modeling in the investigation of human diseases. Hopefully, this paper will provide a useful reference for researchers interested in the application of modeling and computational tools for the investigation of metabolic-related human diseases.
Methods:
This systematic review was conducted according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Elsevier Scopus®, National Library of Medicine PubMed® and Clarivate Web of Science™ databases were enquired, resulting in 566 scientific articles. After applying exclusion and eligibility criteria, a total of 169 papers were selected and individually examined.
Results:
The reviewed papers offer a thorough and up-to-date picture of the latest modeling and computational approaches, based on genome-scale metabolic models, that can be leveraged for the investigation of a large variety of human diseases. The numerous studies have been categorized according to the clinical research area involved in the examined disease. Furthermore, the paper discusses the most typical approaches employed to derive clinically-relevant information using the computational models.
Conclusions:
The number of scientific papers, utilizing GEM-based approaches for the investigation of human diseases, suggests an increasing interest in these types of approaches; hopefully, the present review will represent a useful reference for scientists interested in applying computational modeling approaches to investigate the aetiopathology of human diseases; we also hope that this work will foster the development of novel applications and methods for the discovery of clinically-relevant insights on metabolic-related diseases.}
}
@article{JAGER2021133,
title = {Using agent-based modelling to explore behavioural dynamics affecting our climate},
journal = {Current Opinion in Psychology},
volume = {42},
pages = {133-139},
year = {2021},
note = {Psychology of Climate Change (2021)},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2021.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X21000968},
author = {Wander Jager},
keywords = {Agent based modelling, Social complexity, Computational social science, Social simulation, Artificial societies, Environmental behaviour, Climate, Psychology},
abstract = {This article introduces the methodology of agent-based modelling (ABM), explains how it contributes to understanding the dynamics of climate-relevant behaviour and discusses the challenges to implementing behavioural theory in ABMs. Next, an overview will be given on recent advances in environmentally relevant ABMs. The conclusions address the future of the ABM tool in the context of environmentally relevant behaviour in research and education.}
}
@article{XIE2015262,
title = {Evolutionary sampling: A novel way of machine learning within a probabilistic framework},
journal = {Information Sciences},
volume = {299},
pages = {262-282},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514011384},
author = {Zhenping Xie and Jun Sun and Vasile Palade and Shitong Wang and Yuan Liu},
keywords = {Evolutionary sampling, Support sample model, Monte Carlo Markov chain, Rejection sampling, Online learning, Particle swarm optimization},
abstract = {In many traditional machine learning methods, sampling is only a process of acquiring training data. However, some studies (on sequential Markov chains and particle filters) have demonstrated that sampling can be used for solving some intractable optimization problems in classical learning methods. Along this line of thinking, the relationships between sampling and learning are theoretically exploited in this paper, wherein the key feature of the sampling process is selecting representative samples from original data that can be modeled by a probability distribution. In theory, acquiring reliable samples is not an easy task for an arbitrary probability distribution. Motivated by approaches in evolutionary computation, rejection sampling and function approximation, a novel sampling strategy, called the evolutionary sampling, is proposed in this paper, and a machine learning method, called the evolutionary sampling approach (ESA), is put forward afterwards. Within ESA, a computing model, called the support sample model (SSM), is presented as well and is used to approximate an original density function. Accordingly, a concrete implementation of an evolutionary sampling approach (ESA) is proposed to seek the optimal model parameters of the SSM. Benefiting from the combination of rejection sampling and evolutionary searching, the ESA can theoretically converge to the optimal solution by minimizing the total variation distance, and can do this with high computational efficiency. Moreover, the normalized factor of a density function can be automatically estimated with high precision within the ESA. As a result, the ESA may be suitable for machine learning problems that could be transformed into density function approximation problems within a probabilistic framework. In addition, derived from the rejection sampling strategy, the ESA can also have online learning abilities required by large-scale data stream processing tasks. Theoretical analyses and application studies are carried out in this paper, and the results demonstrate that the ESA, as a novel way of machine learning, has several prominent merits aspired by past researches in machine learning.}
}
@article{WEISSLER19991061,
title = {A Perspective on Standardizing the Predictive Power of Noninvasive Cardiovascular Tests by Likelihood Ratio Computation: 1. Mathematical Principles},
journal = {Mayo Clinic Proceedings},
volume = {74},
number = {11},
pages = {1061-1071},
year = {1999},
issn = {0025-6196},
doi = {https://doi.org/10.4065/74.11.1061},
url = {https://www.sciencedirect.com/science/article/pii/S0025619611650933},
author = {Arnold M. Weissler},
abstract = {The current practice of reporting positive and negative predictive value (PV), sensitivity (Se), and specificity (Sp) as measures of the power of noninvasive cardiovascular tests has significant limitations. A test result's PV and its comparison with other test results are highly dependent on the pretest disease prevalence at which it is determined; the citation of sensitivity and specificity provides no succinct or explicit quantitation of the rule-in and rule-out power of a test. This article presents a rationale for the use of an alternative standard for expressing predictive power in the form of positive and negative likelihood ratios, (+)LR and (-)LR. The likelihood ratios are composite expressions of test power, which incorporate the Se and Sp and their respective complements [(1 - Se) and (1 - Sp)], thus yielding single unambiguous measures of positive and negative predictive power. The likelihood ratios are calculated as follows: (+)LR = Se(l- Sp) and (-)LR = Sp/(I- Se). On analysis of the predictive value equations, the likelihood ratios equal the quotients of the posttest predictive value odds to the pretest prevalence odds for disease and no disease, respectively, as follows: (+)LR = (+)PVOd/POD and (-)LR = (-)PVOn/PON, where (+)PVO d is positive predictive value odds for disease, POD is prevalence odds for disease, (-)PVOn is negative predictive value odds for no disease, and PON is prevalence odds for no disease. Thus, the likelihood ratios are measures of the odds advantage in posttest probability of disease or no disease relative to pretest probability, independent of disease prevalence in the tested population. The quotients of the (+)LR or the (-)LR among test results studied in a common population are direct expressions of their relative predictive power in that population, The likelihood ratio principle is applicable to the evaluation of the predictive power of multiple tests performed in a common population and to estimating predictive power at multiple test thresholds.}
}
@article{BERZ1990473,
title = {Computational aspects of optics design and simulation: COSY INFINITY},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {298},
number = {1},
pages = {473-479},
year = {1990},
issn = {0168-9002},
doi = {https://doi.org/10.1016/0168-9002(90)90649-Q},
url = {https://www.sciencedirect.com/science/article/pii/016890029090649Q},
author = {Martin Berz},
abstract = {The new differential algebraic (DA) techniques allow very efficient treatment and understanding of nonlinear motion in optical systems as well as circular accelerators. To utilize these techniques in their most general way, a powerful software environment is essential. A language with structure elements similar to Pascal was developed. It has object oriented features to allow for a direct utilization of the elementary operations of the DA package. The compiler of the language is written in Fortran 77 to guarantee wide portability. The language was used to write a very general beam optics code, COSY INFINITY. At its lowest level, it allows the computation of the maps of standard beam line elements including fringe fields and system parameters to arbitrary order. The power of the DA approach coupled with an adequate language environment reveals itself in the very limited length of COSY INFINITY of only a few hundred lines. Grouping of elements as well as structures for optimization and study are readily available through the features of the language. Because of the openness of the approach, it offers a lot of power for more advanced purposes. For example, it is very easy to construct new particle optical elements. There are also many ways to efficiently manipulate and analyze the maps.}
}
@article{HICKS2007233,
title = {Lean information management: Understanding and eliminating waste},
journal = {International Journal of Information Management},
