author = {Jenny Nissel and Jiaying Xu and Lihanjing Wu and Zachary Bricken and Jennifer M. Clegg and Hui Li and Jacqueline D. Woolley},
keywords = {Cognitive development, Social development, Possibility, Intuitive theories, Cross-cultural, LIWC},
abstract = {When thinking about possibility, one can consider both epistemic and deontic principles (i.e., physical possibility and permissibility). Cultural influences may lead individuals to weigh epistemic and deontic obligations differently; developing possibility conceptions are therefore positioned to be affected by cultural surroundings. Across two studies, 251 U.S. and Chinese 4-, 6-, and 8-year-olds sampled from major metropolitan areas in Texas and the Hubei, Sichuan, Gansu, and Guangdong Provinces judged the possibility of impossible, improbable, and ordinary events. Across cultures and ages, children judged ordinary events as possible and impossible events as impossible; cultural differences emerged in developing conceptions of improbable events. Whereas U.S. children became more likely to judge these events possible with age, Chinese children's judgments remained consistent with age: Chinese 4- to 8-year-olds judged these events to be possible ∼25% of the time. In Study 2, to test whether this difference was attributable to differential prioritization of epistemic versus deontic constraints, children also judged whether each event was an epistemic violation (i.e., required magic to happen) and a deontic violation (i.e., would result in someone getting in trouble). With age, epistemic judgments were increasingly predictive of possibility judgments for improbable events for U.S. children, and decreasingly so for Chinese children. Contrary to our predictions, deontic judgments were not predictive. We propose that cultural valuation of norms might shape children's developing intuitions about possibility. We discuss our findings in light of three accounts of possibility conceptions, suggesting ways to integrate cultural context into each.}
}
@article{SINGH2024101269,
title = {An empirical approach to understand the role of emotions in code comprehension},
journal = {Journal of Computer Languages},
volume = {79},
pages = {101269},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101269},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000121},
author = {Divjot Singh and Ashutosh Mishra and Ashutosh Aggarwal},
keywords = {Code comprehension, Systematic literature review, Emotions, Cognitive skills},
abstract = {Programming and cognitive skills are two pivotal abilities of programmers to maintain software products. First, this study included a systematic literature review on code comprehension, emotions, cognitive psychology, and belief-desire-intention domains to analyse various code comprehension monitoring techniques, performance metrics, and computational methodologies. Second, a case study is conducted to examine the influence of various emotional stages on programmers’ programming and cognitive skills while comprehending the software code. The categorization of the participants is done empirically based on their expertism level, and the same results are verified using various machine learning models and performance metrics.}
}
@article{RAHARINIRINA2021100332,
title = {Inferring gene regulatory networks from single-cell RNA-seq temporal snapshot data requires higher-order moments},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100332},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100332},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100180X},
author = {N. Alexia Raharinirina and Felix Peppert and Max {von Kleist} and Christof Schütte and Vikram Sunkara},
keywords = {single cell, RNA sequencing, time-course snapshots, Markov chains, chemical master equation, moment equations},
abstract = {Summary
Single-cell RNA sequencing (scRNA-seq) has become ubiquitous in biology. Recently, there has been a push for using scRNA-seq snapshot data to infer the underlying gene regulatory networks (GRNs) steering cellular function. To date, this aspiration remains unrealized due to technical and computational challenges. In this work we focus on the latter, which is under-represented in the literature. We took a systemic approach by subdividing the GRN inference into three fundamental components: data pre-processing, feature extraction, and inference. We observed that the regulatory signature is captured in the statistical moments of scRNA-seq data and requires computationally intensive minimization solvers to extract it. Furthermore, current data pre-processing might not conserve these statistical moments. Although our moment-based approach is a didactic tool for understanding the different compartments of GRN inference, this line of thinking—finding computationally feasible multi-dimensional statistics of data—is imperative for designing GRN inference methods.}
}
@article{ENGLAND2008163,
title = {Rattling the cage: computational models of chaperonin-mediated protein folding},
journal = {Current Opinion in Structural Biology},
volume = {18},
number = {2},
pages = {163-169},
year = {2008},
note = {Theory and simulation / Macromolecular assemblages},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2007.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X08000067},
author = {Jeremy England and Del Lucent and Vijay Pande},
abstract = {Chaperonins are known to maintain the stability of the proteome by facilitating the productive folding of numerous misfolded or aggregation-prone proteins and are thus essential for cell viability. Despite their established importance, the mechanism by which chaperonins facilitate protein folding remains unknown. Computer simulation techniques are now being employed to complement experimental ones in order to shed light on this mystery. Here we review previous computational models of chaperonin-mediated protein folding in the context of the two main hypotheses for chaperonin function: iterative annealing and landscape modulation. We then discuss new results pointing to the importance of solvent (a previously neglected factor) in chaperonin activity. We conclude with our views on the future role of simulation in studying chaperonin activity as well as protein folding in other biologically relevant confined contexts.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@article{NA2023105139,
title = {Towards a neurocomputational account of social controllability: From models to mental health},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {148},
pages = {105139},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105139},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423001082},
author = {Soojung Na and Shawn A. Rhoads and Alessandra N.C. Yu and Vincenzo G. Fiore and Xiaosi Gu},
keywords = {Social controllability, Computational psychiatry, Reinforcement learning, Model-based learning, Model-free learning, Cognitive map},
abstract = {Controllability, or the influence one has over their surroundings, is crucial for decision-making and mental health. Traditionally, controllability is operationalized in sensorimotor terms as one’s ability to exercise their actions to achieve an intended outcome (also termed “agency”). However, recent social neuroscience research suggests that humans also assess if and how they can exert influence over other people (i.e., their actions, outcomes, beliefs) to achieve desired outcomes ("social controllability”). In this review, we will synthesize empirical findings and neurocomputational frameworks related to social controllability. We first introduce the concepts of contextual and perceived controllability and their respective relevance for decision-making. Then, we outline neurocomputational frameworks that can be used to model social controllability, with a focus on behavioral economic paradigms and reinforcement learning approaches. Finally, we discuss the implications of social controllability for computational psychiatry research, using delusion and obsession-compulsion as examples. Taken together, we propose that social controllability could be a key area of investigation in future social neuroscience and computational psychiatry research.}
}
@article{LEUNG2020345,
title = {Limited cognitive ability and selective information processing},
journal = {Games and Economic Behavior},
volume = {120},
pages = {345-369},
year = {2020},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825620300063},
author = {Benson Tsz Kin Leung},
keywords = {Limited ability, Information overload, Information avoidance, Confirmation bias, Wishful thinking, Polarization},
abstract = {This paper studies the information processing behavior of a decision maker (DM) who can only process a subset of all information he receives: before taking an action, the DM receives sequentially a number of signals and decides whether to process or ignore each of them as it is received. The model generates an information processing behavior consistent with that documented in the psychological literature: first, the DM chooses to process signals that are strong; second, his processing strategy exhibits confirmation bias if he has a strong prior belief; third, he tends to process signals that suggest favorable outcomes (wishful thinking). As an application I analyze how the Internet and the induced change in information availability affects the processing behavior of the DM. I show that providing more/better information to the DM could strengthen his confirming bias.}
}
@article{ORJI2022100626,
title = {Assessing the pre-conditions for the pedagogical use of digital tools in the Nigerian higher education sector},
journal = {The International Journal of Management Education},
volume = {20},
number = {2},
pages = {100626},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100626},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000283},
author = {Ifeyinwa Juliet Orji and Frank Ojadi and Ukoha Kalu Okwara},
keywords = {Digitalization, Higher education, TOE theory, Social media, Learning outcomes, Nigeria},
abstract = {Currently, there is a burgeoning interest in digitalization as evidenced in extant literature. Nevertheless, the effect, based on teachers’ own perspectives, of the pedagogical use of digital technologies on learning outcomes in the higher education sector has been under-investigated. Thus, this paper aims to investigate the pre-conditions for the effective adoption of social media tools in the Nigerian higher education sector and to assess the impact of the adoption on specific learning outcomes. A multi-criteria decision-making (MCDM) methodology was proposed for study analysis, aided by views of experts with sufficient teaching experience in Nigerian business school programs. The results indicate that adequate budgetary allocations, technical competence, a sufficient level of privacy, and an effective government regulatory framework are the most important of the investigated pre-conditions. Additionally, the pedagogical use of social media in business school programs is more strongly associated with learning outcomes such as professionalism and strategic thinking, emotional intelligence, and social maturity. Hence, the article offers guidance to decision-makers in the higher education sector on how to actualize the successful adoption of social media for pedagogical use and build effective business strategies at various levels of the digitalization process.}
}
@article{XU2023108916,
title = {Joint optimization task offloading and trajectory control for unmanned-aerial-vehicle-assisted mobile edge computing},
journal = {Computers and Electrical Engineering},
volume = {111},
pages = {108916},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623003403},
author = {Fei Xu and Sen Wang and Weiya Su and Lin Zhang},
keywords = {Edge computing, Computation offloading, Deep reinforcement learning, Unmanned Aerial Vehicle, Trajectory control},
abstract = {The appearance of Mobile Edge Computing (MEC) and Unmanned Aerial Vehicle (UAV) is significant for the future progress of the Internet of Things (IoT). Since the system model with a continuous action space and high-dimensional state space, the joint optimization of UAV trajectory and the computational offloading problem is non-convex, and traditional algorithms for instance ant colony algorithm, genetic algorithm, Actor Critic (AC) algorithm, and Deep Deterministic Policy Gradient (DDPG) algorithm are difficult to cope with. Reasonably formulating the computational task offloading strategy and the trajectory control of the UAV is crucial for the high-efficiency completion of the task. In this paper, a computational offloading and trajectory control system model for UAV-assisted MEC is proposed. We seek to maximize the user ratio of coverage by jointly optimizing computing offload scheduling and UAV trajectories. We propose an improved DDPG algorithm to optimize the objective function and achieve the optimal solution. Meanwhile, our algorithm can achieve an improvement in the user rate of coverage while avoiding obstacles as compared with baseline algorithms, AC, and DDPG.}
}
@article{ZHOU2023126996,
title = {Coal consumption prediction in thermal power units: A feature construction and selection method},
journal = {Energy},
volume = {273},
pages = {126996},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126996},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003900},
author = {Jian Zhou and Wei Zhang},
keywords = {Thermal power units, Coal consumption prediction, Regression analysis, K-means algorithm, Genetic algorithm},
abstract = {Digitization and related facilities have enabled the thermal power generation enterprises to record real-time data of thermal power units. There are many data-driven applications based on real-time monitoring and operational data in power units, while limited studies lay on the operational improvements, especially on coal consumption prediction under all working conditions. We build an intelligent prediction model of coal consumption based on key features selection, working condition clustering, and regression analysis. We combine feature construction and feature selection methods to cope with the problem caused by directly specifying feature subset for model building of traditional prediction method, which may fall into the thinking pattern and miss potentially better feature subset. Besides, to cope with the different coal consumption under different working conditions, we apply cluster analysis to construct a sub-coal consumption prediction model for each cluster category. Numerical results show that compared with other methods, it has the advantages of lower regression error and moderate model complexity, which can provide efficient decision support for operational improvement in thermal power generation.}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{LEONIDOV2022112279,
title = {Strategic stiffening/cooling in the Ising game},
journal = {Chaos, Solitons & Fractals},
volume = {160},
pages = {112279},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112279},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922004891},
author = {Andrey Leonidov and Ekaterina Vasilyeva},
keywords = {Binary choice game, Ising game, Graph, Forward-looking, Myopic, Noise},
abstract = {The dynamic noisy binary choice (Ising) game of forward-looking agents on a complete graph is analysed. It is shown that strategic considerations lead to effective interaction strengthening (noise reduction) as compared to the myopic game. We show that strategic agents are able to come to consensus in the wider range of noise values than myopic ones. Effective population dynamics with time-dependent probabilities reflecting this strategic stiffening/cooling effect is described.}
}
@article{LU2024103920,
title = {The integrated multi-performance fast optimization strategy for battery thermal management system},
journal = {Case Studies in Thermal Engineering},
volume = {54},
pages = {103920},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103920},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X23012261},
author = {Hao Lu and Xiaole Tang and Hongchang Li and Wenjun Zhao and Xiqiang Chang and Weifang Lin},
keywords = {Short-cut computation, Computational fluid dynamics, Weighted average, Optimization algorithm},
abstract = {Increased battery energy density is required to boost electric vehicle endurance; however, this also raises the possibility of thermal runaway and power battery explosion. Improving the cooling system performance requires optimization and enhancement of classical systems. Traditional design approaches struggle to simultaneously enhance multiple aspects of performance, while an optimization based on Computational Fluid Dynamics (CFD) methods is often inefficient. Therefore, by integrating a flow resistance network model (FRNM) with a weighted average optimization algorithm (INFO), an efficient optimization for the comprehensive performance of the system can be achieved. Five optimized systems under different airflow rates were obtained through optimization. A comparison with two existing systems validated the effectiveness of the optimized system. The results demonstrate that, compared to the two reference systems, the optimized system decreases the maximum temperature difference by 65.51 % and 39.07 %, respectively. Furthermore, the improvement in temperature uniformity is more significant, increasing by 63.76 % and 34.40 %, respectively.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{LIANG2019341,
title = {Is ecoregional scale precise enough for lake nutrient criteria? Insights from a novel relationship-based clustering approach},
journal = {Ecological Indicators},
volume = {97},
pages = {341-349},
year = {2019},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18308094},
author = {Zhongyao Liang and Yong Liu and Huili Chen and Yao Ji},
keywords = {Spatial scale, Nutrient criteria, Relationship-based clustering approach, Relationship mapping, Hierarchical clustering, Leave-one-out cross-validation},
abstract = {While the ecoregional lake nutrient criteria have been widely used in the past two decades, the overconfidence on their applicability may mislead the pollution management decisions, considering the spatial heterogeneity within the ecoregion. The exploration of applicability is thereby important, but is hindered by the difficulty in recognizing reliable relationship patterns between the nutrient and management endpoint. We propose a novel relationship-based clustering approach (RCA) to explore whether the ecoregional scale is precise enough for nutrient criteria. The approach (a) simulates relationships using Bayesian Linear Models, (b) clusters lakes according to relationship similarities via relationship mapping and hierarchical clustering, and (c) identifies reliable relationship patterns based on the leave-one-out cross-validation. The RCA is then employed to explore Chlorophyll a-total phosphorus relationships of 34 lakes in four Ecological Drainage Units (EDUs) in the U.S. Long-term water quality data is from a newly established database (LAGOS-NE). The results show that multiple relationship patterns exist in all the EDUs. The ecoregional relationships misestimate the nutrient effect in over a half of lakes. Therefore, we determine that the ecoregional scale is not precise enough for nutrient criteria and the sub-ecoregional scale is then recommended. Besides, the RCA provides a backward thinking for determining the spatial scale and can be used in some other fields where relationship-based clustering is needed.}
}

@article{RODRIGUEZJORDA2025101702,
title = {Linguistic relativity from an enactive perspective: the entanglement of language and cognition},
journal = {Language Sciences},
volume = {108},
pages = {101702},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101702},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000913},
author = {Ulises {Rodríguez Jordá} and Ezequiel A. {Di Paolo}},
keywords = {Linguistic relativity, Enactive approach, Modularity, Post-cognitivism, Languaging},
abstract = {We seek to relate the fields of linguistic relativity (LR) and the enactive approach in cognitive science. We distinguish contemporary research on LR, starting after the mid-1990s, from earlier approaches to the field. Current studies are characterised by a nuanced methodology rooted in the psycholinguistics tradition. While improving on earlier research, they also move away from philosophically oriented discussions about the relation between language and cognition and focus instead on experimentally testing relativistic effects for specific cognitive domains. We claim that this procedure retains some fundamental assumptions from classical cognitive science, precisely those that are challenged by an enactive perspective. These include a commitment to the modularity of mind and a computational understanding of the interactions between cognitive domains. We contend that contemporary LR research is, in fact, compatible with these classical cognitivist ideas, despite superficial points of tension. We then survey recent post-cognitivist approaches to language in cognitive science and explore ways in which LR and the enactive framework could be mutually enriched. Whereas the structural or categorial aspects of language are central for LR research, these are usually downplayed in post-cognitivist approaches, often influenced by the integrationist distinction between first-order linguistic practices and second-order constructs. We advance a specifically enactive perspective that seeks to preserve the systematic features of language while also integrating them within a dynamical understanding of the relation between language and cognition at multiple timescales.}
}
@article{BIRHANE2021100205,
title = {Algorithmic injustice: a relational ethics approach},
journal = {Patterns},
volume = {2},
number = {2},
pages = {100205},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000155},
author = {Abeba Birhane},
keywords = {justice, ethics, Afro-feminism, relational epistemology, data science, complex systems, enaction, embodiment, artificial intelligence, machine learning},
abstract = {Summary
It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”}
}
@incollection{JACOBLOPES202177,
title = {Chapter 5 - Assistant’s tools toward life cycle assessment},
editor = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
booktitle = {Sustainability Metrics and Indicators of Environmental Impact},
publisher = {Elsevier},
pages = {77-90},
year = {2021},
isbn = {978-0-12-823411-2},
doi = {https://doi.org/10.1016/B978-0-12-823411-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234112000062},
author = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
keywords = {Theoretical approach, Sustainability metrics, Life cycle thinking, Social life cycle assessment, Life cycle costing, Life cycle sustainability assessment},
abstract = {This chapter aims to elucidate the main assistant’s tools created to assist in a global assessment of sustainability metrics and indicators. To this end, the chapter will provide a general review of the main assistant’s tools, considering the Life cycle thinking, social life cycle assessment, life cycle cost, and life cycle sustainability assessment tool. In addition, it guides some necessary criteria to be followed to apply each of these tools. Finally, this compilation of information strongly suggests, at the end of the chapter, the application of sensitivity analyses at the end of the process evaluations.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
volume = {377},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{201119,
title = {Evolution of cognition might be down to brain chemistry},
journal = {New Scientist},
volume = {210},
number = {2806},
pages = {19},
year = {2011},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(11)60726-4},
url = {https://www.sciencedirect.com/science/article/pii/S0262407911607264},
abstract = {The prefrontal cortex, the “thinking” part of our brain, has a radically different chemical balance to that of chimps and macaques}
}
@article{ZHAO2023106750,
title = {A cooperative population-based iterated greedy algorithm for distributed permutation flowshop group scheduling problem},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106750},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106750},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300934X},
author = {Hui Zhao and Quan-Ke Pan and Kai-Zhou Gao},
keywords = {Distributed permutation flowshop, Group scheduling, Total flowtime, Iterated greedy algorithm, Co-evolutionary},
abstract = {This paper studies the distributed permutation flowshop group scheduling problem (DPFGSP) with the consideration of minimizing total flowtime (TF), which has important applications in the modern manufacturing process. Based on the characteristics of the problem, a cooperative population-based iterated greedy (CPIG) algorithm is proposed by combining the advantages of the divide-and-rule policy, population-based evolution and iterated greedy algorithm. The CPIG divides the DPFGSP into two coupled sub-problems of group scheduling sub-problem and job scheduling sub-problem, and starts with a single population for simplicity. Unlike in the traditional cooperative co-evolutionary algorithms, the two-coupled sub-problems are addressed with a certain probability that can be determined in favor of solving the whole scheduling problem. Some advanced technologies are used, including the constructive heuristics based initialization, the critical factories based destruction and construction, the new best solution based population updating mechanism. The comprehensive experimental evaluation of 810 instances shows that the CPIG algorithm performs much better than the five state-of-the-art metaheuristics in the literature which are closely related to the considered scheduling problem.}
}
@article{GAGNE201889,
title = {When planning to survive goes wrong: predicting the future and replaying the past in anxiety and PTSD},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {89-95},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618300305},
author = {Christopher Gagne and Peter Dayan and Sonia J Bishop},
abstract = {We increase our probability of survival and wellbeing by minimizing our exposure to rare, extremely negative events. In this article, we examine the computations used to predict and avoid such events and to update our models of the world and action policies after their occurrence. We also consider how these computations might go wrong in anxiety disorders and Post Traumatic Stress Disorder (PTSD). We review evidence that anxiety is linked to increased simulations of the future occurrence of high cost negative events and to elevated estimates of the probability of occurrence of such events. We also review psychological theories of PTSD in the light of newer, computational models of updating through replay and simulation. We consider whether pathological levels of re-experiencing symptomatology might reflect problems reconciling the traumatic outcome with overly optimistic priors or difficulties terminating off-line simulation focused on negative events and over-generalization to states sharing features with those antecedent to the trauma.}
}
@article{CEGIELSKI2016283,
title = {Rethinking the role of Agent-Based Modeling in archaeology},
journal = {Journal of Anthropological Archaeology},
volume = {41},
pages = {283-298},
year = {2016},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278416516000118},
author = {Wendy H. Cegielski and J. Daniel Rogers},
keywords = {ABM, Agent-Based Modeling, Archaeological methods, Simulation, Computational modeling},
abstract = {Agent-Based Modeling (ABM) represents a methodology with significant potential for altering archaeological analytical practice. The continued growth in the number of publications that use ABM provides evidence for the significance of this emerging approach. However, the scope of the research topics investigated has not increased accordingly. A consensus exists among ABM practitioners, that once generally accepted by the field, ABM can make revolutionary advances within the overall archaeological research paradigm. Unresolved concerns within the archaeological community center on whether ABMs are sufficiently grounded in empirical data, are aligned with theoretical trajectories, and on the difficult task of mastering the computational systems. It is worth exploring these aspects of the disjuncture between the mainstream and ABM practitioners for two reasons – to frame a discussion of qualities of ABM that make it transformative and to provide guidelines for broadening ABM’s applicability. With capacity-building in mind, offered here is a practical reference for the non-practitioner archaeologist considering ABM. A glossary is included of key terms used in the text to describe ABM methods and theory.}
}
@incollection{KUMAR2024147,
title = {Chapter Eight - Machine learning model for teaching and emotional intelligence},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {147-168},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000146},
author = {Mohit Kumar and Syam Machinathu Parambil Gangadharan and Nabanita Choudhury},
keywords = {Cognitive thinking, Design thinking, E-Learning, Emotional intelligence, Intelligent quotient, Social neuroscience},
abstract = {Education that is ongoing and permanent for the purpose of adaptable up-skilling and retraining has been identified as a contributory factor, along with a relentless race against time, fast scientific progress, and unanticipated challenges. Students in postsecondary learning require mechanisms that can enable long-term, dependable knowledge production and storage, and this is particularly true in the age after a pandemic that occurred during the development of new technologies. There has been an explosion of e-learning platforms and methodologies that have been developed to remedy this issue; however, not all of them have been as successful as would be ideal. This type of new knowledge is very difficult to execute properly; it needs complex, careful educational and interface design to perform as well as it does and keep learners interested. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. Having a high intelligence quotient does not guarantee a successful and happy life. Success also necessitates self-awareness and emotional control. Our idea was to create a computational paradigm that would educate students in both programming and emotional intelligence. This experiment was successful in addressing the problem of excessive screen use by providing a student interface without displays.}
}
@article{KIREEV1994143,
title = {Approximate molecular electrostatic potential computations: applications to quantitative structure-activity relationships},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {304},
number = {2},
pages = {143-150},
year = {1994},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(96)80006-6},
url = {https://www.sciencedirect.com/science/article/pii/S0166128096800066},
author = {Dmitry B. Kireev and Valery I. Fetisov and Nikolai S. Zefirov},
abstract = {Two new methods for calculating molecular electrostatic potentials are considered, taking into account QSAR requirements. The first of these is based on quantum chemical approximations; the other uses the topology of molecules. A program for displaying potential contour maps generated by various methods is presented. Examples of the successful use of these methods are given.}
}
@article{WISE2024144,
title = {Naturalistic reinforcement learning},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {2},
pages = {144-158},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002127},
author = {Toby Wise and Kara Emery and Angela Radulescu},
keywords = {reinforcement learning, decision-making, naturalistic, computational modeling},
abstract = {Humans possess a remarkable ability to make decisions within real-world environments that are expansive, complex, and multidimensional. Human cognitive computational neuroscience has sought to exploit reinforcement learning (RL) as a framework within which to explain human decision-making, often focusing on constrained, artificial experimental tasks. In this article, we review recent efforts that use naturalistic approaches to determine how humans make decisions in complex environments that better approximate the real world, providing a clearer picture of how humans navigate the challenges posed by real-world decisions. These studies purposely embed elements of naturalistic complexity within experimental paradigms, rather than focusing on simplification, generating insights into the processes that likely underpin humans’ ability to navigate complex, multidimensional real-world environments so successfully.}
}
@article{BIDERMAN2020542,
title = {What Are Memories For? The Hippocampus Bridges Past Experience with Future Decisions},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {7},
pages = {542-556},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301066},
author = {Natalie Biderman and Akram Bakkour and Daphna Shohamy},
keywords = {memory, decision-making, amnesia, hippocampus, value},
abstract = {Many decisions require flexible reasoning that depends on inference, generalization, and deliberation. Here, we review emerging findings indicating that the hippocampus, known for its role in long-term memory, contributes to these flexible aspects of value-based decision-making. This work offers new insights into the role of memory in decision-making and suggests that memory may shape decisions even in situations that do not appear, at first glance, to depend on memory at all. Uncovering the pervasive role of memory in decision-making challenges the way we define what memory is and what it does, suggesting that memory’s primary purpose may be to guide future behavior and that storing a record of the past is just one way to do so.}
}
@article{BANERJEE2015143,
title = {Z*-numbers: Augmented Z-numbers for machine-subjectivity representation},
journal = {Information Sciences},
volume = {323},
pages = {143-178},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515004582},
author = {Romi Banerjee and Sankar K. Pal},
keywords = {Artificial-mindfulness, Machine-consciousness, Machine-qualia, Machine-self, Perception-operators, Thinking machine},
abstract = {Envisaging a futuristic environment of man–machine and machine–machine synergy, this article documents our research on the augmented Z-numbers, the Z*-numbers, for machine-perception encapsulation. The Z*-numbers have been envisioned as operands of endogenous machine-mind processes underlying bespoke comprehension of the real world. Besides information-certainty, as in a Z-number, a Z*-number incorporates context, time and affects as essential factors of subjectivity representation. We have proposed: (a) definitions for certainty and affect parameters—arising out of socio-cultural influences on machine-knowledge, (b) a Z*-number based rudimentary procedure for natural-language comprehension emulation, and (c) primitive perception-operators for ‘machine-mentalese’ simulation using Z*-number information-equivalents. Our work draws from non-symbolic theories of cognition and ‘mindfulness’, human-mind processes—studied through behavioral experiments, and theories of the ‘self’ and ‘qualia’. The article includes detailed discussions of these experiments and consequent insights, analysis of a theoretical run-through of the defined procedure, and correspondence-studies between the Z*-number paradigm and philosophies of the self. Our research raises questions on cognitive biases and autogenous mind-processes that highlight crucial practical challenges in the current realization of a synthetic-mind. All ideas herein aim to contribute to studies on the ‘self’ and its machine-embodiment for the synthesis of an empathetic machine-mind.}
}
@article{GARLING1994355,
title = {Computational-process modelling of household activity scheduling},
journal = {Transportation Research Part B: Methodological},
volume = {28},
number = {5},
pages = {355-364},
year = {1994},
issn = {0191-2615},
doi = {https://doi.org/10.1016/0191-2615(94)90034-5},
url = {https://www.sciencedirect.com/science/article/pii/0191261594900345},
author = {Tommy Gärling and Mei-Po Kwan and Reginald G. Golledge},
abstract = {Models of households' travel choices are an important focus of research. For some time, it has been known that such models need to incorporate how travel depends on activity choices. It is argued that production system models constitute an alternative or necessary complementary approach if the goal is to develop models of interdependent activity and travel choices, or activity scheduling, which are based on behavioral science theories of higher cognitive processes. Several computational-process models (CPMs) which implement production systems as computer programs are reviewed. Currently, no encompassing CPM exists but some may be possible to integrate in a descriptive model of activity scheduling.}
}
@article{GARCIACAIRASCO2021107930,
title = {Searching for a paradigm shift in the research on the epilepsies and associated neuropsychiatric comorbidities. From ancient historical knowledge to the challenge of contemporary systems complexity and emergent functions},
journal = {Epilepsy & Behavior},
volume = {121},
pages = {107930},
year = {2021},
note = {NEWroscience 2018},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2021.107930},
url = {https://www.sciencedirect.com/science/article/pii/S1525505021001645},
author = {Norberto Garcia-Cairasco and Guilherme Podolsky-Gondim and Julian Tejada},
keywords = {Ancestral knowledge, Superstitious versus scientific knowledge, Epilepsies and neuropsychiatric comorbidities, Clinical semiology and neurosurgery methods, experimental and computational modeling, Complexity and emergent properties},
abstract = {In this review, we will discuss in four scenarios our challenges to offer possible solutions for the puzzle associated with the epilepsies and neuropsychiatric comorbidities. We need to recognize that (1) since quite old times, human wisdom was linked to the plural (distinct global places/cultures) perception of the Universe we are in, with deep respect for earth and nature. Plural ancestral knowledge was added with the scientific methods; however, their joint efforts are the ideal scenario; (2) human behavior is not different than animal behavior, in essence the product of Darwinian natural selection; knowledge of animal and human behavior are complementary; (3) the expression of human behavior follows the same rules that complex systems with emergent properties, therefore, we can measure events in human, clinical, neurobiological situations with complexity systems’ tools; (4) we can use the semiology of epilepsies and comorbidities, their neural substrates, and potential treatments (including experimental/computational modeling, neurosurgical interventions), as a source and collection of integrated big data to predict with them (e.g.: machine/deep learning) diagnosis/prognosis, individualized solutions (precision medicine), basic underlying mechanisms and molecular targets. Once the group of symptoms/signals (with a myriad of changing definitions and interpretations over time) and their specific sequences are determined, in epileptology research and clinical settings, the use of modern and contemporary techniques such as neuroanatomical maps, surface electroencephalogram and stereoelectroencephalography (SEEG) and imaging (MRI, BOLD, DTI, SPECT/PET), neuropsychological testing, among others, are auxiliary in the determination of the best electroclinical hypothesis, and help design a specific treatment, usually as the first attempt, with available pharmacological resources. On top of ancient knowledge, currently known and potentially new antiepileptic drugs, alternative treatments and mechanisms are usually produced as a consequence of the hard, multidisciplinary, and integrated studies of clinicians, surgeons, and basic scientists, all over the world. The existence of pharmacoresistant patients, calls for search of other solutions, being along the decades the surgeries the most common interventions, such as resective procedures (i.e., selective or standard lobectomy, lesionectomy), callosotomy, hemispherectomy and hemispherotomy, added by vagus nerve stimulation (VNS), deep brain stimulation (DBS), neuromodulation, and more recently focal minimal or noninvasive ablation. What is critical when we consider the pharmacoresistance aspect with the potential solution through surgery, is still the pursuit of localization-dependent regions (e.g.: epileptogenic zone (EZ)), in order to decide, no matter how sophisticated are the brain mapping tools (EEG and MRI), the size and location of the tissue to be removed. Mimicking the semiology and studying potential neural mechanisms and molecular targets – by means of experimental and computational modeling – are fundamental steps of the whole process. Concluding, with the conjunction of ancient knowledge, coupled to critical and creative contemporary, scientific (not dogmatic) clinical/surgical, and experimental/computational contributions, a better world and of improved quality of life can be offered to the people with epilepsy and neuropsychiatric comorbidities, who are still waiting (as well as the scientists) for a paradigm shift in epileptology, both in the Basic Science, Computational, Clinical, and Neurosurgical Arenas. This article is part of the Special Issue “NEWroscience 2018”.}
}
@incollection{MUBAYI2017249,
title = {Chapter 10 - Computational Modeling Approaches Linking Health and Social Sciences: Sensitivity of Social Determinants on the Patterns of Health Risk Behaviors and Diseases},
editor = {Arni S.R. {Srinivasa Rao} and Saumyadipta Pyne and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {36},
pages = {249-304},
year = {2017},
booktitle = {Disease Modelling and Public Health, Part A},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169716117300172},
author = {Anuj Mubayi},
keywords = {Health risk behaviors, Dynamic models, Data mining, Sensitivity and uncertainty analysis, Ecological models, Social influences},
abstract = {Developing health promotion programs that support healthy lifestyle behaviors require comprehensive understanding of mechanisms that drive such complex social systems. Policy makers can use models and theories to guide this process at the individuals, groups, and communities levels. Individuals can have multiple risky health behaviors including physical inactivity, unhealthy diets, smoking, and alcohol drinking that are often shaped by social and ecological factors. Collective understanding of these factors can provide ability to design and evaluate intervention programs that can change unhealthy or risky behaviors over long period of time. However, it is overwhelming task to optimize intervention based on only empirical and/or cross-sectional studies. Effective long lasting intervention needs a thorough understanding of the role of social and environmental mechanisms at multiple scales on the dynamics of health behaviors. Recent mathematical and computational methods developed in other fields, such as epidemiology and finance, can provide systematic and in-depth understanding of mechanisms. However, the use of such methods in social and behaviors sciences have been limited. In this chapter, some real life working examples of social health behaviors problems are provided which uses some cutting edge methods from dynamical systems and data mining to uncertainty quantification.}
}
@article{GOBERT201581,
title = {Using educational data mining to assess students’ skills at designing and conducting experiments within a complex systems microworld},
journal = {Thinking Skills and Creativity},
volume = {18},
pages = {81-90},
year = {2015},
note = {21st Century Skills: International Advancements and Recent Developments},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2015.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1871187115300067},
author = {Janice D. Gobert and Yoon Jeon Kim and Michael A. {Sao Pedro} and Michael Kennedy and Cameron G. Betts},
keywords = {Complex systems, Inquiry assessment, Performance assessment, Educational data mining, 21st century skills},
abstract = {Many national policy documents underscore the importance of 21st century skills, including critical thinking. In parallel, recent American frameworks for K-12 science education call for the development of critical thinking skills in science, also referred to as science inquiry skills/practices. Assessment of these skills is necessary, as indicated in policy documents; however, this has posed a great challenge for assessment researchers. Recently, some science learning environments seek to assess these science skills. These systems log all students’ interactions within the given system, and if fully leveraged, these logs provide rich assessments of inquiry skills. Here, we describe our environment Inq-ITS (inquiry intelligent tutoring system), that uses educational data mining to assess science inquiry skills, as described as 21st century skills. Additionally, here, we describe how we measure students’ skills at designing controlled experiments, a lynchpin skill of inquiry, in the context of complex systems. In doing so, our work addresses 21st century skill assessment in two ways, namely of inquiry (designing and conducting experiments), and in the context of complex systems, a key topic area of 21st century skills. We use educational data mining to develop our assessment of this skill for complex systems.}
}
@article{MANCINI2022102697,
title = {Out of sight, out of mind? The importance of local context and trust in understanding the social acceptance of biogas projects: A global scale review},
journal = {Energy Research & Social Science},
volume = {91},
pages = {102697},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2022.102697},
url = {https://www.sciencedirect.com/science/article/pii/S2214629622002018},
author = {Eliana Mancini and Andrea Raggi},
keywords = {Social acceptance, Bioenergy, Non-technical barriers, Biogas, Socio-cultural factors, Life Cycle Thinking},
abstract = {Social acceptance is considered the main non-technical barrier to the development of bioenergy projects. This paper presents the results of a systematic literature review aimed to cover a lack in state-of-the-art literature about socio-cultural factors affecting the acceptance of biogas projects at a global scale. Moreover, this study is aimed at identifying which methods are used for studying this phenomenon, with a focus on the Life Cycle Thinking-oriented ones. Journal articles and conference proceedings were considered. At the end of the screening phases, 54 documents were selected and reviewed. The results showed that acceptance concerns two main issues: biogas plants and its presence in a given location and digestate application on fields. This review showed different results between high-income and low-middle-income countries. As regards the former, trust was the most mentioned socio-cultural factor. Education, as well as women's living conditions were considered important in the latter. However, a contextualisation of every outcome based on local peculiarities is needed in order to understand in a better way the accepting/refuting phenomena of the projects. As regards the second objective of this study, Life Cycle Analysis resulted the most widespread Life Cycle Thinking methodology. In conclusion, the outcomes of this work may be useful to identify the non-technical factors and the most suitable approach that should be considered for a successful implementation of site-specific biogas projects.}
}
@article{CRUTCHFIELD199411,
title = {The calculi of emergence: computation, dynamics and induction},
journal = {Physica D: Nonlinear Phenomena},
volume = {75},
number = {1},
pages = {11-54},
year = {1994},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(94)90273-9},
url = {https://www.sciencedirect.com/science/article/pii/0167278994902739},
author = {James P. Crutchfield},
abstract = {Defining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analyzed in terms of how model-building observers infer from measurements the computational capabilities embedded in nonlinear processes. An observer's notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtlely though on how those resources are organized. The descriptive power of the observer's chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data. This paper presents an overview of an inductive framework-hierarchical ϵ-machine reconstruction—in which the emergence of complexity is associated with the innovation of new computational model classes. Complexity metrics for detecting structure and quantifying emergence, along with an analysis of the constraints on the dynamics of innovation, are outlined. Illustrative examples are drawn from the onset of unpredictability in nonlinear systems, finitary nondeterministic processes, and cellular automata pattern recognition. They demonstrate how finite inference resources drive the innovation of new structures and so lead to the emergence of complexity.}
}
@article{XU2024102430,
title = {A temporal approach to online discussion during disasters: Applying SIR infectious disease model to predict topic growth and examining effects of temporal distance},
journal = {Public Relations Review},
volume = {50},
number = {2},
pages = {102430},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102430},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124000092},
author = {Sifan Xu and Xinyan Zhao and Jie Chen},
keywords = {Disaster, SIR, Computational modeling, SIR model, Twitter big data, Climate change, Topic growth, Construal level},
abstract = {Discussions on social media during major disasters are robust and often have multiple frames of reference. Temporal perspectives, however, are still lacking in current understandings of social-mediated discussions during disasters and crises, but incorporating temporal perspectives can significantly enhance environmental scanning efforts as prescribed in the issues management framework. The purpose of the current research is twofold: to apply and validate the SIR (Susceptible-Infectious-Recovered) model to examine topics’ growth over time on social media and to understand how future orientation of social media users (an indicator of temporal distance) affects their construal of a disaster through supervised machine learning. We based our analysis on Twitter discussions during the Texas winter storm in 2021. Results of the study show great fit of the SIR model for topic growth, and that temporal distance affects users’ construal of the event in line with core predictions of construal level theory. Theoretical, methodological, and practical implications on social-mediated discussions related to climate change-induced and -intensified disasters and issues management are discussed.}
}
@article{OREILLY2016547,
title = {Creative Engineers: Is Abductive Reasoning Encouraged enough in Degree Project Work?},
journal = {Procedia CIRP},
volume = {50},
pages = {547-552},
year = {2016},
note = {26th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630395X},
author = {Ciarán J. O’Reilly},
keywords = {Creative design, abductive reasoning, education, degree project},
abstract = {Creativity is considered to be an important ability for an engineer to have, and it is therefore important that the development of this ability is structured into the education of engineering students, along with the ability to apply, analyse and evaluate based on existent knowledge. In this paper, the importance of abduction in creative engineering processes is briefly reviewed. It has been shown that abductive reasoning plays a key role in design as it is the only logical operation that introduces new ideas. Its encouragement within the KTH Royal Institute of Technology's degree projects at the Department of Aeronautical and Vehicle Engineering is analysed by examining the stated intended learning outcomes, and through interviewing students. It is found that abductive reasoning is not explicitly encouraged within the intended learning outcomes of these degree project courses, despite its importance in creative thinking. Although, it is very likely that at least some abduction takes place in the project work, its absence from the intended learning outcomes means that students may not have a felt need to demonstrate their abductive reasoning, and supervisors may encourage only non-creative deductive or inductive reasoning. A more explicit inclusion of abductive reasoning in the intended learning outcomes would help both students and supervisors to include creative thinking in the degree project courses.}
}
@article{JIANG2021106740,
title = {Accelerator for crosswise computing reduct},
journal = {Applied Soft Computing},
volume = {98},
pages = {106740},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106740},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306785},
author = {Zehua Jiang and Keyu Liu and Jingjing Song and Xibei Yang and Jinhai Li and Yuhua Qian},
keywords = {Accelerator, Attribute reduction, Cross computation, Rough set},
abstract = {Attribute reduction, as a technique for selecting qualified attributes which can satisfy the intended constraint related to considered measure, has been widely explored. Notably, one and only one reduct is derived through using one searching strategy in most cases. Nevertheless, only one reduct may be not enough for us to evaluate its effectiveness. To fill such gap, an approach of crosswise computing reduct is proposed for obtaining multiple reducts. The computation of reduct is realized through partitioning the whole data into several groups, and crosswise selecting some groups to form different subsets of data, then computing reducts over these different subsets of data. Moreover, to speed up the process of crosswise computing reduct, an acceleration strategy is designed. The main thinking of our acceleration strategy is to compute the reduct over different subsets of data on the basis of reduct over the whole data. The experimental results over 16 data sets show the following superiorities of our strategy: (1) our approach can decrease the elapsed time of crosswise computing reducts significantly; (2) our approach can not only provide reduct with higher stability, but also maintain the classification performance; (3) the attributes in reduct can provide more stable classification results.}
}
@article{KATTERFELDT201872,
title = {Physical computing with plug-and-play toolkits:Key recommendations for collaborative learning implementations},
journal = {International Journal of Child-Computer Interaction},
volume = {17},
pages = {72-82},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300351},
author = {Eva-Sophie Katterfeldt and Mutlu Cukurova and Daniel Spikol and David Cuartielles},
keywords = {Collaborative learning, Education, Motivation, Physical computing, Programming, Toolkit},
abstract = {Physical computing toolkits have long been used in educational contexts to learn about computational concepts by engaging in the making of interactive projects. This paper presents a comprehensive toolkit that can help educators teach programming with an emphasis on collaboration, and provides suggestions for its effective pedagogical implementation. The toolkit comprises the Talkoo kit with physical computing plug-and-play modules and a visual programming environment. The key suggestions are inspired by the results of the evaluation studies which show that children (aged 14–18 in a sample group of 34 students) are well motivated when working with the toolkit but lack confidence in the kit’s support for collaborative learning. If the intention is to move beyond tools and code in computer education to community and context, thus encouraging computational participation, collaboration should be considered as a key aspect of physical computing activities. Our approach expands the field of programming with physical computing for teenage children with a focus on empowering teachers and students with not only a kit but also its appropriate classroom implementation for collaborative learning.}
}
@article{ROMEROCRISTOBAL2025502215,
title = {Why your doctor is not an algorithm: Exploring logical principles of different clinical inference methods using liver transplantation as a model},
journal = {Gastroenterología y Hepatología (English Edition)},
volume = {48},
number = {3},
pages = {502215},
year = {2025},
issn = {2444-3824},
doi = {https://doi.org/10.1016/j.gastre.2025.502215},
url = {https://www.sciencedirect.com/science/article/pii/S244438242500015X},
author = {Mario Romero-Cristóbal and Magdalena {Salcedo Plaza} and Rafael Bañares},
keywords = {Epistemology, Logic, Clinical reasoning, Machine learning, Liver transplantation, Epistemología, Lógica, Razonamiento clínico, Algoritmos automáticos, Trasplante hepático},
abstract = {The development of machine learning (ML) tools in many different medical settings is largely increasing. However, the use of the resulting algorithms in daily medical practice is still an unsolved challenge. We propose an epistemological approach (i.e., based on logical principles) to the application of computational tools in clinical practice. We rely on the classification of scientific inference into deductive, inductive, and abductive comparing the characteristics of ML tools with those derived from evidence-based medicine [EBM] and experience-based medicine, as paradigms of well-known methods for generation of knowledge. While we illustrate our arguments using liver transplantation as an example, this approach can be applied to other aspects of the specialty. Regarding EBM, it generates general knowledge that clinicians apply deductively, but the certainty of its conclusions is not guaranteed. In contrast, automatic algorithms primarily rely on inductive reasoning. Their design enables the integration of vast datasets and mitigates the emotional biases inherent in human induction. However, its poor capacity for abductive inference (a logical mechanism inherent to human clinical experience) constrains its performance in clinical settings characterized by uncertainty, where data are heterogeneous, results are highly influenced by context, or where prognostic factors can change rapidly.
Resumen
Asistimos en la actualidad a un desarrollo asombroso de las herramientas de aprendizaje automático, lo que se traduce en un número creciente de estudios que ensayan su desempeño ante diferentes problemas médicos. Sin embargo, la implementación de estos algoritmos en la práctica diaria permanece como un reto no completamente abordado. Proponemos una aproximación epistemológica (según los fundamentos lógicos) de la aplicabilidad de las herramientas computacionales en la clínica. Nos basamos en la clasificación de los tipos de inferencia científica en deductiva, inductiva y abductiva, y comparamos las características fundamentales de estas herramientas con las de otros métodos de trasferencia del conocimiento a la práctica diaria (medicina basada en la evidencia [MBE] y medicina basada en la experiencia). Ejemplificamos nuestros razonamientos con el caso del trasplante hepático, si bien pueden ser aplicables a otras áreas de la especialidad. La MBE genera conocimientos generales que el clínico aplica de manera deductiva, a pesar de lo cual la certeza de sus conclusiones no es segura. La inducción predomina en el caso de los algoritmos automáticos. Su diseño permite interrelacionar gran cantidad de datos de manera menos sensible a los sesgos emocionales propios de la inducción humana. Sin embargo, su menor capacidad para la inferencia abductiva (mecanismo lógico propio de la experiencia clínica humana) limita su desempeño en aquellos contextos clínicos que están sujetos a gran incertidumbre, en donde los datos son heterogéneos, los resultados están muy influenciados por el contexto o en los que los factores pronósticos pueden cambiar rápidamente.}
}
@article{SHAFFER199795,
title = {Learning mathematics through design: The anatomy of Escher's world},
journal = {The Journal of Mathematical Behavior},
volume = {16},
number = {2},
pages = {95-112},
year = {1997},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(97)90019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0732312397900195},
author = {David Williamson Shaffer},
abstract = {This article explores one example of an open learning environment created by combining mathematics and design activities in a “mathematics studio”. Two iterations of the mathematics studio experiment in a project at the MIT Media Laboratory known as Escher's World suggest that: (a) students can learn about the mathematical concept of symmetry in a studio learning environment, (b) students learn to use visual thinking to solve mathematical problems in a studio learning environment, and (c) students develop a more positive attitude towards mathematics as a result of working in a studio learning environment. This article uses a qualitative research model to explore the specific characteristics of the mathematics studio that were influential in creating a successful learning environment—in particular, how expressive mathematics activities and expressive computational media give students a sense of control over their learning.}
}
@article{HUANG202233634,
title = {Transition from synaptic simulation to nonvolatile resistive switching behavior based on an Ag/Ag:ZnO/Pt memristor},
journal = {RSC Advances},
volume = {12},
number = {52},
pages = {33634-33640},
year = {2022},
issn = {2046-2069},
doi = {https://doi.org/10.1039/d2ra05483c},
url = {https://www.sciencedirect.com/science/article/pii/S2046206922032296},
author = {Yong Huang and Jiahao Yu and Yu Kong and Xiaoqiu Wang},
abstract = {ABSTRACT
The advent of memristors and the continuing research and development in the field of brain-inspired computing could allow realization of a veritable “thinking machine”. In this study, ZnO-based memristors were fabricated using a radio frequency magnetron sputtering method. The ZnO oxide layer was prepared by incorporating silver nanocrystals (NCs). Several synaptic functions, i.e. nonlinear transmission characteristics, short-term potentiation, long-term potentiation/depression, and pair-pulse facilitation, were imitated in the memristor successfully. Furthermore, the transition from synaptic behaviors to bipolar resistive switching behaviors of the device was also observed under repeated stimulus. It is speculated that the switching mechanism is due to the formation and rupture of the conductive Ag filaments and the corresponding electrochemical metallization. The experimental results demonstrate that the Ag/Ag:ZnO/Pt memristor with resistive switching and several synaptic behaviors has a potential application in neuromorphic computing and data storage systems.}
}
@article{LOPEZSILVA202446,
title = {‘Are these my thoughts?’: A 20-year prospective study of thought insertion, thought withdrawal, thought broadcasting, and their relationship to auditory verbal hallucinations},
journal = {Schizophrenia Research},
volume = {265},
pages = {46-57},
year = {2024},
note = {Hallucinations: Neurobiology and Patient Experience},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002778},
author = {Pablo López-Silva and Martin Harrow and Thomas H. Jobe and Michele Tufano and Helen Harrow and Cherise Rosen},
keywords = {Schizophrenia, Psychosis, Thought insertion, Thought withdrawal, Thought broadcasting, Auditory-verbal hallucinations},
abstract = {The co-occurrence of delusions and other symptoms at the onset of psychosis is a challenge for theories about the aetiology of psychosis. This paper explores the relatedness of delusions about the experience of thinking (thought insertion, thought withdrawal, and thought broadcasting) and auditory verbal hallucinations by describing their trajectories over a 20-year period in individuals diagnosed with schizophrenia, affective and other psychosis, and unipolar depression nonpsychosis. The sample consisted of 407 participants who were recruited at index hospitalization and evaluated over six follow-ups over 20 years. The symptom structure associated with thought insertion included auditory verbal hallucinations, somatic hallucinations, other hallucinations, delusions of thought-dissemination, delusions of control, delusion of self-depreciation, depersonalization and anxiety. The symptom constellation of thought withdrawal included somatic hallucinations, other hallucinations, delusions of thought dissemination, delusions of control, sexual delusions, depersonalization, negative symptoms, depression, and anxiety. The symptom constellation of thought broadcasting included auditory verbal hallucinations, somatic hallucinations, delusions of thought-dissemination, delusion of self-depreciation, fantastic delusions, sexual delusions, and depersonalization. Auditory verbal hallucinations and delusions of self-depreciation were significantly associated with both thought insertion and thought broadcasting. Thought insertion and thought withdrawal were significantly associated with other hallucinations, delusions of control, and anxiety; thought withdrawal and thought broadcasting were significantly related to sexual delusions. We hypothesize that specific symptom constellations over time might be explained as the product of pseudo-coherent realities created to give meaning to the experience of the world and the self of individuals in psychosis based on both prior top-down and ongoing bottom-up elements.}
}
@article{BEDNORZ2024101169,
title = {Effects of domain-specific linguistic factors on the difficulty of mathematics tasks},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101169},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000464},
author = {David Bednorz and Michael Kleine and Rudolf {vom Hofe}},
keywords = {Mathematical tasks, Task features, linguistic complexity, Task difficulty},
abstract = {Linguistic features as a task-related feature influence the difficulty of mathematical tasks. To reduce this influence (e.g., in testing situations), studies on linguistic simplification focus on modifying linguistic features. These studies show little or no effect on increasing test performance. An open question is whether a quantitative–exploratory approach with texts from a specific domain can be an additional model for reducing the linguistic influence on mathematical tasks. To answer this question, generalized linear mixed models were used to determine the effects of linguistic factors, the requirements of the items, and the effects of linguistic factors when differentiating the requirements of the items, while controlling for further person- and item-related effects. The results show that linguistic factors can have either a negative or positive influence on test performance. The findings indicate that for mathematics assessments and teaching, it might be essential to consider the influence of language factors and task requirements.}
}
@article{BINA2020102475,
title = {Beyond techno-utopia and its discontents: On the role of utopianism and speculative fiction in shaping alternatives to the smart city imaginary},
journal = {Futures},
volume = {115},
pages = {102475},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.102475},
url = {https://www.sciencedirect.com/science/article/pii/S0016328719303374},
author = {Olivia Bina and Andy Inch and Lavínia Pereira},
keywords = {Smart cities, Ways of knowing, Urban imaginaries, Utopianism, Fiction},
abstract = {In recent years, the ösmart city’ has become established in policy and planning discourse, embedding visions of an urban future where ubiquitous technology offers efficient solutions to the pathologies of the contemporary city. In response, a rapidly growing social-scientific literature is critically exploring how the smart city imaginary (SCI) promotes ötechno-utopian’ fantasies, ignoring the risks of a technologically determined future. In this paper we begin by considering SCI as emblematic of the colonization of contemporary (urban) futures by vested interests, arguing for the need for diverse and plural imaginaries and thus for a re-engagement of the social sciences. We explore how critical social scientific contributions to shaping futures might be deepened through further engagement with utopian theory and speculative fiction, two traditions of future-orientated thinking that seek to combine critique with constructive thinking about alternatives. We therefore contribute to ö50 + 50 Theme 2: Framing Futures in 2068-the limits of and opportunities for futures research’ by 1) extending critique of contemporary claims about (smart urban) futures, and; 2) exploring how utopianism and fiction can expand ways of thinking, imagining and knowing futures.}
}
@article{BAKER2021101933,
title = {Who is marginalized in energy justice? Amplifying community leader perspectives of energy transitions in Ghana},
journal = {Energy Research & Social Science},
volume = {73},
pages = {101933},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621000268},
author = {Erin Baker and Destenie Nock and Todd Levin and Samuel A. Atarah and Anthony Afful-Dadzie and David Dodoo-Arhin and Léonce Ndikumana and Ekundayo Shittu and Edwin Muchapondwa and Charles Van-Hein Sackey},
abstract = {There is a divide in energy access studies, between technologically-focused modeling papers in engineering and economics, and energy justice frameworks and principles grounded in social sciences. Quantitative computational models are necessary when analyzing energy, and more specifically electricity, systems, as they are technologically-complex systems that can diverge from intuitive patterns. To assure energy justice, these models must be reflective of, and informative to, a wide range of stakeholders, including households and communities alongside utilities, governments, and others. Yet, moving from a qualitative understanding of preferences to quantitative modeling is challenging. In this perspective piece, we pilot the use of the value-focused thinking framework to inform stakeholder engagement. The result is a strategic objective hierarchy that highlights the tradeoffs and the social, economic and technological factors that need to be measured in models. We apply the process in Ghana, using a survey, stakeholder workshops, and follow-up interviews to uncover key tradeoffs and stakeholder-derived objectives. We discuss three key areas that have been rarely, if ever, well-represented in energy models: (1) the relationship between the dynamics of electricity end-use and the technology and economic structure of the system; (2) explicit tradeoffs between electricity access, cost, and reliability as defined by stakeholders; and (3) the definition of new objectives, such as minimizing hazards related to theft. We conclude that this model of engagement provides an opportunity to tie together rigorous qualitative analysis and stakeholder engagement with crucial quantitative models of the electricity system.}
}
@article{SIMPSON2017166,
title = {Preparing industry for additive manufacturing and its applications: Summary & recommendations from a National Science Foundation workshop},
journal = {Additive Manufacturing},
volume = {13},
pages = {166-178},
year = {2017},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860416302019},
author = {Timothy W. Simpson and Christopher B. Williams and Michael Hripko},
keywords = {Additive manufacturing, Design for additive manufacturing, STEM education, 3D printing, Workforce development},
abstract = {Accompanying the increasing advances and interest in Additive Manufacturing (AM) technologies is an increasing demand for an industrial workforce that is knowledgeable about the technologies and how to apply them to solve real-world problems. As a step towards addressing this knowledge gap, a workshop was held at the National Science Foundation (NSF) to discuss the educational needs to prepare industry for AM and its use in different fields. The workshop participants – 66 representatives from academia, industry, and government – identified several key educational themes: (1) AM processes and process/material relationships, (2) engineering fundamentals with an emphasis on materials science and manufacturing, (3) professional skills for problem solving and critical thinking, (4) design practices and tools that leverage the design freedom enabled by AM, and (5) cross-functional teaming and ideation techniques to nurture creativity. This paper summarizes the industry speakers and presentations from the workshop, along with several new educational partnerships identified by small working groups. Based on the presentations and partnerships, the following recommendations are offered to advance the AM workforce. First, ensure that all AM curricula provide students with an understanding of (i) AM and traditional manufacturing processes to enable them to effectively select the appropriate process for product realization; (ii) the relationships between AM processes and material properties; and (iii) “Design for AM”, including computational tools for AM design as well as frameworks for process selection, costing, and solution generation that take advantage of AM capabilities. Second, establish a national network for AM education that, by leveraging existing “distributed” educational models and NSF’s Advanced Technology Education (ATE) Programs, provides open source resources as well as packaged activities, courses, and curricula for all educational levels (K-Gray). Third, support K-12 educational programs in STEAM (STEM plus the arts) and across all formal and informal learning environments in order to learn the unique capabilities of AM while engaging students in hands-on, tactile, and visual learning activities to prepare them for jobs in industry while learning how to think differently when designing for AM. Fourth, provide support for collaborative and community-oriented maker spaces that promote awareness of AM among the public and provide AM training programs for incumbent workers in industry and students seeking alternative pathways to gain AM knowledge and experience. Recommendations for scaling and coordination across local, regional, and national levels are also discussed to create synergies among the proposed activities and existing efforts.}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{ALANO20221,
title = {Professor Richard Carter (1945–2021)},
journal = {Trends in Parasitology},
volume = {38},
number = {1},
pages = {1-3},
year = {2022},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471492221002609},
author = {Pietro Alano and Richard Culleton and Christian Doerig and Louis Miller},
abstract = {The malaria research community lost a pioneer when Professor Richard Carter passed away at the age of 76 on 4 September 2021. Richard was an exceptionally brilliant malariologist, always inquisitive and gifted with an unorthodox way of thinking.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{SHEFFIELD2024100333,
title = {Understanding Cognitive Behavioral Therapy for Psychosis Through the Predictive Coding Framework},
journal = {Biological Psychiatry Global Open Science},
volume = {4},
number = {4},
pages = {100333},
year = {2024},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324000466},
author = {Julia M. Sheffield and Aaron P. Brinen and Brandee Feola and Stephan Heckers and Philip R. Corlett},
keywords = {Belief updating, CBTp, Persecutory delusions, Predictive coding, Psychotherapy, Volatility},
abstract = {Psychological treatments for persecutory delusions, particularly cognitive behavioral therapy for psychosis, are efficacious; however, mechanistic theories explaining why they work rarely bridge to the level of cognitive neuroscience. Predictive coding, a general brain processing theory rooted in cognitive and computational neuroscience, has increasing experimental support for explaining symptoms of psychosis, including the formation and maintenance of delusions. Here, we describe recent advances in cognitive behavioral therapy for psychosis–based psychotherapy for persecutory delusions, which targets specific psychological processes at the computational level of information processing. We outline how Bayesian learning models employed in predictive coding are superior to simple associative learning models for understanding the impact of cognitive behavioral interventions at the algorithmic level. We review hierarchical predictive coding as an account of belief updating rooted in prediction error signaling. We examine how this process is abnormal in psychotic disorders, garnering noisy sensory data that is made sense of through the development of overly strong delusional priors. We argue that effective cognitive behavioral therapy for psychosis systematically targets the way sensory data are selected, experienced, and interpreted, thus allowing for the strengthening of alternative beliefs. Finally, future directions based on these arguments are discussed.}
}
@incollection{BUJA2005391,
title = {14 - Computational Methods for High-Dimensional Rotations in Data Visualization},
editor = {C.R. Rao and E.J. Wegman and J.L. Solka},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {24},
pages = {391-413},
year = {2005},
booktitle = {Data Mining and Data Visualization},
issn = {0169-7161},
doi = {https://doi.org/10.1016/S0169-7161(04)24014-7},
url = {https://www.sciencedirect.com/science/article/pii/S0169716104240147},
author = {Andreas Buja and Dianne Cook and Daniel Asimov and Catherine Hurley},
abstract = {There exist many methods for visualizing complex relations among variables of a multivariate dataset. For pairs of quantitative variables, the method of choice is the scatterplot. For triples of quantitative variables, the method of choice is 3D data rotations. Such rotations let us perceive structure among three variables as shape of point scatters in virtual 3D space. Although not obvious, three-dimensional data rotations can be extended to higher dimensions. The mathematical construction of high-dimensional data rotations, however, is not an intuitive generalization. Whereas three-dimensional data rotations are thought of as rotations of an object in space, a proper framework for their high-dimensional extension is better based on rotations of a low-dimensional projection in high-dimensional space. The term “data rotations” is therefore a misnomer, and something along the lines of “high-to-low dimensional data projections” would be technically more accurate. To be useful, virtual rotations need to be under interactive user control, and they need to be animated. We therefore require projections not as static pictures but as movies under user control. Movies, however, are mathematically speaking one-parameter families of pictures. This article is therefore about one-parameter families of low-dimensional projections in high-dimensional data spaces. We describe several algorithms for dynamic projections, all based on the idea of smoothly interpolating a discrete sequence of projections. The algorithms lend themselves to the implementation of interactive visual exploration tools of high-dimensional data, such as so-called grand tours, guided tours and manual tours.}
}
@incollection{MARWALA202185,
title = {Chapter 7 - Bounded rational counterfactuals},
editor = {Tshilidzi Marwala},
booktitle = {Rational Machines and Artificial Intelligence},
publisher = {Academic Press},
pages = {85-96},
year = {2021},
isbn = {978-0-12-820676-8},
doi = {https://doi.org/10.1016/B978-0-12-820676-8.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206768000120},
author = {Tshilidzi Marwala},
keywords = {Rational counterfactuals, Bounded rationality, Optimization, Artificial intelligence},
abstract = {The rational counterfactual is identified from the factual and the knowledge of the laws that govern the relationships between the antecedent and the consequent of the factual, which maximizes the attainment of the desired consequent. However, the attainment of the desired consequent is not perfect and is, in fact, limited, which makes these counterfactuals bounded rational counterfactuals. In counterfactual thinking, factual statements such as “The COVID-19 afflicted the world, and the world economy contracted by 3%,” has the counterfactual “The COVID-19 did not afflict the world, and the world economy grew by 3%.” In this chapter, we use intelligent machines that use AI to build bounded rational counterfactuals. It is observed that intelligent machines can achieve bounded rational counterfactual better than human agents. In general, quantifiable factual easily has bounded rational counterfactuals when compared to qualitative counterfactuals.}
}
@article{SCHREIBER20142544,
title = {A few bad ideas on the way to the triumph of parallel computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2544-2547},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731513002177},
author = {Robert Schreiber},
keywords = {Parallelism, Amdahl, Automatic parallelization, Accelerators, Exascale},
abstract = {Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here.}
}
@article{NISHANT2020102104,
title = {Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda},
journal = {International Journal of Information Management},
volume = {53},
pages = {102104},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102104},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220300967},
author = {Rohit Nishant and Mike Kennedy and Jacqueline Corbett},
keywords = {Agenda for practice, AI, Artificial intelligence, Climate change, Environmental governance, Natural environment, Research agenda, Sustainability},
abstract = {Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.}
}
@article{BEATY2017189,
title = {Creative constraints: Brain activity and network dynamics underlying semantic interference during idea production},
journal = {NeuroImage},
volume = {148},
pages = {189-196},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917300125},
author = {Roger E. Beaty and Alexander P. Christensen and Mathias Benedek and Paul J. Silvia and Daniel L. Schacter},
keywords = {Creativity, Divergent thinking, Cognitive control, Functional connectivity, Default network, Executive control network},
abstract = {Functional neuroimaging research has recently revealed brain network interactions during performance on creative thinking tasks—particularly among regions of the default and executive control networks—but the cognitive mechanisms related to these interactions remain poorly understood. Here we test the hypothesis that the executive control network can interact with the default network to inhibit salient conceptual knowledge (i.e., pre-potent responses) elicited from memory during creative idea production. Participants studied common noun-verb pairs and were given a cued-recall test with corrective feedback to strengthen the paired association in memory. They then completed a verb generation task that presented either a previously studied noun (high-constraint) or an unstudied noun (low-constraint), and were asked to “think creatively” while searching for a novel verb to relate to the presented noun. Latent Semantic Analysis of verbal responses showed decreased semantic distance values in the high-constraint (i.e., interference) condition, which corresponded to increased neural activity within regions of the default (posterior cingulate cortex and bilateral angular gyri), salience (right anterior insula), and executive control (left dorsolateral prefrontal cortex) networks. Independent component analysis of intrinsic functional connectivity networks extended this finding by revealing differential interactions among these large-scale networks across the task conditions. The results suggest that interactions between the default and executive control networks underlie response inhibition during constrained idea production, providing insight into specific neurocognitive mechanisms supporting creative cognition.}
}
@article{LIU2024101910,
title = {Understanding ceiling temperature as a predictive design parameter for circular polymers},
journal = {Cell Reports Physical Science},
volume = {5},
number = {4},
pages = {101910},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.101910},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424001462},
author = {Xiaoyang Liu and Shivani Kozarekar and Alexander Shaw and Tie-Qi Xu and Eugene Y.-X. Chen and Linda J. Broadbelt},
keywords = {circular polymers, ceiling temperature, thermodynamic parameters, uncertainty propagation, density functional theory},
abstract = {Summary
The rise of polymeric materials marks a notable achievement of the past century, yet challenges in recycling have led to their accumulation in various environments. Efforts to address this include advancements in mechanical recycling, degradation processes, and chemical recycling techniques, particularly chemical recycling to monomer, which offers a path toward a circular economy for plastics. In this perspective, we discuss how ceiling temperature (Tc) can be used as a design parameter for circular (closed-loop recyclable) polymers and provide an overview of typical experimental approaches for deriving Tc, focusing on ΔHp and ΔSp as the key parameters for prediction. The concept of Tc is heavily embedded in the polymer literature and provides a simple but still useful way of quickly ranking different polymers in terms of their relative thermodynamic stability of polymer versus monomer states. While Tc in the bulk state as an intrinsic value is a desirable quantity, it is infeasible in many cases to measure equilibrium states in the bulk; thus, many researchers have focused on investigating Tc in solution, where there may be dependencies of Tc on the solvent, concentration, or other factors, resulting in a family of apparent Tc values at each set of conditions. We thus explore computational studies as a complement to experimental measurements of Tc. To this end, we focus here on the advantages, obstacles, and outlook of the establishment of predictive computational approaches to calculate key thermodynamic parameters related to polymer circularity, namely ΔHp, ΔSp, ΔGp, and Tc values.}
}
@article{BLOTE2000221,
title = {Mental computation and conceptual understanding},
journal = {Learning and Instruction},
volume = {10},
number = {3},
pages = {221-247},
year = {2000},
issn = {0959-4752},
doi = {https://doi.org/10.1016/S0959-4752(99)00028-6},
url = {https://www.sciencedirect.com/science/article/pii/S0959475299000286},
author = {Anke W. Blöte and Anton S. Klein and Meindert Beishuizen},
keywords = {Arithmetic, Procedural flexibility, Conceptual understanding},
abstract = {The goal of this study was to assess the strategic flexibility of students in mental arithmetic up to the number 100. Sixty Dutch second-graders who took part in an experimental ‘realistic arithmetic’ program participated in the study. Results showed that students' preference for certain mathematical procedures depended on the number characteristics of the problems. This indicates that the students had a good conceptual understanding of numbers and procedures. Their actual use of these procedures, however, was somewhat limited. Most problems were solved within a sequential structure. A completely different procedure was used for solving subtraction problems that had a very small difference between the two numbers. Furthermore, it was found that a substantial increase in the students' use of a base-ten procedure occurred after the introduction of this procedure in the mathematics curriculum. Students' preference for this procedure also increased, although to a lesser extent. Another finding of the study was that students exhibited more flexible strategic behaviour with context problems than with numerical-expression problems.}
}
@article{SZNYCER2025106652,
title = {Emotional tears: What they are and how they work},
journal = {Evolution and Human Behavior},
volume = {46},
number = {1},
pages = {106652},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106652},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000017},
author = {Daniel Sznycer and Asmir Gračanin and Debra Lieberman},
keywords = {Tears, Value, Emotion, Motivation, Signaling},
abstract = {Although much is known about emotional tears from the perspectives of neurobiology and behavior, the production of emotional tears and the responses they elicit depend sensitively on a rich set of computations—one which has received little attention to date. This review article aims to close this gap. Emotional tearing occurs during negative events (e.g., injuries) and positive events (e.g., achievements). Episodes of tearing appear to be united by tearers' subjective imputation of negative or positive value to certain internal or external phenomena. Knowing the degree to which objects, organisms, events, and states of affairs enhance or diminish one's prospects—the value of things—is a pressing matter for humans and other organisms. Value information is produced for internal consumption, to be used by behavior-regulating mechanisms in the focal individual. But some evaluations are made available, in addition, to other people, through tearing and other forms of verbal and non-verbal communication. Tearing may function as an implicit plea for receivers (the tear targets) to minimize the costs imposed on the tearer by nature, by third-parties, or by the tear targets themselves—common when the tearer has lower formidability or wherewithal than tear targets do. In addition, tearing may exhort tear targets to infer and register which things the tearer values, positively or negatively. Here, we characterize tears, describe the game-theoretic logic of bargaining from a position of weakness, outline the computational systems that regulate the production of and responses to emotional tears, and review findings about emotional tearing that are relevant to the signaling hypothesis.}
}
@article{ELAZZAOUI2023119595,
title = {A digital twin-based edge intelligence framework for decentralized decision in IoV system},
journal = {Information Sciences},
volume = {649},
pages = {119595},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119595},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011805},
author = {Abir {El Azzaoui} and Sekione Reward Jeremiah and Neal N. Xiong and Jong Hyuk Park},
keywords = {IoV, Digital twin, Smart contracts, Smart transportation},
abstract = {The Internet of Vehicle (IoV) is an emerging technology for the development of future smart cities. With the fast and exponential growing rate of Internet of Things (IoT), the smart transportation field is ushering in a revolutionary advancement. Smart transportation systems facilitate better informed, more coordinated, and smarter use of transport networks, with the use of advanced information and communication technologies applied to vehicles to help improve traffic management, minimize congestion, improve safety, and ultimately provide a more intelligent use of transport networks. Smart transportation is an integral part of modern-day smart city projects. However, the world climate institution has reported that carbon emissions from the overall transportation system accounts for one-fifth of global carbon dioxide with a sum of around 24% of energy. Electric vehicles (EV) represent a solution for this issue, yet, it is not sustainable. The communication between EVs and a roadside unit (RSU), and the continuous computational power required to support an IoV system also requires a reliance on energy harvesting. With this in mind, in this paper, we propose a decentralized trust management solution for IoV systems to reduce both carbon footprint and offload the computation power required. Our solution resides in developing the digital twin of vehicles on an intelligent edge environment to simulate the physical vehicle and handle the required data processing. Also, we implement a smart contract model for fast, secure, and sustainable on-road battery recharging between EVs.}
}
@article{GAO2022509,
title = {An integrated simulation method for PVSS parametric design using multi-objective optimization},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {3},
pages = {509-526},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S209526352100087X},
author = {Qing Gao and Ying Yang and Qian Wang},
keywords = {Integrated simulation, PV shading System, Parametric design, Multi-objective optimization, Thermal-daylighting balance},
abstract = {An adequate strategy for achieving energy efficiency when designing a photovoltaic shading system (PVSS) shall find an equilibrium between sunlight heat gain and daylight transmittances through effective analysis tools in a building's early design phases. However, traditional simulation methods are either time-consuming or lacking architectonical thinking. This paper proposes a new method for architects to integrate thermal and daylighting performance by using parametric script modelling and optimize their balance with multi-objective optimization (MOO) algorithm in PVSS design. A case study was conducted to demonstrate the workflow of proposed integrated simulation method in PVSS design, and further compared the results with that of three single-objective optimizations under the same design requirement. The findings show that the integrated framework is a feasible method for PVSS design and can be extended into the design of other advance shading system or building integrated photovoltaic.}
}
@incollection{TEZDUYAR199521,
title = { - Massively parallel finite element computation of 3d flows - mesh update strategies in computation of moving boundaries and interfaces°†},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {21-30},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50131-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501316},
author = {T. Tezduyar and S. Aliabadi and M. Behr and A. Johnson and S. Mittal},
abstract = {Publisher Summary
This chapter describes the parallel implicit finite element computations of compressible and incompressible flows with the Connection Machine (CM)—CM-200 and CM-5. The parallel implementations are based on the assumption that the mesh is unstructured. The computations of flow problems involving moving boundaries and interfaces are achieved by using the deformable-spatial-domain or stabilized space-time method. In this method, with special mesh update strategies, the frequency of remeshing is minimized. This avoids the projection errors generated by remeshing and also avoids the cost associated with repeated mesh generation and parallelization setup. This method and its implementation on the massively parallel supercomputers provide a new capability to solve a large class of practical problems involving free surfaces, two-liquid interfaces, and fluid-structure interactions. Now 3D incompressible flow computations can be carried out at sustained speeds of up to 7.0 GigaFLOPS on the CM-5. The 3D compressible flow computations are carried out at sustained speeds of up to 12.2 GigaFLOPS on the CM-5. This parallel performance is significant in the sense that now there is a new level of computational capability in finite element solution of 3D flow problems. Several 3D flow problems are solved using these parallel and update mesh strategies. The chapter discusses the computation of incompressible flow occurring between two concentric cylinders, sloshing in a liquid-filled container subjected to vertical vibrations, and supersonic flow past a delta-wing.}
}
@article{CIULLO2021100349,
title = {A framework for building climate storylines based on downward counterfactuals: The case of the European Union Solidarity fund},
journal = {Climate Risk Management},
volume = {33},
pages = {100349},
year = {2021},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100349},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321000784},
author = {Alessio Ciullo and Olivia Martius and Eric Strobl and David N. Bresch},
keywords = {Climate storylines, Downward counterfactuals, European Union Solidarity Fund},
abstract = {Recent research introduced the concept of climate storylines as an alternative approach to estimate climate impact and better deal with uncertainties. A climate storyline is an event-based approach which aims at building “physically self-consistent unfolding of past events, or of plausible future events or pathways”. As such, climate storylines may profit from downward counterfactual thinking, which aims at analyzing how past events could have been worse. Notwithstanding the various applications of downward counterfactual thinking in the natural risk management literature, no study relates this with the climate storyline approach. The main goal of this paper is thus to introduce a framework that supports the development of climate storylines from downward counterfactuals. The framework is event-oriented, it focuses on impact, and it is designed to be applied in a participatory fashion. As a proof-of-concept application, we study the impact of tropical cyclones on the European Union Solidarity Fund (EUSF) without conducting a participatory analysis. Tropical cyclones represent a serious threat for the European outermost regions, and their impact to the EUSF capital availability has never been studied. We find that payouts due to tropical cyclones can hamper a recovery of the fund if large payouts concurrently occur in mainland Europe. To avoid this also considering future changes, an increase in capitalization up to 90 % percent may be required.}
}
@article{KUAI2021150,
title = {Multi-source brain computing with systematic fusion for smart health},
journal = {Information Fusion},
volume = {75},
pages = {150-167},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000646},
author = {Hongzhi Kuai and Ning Zhong and Jianhui Chen and Yang Yang and Xiaofei Zhang and Peipeng Liang and Kazuyuki Imamura and Lianfang Ma and Haiyuan Wang},
keywords = {Brain computing, Brain informatics, Data-Brain, Smart health, Systematic fusion, Intelligence system},
abstract = {With the progress of artificial intelligence, big data and functional neuroimaging technologies, brain computing has rapidly advanced our understanding of brain intelligence and brain disorders. We argue that existing data analytical methods have become insufficient for brain computing when dealing with multiple brain big data sources, because such methods mainly focus on flattening strategies and fail to work well for systematic understanding of the constituent elements of cognition, emotion and disease, as well as the intra- and inter-relations within and among themselves. To address this problem, we present in this paper a novel multi-source brain computing platform by Data-Brain driven systematic fusion. First, we formalize a series of behaviors surrounding the Brain Informatics-based investigation process, and present a conceptual model to systematically represent content and context of functional neuroimaging data. Then, we propose the systematic brain computing framework with multi-aspect fusion and inference to understand brain specificity and give uncertainty quantification, as well as its inspiration and applications for translational studies on brain health. In particular, a graph matching-based task search algorithm is introduced to help systematic experimental design and data sampling with multiple cognitive tasks. The study increases the interpretability and transparency of brain computing findings by inferring and testing multiple hypotheses taking into consideration the effect of evidence combination. Finally, multiple sources of knowledge (K), information (I) and data (D) are driven by a KID loop as the thinking space to inspire never-ending learning and multi-dimensional interactions in the connected social–cyber–physical spaces. Experimental results have demonstrated the efficacy of the proposed brain computing method with systematic fusion.}
}
@article{SUYOTO2015328,
title = {Parametric Approach as a Tool for Decision-making in Planning and Design Process. Case study: Office Tower in Kebayoran Lama},
journal = {Procedia - Social and Behavioral Sciences},
volume = {184},
pages = {328-337},
year = {2015},
note = {REFLECTIONS ON CREATIVITY: PUBLIC ENGAGEMENT AND THE MAKING OF PLACE},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815033479},
author = {William Suyoto and Aswin Indraprastha and Heru W. Purbo},
keywords = {parametric design, discrete method, office tower, building modeling},
abstract = {This study offers discrete method in parametric design to solve problems during design process (programming, site planning, massing, structure planning, and facade planning). This study is applied in the design of office tower in Kebayoran Lama, Jakarta. The objective of the study is to explore the uses of parametric design method, yet, maintains its time feasibility. The result of the study is a method for planning and design that is more advantageous than the conventional ones in terms of simultaneous, coordinated and accountable. This method enables designer to do many iterations and monitor changes during the design process. However, the method needs a higher skill in logical thinking during the process, which demands time.}
}
@incollection{CAPPAI2024804,
title = {Molecular Dynamics Simulations of Thermal Transport in Solid State Systems},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {804-820},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00095-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000957},
author = {Antonio Cappai and Claudio Melis and Luciano Colombo and Riccardo Dettori},
keywords = {Atomistic simulations, Classical molecular dynamics, Computational methods, Harmonic and anharmonic vibrational properties, Lattice thermal conductivity, Nanoscale thermal transport, Non-equilibrium thermodynamics, Thermal management, Thermal sciences, Thermoelectricity},
abstract = {In this chapter, we provide a synoptic review of the theoretical/computational approaches currently used to characterize thermal transport at the nanoscale, a topic of paramount importance for several applications and technological thermal management requirements. We focus in particular on the description of the atomistic techniques based on equilibrium (EMD), non-equilibrium (NEMD), and approach to equilibrium (AEMD) molecular dynamics (MD), which allow to efficiently describe relatively large and structurally complex systems with a reduced computational cost as compared to fully "ab-initio" techniques. We describe the theoretical background for each simulation strategy, as well as their implementation in state-of-the-art MD codes by underlying their intrinsic limitations and providing strategies to control some of them. We finally perform a series of benchmark calculations on bulk crystalline silicon by showing that the estimated thermal conductivity is weakly dependent on the specific strategy actually employed, while the overall computational cost is largely dependent on it.}
}
@article{GUNNING2021169,
title = {Brain-based mechanisms of late-life depression: Implications for novel interventions},
journal = {Seminars in Cell & Developmental Biology},
volume = {116},
pages = {169-179},
year = {2021},
note = {Special Issue: Myelin edited by Gonçalo Castelo-Branco and Roman Chrast / Special issue: Aging in the nervous system edited by Mara Mather},
issn = {1084-9521},
doi = {https://doi.org/10.1016/j.semcdb.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084952121001117},
author = {Faith M. Gunning and Lauren E. Oberlin and Maddy Schier and Lindsay W. Victoria},
keywords = {Aging, Depression, Functional connectivity, White matter, Apathy, Executive function},
abstract = {Late-life depression (LLD) is a particularly debilitating illness. Older adults suffering from depression commonly experience poor outcomes in response to antidepressant treatments, medical comorbidities, and declines in daily functioning. This review aims to further our understanding of the brain network dysfunctions underlying LLD that contribute to disrupted cognitive and affective processes and corresponding clinical manifestations. We provide an overview of a network model of LLD that integrates the salience network, the default mode network (DMN) and the executive control network (ECN). We discuss the brain-based structural and functional mechanisms of LLD with an emphasis on their link to clinical subtypes that often fail to respond to available treatments. Understanding the brain networks that underlie these disrupted processes can inform the development of targeted interventions for LLD. We propose behavioral, cognitive, or computational approaches to identifying novel, personalized interventions that may more effectively target the key cognitive and affective symptoms of LLD.}
}
@article{TWORZYDLO199387,
title = {Towards an automated environment in computational mechanics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {104},
number = {1},
pages = {87-143},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90208-F},
url = {https://www.sciencedirect.com/science/article/pii/004578259390208F},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to an automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. This includes algorithmic approaches, based on error estimation. adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here and confirm the feasibility of developing fully automated design environments.}
}
@article{HAMALAINEN199619,
title = {Accelerating genetic algorithm computation in tree shaped parallel computer},
journal = {Journal of Systems Architecture},
volume = {42},
number = {1},
pages = {19-36},
year = {1996},
issn = {1383-7621},
doi = {https://doi.org/10.1016/1383-7621(96)00009-4},
url = {https://www.sciencedirect.com/science/article/pii/1383762196000094},
author = {Timo Hämäläinen and Harri Klapuri and Jukka Saarinen and Pekka Ojala and Kimmo Kaski},
keywords = {Parallel genetic algorithms, Parallel implementation, Parallel computing, Tree shape architecture},
abstract = {Realizations of genetic algorithms (GAs) in a tree shape parallel computer architecture are presented using different levels of parallelism. In addition, basic models for parallel GAs are considered. The tree shape parallel computer system, GAPA (Genetic Algorithm Parallel Accelerator) with special hardware for GA computation, is described in detail. Also mappings for centralized and distributed GA models are given and their performance has been measured for different population sizes.}
}
@incollection{LI2014249,
title = {Chapter 8 - Image Processing at Your Fingertips: The New Horizon of Mobile Imaging},
editor = {Joel Trussell and Anuj Srivastava and Amit K. Roy-Chowdhury and Ankur Srivastava and Patrick A. Naylor and Rama Chellappa and Sergios Theodoridis},
series = {Academic Press Library in Signal Processing},
publisher = {Elsevier},
volume = {4},
pages = {249-264},
year = {2014},
booktitle = {Academic Press Library in Signal Processing: Volume 4},
issn = {2351-9819},
doi = {https://doi.org/10.1016/B978-0-12-396501-1.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012396501100008X},
author = {Xin Li},
keywords = {Mobile imaging, Mobile computing, Interactive image processing, Human network interaction},
abstract = {In this chapter, we briefly review the history of mobile imaging and current trend of mobile computing—namely interacting with a computer without an interface. Specifically, we highlight a list of image processing problems at fingertips: intelligent image acquisition, interactive image matting, dynamic image mosaicing and supervised image restoration. The unifying theme is how human interaction can reshape our thinking of conventional image processing algorithms. Several promising applications related to fingertip image processing are discussed at the end.}
}
@article{DUENASDIEZ2019514,
title = {How Chemistry Computes: Language Recognition by Non-Biochemical Chemical Automata. From Finite Automata to Turing Machines},
journal = {iScience},
volume = {19},
pages = {514-526},
year = {2019},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589004219302858},
author = {Marta Dueñas-Díez and Juan Pérez-Mercader},
keywords = {Chemistry, Chemical Reaction, Computer Science, Theory of Computation},
abstract = {Summary
Every problem in computing can be cast as decision problems of whether strings are in a language or not. Computations and language recognition are carried out by three classes of automata, the most complex of which is the Turing machine. Living systems compute using biochemistry; in the artificial, computation today is mostly electronic. Thinking of chemical reactions as molecular recognition machines, and without using biochemistry, we realize one automaton in each class by means of one-pot, table top chemical reactors: from the simplest, Finite automata, to the most complex, Turing machines. Language acceptance/rejection criteria by automata can be formulated using energy considerations. Our Turing machine uses the Belousov-Zhabotinsky chemical reaction and checks the same symbol in an Avogadro′s number of processors. Our findings have implications for chemical and general computing, artificial intelligence, bioengineering, the study of the origin and presence of life on other planets, and for artificial biology.}
}
@article{JIANG2024122157,
title = {Explicit potential function and fast algorithm for computing potentials in α×β conic surface resistor network},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122157},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122157},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026593},
author = {Xiaoyu Jiang and Gaojun Zhang and Yanpeng Zheng and Zhaolin Jiang},
keywords = {Resistor network, Chebyshev polynomials, DST-IV, Fast algorithm},
abstract = {Resistor network research is of great importance, yet many resistor networks and their large-scale fast computations have not received sufficient attention. This paper proposes a new resistor network with idiosyncratic shape, i.e., a α×β conic surface (CS) resistor network that resembles the upper part of a three-dimensional Dirac function. Utilizing the Recursion Transform (RT-V) method of Tan, a recursive matrix equation model is constructed based on Kirchhoff’s law and nodal voltages, which contains the modified tridiagonal Toeplitz matrix. By using the orthogonal matrix transformation, the eigenvalues and eigenvectors of the modified tridiagonal Toeplitz are obtained. The discrete sine transform of the fourth type (DST−IV) is utilized to solve node voltages, while the explicit potential function is represented by the Chebyshev polynomials of the second kind. In addition, explicit potential functions for some special cases are provided, and the potential distribution is illustrated using dynamic three-dimensional graph. To achieve a rapid calculation of the potential, a fast algorithm based on the multiplication of DST-IV with a vector is proposed. In the end, analysis of computational efficiency for the explicit potential function and the fast algorithm are shown.}
}
@article{DORESWAMY2023,
title = {Attributes That Influence Human Decision-Making in Complex Health Services: Scoping Review},
journal = {JMIR Human Factors},
volume = {10},
year = {2023},
issn = {2292-9495},
doi = {https://doi.org/10.2196/46490},
url = {https://www.sciencedirect.com/science/article/pii/S2292949523001153},
author = {Nandini Doreswamy and Louise Horstmanshof},
keywords = {human attributes, human decision-making, rationality, rational decision-making, health policy, health regulation, health services, },
abstract = {Background
Humans currently dominate decision-making in both clinical health services and complex health services such as health policy and health regulation. Many assumptions inherent in health service models today are underpinned by Ramsey’s Expected Utility Theory, a prominent theory in the field of economics that is rooted in rationality. Rational, evidence-based metrics currently dominate the culture of decision-making in health policy and regulation. However, as the COVID-19 pandemic has shown, rational metrics alone may not suffice in making better policy and regulatory decisions. There are ethical and moral considerations and other complex factors that cannot be reduced to evidence-based rationality alone. Therefore, this scoping review was undertaken to identify and map the attributes that influence human decision-making in complex health services.
Objective
The objective is to identify and map the attributes that influence human decision-making in complex health services that have been reported in the peer-reviewed literature.
Methods
This scoping review was designed to answer the following research question: what attributes have been reported in the literature that influence human decision-making in complex health services? A clear, reproducible methodology is provided. It is reported in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) standards and a recognized framework. As the topic of interest merited broad review to scope and understand literature from a holistic viewpoint, a scoping review of literature was appropriate here. Inclusion and exclusion criteria were developed, and a database search undertaken within 4 search systems—ProQuest, Scopus, PubMed, and Web of Science.
Results
The results span 46 years, from 1976 to 2022. A total of 167 papers were identified. After removing duplicates, 81 papers remained. Of these, 77 papers were excluded based on the inclusion and exclusion criteria. The remaining 4 papers were found to be relevant. Citation tracking was undertaken, identifying 4 more relevant papers. Thus, a total of 8 papers were included. These papers were reviewed in detail to identify the human attributes mentioned and count the frequency of mentions. A thematic analysis was conducted to identify the themes.
Conclusions
The results highlight key themes that underline the complex and nuanced nature of human decision-making. The results suggest that rationality is entrenched and may influence the lexicon of our thinking about decision-making. The results also highlight the counter narrative of decision-making underpinned by uniquely human attributes. This may have ramifications for decision-making in complex health services today. The review itself takes a rational approach, and the methods used were suited to this.
International Registered Report Identifier (IRRID)
RR2-10.2196/42353}
}
@article{WESSMANENZINGER2019105,
title = {Grade 5 children’s drawings for integer addition and subtraction open number sentences},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {105-128},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300731},
author = {Nicole M. Wessman-Enzinger},
keywords = {Integers, Integer addition and subtraction, Learner-generated drawings, Number line, Models, Instructional models, Student thinking},
abstract = {Three Grade 5 children participated in a microgenetic study embedded in 12-week teaching experiment on integer addition and subtraction. They solved open number sentences in four individual sessions across the 12-weeks and produced drawings. Through the lens of learner-generated drawings and qualitative analysis, these drawings provide perspective into the children’s thinking about integer addition and subtraction. The following categories are described: Single and Double Set of Objects, Number Sequences, Empty Number Lines, Number Lines, Number Sentences, Sign Emphasis, and Answer in Box Only. One student drew sets of objects frequently and the other students drew number lines more. Descriptions of how use of their drawings changed over time are provided. Implications point to a re-examination of integer instructional models and insight into potential learning progressions.}
}
@article{DELEERSNYDER2024105602,
title = {A multidimensional AI-trained correction to the 1D approximate model for Airborne TDEM sensing},
journal = {Computers & Geosciences},
volume = {188},
pages = {105602},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424000852},
author = {Wouter Deleersnyder and David Dudal and Thomas Hermans},
keywords = {Forward modelling, Machine Learning, Surrogate modelling, Electromagnetics, Airborne},
abstract = {The computational resources required to solve the full 3D inversion of time-domain electromagnetic data are immense. To overcome the time-consuming 3D simulations, we construct a surrogate model, more precisely, a data-driven statistical model to replace the 3D simulations. It is trained on 3D data and predicts the approximate output much faster. We construct a surrogate model that predicts the discrepancy between a 1D subsurface model and a deviation of the 1D assumption. The latter response is fastly computable with a semi-analytical 1D forward model. We exemplify the approach on a two-layered case. The results are encouraging even with few training samples. Given the computational cost related to the 3D simulations, there are limitations in the number of training samples that can be generated. In addition, certain applications require a wide range of parameters to be sampled, such as the electrical conductivity parameters in a saltwater intrusion case. The challenge of this work is achieving the best possible accuracy with only a few thousand samples. We propose to view the performance in terms of learning gain, representing the gain from the surrogate model whilst still acknowledging a residual discrepancy. Our works open new avenues for effectively simulating 3D TDEM data.}
}
@incollection{TONDEUR202424,
title = {Chapter 3 - Quality improvement movements},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {24-70},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00022-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000223},
author = {Yves Tondeur},
keywords = {Accreditation & technology-in-use, Commitment-based approach, Empirical vs. rational methods, Isomer selectivity, Isotope dilution, Known & documented quality, Legislating competition, Methods innovation rule, Performance assessment, Precision & trueness, Purpose of quality control samples, Quick fixes vs. fundamental solution, Structural conflicts},
abstract = {Emerging over recent years is the notion that quality improvements are hard to come by, when in fact, countless opportunities to integrate new developments are overlooked primarily because of the restrictive ways in which analytical protocols have been written and enforced, or the misconceptions about their function. A point of instability was reached. The manifestation of a quality malaise can be seen through the efforts by many to enhance quality by attempting to transfer the responsibility for quality back to those doing the work and to those who need the work products. The testing industry is now forced into abandoning old formal structures, mental models, and behaviors. Realigning our thinking, discovering the limits, and identifying the structural conflicts are essential if one wants to improve quality. This chapter makes clear that doing the same thing than before better is not enough, or to solely implement quick fixes can be wasteful; somehow, we need to ensure the application of the method coevolves with its environment. As chemists, what can we do?}
}
@incollection{BATCHELDER2015808,
title = {Mathematical Psychology: History},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {808-815},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43059-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086843059X},
author = {William H. Batchelder},
keywords = {Bayesian theory, Choice, Correlation coefficient, Decision making, Differential psychology, Experimental psychology, Factor analysis, Game theory, Information science, Learning theory, Marginal utility, Measurement theory, Memory, Paired-comparison scaling, Psychometrics, Psychophysics, Response time, Scaling, Signal detection theory, Stochastic processes, Theory of grammar, Utility theory},
abstract = {The application of mathematics to certain problems within the field of psychology dates back to at least the seventeenth century. This article reviews some of these early applications, most of which either involve theories for experimental phenomena or statistical methods for measuring individual differences. These later applications led to the field of psychometrics in the 1930s; and the former led to the field of mathematical psychology in the 1950s, and both fields are active today. Early mathematical psychology was characterized by testable, formal theories in the areas of learning and memory, perception and psychophysics, choice and decision making, language and thinking, and measurement and scaling; and these areas still characterize the field today.}
}
@article{ORAN1992251,
title = {Reactive-flow computations on a massively parallel computer},
journal = {Fluid Dynamics Research},
volume = {10},
number = {4},
pages = {251-271},
year = {1992},
issn = {0169-5983},
doi = {https://doi.org/10.1016/0169-5983(92)90025-R},
url = {https://www.sciencedirect.com/science/article/pii/016959839290025R},
author = {Elaine S. Oran and Jay P. Boris and C.Richard DeVore},
abstract = {Results are described of recent research and model developments for performing large-scale multidimensional compressible reacting-flow computations on the Connection Machine, a very fine-grained, parallel computer capable of multigigaflop performance. We are interested both in having general-purpose computer programs for routine production computations and in evaluating the architecture of the computer for a wide range of computational fluid dynamics and reacting-flow applications. We describe the hurdles involved in rethinking the structure and the algorithms to best suit this kind of computer, provide some relative timings for different programs, and describe ways of dealing with special constraints (such as periodic boundary conditions) imposed by the architecture. Finally, representative results are presented for several reacting and nonreacting computations, including the development and propagation of a spark in a hydrogen-oxygen mixture, an imploding detonation, and the generation of beam-channel turbulence.}
}
@incollection{ZHUGE201655,
title = {4 - The think lens},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {55-65},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000044},
author = {Hai Zhuge},
keywords = {Think lens, Semantic lens, models, semantic images, multi-dimensional, semantic link network},
abstract = {The nature of many research problems is about scale and dimension of observation and thinking. Whether the patterns and rules on one scale still hold on the other scale? Whether the patterns and rules on one dimension or some dimensions still hold on the other dimension or some other dimensions? Summarization is also about the scale and the dimension of motivation, representation and thinking. Human eyes can focus on not only a part of a representation but also the whole from a certain distance like the lens of camera. The think lens is a mechanism that can zoom in and out while observing, searching, mapping, analysing, planning, predicting, calculating, reasoning, imaging, and representing patterns through semantic computing on various representations according to some principles and rules. This section presents a concept model of the think lens for realising general summarisation in cyber-physical society.}
}
@article{BRAUND2013175,
title = {First steps in teaching argumentation: A South African study},
journal = {International Journal of Educational Development},
volume = {33},
number = {2},
pages = {175-184},
year = {2013},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2012.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0738059312000417},
author = {Martin Braund and Zena Scholtz and Melanie Sadeck and Robert Koopman},
keywords = {Critical thinking, Argumentation, Student teachers, Science},
abstract = {South African student teachers were studied to see how they coped with requirements to teach science using argumentation. Lesson observations, plans, reflective logs, post-teaching interviews and assessment of pupils’ argumentation were used to compare student teachers’ preparedness and interactions with pupils. Two clusters of students were identified representing high preparedness and low interaction. A high degree of preparedness alone did not guarantee high levels of argumentation. Schools’ educational situations were independent of success in teaching argumentation. The outcomes and implications for further development of teaching critical thinking are discussed.}
}
@article{LU2025121076,
title = {The neuroscientific basis of flow: Learning progress guides task engagement and cognitive control},
journal = {NeuroImage},
volume = {308},
pages = {121076},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121076},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925000783},
author = {Hairong Lu and Dimitri {Van der Linden} and Arnold B. Bakker},
keywords = {Learning progress, Flow experience, Engagement, Cognitive control, Goal-directed behavior},
abstract = {People often strive for deep engagement in activities, a state typically associated with feelings of flow - full task absorption accompanied by a sense of control and enjoyment. The intrinsic factors driving such engagement and facilitating subjective feelings of flow remain unclear. Building on computational theories of intrinsic motivation, this study examines how learning progress predicts engagement and directs cognitive control. Results showed that task engagement, indicated by feelings of flow and low distractibility, is a function of learning progress. Electroencephalography data further revealed that learning progress is associated with enhanced proactive preparation (e.g., reduced pre-stimulus contingent negativity variance and parietal alpha desynchronization) and improved feedback processing (e.g., increased P3b amplitude and parietal alpha desynchronization). The impact of learning progress on cognitive control is observed at the task-block and goal-episode levels, but not at the trial level. This suggests that learning progress shapes cognitive control over extended periods as progress accumulates. These findings highlight the critical role of learning progress in sustaining engagement and cognitive control in goal-directed behavior.}
}
@article{KOWALSKI2020103693,
title = {Effects of attention training technique on brain function in high- and low-cognitive-attentional syndrome individuals: Regional dynamics before, during, and after a single session of ATT},
journal = {Behaviour Research and Therapy},
volume = {132},
pages = {103693},
year = {2020},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2020.103693},
url = {https://www.sciencedirect.com/science/article/pii/S0005796720301479},
author = {Joachim Kowalski and Małgorzata Wierzba and Marek Wypych and Artur Marchewka and Małgorzata Dragan},
keywords = {Attention training technique, Metacognitve therapy, Attention, fMRI, S-REF},
abstract = {Objective
Attention Training Technique (ATT) is a key therapeutic tool in metacognitive therapy. There are numerous studies on the behavioral effects of ATT, however the neural mechanisms at work in the training are yet to be uncovered. To date there have been no controlled fMRI studies of ATT.
Method
We conducted a randomized double-blind controlled study of two groups with varying levels of cognitive-attentional syndrome (CAS). Groups with high (n = 43) and low (n = 46) levels of CAS underwent a single session of ATT or a control condition (CON) in an MRI scanner. Participants underwent resting state functional MRI (rsfMRI) sessions and rumination induction sessions both pre- and post-intervention Functional connectivity analyses and inter-subject correlations analyses were computed. We also collected data on emotion and attention functioning pre- and post-intervention.
Results
We did not observe any behavioral effects of ATT. However, direct comparison between ATT and CON sessions revealed greater inter-subject correlations in almost all hubs belonging to the studied functional networks. Moreover, subjects who received ATT showed diminished connectivity in the fronto-parietal network during ruminations and diminished connectivity of the precuneus with lateral occipital cortices and the intraparietal sulcus in abstract thinking and rsfMRI, respectively. Furthermore, some of the observed effects in functional connectivity and inter-subject correlations were specific to different levels of CAS.
Conclusions
Our results may support a proposed neural mechanism for ATT: disengagement of attention from CAS-type processing in either low- or high-CAS individuals. It is also possible that some neural effects of ATT are specific to individuals with different levels of CAS.}
}
@article{SCHULTZ2022104766,
title = {Animacy and the prediction of behaviour},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {140},
pages = {104766},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104766},
url = {https://www.sciencedirect.com/science/article/pii/S014976342200255X},
author = {Johannes Schultz and Chris D. Frith},
keywords = {Animacy, Action prediction, Goal-directed action, Mentalizing, Theory-of-Mind, Intentions, Economic games, Social cognition},
abstract = {To survive, all animals need to predict what other agents are going to do next. We review neural mechanisms involved in the steps required for this ability. The first step is to determine whether an object is an agent, and if so, how sophisticated it is. This involves brain regions carrying representations of animate agents. The movements of the agent can then be anticipated in the short term based solely on physical constraints. In the longer term, taking into account the agent’s goals and intentions is useful. Observing goal directed behaviour activates the neural action observation network, and predicting future goal directed behaviour is helped by the observer’s own action generating mechanisms. Intentions are critically important in determining actions when interacting with other agents, as several intentions can lie behind an action. Here, interpretation is helped by prior beliefs about the agent and the brain’s mentalising system is engaged. Biologically-constrained computational models of action recognition exist, but equivalent models for understanding intentional agents remain to be developed.}
}
@article{MENG201248,
title = {Extracting linguistic rules from data sets using fuzzy logic and genetic algorithms},
journal = {Neurocomputing},
volume = {78},
number = {1},
pages = {48-54},
year = {2012},
note = {Selected papers from the 8th International Symposium on Neural Networks (ISNN 2011)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2011.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231211004711},
author = {Dan Meng and Zheng Pei},
keywords = {Computing with Words, Linguistic rules, Fuzzy logic, Genetic algorithms},
abstract = {Linguistic rules in natural language are useful and consistent with human way of thinking. They are very important in multi-criteria decision making due to their interpretability. In this paper, our discussions concentrate on extracting linguistic rules from data sets. In the end, we firstly analyze how to extract complex linguistic data summaries based on fuzzy logic. Then, we formalize linguistic rules based on complex linguistic data summaries, in which, the degree of confidence of linguistic rules from a data set can be explained by linguistic quantifiers and its linguistic truth from the fuzzy logical point of view. In order to obtain a linguistic rule with a higher degree of linguistic truth, a genetic algorithm is used to optimize the number and parameters of membership functions of linguistic values. Computational results show that the proposed method is an alternative method for extracting linguistic rules with linguistic truth from data sets.}
}
@incollection{MONLEZUN2023159,
title = {Chapter 6 - AI+patient safety: adaptive, embedded, intelligent},
editor = {Dominique J. Monlezun},
booktitle = {The Thinking Healthcare System},
publisher = {Academic Press},
pages = {159-182},
year = {2023},
isbn = {978-0-443-18906-7},
doi = {https://doi.org/10.1016/B978-0-443-18906-7.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443189067000076},
author = {Dominique J. Monlezun},
keywords = {Bias, Blockchain, Clinical alarms, Clinical reports, Command center intelligence, Data privacy, Data security, Design thinking, Drug safety, Explainability, Patient safety, Reproducibility},
abstract = {This chapter discusses the AI transformation of patient safety within modern healthcare systems, particularly those digitally extended with telehealth. It carefully details the definitions and debates in patient safety (including the major actors and researchers in this community who share a general critique about the slow to absent sustained, substantive, and equally shared improvements in healthcare systems' safe delivery of patient care). The chapter provides the standard (including WHO) and innovative though still practical conceptualizations of patient safety and then progresses to recent more promising recent advances in human-centered, standardized, and AI-enabled patient safety (including safety as design thinking and system strategy). The chapter illustrates these developments with concrete use cases (in AI-enabled drug safety, clinical reports, and alarms) and augmentation with automation (including embedded, ambient, and command center safety intelligence). The chapter concludes by considering new and growing challenges in AI-driven patient safety (including data security, privacy, bias, and inconsistency) and emerging solutions (including blockchain, bias reduction, reproducibility, explainability, effectiveness, and safety in embedded design).}
}
@article{CHANG2017160,
title = {Dynamic modeling approaches to characterize the functioning of health systems: A systematic review of the literature},
journal = {Social Science & Medicine},
volume = {194},
pages = {160-167},
year = {2017},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617305300},
author = {Angela Y. Chang and Osondu Ogbuoji and Rifat Atun and Stéphane Verguet},
keywords = {Health systems, Dynamic modeling, Systems thinking, System dynamics},
abstract = {Universal Health Coverage (UHC) is one of the targets for the United Nations Sustainable Development Goal 3. The impetus for UHC has led to an increased demand for time-sensitive tools to enhance our knowledge of how health systems function and to evaluate impact of system interventions. We define the field of “health system modeling” (HSM) as an area of research where dynamic mathematical models can be designed in order to describe, predict, and quantitatively capture the functioning of health systems. HSM can be used to explore the dynamic relationships among different system components, including organizational design, financing and other resources (such as investments in resources and supply chain management systems) – what we call “inputs” – on access, coverage, and quality of care – what we call “outputs”, toward improved health system “outcomes”, namely increased levels and fairer distributions of population health and financial risk protection. We undertook a systematic review to identify the existing approaches used in HSM. We identified “systems thinking” – a conceptual and qualitative description of the critical interactions within a health system – as an important underlying precursor to HSM, and collated a critical collection of such articles. We then reviewed and categorized articles from two schools of thoughts: “system dynamics” (SD)” and “susceptible-infected-recovered-plus” (SIR+). SD emphasizes the notion of accumulations of stocks in the system, inflows and outflows, and causal feedback structure to predict intended and unintended consequences of policy interventions. The SIR + models link a typical disease transmission model with another that captures certain aspects of the system that impact the outcomes of the main model. These existing methods provide critical insights in informing the design of HSM, and provide a departure point to extend this research agenda. We highlight the opportunity to advance modeling methods to further understand the dynamics between health system inputs and outputs.}
}
@article{PALIOURA2025105186,
title = {“Storytelling and educational robotics: A scoping review (2004–2024)”},
journal = {Computers & Education},
volume = {225},
pages = {105186},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105186},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002008},
author = {Maria Palioura and Theodosios Sapounidis},
keywords = {Educational robotics, Storytelling, Scoping review},
abstract = {Storytelling has been used for years in educational practice and Educational Robotics is a rapidly growing field worldwide. Accordingly, researchers have attempted to combine Storytelling and Robotics in education. However, no systematic record exists on this combination. Therefore, we conducted a scoping review of 82 papers out of 5272 articles published in 5 Databases in the last 20 years to map the conducted research so far. In detail: the educational levels and the school subjects in which storytelling and educational robotics are applied, the types of robots used, the duration, the sample size, participants' age and the skills that students may develop through this combination. Additionally, we analyzed, grouped, and presented the tools used for measuring the potential effects of storytelling and educational robotics. Finally, the students' role in the activities was sought. Based on our findings, most interventions mainly addressed preschool and primary school students, the robots used the most are humanoid, most interventions did not exceed 6 h, and the number of participants was less than 20 students. Besides, most interventions tried to develop students’ skills (communication, creativity, collaboration) and attitudes (engagement, motivation, participation) with qualitative tools borrowed from other domains (e.g. psychology, healthcare). This scoping demonstrates a gap in the use of storytelling and educational robotics in secondary and university education and subjects like history, geography, etc. Finally, this combination seems to have the potential to enhance the educational process, but more research is needed to shed light on all the aspects of the combination.}
}
@article{GAO2025102628,
title = {An organic artificial synaptic memristor for neuromorphic computing},
journal = {Applied Materials Today},
volume = {43},
pages = {102628},
year = {2025},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2025.102628},
url = {https://www.sciencedirect.com/science/article/pii/S2352940725000472},
author = {Kaikai Gao and Bai Sun and Bo Yang and Zelin Cao and Yu Cui and Mengna Wang and Chuncai Kong and Guangdong Zhou and Sihai Luo and Xiaoliang Chen and Jinyou Shao},
keywords = {Artificial synaptic, Organic material, Neuromorphic computing, Dataset recognition, Artificial intelligence},
abstract = {Developing an artificial synaptic device that can simulate the learning and memory abilities of the human brain is a key step toward achieving neuromorphic computing. Although traditional transistors and emerging memristors are considered potential candidates for achieving these functions, their manufacturing often relies on non-renewable semiconductor materials. Here, we have successfully fabricated a flexible artificial synaptic device with a typical memristive sandwich structure (Ag/PMMA/SLE/Ti) utilizing the cost-effective organic material sodium lignosulfonate (SLE) as the dielectric layer. This device effectively achieves short-term plasticity (STP), spike-number-dependent plasticity (SNDP), and long-term potentiation/depression (LTP/LTD). Furthermore, the conductance of the designed artificial synapse corresponds to synaptic weights, which can be recognized by neuromorphic computation on CYCLE and MNIST datasets (small/large sizes) with an accuracy of 33.8 % and 89.3 %/91.0 %, respectively. Therefore, this artificial synaptic device exhibits the flexibility to serve in various wearable scenarios, including intelligent electronic skin (e-skin). Additionally, the excellent biocompatibility of SLE aligns well with the concept of green electronics.}
}
@article{SHEARER2021,
title = {Foodborne Illness Outbreak Investigation for One Health Postsecondary Education},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {2},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00129-21},
url = {https://www.sciencedirect.com/science/article/pii/S193578772100143X},
author = {Adrienne E. H. Shearer and Kalmia E. Kniel},
keywords = {food safety, investigation, One Health, education, microbiology, public health, escape room, problem-based learning, epidemiology, environment},
abstract = {One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups.
ABSTRACT
One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups. Tasks addressed epidemiology, laboratory, environment, traceback, recall, and prevention concepts. Gamification of the exercise involved generation of a numerical code to unlock a combination lock as an indication of successful organization, compilation, and interpretation of data. Students presented investigation findings and responses to critical thought questions on their roles. Student surveys on engagement and self-perceived change in conceptual understanding indicated that nearly all expressed increased understanding of outbreak investigations, safe food production, and environmental water as a transmission vehicle. Volunteered learned concepts indicated enhanced appreciation for the complexity of food safety and interdisciplinary connections. Students enjoyed the exercise (92%) and cited the clues and group interaction among the most enjoyable features. Objective assessment of student conceptual learning with the subset of students who conducted the investigation individually (n = 58) demonstrated significant increase in correct test responses (49% pretest; 76% posttest) after completion of the investigation for all questions combined and across all learning objectives. These data demonstrate the value of a foodborne disease investigation with escape room gamification features for engaging students in One Health concepts and exercising problem-solving, critical thinking, and skills for independent and collaborative work.}
}
@incollection{ANGELETOS2023613,
title = {Chapter 20 - Dampening general equilibrium: incomplete information and bounded rationality☆☆This chapter subsumes an older paper of ours, entitled “Dampening General Equilibrium: From Micro to Macro” (Angeletos and Lian, 2017). We have benefited from the comments of various colleagues, especially those of the editors, Rüdiger Bachmann, Wilbert van der Klaauw, and Giorgio Topa. Angeletos acknowledges the support of the National Science Foundation under Grant Number SES-1757198.},
editor = {Rüdiger Bachmann and Giorgio Topa and Wilbert {van der Klaauw}},
booktitle = {Handbook of Economic Expectations},
publisher = {Academic Press},
pages = {613-645},
year = {2023},
isbn = {978-0-12-822927-9},
doi = {https://doi.org/10.1016/B978-0-12-822927-9.00028-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229279000288},
author = {George-Marios Angeletos and Chen Lian},
keywords = {General equilibrium, incomplete information, higher-order beliefs, Level- Thinking, reflective equilibrium, coordination, beauty contests},
abstract = {We review how realistic frictions in information and/or rationality arrest general equilibrium (GE) feedbacks. In one specification, we maintain rational expectations but remove common knowledge of aggregate shocks. In another, we replace rational expectations with Level-k Thinking or a smooth variant thereof. Two other approaches, heterogeneous priors and cognitive discounting, capture the same essence while offering a gain in tractability. Relative to the full-information rational-expectation (FIRE) benchmark, all these modifications amount to attenuation of GE effects, especially in the short run. This in turn translates to either under- or overreaction in aggregate outcomes, depending on whether GE feedbacks are positive or negative in the first place. We review a few applications, with emphasis on monetary and fiscal policy. We finally discuss how the available evidence on expectations, along with other considerations, can help guide the choice among the various alternatives, as well as between them and FIRE.}
}
@article{BLELLOCH199490,
title = {Parallel solutions to geometric problems in the scan model of computation},
journal = {Journal of Computer and System Sciences},
volume = {48},
number = {1},
pages = {90-115},
year = {1994},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(05)80023-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022000005800236},
author = {Guy E. Blelloch and James J. Little},
abstract = {This paper describes several parallel algorithms that solve geometric problems. The algorithms are based on a vector model of computation-the scan model. The purpose of this paper is both to show how the model can be used and to formulate a set of practical algorithms. The scan model is based on a small set of operations on vectors of atomic values. It differs from the P-RAM models both in that it includes a set of scan primitives, also called parallel prefix computations, and in that it is a strictly data-parallel model. A very useful abstraction in the scan model is the segment abstraction, the subdivision of a vector into a collection of independent smaller vectors. The segment abstraction permits a clean formulation of divide-and-conquer algorithms and is used heavily in the algorithms described in this paper. Within the scan model, using the operations and routines defined, the paper describes a k-D tree algorithm requiring O(lg n) calls to the primitives for n points, a closest-pair algorithm requiring O(lg n) calls to the primitives, a line-drawing algorithm requiring O(1) calls to the primitives, a line-of-sight algorithm requiring O(1) calls to the primitives, and finally, three different convex-hull algorithms. The last convex-hull algorithm, merge-hull, utilizes a generalized binary search technique using divide-and-conquer with the segment abstraction. The paper also describes how to implement the CREW version of Cole's merge sort in O(lg n) calls to the primitives. All these algorithms should be noted for their simplicity rather than their complexity; many of them are parallel versions of known serial algorithms. Most of the algorithms discussed in this paper have been implemented on the Connection Machine, a highly parallel single instruction multiple data computer.}
}
@article{GINOSAR20231858,
title = {Are grid cells used for navigation? On local metrics, subjective spaces, and black holes},
journal = {Neuron},
volume = {111},
number = {12},
pages = {1858-1875},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323002234},
author = {Gily Ginosar and Johnatan Aljadeff and Liora Las and Dori Derdikman and Nachum Ulanovsky},
abstract = {Summary
The symmetric, lattice-like spatial pattern of grid-cell activity is thought to provide a neuronal global metric for space. This view is compatible with grid cells recorded in empty boxes but inconsistent with data from more naturalistic settings. We review evidence arguing against the global-metric notion, including the distortion and disintegration of the grid pattern in complex and three-dimensional environments. We argue that deviations from lattice symmetry are key for understanding grid-cell function. We propose three possible functions for grid cells, which treat real-world grid distortions as a feature rather than a bug. First, grid cells may constitute a local metric for proximal space rather than a global metric for all space. Second, grid cells could form a metric for subjective action-relevant space rather than physical space. Third, distortions may represent salient locations. Finally, we discuss mechanisms that can underlie these functions. These ideas may transform our thinking about grid cells.}
}
@article{BELLO2025101316,
title = {Self-control on the path toward artificial moral agency},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101316},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001104},
author = {Paul Bello and Will Bridewell},
keywords = {Self-control, Attention, Cognitive architecture},
abstract = {The ability of agents to commit to their plans and see them through is a core concept in the philosophy of action (Bratman, 1987, Holton, 2009) and is considered to be a defining feature of having an intention. Seeing plans through in the face of highly compelling opportunities for action that are incompatible with our current commitments requires self-control. In this review paper, we draw upon ancient and modern literature on self-control along with contemporary ideas about the cognitive architecture supporting intentional action to argue that any computational account of moral agency must include an approach to self-control. In addition, we extract and develop a list of necessary features of the phenomena against which individual modeling efforts can be compared. The ARCADIA cognitive system will be discussed in light of this list of features and used to demonstrate both success and failure in a highly simplified self-control dilemma. Finally, we end by discussing a path toward more functionally complete models of agency and control, along with offering perfunctory thoughts on some of the more conceptually challenging issues to address in the future.}
}
@article{ZHANG2025100889,
title = {Identification of Critical Phosphorylation Sites Enhancing Kinase Activity With a Bimodal Fusion Framework},
journal = {Molecular & Cellular Proteomics},
volume = {24},
number = {1},
pages = {100889},
year = {2025},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2024.100889},
url = {https://www.sciencedirect.com/science/article/pii/S1535947624001798},
author = {Menghuan Zhang and Yizhi Zhang and Keqin Dong and Jin Lin and Xingang Cui and Yong Zhang},
keywords = {kinase activity, critical phosphorylation site, deep learning, phosphorylation mass spectrometry, embedding},
abstract = {Phosphorylation is an indispensable regulatory mechanism in cells, with specific sites on kinases that can significantly enhance their activity. Although several such critical phosphorylation sites (phos-sites) have been experimentally identified, many more remain to be explored. To date, no computational method exists to systematically identify these critical phos-sites on kinases. In this study, we introduce PhoSiteformer, a transformer-inspired foundational model designed to generate embeddings of phos-sites using phosphorylation mass spectrometry data. Recognizing the complementary insights offered by protein sequence data and phosphorylation mass spectrometry data, we developed a classification model, CSPred, which employs a bimodal fusion strategy. CSPred combines embeddings from PhoSiteformer with those from the protein language model ProtT5. Our approach successfully identified 77 critical phos-sites on 58 human kinases. Two of these sites, T517 on PKG1 and T735 on PRKD3, have been experimentally verified. This study presents the first systematic and computational approach to identify critical phos-sites that enhance kinase activity.}
}
@article{LIMONGELLI199289,
title = {Abstract specification of structures and methods in symbolic mathematical computation},
journal = {Theoretical Computer Science},
volume = {104},
number = {1},
pages = {89-107},
year = {1992},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(92)90167-E},
url = {https://www.sciencedirect.com/science/article/pii/030439759290167E},
author = {C. Limongelli and M. Temperini},
abstract = {This paper describes a methodology based on the object-oriented programming paradigm, to support the design and implementation of a symbolic computation system. The requirements of the system are related to the specification and treatment of mathematical structures. This treatment is considered from both the numerical and the symbolic points of view. The resulting programming system should be able to support the formal definition of mathematical data structures and methods at their highest level of abstraction, to perform computations on instances created from such definitions, and to handle abstract data structures through the manipulation of their logical properties. Particular consideration is given to the correctness aspects. Some examples of convenient application of the proposed design methodology are presented.}
}
@article{ISLAM2022100280,
title = {Industry 4.0: Skill set for employability},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100280},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000341},
author = {Md. Aminul Islam},
keywords = {Industry 4.0, Skills, Competencies, Graduates, A lower middle-income country, Bangladesh},
abstract = {This paper aims at finding whether students are ready to perform in the modern competitive business job arena. Most importantly, if they have the required skills and competencies to catch the opportunity offered by companies at the fourth industrial revolution where we notice the trend of automation and data exchange and IoT, cloud computing and cognitive computing have taken the lead. Our target participants in the survey include students from public and private universities in Bangladesh who will perform in the job market and who are already in the market. This is how we can bridge the gap between employers' expectations and students' perceptions of skills and competencies they acquire before entering the job market. After surveying and analyzing data collected from 361 undergraduate and graduate-level students, we found that both business and technology impact employment. Students are aware of the changing job market scenario, and they are trying to have those skills which will make them competent compared to the early years, but they are not prepared enough to accept the challenges faced in industry 4.0. This paper will be helpful for both the academicians to be aware of the future trend of the market so that they can prepare students to fight the challenges and do future research on them. At the same time, employers can get some ideas how students are thinking right now and how much training and development opportunity they should arrange for the newly recruited graduates who have lack expertise but if they are trained up, can be a source of strength for the companies.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{BROWN201511,
title = {On unifiers, diversifiers, and the nature of pattern recognition},
journal = {Pattern Recognition Letters},
volume = {64},
pages = {11-20},
year = {2015},
note = {Philosophical Aspects of Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001312},
author = {Gavin Brown},
keywords = {Nature of pattern recognition, Unifying, Diversifying, Dyson},
abstract = {We study a dichotomy of scientific styles, unifying and diversifying, as proposed by Freeman J. Dyson. We discuss the extent to which the dichotomy transfers from the natural sciences (where Dyson proposed it) to the field of Pattern Recognition. To address this we must firstly ask what it means to be a “unifier” or “diversifier” in a field, and what are the relative merits of each style of thinking. Secondly, given that Dyson applied this to the sciences, does it also apply in a field known to be a blend of science and engineering? Parallels are drawn to Platonic/Aristotelian views, and to Cartesian/Baconian science, and questions are asked on what drives the Kuhnian paradigm shifts of our field. This article is intended not to marginalise individuals into categories (unifier/diversifier) but instead to demonstrate the utility of philosophical reflection on our field, showing the depth and complexities a seemingly simple idea can unearth.}
}
@article{DEVINK20222744,
title = {Cooperativity as quantification and optimization paradigm for nuclear receptor modulators††Electronic supplementary information (ESI) available: Experimental details, supporting figures and tables. See DOI: 10.1039/d1sc06426f},
journal = {Chemical Science},
volume = {13},
number = {9},
pages = {2744-2752},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d1sc06426f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023017297},
author = {Pim J. {de Vink} and Auke A. Koops and Giulia D'Arrigo and Gabriele Cruciani and Francesca Spyrakis and Luc Brunsveld},
abstract = {ABSTRACT
Nuclear Receptors (NRs) are highly relevant drug targets, for which small molecule modulation goes beyond a simple ligand/receptor interaction. NR–ligands modulate Protein–Protein Interactions (PPIs) with coregulator proteins. Here we bring forward a cooperativity mechanism for small molecule modulation of NR PPIs, using the Peroxisome Proliferator Activated Receptor γ (PPARγ), which describes NR–ligands as allosteric molecular glues. The cooperativity framework uses a thermodynamic model based on three-body binding events, to dissect and quantify reciprocal effects of NR–coregulator binding (KID) and NR–ligand binding (KIID), jointly recapitulated in the cooperativity factor (α) for each specific ternary ligand·NR·coregulator complex formation. These fundamental thermodynamic parameters allow for a conceptually new way of thinking about structure–activity-relationships for NR–ligands and can steer NR modulator discovery and optimization via a completely novel approach.}
}
@article{AUMER2024100111,
title = {Impaired cognitive flexibility in schizophrenia: A systematic review of behavioral and neurobiological findings},
journal = {Biomarkers in Neuropsychiatry},
volume = {11},
pages = {100111},
year = {2024},
issn = {2666-1446},
doi = {https://doi.org/10.1016/j.bionps.2024.100111},
url = {https://www.sciencedirect.com/science/article/pii/S2666144624000297},
author = {Philipp Aumer and Geva A. Brandt and Dusan Hirjak and Florian Bähner},
keywords = {Schizophrenia, Cognitive flexibility, Set-shifting, Wisconsin card sorting test, Intra-extra dimensional set shift, Cambridge Neuropsychological Test Automated Battery},
abstract = {Background and hypothesis
Impaired cognitive flexibility in schizophrenia (SZ) is well documented and correlation with worse functional outcome indicates clinical relevance. Paradigms that assess cognitive flexibility include the Wisconsin Card Sorting Test (WCST) and the Cambridge Neuropsychological Test Automated Battery’s (CANTAB) Intra-Extra Dimensional Set Shift (IED). This systematic review provides an overview of the current state of research on cognitive flexibility in schizophrenia and points out relevant areas of non-consensus.
Methods
Two electronic databases (Embase and PubMed) were searched for records published from 1993 to 2024 on adult SZ patients that were assessed for cognitive flexibility/set-shifting ability using WCST and/or IED.
Results
38 studies were included in the review, most of which reported significantly worse performance of SZ patients in WCST and/or IED compared to healthy controls (HC). Most publications focused on the specific profile of cognitive inflexibility. Other aspects included progression of cognitive inflexibility over the course of the illness, neurobiological correlates, IQ as a possible confounder and whether cognitive inflexibility is a heritable trait.
Conclusion
Included studies show that cognitive inflexibility rather reflects a stable trait than a state, indicating a lasting prefrontal impairment in SZ. Further longitudinal studies are needed to clarify how these deficits evolve during progression of the disorder. Neither antipsychotic medication nor intelligence seem to explain impaired cognitive flexibility. However, a disease-specific cognitive phenotype has not yet been established and additional research on neuro-computational mechanisms is thus needed to identify possible targets for interventional studies.}
}
@article{ZHANG2024103588,
title = {Anonymous data sharing scheme for resource-constrained internet of things environments},
journal = {Ad Hoc Networks},
volume = {163},
pages = {103588},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103588},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524001999},
author = {Zetian Zhang and Jingyu Wang and Lixin Liu and Yongfeng Li and Yun Hao and Hanqing Yang},
keywords = {Anonymity, Resource-constrained, Integrity verification, Data sharing, Accountability, Revocation},
abstract = {With the rapid development of Internet of Things (IoT) technology in industrial, agricultural, medical and other fields, IoT terminal devices face security and privacy challenges when sharing data. Among them, ensuring data confidentiality, achieving dual-side privacy protection, and performing reliable data integrity verification are basic requirements. Especially in resource-constrained environments, limitations in the storage, computing, and communication capabilities of devices increase the difficulty of implementing these security safeguards. To address this problem, this paper proposes a resource-constrained anonymous data-sharing scheme (ADS-RC) for the IoT. In ADS-RC, we use elliptic curve operations to replace computation-intensive bilinear pairing operations, thereby reducing the computational and communication burden on end devices. We combine an anonymous verifiable algorithm and an attribute encryption algorithm to ensure double anonymity and data confidentiality during the data-sharing process. To deal with potential dishonest behavior, this solution supports the revocation of malicious user permissions. In addition, we designed a batch data integrity verification algorithm and stored verification evidence on the blockchain to ensure the security and traceability of data during transmission and storage. Through experimental verification, the ADS-RC scheme achieves reasonable efficiency in correctness, security and efficiency, providing a new solution for data sharing in resource-constrained IoT environments.}
}
@article{NEVES2009834,
title = {Structuring an MCDA model using SSM: A case study in energy efficiency},
journal = {European Journal of Operational Research},
volume = {199},
number = {3},
pages = {834-845},
year = {2009},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2009.01.053},
url = {https://www.sciencedirect.com/science/article/pii/S0377221709002033},
author = {L.P. Neves and L.C. Dias and C.H. Antunes and A.G. Martins},
keywords = {Problem structuring methods, Multiple criteria analysis, SSM, Value Focused Thinking, Energy efficiency},
abstract = {This work presents the use of a problem structuring method, Soft Systems Methodology (SSM), to structure a Multi-Criteria Decision Analysis (MCDA) model, aimed at appraising energy efficiency initiatives. SSM was useful to help defining clearly the decision problem context and the main actors involved, as well as to unveil the relevant objectives for each stakeholder. Keeney’s Value Focused Thinking approach was then used to refine and structure the list of objectives according to the perspective of the main evaluators identified. In addition to describing this particular case study, this paper aims at providing some general guidelines on how SSM may facilitate the emergence of objectives for MCDA models.}
}
@incollection{KRAAK2009468,
title = {Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {468-480},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00033-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044910400033X},
author = {M.-J. Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated-multiple-views, Geocomputation, Geoservices, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual exploration, Visual representation, Visual thinking},
abstract = {Recent developments in information and communication technology (ICT) have introduced many new opportunities, and have influenced many scientific disciplines in application of their methods and techniques. From a mapping perspective, this includes cartography and related disciplines like scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GI Science. Interactivity and dynamics are prominent keywords and allow one not only to apply maps and diagrams to present-known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. This allows for simultaneous alternative views of the data and stimulates visual thinking, resulting in geovisualization.}
}
@article{ROBERTSON2009136,
title = {Impact of CAD tools on creative problem solving in engineering design},
journal = {Computer-Aided Design},
volume = {41},
number = {3},
pages = {136-146},
year = {2009},
note = {Computer Support for Conceptual Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448508001334},
author = {B.F. Robertson and D.F. Radcliffe},
keywords = {CAD, Creativity, Conceptual design},
abstract = {This paper presents the results of a survey of CAD users that examined the ways in which their computational environment may influence their ability to design creatively. This extensive online survey builds upon the findings of an earlier observational case study of the use of computer tools by a small engineering team. The case study was conducted during the conceptual and detailed stages of the design of a first-to-world product. Four mechanisms by which CAD tools may influence the creative problem solving process were investigated: enhanced visualisation and communication, circumscribed thinking, premature design fixation and bounded ideation. The prevalence of these mechanisms was examined via a series of questions that probed the user’s mode of working, attitudes, and responses to hypothetical situations. The survey showed good support for the first three mechanisms and moderate support for the fourth. The results have important implications for both the users and designers of CAD tools.}
}
@article{ASGARI20251631,
title = {Uncovering the role of big data analytics on the resilience of agri-food supply chains: a systematic literature review},
journal = {Procedia Computer Science},
volume = {253},
pages = {1631-1639},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.225},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002339},
author = {Alireza Asgari},
keywords = {Big Data Analytics, Resilience, Agri-food Supply Chain, systematic literature review},
abstract = {In an era marked by frequent disruptions, ensuring the resilience of agri-food supply chains is critical for maintaining food availability and accessibility. The significant increase in generation and complexity of data due to the advancement of computational power and relational databases, has made the discovery of new information through the process of data collection and manipulation possible. Big data analytics can contribute significantly to attaining agri-food supply chain and has many implications inside the food sector. This systematic literature review investigates the role of big data analytics in bolstering agri-food supply chain’s ability to anticipate, respond to, and recover from disruptions. The findings reveal a positive influence of predictive and prescriptive analytics on readiness and response phases of resilience, particularly on capabilities such as risk management, flexibility, and agility. Based on these findings, a comprehensive framework is proposed which maps the current state-of-the-art in addition to identifying gaps in the current literature, offering valuable insights for practitioners and guiding future research.}
}
@article{DEMSZKY2025105183,
title = {Automated feedback improves teachers’ questioning quality in brick-and-mortar classrooms: Opportunities for further enhancement},
journal = {Computers & Education},
volume = {227},
pages = {105183},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105183},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001970},
author = {Dorottya Demszky and Jing Liu and Heather C. Hill and Shyamoli Sanghi and Ariel Chung},
keywords = {Computer-assisted instruction, Brick-and-mortar classroom, Natural language processing, Automated teacher feedback, Randomized controlled trial, Focusing questions, Mixed-methods study, K-12 instruction},
abstract = {AI-powered professional learning tools that provide teachers with individualized feedback on their instruction have proven effective at improving instruction and student engagement in virtual learning contexts. Despite the need for consistent, personalized professional learning in K-12 settings, the effectiveness of automated feedback tools in traditional classrooms remains unexplored. We present results from 224 Utah mathematics and science teachers who engaged in a pre-registered randomized controlled trial, conducted in partnership with TeachFX, to assess the impact of automated feedback in K-12 classrooms. This feedback targeted “focusing questions” — questions that probe students’ thinking by pressing for explanations and reflection. We find that teachers opened emails containing the automated feedback about 53–65% of the time, and the feedback increased their use of focusing questions by 20% (p < 0.01) compared to the control group. The feedback did not impact other teaching practices. Qualitative interviews with 13 teachers revealed mixed perceptions of the automated feedback. Some teachers appreciated the reflective insights, while others faced barriers such as skepticism about accuracy, data privacy concerns, and time constraints. Our findings highlight the promises and areas of improvement for implementing effective and teacher-friendly automated professional learning tools in brick-and-mortar classrooms.}
}
@article{NASSIRI201329,
title = {Computational modelling of long bone fractures fixed with locking plates – How can the risk of implant failure be reduced?},
journal = {Journal of Orthopaedics},
volume = {10},
number = {1},
pages = {29-37},
year = {2013},
issn = {0972-978X},
doi = {https://doi.org/10.1016/j.jor.2013.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0972978X13000020},
author = {M. Nassiri and B. MacDonald and J.M. O'Byrne},
keywords = {Modeling, Fracture, Locking, Plate, Failure},
abstract = {Background and purpose
The Locking Compression Plate (LCP) is part of a new plate generation requiring an adapted surgical technique and new thinking about commonly used concepts of internal fixation using plates. Knowledge of the fixation stability provided by these new plates is very limited and clarification is still necessary to determine how the mechanical stability and the risk of implant failure can best be controlled.
Methods
Upon validation, a finite element model of an LCP attached to a cylinder was developed to simulate and analyse the biomechanics of a transverse long bone fracture fixed with a locking plate. Of special interest were the factors influencing the mechanical conditions at the fracture site, the control of interfragmentary movement and implant failure.
Results
Several factors were shown to influence stability in compression. Increasing translation and/or fracture angle post fixation reduced construct stability. Axial stiffness was also influenced by the working length and plate-bone distance. The fracture gap had no effect on the construct stability when no bone contact occurred during loading. Stress analysis of the LCP demonstrated that the maximum Von Mises stresses were found in the innermost screws at the screw-head junction.
Interpretation
For the clinical use of the LCP as a locked internal fixator in fractures with an interfragmentary gap of 1 mm, at least two to four plate holes near the fracture gap should be omitted to allow fracture motion and bone contact to occur. This will also achieve a larger area of stress distribution on the plate and reduce the likelihood of fatigue failure due to cyclic loading.}
}
@article{GOMEZMARTINEZ2025129806,
title = {A bioinspired model of decision making guided by reward dimensions and a motivational state},
journal = {Neurocomputing},
pages = {129806},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129806},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225004783},
author = {Diana G. Gómez-Martínez and Alison Muñoz-Capote and Oscar Hernández and Francisco Robles and Félix Ramos},
keywords = {Decision-making, Magnitude reward, Probability reward, Decision criteria, Motivational state, Nondeterministic model},
abstract = {The decision-making process is a critical component of computational systems, whose processing involves the evaluation of various alternatives presented as possible solutions to a given problem, depending on the current context. This paper seeks to show how a neuroscience-based decision-making mechanism (DMM) integrating decision criteria, knowledge of reward stimuli, and motivational information helps to contribute to producing human-like adaptive behavior. To fulfill this objective, a computational model on DMM is proposed. The alternatives in this proposed model are constructed based on preferences, and the selection of the best alternative is guided by a goal-directed control scheme influenced by a motivational state (MS). The formation of preferences considers some dimensions of the reward, e.g., magnitude, probability of receiving the reward, incentive salience, and affective value. To validate the model exhibits a behavior considering parameters human being uses to compute its behavior, a case study was proposed. The case study’s objective is to gain the maximum reward (food) from the choice of a 4-choice card (a variation of Iowa Gambling Test), each card has a reward and a contingency probability associated with it. The analysis of the results of the case study shows that the model presents a short exploitation stage to find the contingency rule and choose the best option frequently according to some studies, also observed that the utility value of the card influenced the MS of hunger and other factors play a critical role in the DMM.}
}
@article{DAS2025111780,
title = {ANN-based prediction of aerodynamic force coefficients and shape optimization using MOEO of a high-rise building with varying cross-section along height},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111780},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.111780},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225000166},
author = {Arghyadip Das and Sujit Kumar Dalui},
keywords = {Tall building, Computational fluid dynamics (CFD), Corner recession, Force coefficient, Artificial neural network (ANN), Multi-objective equilibrium optimization (MOEO)},
abstract = {The main goal of this study is to find the effect of various geometrical configurations with varying cross-sections along the height of a corner recessed square tall building, considering the wind flow around the buildings from 0° to 360°. The design variables considered in this study are the amount of corner recessing (S), the height of the square cross-section (h) and the wind incidence angle (Ø). The design variables are generated using the simple random sampling technique in MATLAB. A series of numerical analyses have been performed for those randomly generated design variables using Computational Fluid Dynamics (CFD) in the ANSYS-CFX module to evaluate the along and across-wind force coefficients (Cfx and Cfy). The numerical simulations have been carried out using the k-ε turbulence model on a length scale of 1:300. The results of the CFD simulations are used to train the artificial neural network (ANN) of Cfx and Cfy. After simulating the networks of Cfx and Cfy, a shape optimization study has been carried out using Multi-Objective Equilibrium Optimization (MOEO) to find the optimal shapes of various building configurations considering different weights of the design function values (Cfx and Cfy). The results of the shape optimization study have been validated with separate CFD investigations and several wind tunnel experiments.}
}
@article{HERTENSTEIN20191213,
title = {Modulation of creativity by transcranial direct current stimulation},
journal = {Brain Stimulation},
volume = {12},
number = {5},
pages = {1213-1221},
year = {2019},
issn = {1935-861X},
doi = {https://doi.org/10.1016/j.brs.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1935861X19302293},
author = {Elisabeth Hertenstein and Elena Waibel and Lukas Frase and Dieter Riemann and Bernd Feige and Michael A. Nitsche and Christoph P. Kaller and Christoph Nissen},
keywords = {Creativity, Flexibility, Transcranial direct current stimulation, Frontal cortex, Electroencephalography},
abstract = {Background
Creativity is the use of original ideas to accomplish something innovative. Previous research supports the notion that creativity is facilitated by an activation of the right and/or a deactivation of the left prefrontal cortex. In contrast, recent brain imaging studies suggest that creativity improves with left frontal activation.
Objective
The present study was designed to further elucidate the neural basis of and ways to modulate creativity, based on the modulation of prefrontal cortical activity through the non-invasive brain stimulation technique transcranial direct current stimulation (tDCS).
Methods
Ninety healthy University students performed three tasks on major aspects of creativity: conceptual expansion (Alternate Uses Task, AUT), associative thinking (Compound Remote Associate Task, CRA), and set shifting ability (Wisconsin Card Sorting Task, WCST). Simultaneously, they received cathodal stimulation of the left and anodal stimulation of the right inferior frontal gyrus (IFG), the reverse protocol, or sham stimulation.
Results
The main pattern of results was a superior performance with bilateral left cathodal/right anodal stimulation, and an inferior performance in the reversed protocol compared to sham stimulation. As a potential underlying physiological mechanism, resting state EEG beta power, indicative of enhanced cortical activity, in the right frontal area increased with anodal stimulation and was associated with better performance.
Conclusion
The findings provide new insights into ways of modulating creativity, whereby a deactivation of the left and an activation of the right prefrontal cortex with tDCS is associated with increased creativity. Potential future applications might include tDCS for patients with mental disorders and for healthy individuals in creative professions.}
}
@article{PIVIK2012548,
title = {Eating breakfast enhances the efficiency of neural networks engaged during mental arithmetic in school-aged children},
journal = {Physiology & Behavior},
volume = {106},
number = {4},
pages = {548-555},
year = {2012},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2012.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0031938412001394},
author = {R.T. Pivik and Kevin B. Tennal and Stephen D. Chapman and Yuyuan Gu},
keywords = {Morning nutrition, Mental arithmetic, Preadolescents, Time–frequency analysis},
abstract = {To determine the influence of a morning meal on complex mental functions in children (8–11y), time–frequency analyses were applied to electroencephalographic (EEG) activity recorded while children solved simple addition problems after an overnight fast and again after having either eaten or skipped breakfast. Power of low frequency EEG activity [2Hertz (Hz) bands in the 2–12Hz range] was determined from recordings over frontal and parietal brain regions associated with mathematical thinking during mental calculation of correctly answered problems. Analyses were adjusted for background variables known to influence or reflect the development of mathematical skills, i.e., age and measures of math competence and math fluency. Relative to fed children, those who continued to fast showed greater power increases in upper theta (6–8Hz) and both alpha bands (8–10Hz; 10–12Hz) across sites. Increased theta suggests greater demands on working memory. Increased alpha may facilitate task-essential activity by suppressing non-task-essential activity. Fasting children also had greater delta (2–4Hz) and greater lower-theta (4–6Hz) power in left frontal recordings—indicating a region-specific emphasis on both working memory for mental calculation (theta) and activation of processes that suppress interfering activity (delta). Fed children also showed a significant increase in correct responses while children who continued to fast did not. Taken together the findings suggest that neural network activity involved in processing numerical information is functionally enhanced and performance is improved in children who have eaten breakfast, whereas greater mental effort is required for this mathematical thinking in children who skip breakfast.}
}
@article{BREIT2025102621,
title = {Mathematics achievement and learner characteristics: A systematic review of meta-analyses},
journal = {Learning and Individual Differences},
volume = {118},
pages = {102621},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2024.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1041608024002140},
author = {Moritz Breit and Michael Schneider and Franzis Preckel},
keywords = {Mathematics achievement, meta-analysis, Systematic review, Math talent, Predictors},
abstract = {Learners' individual differences in mathematics achievement are associated with individual differences in psychological characteristics. A number of meta-analyses have quantified the strengths of these correlations. However, these findings are scattered across different strands of the literature. The present systematic review aims to integrate these strands by providing an overview of meta-analyses of psychological correlates of mathematics achievement. We conducted a systematic literature search and included 30 meta-analyses, reporting correlations between mathematics achievement and 66 variables based on 13,853 effect sizes and an estimated 4,658,717 participants. The correlations are rank-ordered by size and complemented with information about the meta-analyses, their inclusion criteria, and methods. The results show strong associations of mathematics achievement with verbal skills and abilities, prior knowledge, intelligence, creativity, math-specific skills, math self-concept, self-regulation, meta-cognition, and executive functions. Relatively weaker relations were observed for emotional intelligence, achievement goals, academic emotions, and the Big Five personality traits.}
}
@article{REZAPOUR2024108003,
title = {Learning experience assessment through players chat content in multiplayer online games},
journal = {Computers in Human Behavior},
volume = {151},
pages = {108003},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003540},
author = {Mohammad Mahdi Rezapour and Afsaneh Fatemi and Mohammad Ali Nematbakhsh},
keywords = {Learning experience assessment, Multiplayer online game, Natural language processing, BERT, Stealth assessment, Game-based learning},
abstract = {Assessing players’ learning experiences in a proper manner is a fundamental aspect of successful game-based learning programs. One notable characteristic of these programs is stealth assessment, which involves integrating formative assessment into the learning environment without disrupting the learning process. In multiplayer online games (MOGs), the in-game online chat system is a commonly used tool that enables players to communicate through text or voice messages during gameplay. However, there is a lack of specific research on incorporating players’ in-game chat content for computational learning experience assessment, which could enhance the validity of stealth assessment. This study proposes a stealth assessment method based on natural language processing to highlight the significance of players’ in-game chat data in estimating learners’ skills in MOGs. A natural language processing model is developed using a distilled version of the Google BERT pre-trained model. The evaluations demonstrate that the proposed method accurately estimates a player’s skill level by analyzing a few chat messages from the player. This method has the potential to make a profound impact on the field of game-based learning by enabling more precise assessment and supporting the design of tailored interventions and adaptive learning systems. This study pioneers computational skill assessment through chats in MOGs, opening up new opportunities for future investigations in skill assessment and having the potential to transform the field of game-based learning.}
}
@article{AWD2023107403,
title = {A review on the enhancement of failure mechanisms modeling in additively manufactured structures by machine learning},
journal = {Engineering Failure Analysis},
volume = {151},
pages = {107403},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107403},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723003576},
author = {Mustafa Awd and Lobna Saeed and Frank Walther},
keywords = {Modeling, Simulation, Finite Element Analysis, Analytical Models, Multi-Physics, Data-Driven Modeling, Additive Manufacturing, Failure Mechanisms},
abstract = {This review discusses the feasibility of using microstructure- and defect-sensitive models to predict the fatigue behavior of additively generated materials through a non-exclusive qualitative assessment of the current literature on the structural integrity of additively manufactured structures. The time it takes to implement additively manufactured structures is reduced if a computational model can predict and enhance their mechanical performance. Computational modeling techniques can express nonlinear, multimodal functions in failure analysis of engineering materials, reducing environmental waste and providing sustainable technology. Machine learning is used in manufacturing and industrial sectors to optimize process parameters and model data-driven correlations between processes, structures, and properties. Machine learning and artificial intelligence can be combined to enable atomistic-scale damage tolerance design, which can be customized using computer programming. The main advantage of machine learning is that it can be well integrated with finite element, analytical, or empirical modeling with a significant increase in the yield of the model being used.}
}
@article{READ200577,
title = {Early computational processing in binocular vision and depth perception},
journal = {Progress in Biophysics and Molecular Biology},
volume = {87},
number = {1},
pages = {77-108},
year = {2005},
note = {Biophysics of Excitable Tissues},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2004.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S007961070400063X},
author = {Jenny Read},
abstract = {Stereoscopic depth perception is a fascinating ability in its own right and also a useful model of perception. In recent years, considerable progress has been made in understanding the early cortical circuitry underlying this ability. Inputs from left and right eyes are first combined in primary visual cortex (V1), where many cells are tuned for binocular disparity. Although the observation of disparity tuning in V1, combined with psychophysical evidence that stereopsis must occur early in visual processing, led to initial suggestions that V1 was the neural correlate of stereoscopic depth perception, more recent work indicates that this must occur in higher visual areas. The firing of cells in V1 appears to depend relatively simply on the visual stimuli within local receptive fields in each retina, whereas the perception of depth reflects global properties of the stimulus. However, V1 neurons appear to be specialized in a number of respects to encode ecologically relevant binocular disparities. This suggests that they carry out essential pre-processing underlying stereoscopic depth perception in higher areas. This article reviews recent progress in developing accurate models of the computations carried out by these neurons. We seem close to achieving a mathematical description of the initial stages of the brain's stereo algorithm. This is important in itself––for instance, it may enable improved stereopsis in computer vision––and paves the way for a full understanding of how depth perception arises.}
}

@article{SIGAYRET2022104505,
title = {Unplugged or plugged-in programming learning: A comparative experimental study},
journal = {Computers & Education},
volume = {184},
pages = {104505},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104505},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000768},
author = {Kevin Sigayret and André Tricot and Nathalie Blanc},
keywords = {Elementary education, Improving classroom teaching, Programming and programming languages, Teaching/learning strategies},
abstract = {In recent years, computer programming has reappeared in school curricula with the aim of transmitting knowledge and skills beyond the simple ability to code. However, there are different ways of teaching this subject and very few experimental studies compare plugged-in and unplugged programming learning. The purpose of this study is to highlight the impact of plugged-in or unplugged learning on students' performance and subjective experience. To this end, we designed an experimental study with 217 primary school students divided into two groups and we measured their knowledge of computational concepts, ability to solve algorithmic problem, motivation toward the instruction, self-belief and attitude toward science. The programming sessions were designed to be similar between the two conditions, only the tools were different. Computers and Scratch software were used in the plugged-in group while the unplugged group used paper instructions, pictures, figurines and body movements instead. The results show better learning performance in the plugged-in group. Furthermore, although motivation dropped slightly in both groups, this drop was only significant in the unplugged condition. Gender also seems to be an important factor, as girls exhibit a lower post-test motivation and a lower willingness to pursue their practice in programming outside the school context. However, this effect on motivation was only observable in the plugged-in group which suggests that educational programming software may have a positive but gendered motivational impact.}
}
@article{GAO20222707,
title = {Similarity reductions for a generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation in fluid dynamics},
journal = {Chinese Journal of Physics},
volume = {77},
pages = {2707-2712},
year = {2022},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0577907322001228},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Fluid dynamics, Generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation, Similarity reductions, Symbolic computation},
abstract = {Rather intriguing, the paper Chin. J. Phys. 73 (2021) 600-612 has studied a (3+1)-dimensional B-type Kadomtsev–Petviashvili equation in fluid dynamics, while fluid dynamics has a wide range of applications, including those for geophysics, mechanical engineering, civil engineering, chemical engineering, astrophysics and biology. In this paper, taking into consideration certain nonlinear waves in fluid dynamics, we investigate a generalized variable-coefficient version of the aforementioned equation. Making use of symbolic computation, with respect to the amplitude or elevation of the relevant wave, we construct out two sets of the similarity reductions, which rely on the variable coefficients in the generalized equation.}
}
@article{EDELMANN20181,
title = {Formal studies of culture: Issues, challenges, and current trends},
journal = {Poetics},
volume = {68},
pages = {1-9},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X18301323},
author = {Achim Edelmann and John W. Mohr},
keywords = {Formal study of culture, Cultural matrix approach, Measuring duality, Formalist theorization of culture, Computational hermeneutics},
abstract = {Over the last two decades, the formal study of culture has grown into one of the most exciting, systematic, and dynamic sub-fields in sociology. In this essay, we take stock of recent developments in this field. We highlight four emerging themes: (1) the maturation of the field that has occurred over the last two decades, (2) the rise and formalization of the “cultural matrix” approach to studying culture, (3) the development of various efforts to advance a more formal theory of culture, and (4) the proliferation of Big Data and the development of new kinds of quantitative and computational approaches to the study of culture, including the emergence of a new area focused on “computational hermeneutics.” We conclude by discussing future opportunities, challenges, and questions in formalizing culture.}
}
@article{ZHOU2022100001,
title = {Science in One Health: A new journal with a new approach},
journal = {Science in One Health},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949704322000014},
author = {Xiao-Nong Zhou and Marcel Tanner},
keywords = {One Health, Human health, Animal health, Ecosystem health, Research and implementation science},
abstract = {One Health recognizes the close links and interdependence among human health, animal health and environmental health. With the pandemic of COVID-19 and the risk of many emerging or reemerging infectious diseases of zoonotic nature as well as the spreading antimicrobial resistance, One Health has become one of top concerns globally, as it entails the essential global public health challenges from antimicrobial resistance over zoonoses, to climate change, food security and societal well-being. Research priorities in One Health include the study on interactions of human-animal-plants-nature ecology interface, systems thinking, integrated surveillance and response systems, and the overall One Health governance as part of the global health and sustainability governance. The now launched journal, Science in One Health, aims to be a resource platform that disseminates scientific evidence, knowledge, and tools on the One Health approaches and respective possible socio-ecological interventions. Thus, aims at providing fruitful exchanges of information and experience among researchers, and decision makers as well as public health actors.}
}
@article{FISCHER199721,
title = {Computational environments supporting creativity in the context of lifelong learning and design},
journal = {Knowledge-Based Systems},
volume = {10},
number = {1},
pages = {21-28},
year = {1997},
note = {Information Technology Support for Creativity},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(97)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S0950705197000105},
author = {Gerhard Fischer and Kumiyo Nakakoji},
keywords = {Creativity support, Domain-oriented design environments (DODEs), Lifelong-learning},
abstract = {Much of our intelligence and creativity results from the collective memory of communities of practice and of the artifacts and technology surrounding them. Rather than studying individual creativity in isolation, we have developed a conceptual framework of creativity in the context of everyday practice — where design activities prevail and learning is constantly required. The conceptual framework explores new role distributions between people and computers based on theories that view design as reflection-in-action and breakdowns as opportunities for learning and creativity. We use an example from the domain of multimedia information design to illustrate how creativity is supported by domain-oriented design environments. The paper describes the mechanisms, architectures and processes underlying these environments.}
}
@article{VARGASCARPINTERO2025120104,
title = {Development of an integrated multi-criteria framework to assess the implementation potential of biobased value chains and webs with a territorial approach},
journal = {Industrial Crops and Products},
volume = {223},
pages = {120104},
year = {2025},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2024.120104},
url = {https://www.sciencedirect.com/science/article/pii/S0926669024020818},
author = {Ricardo Vargas-Carpintero},
keywords = {Biobased value chain, Biobased value web, Biorefinery, Territorial bioeconomy system, Multi-criteria assessment, Land-based bioeconomy},
abstract = {Biobased value chains and webs (BVCW) encompass value adding activities and actors from biomass production, its processing into biobased products for manifold sectors, until their commercialization and use. BVCW are part of territorial bioeconomy systems and are shaped by contextual settings. The design and development of BVCW involve strategic decisions towards their sustainable implementation. Throughout the design and development of BVCW, the adoption of an integral approach that links technical aspects of biomass-to-product pathways with non-technical aspects and context factors is necessary to increase the BVCW implementation potential. Accordingly, an active incorporation of the territorial context of BVCW in the design process is required. In view of these requirements, in this study an integrated, multi-criteria framework is developed to assess the implementation potential in BVCW design. For this purpose, key elements from existing biorefinery and biomass supply chain design methodologies are identified and integrated in a multi-criteria framework that allows the consideration of both an internal and external perspective of the BVCW in relation to the context. The conceptualized framework serves as an evaluation approach to check the implementability of biomass-to-product pathways BVCW configurations in form of by means of a multi-criteria catalogue. The set of criteria integrates relevant aspects for the design and development of BVCW from land-based biomass (e.g. crops and crop residues). It entails key criteria related to the functionality of the biomass-to-product pathway in technical-economic terms and the surrounding biophysical, social and economic context. The further operationalization of the multi-criteria catalogue by means of an indicator-based assessment could enable the prioritization and selection of BVCW configurations with best implementation potential. In this way, the framework provides a practical approach for decision-makers, local actors and researchers involved in the design and development of BVCW tailored to the territorial context.}
}
@article{EDELSON2021100986,
title = {How fuzzy-trace theory predicts development of risky decision making, with novel extensions to culture and reward sensitivity},
journal = {Developmental Review},
volume = {62},
pages = {100986},
year = {2021},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2021.100986},
url = {https://www.sciencedirect.com/science/article/pii/S0273229721000411},
author = {Sarah M. Edelson and Valerie F. Reyna},
keywords = {Risk-taking, Risky decision making, Reward sensitivity, COVID-19, Fuzzy-trace theory, Adolescence},
abstract = {Comprehensive meta-analyses of risky decision making in children, adolescents, and adults have revealed that age trends in disambiguated laboratory tasks confirmed fuzzy-trace theory’s prediction that preference for risk decreases monotonically from childhood to adulthood. These findings are contrary to predictions of dual systems or neurobiological imbalance models. Assumptions about increasing developmental reliance on mental representations of the gist of risky options are essential to account for this developmental trend. However, dual systems theory appropriately emphasizes how cultural context changes behavioral manifestation of risk preferences across age and neurobiological imbalance models appropriately emphasize developmental changes in reward sensitivity. All of the major theories include the assumption of increasing behavioral inhibition. Here, we integrate these theoretical constructs—representation, cultural context, reward sensitivity, and behavioral inhibition—to provide a novel framework for understanding and improving risky decision making in youth. We also discuss the roles of critical tests, scientific falsification, disambiguating assessments of psychological and neurological processes, and the misuse of such concepts as ecological validity and reverse inference. We illustrate these concepts by extending fuzzy-trace theory to explain why youth are a major conduit of viral infections, including the virus that causes COVID-19. We conclude by encouraging behavioral scientists to embrace new ways of thinking about risky decision making that go beyond traditional stereotypes about adolescents and that go beyond conceptualizing ideal decision making as trading off degrees of risk and reward.}
}
@article{GABRIEL2008330,
title = {A friend is a present you give to your “Self”: Avoidance of intimacy moderates the effects of friends on self-liking},
journal = {Journal of Experimental Social Psychology},
volume = {44},
number = {2},
pages = {330-343},
year = {2008},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022103107001126},
author = {Shira Gabriel and Mauricio Carvallo and Lisa M. Jaremka and Brooke Tippin},
keywords = {The self, Social comparison, Friendship, Avoidance of intimacy, Attachment style},
abstract = {The current research proposes that thinking about friends improves feelings about the self and does so differentially depending on avoidance of intimacy. Based on previous findings that individuals who avoid intimacy in relationships (avoidant individuals) contrast their self-concepts with primed friends whereas those who pursue intimacy in relationships (non-avoidant individuals) assimilate their self-concepts to primed friends [Gabriel, S., Carvallo, M., Dean, K., Tippin, B. D., & Renaud, J. (2005). How I see “Me” depends on how I see “We”: The role of avoidance of intimacy in social comparison. Personality and Social Psychology Bulletin, 31, 156–157], we predicted that friends who embody negative aspects of self would lead avoidant individuals to like themselves more, whereas friends who embody positive aspects of self would lead non-avoidant individuals to like themselves more. A pretest determined that good friends were seen as more similar to positive and ideal aspects of the self, whereas friends about whom participants had more mixed feelings (ambivalent friends) were seen as more similar to disliked and feared aspects of the self. Four experiments supported the main hypotheses. In Experiment 1, non-avoidant individuals like themselves more when good friends were primed. In Experiment 2, avoidant individuals like themselves more when ambivalent friends were primed. In Experiment 3, non-avoidant individuals liked themselves better after thinking about a friend’s positive traits, whereas avoidant individuals liked themselves better after thinking about a friend’s negative traits. In Experiment 4, all individuals under self-esteem threat strategically brought friends to mind who would help them like themselves more.}
}
@article{LAVALLEY2024108825,
title = {Transdiagnostic failure to adapt interoceptive precision estimates across affective, substance use, and eating disorders: A replication and extension of previous results},
journal = {Biological Psychology},
volume = {191},
pages = {108825},
year = {2024},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2024.108825},
url = {https://www.sciencedirect.com/science/article/pii/S030105112400084X},
author = {Claire A. Lavalley and Navid Hakimi and Samuel Taylor and Rayus Kuplicki and Katherine L. Forthman and Jennifer L. Stewart and Martin P. Paulus and Sahib S. Khalsa and Ryan Smith},
keywords = {Interoception, Depression, Anxiety, Substance use, Eating disorders, Precision, Priors, Bayesian perception, Computational modeling},
abstract = {Recent Bayesian theories of interoception suggest that perception of bodily states rests upon a precision-weighted integration of afferent signals and prior beliefs. In a previous study, we fit a computational model of perception to behavior on a heartbeat tapping task to test whether aberrant precision-weighting could explain misestimation of cardiac states in psychopathology. We found that, during an interoceptive perturbation designed to amplify afferent signal precision (inspiratory breath-holding), healthy individuals increased the precision-weighting assigned to ascending cardiac signals (relative to resting conditions), while individuals with anxiety, depression, substance use disorders, and/or eating disorders did not. In this pre-registered study, we aimed to replicate and extend our prior findings in a new transdiagnostic patient sample (N = 285) similar to the one in the original study. As expected, patients in this new sample were also unable to adjust beliefs about the precision of cardiac signals – preventing the ability to accurately perceive changes in their cardiac state. Follow-up analyses combining samples from the previous and current study (N = 719) also afforded power to identify group differences between narrower diagnostic categories, and to examine predictive accuracy when logistic regression models were trained on one sample and tested on the other. With this confirmatory evidence in place, future studies should examine the utility of interoceptive precision measures in predicting treatment outcomes and test whether these computational mechanisms might represent novel therapeutic targets.}
}
@article{CHASTAIN200083,
title = {Cultivating design competence: online support for beginning design studio},
journal = {Automation in Construction},
volume = {9},
number = {1},
pages = {83-91},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00053-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000539},
author = {Thomas Chastain and Ame Elliott},
abstract = {A primary lesson of a beginning design studio is the development of a fundamental design competence. This entails acquiring skills of integration, projection, exploration, as well as critical thinking—forming the basis of thinking “like a designer”. Plaguing the beginning architectural design student as she develops this competence are three typical problems: a lagging visual intelligence, a linking of originality with creativity, and the belief that design is an act of an individual author instead of a collaborative activity. We believe that computation support for design learning has particular attributes for helping students overcome these problems. These attributes include its inherent qualities for visualization, for explicitness, and for sharing. This paper describes five interactive multi-media exercises exploiting these attributes which were developed to support a beginning design studio. The paper also reports how they have been integrated into the course curriculum. Le développement des compétences en design: support on-line pour le studio de design élémentaire Une des premières leçons lors du studio de design est le développement d’une compétence fondamentale en conception. Ceci implique l’acquisition des habiletés d’intégration, de projection, d’exploration ainsi que la pensée critique—antérieurement les bases de la façon de penser nommée “comme un concepteur”. Il y a trois problèmes fondamentaux qui pèsent sur l’étudiant débutant en architecture lors du développement cette compétence: une intelligence visuelle insuffisante, le fait de lier l’originalité à la créativité, et la croyance que le processus de conception est une activité individuelle, plutôt que collaborative. Nous sommes de l’avis que le soutien en informatique lors de l’apprentissage de la conception architecturale posséde des attributs bien particuliers pour aider les étudiants à surmonter ces difficultés. Ces attributs comprennent des qualités inhérentes pour la visualisation, pour être explicite, et pour le partage. Ce papier décrit cinq exercices de médias interactifs qui exploitent ces attributs, et qui ont été développés pour supporter un studio de design élémentaire. Il présente aussi un reportage sur la façon dont ces exercices ont été intégrés dans le curriculum du cours.}
}
@incollection{CARPENDALE2013125,
title = {Chapter Six - A Relational Developmental Systems Approach to Moral Development},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {125-153},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000063},
author = {Jeremy I.M. Carpendale and Stuart I. Hammond and Sherrie Atwood},
keywords = {Developmental systems theory, Moral development, Moral norms, Nativism, Social interaction},
abstract = {Morality and cooperation are central to human life. Psychological explanations for moral development and cooperative behavior will have biological and evolutionary dimensions, but they can differ radically in their approach to biology. In particular, many recent proposals have pursued the view that aspects of morality are innate. We briefly review and critique two of these claims. In contrast to these nativist assumptions about the role of biology in morality, we present an alternative approach based on a relational developmental systems view of moral development. The role for biology in this approach is in setting up the conditions—the developmental system—in which forms of interaction and later forms of thinking emerge.}
}
@article{NG2024100090,
title = {Using cospaces in augmented reality digital story creation: A thematic analysis},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100090},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000400},
author = {Davy Tsz Kit Ng and Wan Yee Winsy Lai and Morris Siu-yung Jong and Chi Wui Ng},
keywords = {Digital storytelling, CoSpaces, Online community, Augmented reality, Language learning},
abstract = {With the digital affordances of augmented reality (AR) technologies, research has shown their value for contextualized, interactive and collaborative language learning through supporting real-world immersion. In recent years, CoSpaces has been a popular AR learning tool with an extensive library of 3D models and constructive gadgets, as well as a visual programming platform. With this tool, students can create projects of digital stories by building personalized AR artifacts, scenes, and storylines, and then share their projects in a dynamic and global community of children. This study examined the characteristics of 39 selected CoSpaces’ open projects via thematic analysis and categorization into five learning contexts: (1) art, history, culture and design, (2) STEM, (3) classroom English and everyday communication, (4) fairy tale/literature, and (5) campus tour. Furthermore, this study identified six language learning competencies derived from digital story creation: (1) discovering knowledge, (2) connecting to prior experience and knowledge, (3) conducting research, (4) problem-solving, (5) expressing and creating digitally, as well as (6) presenting, appreciating and evaluating. Digital literacy refers to the ability to use technology to find, evaluate, create, and communicate information. In addition, three major types of digital literacy skills necessary for AR digital storytelling processes have been identified, encompassing digital creativity, technoligcal proficiency, and research skills. Our results contribute to discovering educational values in developing digital language competency through AR digital story creation. Recommendations are offered for future research and for educators to design appropriate AR learning experiences.}
}
@article{CHANG2023101823,
title = {Stakeholder requirement evaluation of smart industrial service ecosystem under Pythagorean fuzzy environment for complex industrial contexts: A case study of renewable energy park},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101823},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101823},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002816},
author = {Yuan Chang and Xinguo Ming and Zhihua Chen and Tongtong Zhou and Xiaoqiang Liao and Wenyan Song},
keywords = {Smart industrial product-service system (IPS), Requirement evaluation, Service ecosystem, Pythagorean fuzzy sets, Multi-criteria decision making, Viable systems model},
abstract = {This study focuses on ways to systematically evaluate stakeholder requirements when developing a smart industrial service ecosystem (SISE) in a complex industrial context. The SISE development requires considering the service requirement from both the complex industrial context and service ecosystem manners. This study proposes a systematic framework for stakeholder requirement evaluation in SISE. The first part of the framework is the industrial context-viable system model with ecological thinking (IC-VESM) to elicit the service requirements for the SISE, which facilitates a systematic analysis of the service value proposition and service requirement elicitation in the operational lifecycle of an entire industrial context. This second part of the framework proposes a method for evaluating service requirements that is both feasible and systematic. This is achieved by combining the Fuzzy Kano and AHP methods in a Pythagorean fuzzy (PF) environment. The PF Kano computes the categories and determines the weights of service requirements from a consumer perspective, while the PF AHP hierarchically analyzes the service requirements and provides pairwise comparison paths for design experts. Finally, an illustrative case study in a renewable energy context was used to demonstrate the feasibility and effectiveness of the methodology. The proposed theoretical model provides more reliable and systematic outcomes than traditional methods when eliciting service requirements and evaluating complex smart industrial service solutions. The study has practical implications by providing useful insights for companies to recognize key smart service requirements in complex industrial contexts and to improve sustainable development.}
}
@article{RASANAN2024857,
title = {Beyond discrete-choice options},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {9},
pages = {857-870},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400175X},
author = {Amir Hosein Hadian Rasanan and Nathan J. Evans and Laura Fontanesi and Catherine Manning and Cynthia Huang-Pollock and Dora Matzke and Andrew Heathcote and Jörg Rieskamp and Maarten Speekenbrink and Michael J. Frank and Stefano Palminteri and Christopher G. Lucas and Jerome R. Busemeyer and Roger Ratcliff and Jamal Amani Rad},
abstract = {While decision theories have evolved over the past five decades, their focus has largely been on choices among a limited number of discrete options, even though many real-world situations have a continuous-option space. Recently, theories have attempted to address decisions with continuous-option spaces, and several computational models have been proposed within the sequential sampling framework to explain how we make a decision in continuous-option space. This article aims to review the main attempts to understand decisions on continuous-option spaces, give an overview of applications of these types of decisions, and present puzzles to be addressed by future developments.}
}
@article{CALUDE2023844,
title = {What perceptron neural networks are (not) good for?},
journal = {Information Sciences},
volume = {621},
pages = {844-857},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013743},
author = {Cristian S. Calude and Shahrokh Heidari and Joseph Sifakis},
keywords = {Perceptrons, Sensitive and robust functions, Quantum computing},
abstract = {Perceptron Neural Networks (PNNs) are essential components of intelligent systems because they produce efficient solutions to problems of overwhelming complexity for conventional computing methods. Many papers show that PNNs can approximate a wide variety of functions, but comparatively, very few discuss their limitations and the scope of this paper. To this aim, we define two classes of Boolean functions – sensitive and robust –, and prove that an exponentially large set of sensitive functions are exponentially difficult to compute by multi-layer PNNs (hence incomputable by single-layer PNNs). A comparatively large set of functions in the second one, but not all, are computable by single-layer PNNs. Finally, we used polynomial threshold PNNs to compute all Boolean functions with quantum annealing and present in detail a QUBO computation on the D-Wave Advantage. These results confirm that the successes of PNNs, or lack of them, are in part determined by properties of the learned data sets and suggest that sensitive functions may not be (efficiently) computed by PNNs.}
}
@article{GREENE201766,
title = {The rat-a-gorical imperative: Moral intuition and the limits of affective learning},
journal = {Cognition},
volume = {167},
pages = {66-77},
year = {2017},
note = {Moral Learning},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717300690},
author = {Joshua D. Greene},
keywords = {Deontology, Utilitarianism, Consequentialism, Reinforcement learning, Model-free learning, Machine learning, Ethics, Normative ethics, Moral judgment},
abstract = {Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I’ve argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions.}
}
@article{WARD202154,
title = {On value-laden science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {54-62},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301783},
author = {Zina B. Ward},
keywords = {Values, Values in science, Argument from inductive risk, Motivating reasons, Justifying reasons},
abstract = {Philosophical work on values in science is held back by widespread ambiguity about how values bear on scientific choices. Here, I disambiguate several ways in which a choice can be value-laden and show that this disambiguation has the potential to solve and dissolve philosophical problems about values in science. First, I characterize four ways in which values relate to choices: values can motivate, justify, cause, or be impacted by the choices we make. Next, I put my proposed taxonomy to work, using it to clarify one version of the argument from inductive risk. The claim that non-epistemic values must play a role in scientific choices that run inductive risk makes most sense as a claim about values being needed to justify such choices. The argument from inductive risk is not unique: many philosophical arguments about values in science can be more clearly understood and assessed by paying close attention to how values and choices are related.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{SPARAPANI2023102186,
title = {Factors associated with classroom participation in preschool through third grade learners on the autism spectrum},
journal = {Research in Autism Spectrum Disorders},
volume = {105},
pages = {102186},
year = {2023},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2023.102186},
url = {https://www.sciencedirect.com/science/article/pii/S1750946723000867},
author = {Nicole Sparapani and Nancy Tseng and Laurel Towers and Sandy Birkeneder and Sana Karimi and Cameron J. Alexander and Johanna Vega Garcia and Taffeta Wood and Amanda Dimachkie Nunnally},
keywords = {Autism, Instructional opportunities, Mathematical tasks, Teacher language, Active engagement, Spontaneous communication},
abstract = {Background
Access to mathematics instruction that involves opportunities for critical thinking and procedural fluency promotes mathematics learning. Studies have outlined effective strategies for teaching mathematics to children on the autism spectrum, however, the focus of these interventions often represent a narrow set of mathematical skills and concepts centered on procedural learning without linking ideas to underlying concepts.
Methods
This study utilized classroom video observations to evaluate the variability in and nature of mathematical learning opportunities presented to 76 autistic students within 49 preschool–3rd grade general and special education learning contexts. We examined teacher instructional practices and student participation across 109 mathematical tasks within larger mathematics lessons.
Results
Students were most often presented with mathematical tasks that required low-level cognitive demand, such as tasks focusing on rote memorization and practicing predetermined steps to solve basic algorithms. Furthermore, the nature of the mathematical task was linked with the language that teachers used, and this in turn, was associated with students’ participation within the learning opportunity.
Conclusions
Our findings indicate that features of talk within specific types of mathematical tasks, including math-related talk and responsive language, were associated with increased student active engagement and spontaneous communication. The knowledge gained from this study contributes to the development of optimized instructional practices for school-aged children on the autism spectrum—information that could be used to prepare both preservice and in-service teachers.}
}
@article{YANG2024109519,
title = {Global optimization strategy of prosumer data center system operation based on multi-agent deep reinforcement learning},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109519},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109519},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao and Xue Tian},
keywords = {Data center system, Global cooperative optimization, D3QN, VDN},
abstract = {The escalating issues of high energy consumption and carbon emissions in data centers (DCs) necessitate the optimization of system operations. However, early optimization strategies were overly simplistic and lacked automated updating and iterative capabilities. With the evolution of artificial intelligence (AI), researchers have applied deep reinforcement learning (DRL) algorithms to system operations. However, the optimization focus has been limited to the internal systems, lacking global optimization. In this paper, a global optimization control strategy based on the Dueling double-deep Q network (D3QN) and value decomposition network (VDN) algorithms is proposed to make the DCs system operate more closely with the upstream, midstream, and downstream. By adjusting battery charging/discharging capacity, computational workload, and waste heat utilization heating temperature global synergistic optimization is achieved. Compared with without optimization, renewable energy waste, operation cost, total electricity consumption, and grid electricity consumption are reduced by 18.37%, 9.78%, 4.01%, and 29.74%, respectively. Additionally, a detailed comparison between non-algorithmic optimization and algorithmic optimization is provided, offering valuable insights for substantial energy savings and emissions reduction in DCs. The results demonstrate the importance of fully exploring the interactive potential between upstream energy supply, midstream computational workload, and downstream waste heat recovery to achieve synergistic global optimization of “computing power", “thermal energy" and “electrical energy" for the sustainable and green development of DCs or other prosumer buildings.}
}
@article{MORAWSKI200231,
title = {Are measurement-oriented courses getting too difficult for Polish students?},
journal = {Measurement},
volume = {32},
number = {1},
pages = {31-38},
year = {2002},
issn = {0263-2241},
doi = {https://doi.org/10.1016/S0263-2241(01)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0263224101000537},
author = {Roman Z Morawski},
keywords = {Measurement, Abstract thinking, Experimentation skills, Teaching methodology},
abstract = {The measurement is assumed to be the most reliable means of acquiring information on physical reality; consequently, measurement science and technology is of fundamental importance for all the branches of engineering which have to deal with real-world objects and phenomena. Unfortunately, the ability of today’s students of engineering to grasp basic ideas of measurement science and to master basic skills related to measurement technology seems to be seriously endangered, inter alia, by the omnipresence of computer-related topics in engineering curricula. Paradoxically, it is also endangered by some cultural changes that undermine the historically established role of abstract thinking in the development of Latin civilisation. Educators cannot avoid the question: what kind of remedial measures should be undertaken? This paper aims to contribute to better understanding of various difficulties the teachers of measurement-related courses are facing today.}
}
@article{PIETARINEN2025105410,
title = {Synechism 2.0: Contours of a new theory of continuity in bioengineering},
journal = {BioSystems},
volume = {250},
pages = {105410},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105410},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725000206},
author = {Ahti-Veikko Pietarinen and Vera Shumilina},
keywords = {Charles S. Peirce, Synechism, Collective agency, Synthetic intelligence, Michael E. Levin, Bioengineering, Bioelectricity},
abstract = {The methodological principle of synechism, the all-pervading continuity first proposed by Charles Peirce in 1892, is reinvigorated in the present paper to prompt a comprehensive reevaluation of the integrated concepts of life, machines, agency, and intelligence. The evidence comes from the intersections of synthetic bioengineering, developmental biology, and cognitive and computational sciences. As a regulative principle, synechism, “that continuity governs the whole domain of experience in every element of it”, has been shown to infiltrate fundamental issues of contemporary biology, including cognition in different substrates, embodied agency, collectives (swarm and nested), intelligence on multiple scales, and developmental bioelectricity in morphogenesis. In the present paper, we make explicit modern biology's turn to this fundamental feature of science in its rejection of conceptual binaries, preference for collectives over individuals, quantitative over qualitative, and multiscale applicability of the emerging hypotheses about the integration of the first principles of the diversity of life. Specifically, synechism presents itself as the bedrock for research encompassing biological machines, chimaeras, organoids, and Xenobots. We then review a synechistic framework that embeds functionalist, information-theoretic, pragmaticist and inferentialist approaches to springboard to continuum-driven biosystemic behaviour.}
}
@article{WANG20071997,
title = {DIANA: A computer-supported heterogeneous grouping system for teachers to conduct successful small learning groups},
journal = {Computers in Human Behavior},
volume = {23},
number = {4},
pages = {1997-2010},
year = {2007},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2006.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563206000094},
author = {Dai-Yi Wang and Sunny S.J. Lin and Chuen-Tsai Sun},
keywords = {Cooperative learning, Small-group learning, Computer assisted grouping system, Group composition, Thinking styles, University students},
abstract = {Teachers interested in small-group learning can benefit from using psychological factors to create heterogeneous groups. In this paper we describe a computer-supported grouping system named DIANA that uses genetic algorithms to achieve fairness, equity, flexibility, and easy implementation. Grouping was performed so as to avoid the creation of exceptionally weak groups. We tested DIANA with 66 undergraduate computer science students assigned to groups of three either randomly (10 groups) or using an algorithm reflecting [Sternberg, R. J. (1994). Thinking styles: theory and assessment at the interface between intelligence and personality. In R. J. Sterberg, & P. Ruzgis (Eds.), Personality and Intelligence (pp. 169–187). New York: Cambridge University Press.] three thinking styles (12 groups). The results indicate that: (a) the algorithm-determined groups were more capable of completing whatever they were “required to do” at a statistically significant level, (b) both groups were equally capable of solving approximately 80% of what they “chose to do,” and (c) the algorithm-determined groups had smaller inter-group variation in performance. Levels of satisfaction with fellow group member attitudes, the cooperative process, and group outcomes were also higher among members of the algorithm-determined groups. Suggestions for applying computer-supported group composition systems are offered.}
}
@article{ROBINSON20231189,
title = {Opportunities and challenges for microbiomics in ecosystem restoration},
journal = {Trends in Ecology & Evolution},
volume = {38},
number = {12},
pages = {1189-1202},
year = {2023},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534723002112},
author = {Jake M. Robinson and Riley Hodgson and Siegfried L. Krauss and Craig Liddicoat and Ashish A. Malik and Belinda C. Martin and Jakki J. Mohr and David Moreno-Mateos and Miriam Muñoz-Rojas and Shawn D. Peddle and Martin F. Breed},
keywords = {ecosystem restoration, microbiome, microbiomics, metagenomics, restoration ecology, innovation},
abstract = {Microbiomics is the science of characterizing microbial community structure, function, and dynamics. It has great potential to advance our understanding of plant–soil–microbe processes and interaction networks which can be applied to improve ecosystem restoration. However, microbiomics may be perceived as complex and the technology is not accessible to all. The opportunities of microbiomics in restoration ecology are considerable, but so are the practical challenges. Applying microbiomics in restoration must move beyond compositional assessments to incorporate tools to study the complexity of ecosystem recovery. Advances in metaomic tools provide unprecedented possibilities to aid restoration interventions. Moreover, complementary non-omic applications, such as microbial inoculants and biopriming, have the potential to improve restoration objectives by enhancing the establishment and health of vegetation communities.}
}
@article{BAKER2021101933,
title = {Who is marginalized in energy justice? Amplifying community leader perspectives of energy transitions in Ghana},
journal = {Energy Research & Social Science},
volume = {73},
pages = {101933},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621000268},
author = {Erin Baker and Destenie Nock and Todd Levin and Samuel A. Atarah and Anthony Afful-Dadzie and David Dodoo-Arhin and Léonce Ndikumana and Ekundayo Shittu and Edwin Muchapondwa and Charles Van-Hein Sackey},
abstract = {There is a divide in energy access studies, between technologically-focused modeling papers in engineering and economics, and energy justice frameworks and principles grounded in social sciences. Quantitative computational models are necessary when analyzing energy, and more specifically electricity, systems, as they are technologically-complex systems that can diverge from intuitive patterns. To assure energy justice, these models must be reflective of, and informative to, a wide range of stakeholders, including households and communities alongside utilities, governments, and others. Yet, moving from a qualitative understanding of preferences to quantitative modeling is challenging. In this perspective piece, we pilot the use of the value-focused thinking framework to inform stakeholder engagement. The result is a strategic objective hierarchy that highlights the tradeoffs and the social, economic and technological factors that need to be measured in models. We apply the process in Ghana, using a survey, stakeholder workshops, and follow-up interviews to uncover key tradeoffs and stakeholder-derived objectives. We discuss three key areas that have been rarely, if ever, well-represented in energy models: (1) the relationship between the dynamics of electricity end-use and the technology and economic structure of the system; (2) explicit tradeoffs between electricity access, cost, and reliability as defined by stakeholders; and (3) the definition of new objectives, such as minimizing hazards related to theft. We conclude that this model of engagement provides an opportunity to tie together rigorous qualitative analysis and stakeholder engagement with crucial quantitative models of the electricity system.}
}
@incollection{KERN202469,
title = {Chapter 5 - The turbinates—an overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {69-96},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000056},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Acetylcholine, secondary atrophic rhinitis, autonomic nervous system, turbinate anatomy, middle turbinate anatomy, inferior turbinate anatomy, capacitance vessels (sinusoids), “diffusor function,” functional residual capacity of the nose (FRCn), “,” hypertrophy (increase in cell ), hyperplasia (increase in cell ), nasal cycle, nasal obstruction, on-urgical urbinate eduction djunctive rocedure (n-sTRAP), out-fracture (lateralization), squamous metaplasia, submucous resection, ozaena, “resistor function,” total inferior turbinectomy, turbinates, turbinectomy, turbinoplasty, acoustic rhinometry, and rhinomanometry},
abstract = {This chapter presents an overview of the turbinates. To the best of our knowledge, it was a New Yorker, William M. Jarvis, MD, who in 1882 described three cases of utilizing a snare to affect a partial turbinectomy. At the dawn of the 20th century, most surgeons were promoting total inferior turbinectomy not only for nasal airway obstruction but astoundingly also for hearing impairment and tinnitus. The turbinate enlargement or “hypertrophy” is neither the cause nor a complication of hearing loss. Fortunately, and for the most part, dazed blunders and egregious errors in thinking by esteemed experts, for the most part, have remedied itself through scientific studies, since the late 19th century. This chapter traces the more than a century long history of turbinate thinking and surgery offering both sides of the turbinate debate in their “own words.” All the various procedures used to reduce the inferior turbinate are presented. To be as fair minded as possible, numerous authors are quoted, spanning more than one hundred years; some observed and reported the serious adverse effects of aggressive turbinate surgery, pleading for a conservative approach to inferior turbinate surgical intervention, while others claimed that turbinectomy was without any serious sequelae which is challenged by the facts.}
}
@incollection{ZHUGE2016149,
title = {15 - Limitations and challenges},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {149-151},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000159},
author = {Hai Zhuge},
keywords = {Summarization, limitations, challenges, representations, computing},
abstract = {The limitation of summarisation lies in the natural differences between human and machine, between languages, and between the ways of observation and thinking of authors and those of readers. The significant evolution of documents in form and function in cyber-physical society challenges the paradigm of summarization research.}
}
@article{ROMAN1992161,
title = {Pavane: a system for declarative visualization of concurrent computations},
journal = {Journal of Visual Languages & Computing},
volume = {3},
number = {2},
pages = {161-193},
year = {1992},
issn = {1045-926X},
doi = {https://doi.org/10.1016/1045-926X(92)90014-D},
url = {https://www.sciencedirect.com/science/article/pii/1045926X9290014D},
author = {Gruia-Catalin Roman and Kenneth C Cox and C.Donald Wilcox and Jerome Y Plun},
abstract = {This paper describes the conceptual model, specification method and visualization methodology for Pavane—a visualization environment concerned with exploring, monitoring and presenting concurrent computations. The underlying visualization model is declarative in the sense that visualization is treated as a mapping from program states to a three-dimensional world of geometric objects. The latter is rendered in full color and may be examined freely by a viewer who is allowed to navigate through the geometric world. The state-to-geometry mapping is defined as a composition of several simpler mappings. The choice is determined by methodological and architectural considerations. This paper shows how this decomposition was molded by two methodological objectives: (1) the desire visually to capture abstract formal properties of programs (e.g. safety and progress) rather than operational details; and (2) the need to support complex animations of atomic computational events. All mappings are specified using a rule-based notation; rules may be added, deleted and modified at any time during the visualization. An algorithm for termination detection in diffusing computations is used to illustrate the specification method and to demonstrate its conceptual elegance and flexibility. A concurrent version of a popular artificial intelligence program provides a vehicle for demonstrating how we derive graphical representations and animation scenarios from key formal properties of the program, i.e. from those safety and progress assertions about the program which turn out to be important in verifying its correctness.}
}
@incollection{KRINGELBACH2019139,
title = {24 - Whole-brain modeling of neuroimaging data: Moving beyond correlation to causation},
editor = {Amir Raz and Robert T. Thibault},
booktitle = {Casting Light on the Dark Side of Brain Imaging},
publisher = {Academic Press},
pages = {139-143},
year = {2019},
isbn = {978-0-12-816179-1},
doi = {https://doi.org/10.1016/B978-0-12-816179-1.00024-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161791000244},
author = {Morten L. Kringelbach and Gustavo Deco},
keywords = {Whole-brain modeling, neuroimaging, causative mechanisms},
abstract = {Neuroimaging has offered an unprecedented window on human brain activity. While this advance has led to great expectations, many neuroscientists have grown increasingly frustrated with the lack of causal insights that this technique has provided into human brain function, in turn, leading to heated discussions on the potential rise of neophrenology. Elsewhere in this book, you can read about the apparent failure of brain imaging to tell us much new or meaningful about thinking and cognition in general. Such views are true to a certain extent; brain imaging often takes indirect measures of neural activity such as blood flow and, just because such brain measures correlate with behavioral output, does not mean that they cause the output. But, these new tools do measure important information about brain activity that could potentially tell us a great deal about brain and mind.}
}
@article{BAMBINI2022106196,
title = {It is time to address language disorders in schizophrenia: A RCT on the efficacy of a novel training targeting the pragmatics of communication (PragmaCom)},
journal = {Journal of Communication Disorders},
volume = {97},
pages = {106196},
year = {2022},
issn = {0021-9924},
doi = {https://doi.org/10.1016/j.jcomdis.2022.106196},
url = {https://www.sciencedirect.com/science/article/pii/S0021992422000156},
author = {Valentina Bambini and Giulia Agostoni and Mariachiara Buonocore and Elisabetta Tonini and Margherita Bechi and Ilaria Ferri and Jacopo Sapienza and Francesca Martini and Federica Cuoco and Federica Cocchi and Luca Bischetti and Roberto Cavallaro and Marta Bosia},
keywords = {Pragmatics, Schizophrenia, Rehabilitation, Concretism, Metaphor, Functioning},
abstract = {Introduction: Language and communication disruptions in schizophrenia are at the center of a large body of investigation. Yet, the remediation of such disruptions is still in its infancy. Here we targeted what is known to be one of the most damaged language domains in schizophrenia, namely pragmatics, by conducting a pragmatics-centered intervention with a randomized controlled trial design and assessing also durability and generalization. To the best of our knowledge, this is the first study with these characteristics. Methods: Inspired by the Gricean account of natural language use, we tailored a novel treatment addressing the pragmatics of communication (PragmaCom) and we tested its efficacy in a sample of individuals with schizophrenia randomized to the experimental group or to an active control group. The primary outcome with respect to the efficacy of the PragmaCom was measured by changes in pragmatic abilities (as evaluated with the global score of the Assessment of Pragmatic Abilities and Cognitive Substrates test) from baseline to 12 weeks and at 3-month follow-up. The secondary outcome was measured by changes in metaphor comprehension, abstract thinking, and global functioning from baseline to 12 weeks and at 3-month follow-up. Results: Relative to the control group, at post-test the PragmaCom group showed greater and enduring improvement in global pragmatic skills and in metaphor comprehension. At follow-up, these improvements persisted and the PragmaCom exerted beneficial effects also on functioning. Conclusions: Despite the limited sample size, we believe that these findings offer initial yet encouraging evidence of the possibility to improve pragmatic skills with a theoretically grounded approach and to obtain durable and clinically relevant benefits. We argue that it is time that therapeutic efforts embrace communicative dysfunctions in order to improve illness outcome.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{YAO2022107747,
title = {Regional attention reinforcement learning for rapid object detection},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107747},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107747},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200057X},
author = {Hongge Yao and Peng Dong and Siyi Cheng and Jun Yu},
keywords = {Regional attention, Reinforcement learning, Object detection, Information fusion, Location and recognition},
abstract = {When people observe a picture, they first pay attention to local areas of the picture, rather than the whole areas, then combine them with previous experience in the brain, and finally make judgments through thinking. This is human visual logic. In this paper, we propose a regional attention reinforcement learning model for object detection. The proposed model uses human visual logical to solve the detection problem of small and complex targets in the picture. The model uses a recurrent network structure as the main framework to extract historical information, and fuse the historical information with the current concerned information. At each recurrent time step, it can pay attention to the fused information, especially pay more attention to the information that may have objects. Experimental results show that the proposed method has more than 5% improved in recognition accuracy to the conventional methods. In terms of FLOPs, the conventional methods normally require 170 M, while the proposed method only needs 25.4M This means that the proposed method has higher detection efficiency.}
}
@incollection{FINI2019161,
title = {Chapter 7 - Sustainable Procurement and Transport of Construction Materials},
editor = {Vivian W.Y. Tam and Khoa N. Le},
booktitle = {Sustainable Construction Technologies},
publisher = {Butterworth-Heinemann},
pages = {161-209},
year = {2019},
isbn = {978-0-12-811749-1},
doi = {https://doi.org/10.1016/B978-0-12-811749-1.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117491000055},
author = {Alireza Ahmadian Fard Fini and Ali Akbarnezhad},
keywords = {Sustainable construction materials, life cycle thinking, procurement and transport, prefabrication technologies},
abstract = {Construction industry is the largest global consumer of materials. This huge share comes with the huge responsibility to account for economic, environmental, and social impacts associated with the materials through adoption of sustainable procurement strategies. Sustainable material procurement requires reconciliation among economic, environmental and social impacts of procurement decisions throughout the life cycle of materials. However, this is challenging mainly due to the broad range of economic, environmental and social impacts associated with different stages of material’s life cycle as well as the overlapping impacts that various supply decisions may have on multiple performance areas. Current practices of material procurement are, on the other hand, predominantly influenced by economy of construction stage and little attention is paid to environmental and social considerations over a long-term horizon. Moreover, material supply decisions made currently in practice are commonly traditional and tend to largely overlook the opportunities made available by advances in material science, computing, and decision-making areas. This chapter starts by presenting an overview of sustainability challenges associated with current material procurement practices to highlight the need for adoption of new sustainable approaches and technologies. It then continues by highlighting the challenges associated with adoption of new approaches and the important sustainability criteria to be considered in selection of new sustainable materials, technologies, and procurement strategies. A comprehensive decision-making framework for identifying the most sustainable procurement options in a construction project among various procurement options available is then presented. The framework is founded on the concepts of life cycle thinking and supply chain structure which are incorporated in to a computational module to compare the life cycle impacts of various supply decision based on the selection criteria determined collaboratively by different project stakeholders. The results of such comparative analysis leads to a ranking of various procurement decision alternatives comprised of different combinations of supply decision including material type, material supply structure, location of supplier, and mode of transport.}
}
@article{NADOLSKI2019210,
title = {Complex systems analysis of hybrid warfare},
journal = {Procedia Computer Science},
volume = {153},
pages = {210-217},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307318},
author = {Molly Nadolski and James Fairbanks},
keywords = {Multi-level modelling, Sociotechnical systems, Complex Systems, Toolsets, Unstructured Spaces, Conceptual Modeling, Quantitative Modeling},
abstract = {Being empowered with the appropriate toolset will enable decision-makers to analyze how best to intervene in ever-changing complex systems. This research project explored deconstructing qualitative methods to identify and document requirements to connect the models to computational social science approaches. Previous efforts from our research provided a customizable toolset that assesses the current and future impact that decisions, policies, or strategies can deliver in a system to tackle particularly complex problems. This paper presents a portion of the research effort that developed a threat analysis framework by establishing formally documented research methods to effectively combine conceptual and computational tools. This enables more accurate, efficient, and foresightful knowledge capture and depictions of a particular problem space. The case that the tools and approach are tested against is Russia’s application of grey zone warfare tools in Moldova.}
}
@article{JACKSON2012370,
title = {Information technology use and creativity: Findings from the Children and Technology Project},
journal = {Computers in Human Behavior},
volume = {28},
number = {2},
pages = {370-376},
year = {2012},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563211002147},
author = {Linda A. Jackson and Edward A. Witt and Alexander Ivan Games and Hiram E. Fitzgerald and Alexander {von Eye} and Yong Zhao},
keywords = {Videogames, Creativity, Children, Technology use},
abstract = {This research examined relationships between children’s information technology (IT) use and their creativity. Four types of information technology were considered: computer use, Internet use, videogame playing and cell phone use. A multidimensional measure of creativity was developed based on Sternberg and Lubart, 1999, Subrahmanyam et al., 2006 test of creative thinking. Participants were 491 12-year olds; 53% were female, 34% were African American and 66% were Caucasian American. Results indicated that videogame playing predicted of all measures of creativity. Regardless of gender or race, greater videogame playing was associated with greater creativity. Type of videogame (e.g., violent, interpersonal) was unrelated to videogame effects on creativity. Gender but not race differences were obtained in the amount and type of videogame playing, but not in creativity. Implications of the findings for future research to test the causal relationship between videogame playing and creativity and to identify mediator and moderator variables are discussed.}
}
@article{NIKIFORIDOU2010795,
title = {Statistical literacy at university level: the current trends},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {795-799},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.236},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810023414},
author = {Zoi Nikiforidou and Aspasia Lekka and Jenny Pange},
keywords = {statistical literacy, Statistics Education},
abstract = {Active and critical citizens, in contemporary information-driven societies, are considered to possess capacities and skills of statistical literacy. There are numerous definitions and descriptions concerning statistical literacy, statistical reasoning and statistical thinking. Thus, all these terms converge to the principle that statistical citizenship develops from school settings and relates mainly to the processes of evaluating, interpreting and communicating data. If these are not acquired on time, then students build up errors and misunderstandings. In the current paper general issues concerning Statistics Education at the University level are addressed and aspects for future research are stressed in terms of technology use, content and pedagogic approaches.}
}
@article{GUO2017677,
title = {Research on Element Importance of Shafting Installation Based on QFD and FMEA},
journal = {Procedia Engineering},
volume = {174},
pages = {677-685},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302059},
author = {Qi Guo and Kuangjie Sheng and Zheng Wang and Xilin Zhang and hengyi Yang and Rui Miao},
keywords = {Quality Function Deployment, Failure Mode and Effects Analysis, Marine Shafting, Comprehensive Importance},
abstract = {Development in today's shipbuilding economy is transforming from the quantitative growth to the quality growth. Quality function deployment (QFD) and failure mode and effects analysis (FMEA) adopt different ways of thinking, they remedy their respective limitations for each other, that can effectively guide the quality control. This paper is combined of HuDong ZhongHua Shipbuilding (group) co. LTD.’s shafting installation process, starting from the QFD customer requirements for finding the importance of production process elements and correction by FMEA, ultimately acquire the comprehensive importance of shafting installation process elements.}
}
@article{SZYJEWSKI20203476,
title = {Future management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3476-3485},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031944X},
author = {Zdzisław Szyjewski},
keywords = {Future management, forecasting the future, new technologies},
abstract = {Accurate forecasting, good identification of trends is the basis of business success. Strategic management methods and techniques that use experience and historical economic data are not adequate to the rapidly changing business environment. In particular, technological changes, and in particular the widespread use of ICT, forces a new approach to management style and changes in the way data is acquired on the basis of which future decisions are made. Innovation thinking, a flexible and dynamic approach to making future-oriented decisions using new technologies are the foundations of future management. Therefore, the aim of the paper is to show the role and position of technology in creating the future.}
}
@incollection{RIVELA202293,
title = {Chapter 6 - Life Cycle Sustainability Assessment-based tools},
editor = {Carmen Teodosiu and Silvia Fiore and Almudena Hospido},
booktitle = {Assessing Progress Towards Sustainability},
publisher = {Elsevier},
pages = {93-118},
year = {2022},
isbn = {978-0-323-85851-9},
doi = {https://doi.org/10.1016/B978-0-323-85851-9.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858519000183},
author = {Beatriz Rivela and Brandon Kuczenski and Dolores Sucozhañay},
keywords = {Life Cycle Thinking, Life Cycle Assessment, Life Cycle Costing, Social Life Cycle Assessment, Life Cycle Sustainability Assessment},
abstract = {This chapter establishes a baseline of ideas of what Life Cycle Thinking means: going beyond the traditional focus, understanding and including the whole environmental, social, and economic implications of decision-making processes to identify potential conflicts, synergies, and trade-offs. The life cycle methodologies for sustainability assessment are described, providing an overview of the tools and criteria currently applied, available software and databases, and ongoing challenges. While Environmental Life Cycle Assessment (LCA) is a consolidated tool, based on the ISO standards, Life Cycle Costing (LCC), the tool aimed at the assessment of the economic domain using a life cycle perspective, has not been widely integrated into sustainability assessment until the last decade. Concerning the social dimension, Social Life Cycle Assessment (S-LCA) is still at an early stage of development, but it is a promising methodology to face the challenge of integrating the social aspects towards a holistic approach to sustainable development.}
}
@article{GRAF2021100836,
title = {A cycle for validating a learning progression illustrated with an example from the concept of function},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100836},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100836},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320301000},
author = {Edith Aurora Graf and Peter W. {van Rijn} and Cheryl L. Eames},
keywords = {Learning progressions, Learning trajectories, Validation, Empirical recovery, Mathematics assessment},
abstract = {A learning progression, or learning trajectory, describes the evolution of student thinking from early conceptions to the target understanding within a particular domain. As a complex theory of development, it requires conceptual and empirical support. In earlier work, we proposed a cycle for the validation of a learning progression with four steps: 1) Theory Development, 2) Examination of Empirical Recovery, 3) Comparison to Competing Models, and 4) Evaluation of Instructional Efficacy. A group of experts met to discuss the application of learning sciences to the design, use, and validation of classroom assessment. Learning progressions, learning trajectories, and how they can support classroom assessment were the main focuses. Revisions to the cycle were suggested. We describe the adapted cycle and illustrate how the first third of it has been applied towards the validation of a learning progression for the concept of function.}
}
@article{DOOLITTLE2019889,
title = {Making Evolutionary Sense of Gaia},
journal = {Trends in Ecology & Evolution},
volume = {34},
number = {10},
pages = {889-894},
year = {2019},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534719301417},
author = {W. Ford Doolittle},
keywords = {Gaia hypothesis, evolution, differential persistence, clade selection},
abstract = {The Gaia hypothesis in a strong and frequently criticized form assumes that global homeostatic mechanisms have evolved by natural selection favoring the maintenance of conditions suitable for life. Traditional neoDarwinists hold this to be impossible in theory. But the hypothesis does make sense if one treats the clade that comprises the biological component of Gaia as an individual and allows differential persistence – as well as differential reproduction – to be an outcome of evolution by natural selection. Recent developments in theoretical and experimental evolutionary biology may justify both maneuvers.}
}
@article{TRAENKLE1994197,
title = {Solving microstructure electrostatistics with MIMD parallel supercomputers and Split-C},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {53},
pages = {197-213},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)85049-6},
url = {https://www.sciencedirect.com/science/article/pii/0377025794850496},
author = {Frank Traenkle and Matthew I. Frank and Mary K. Vernon and Sangtae Kirn},
keywords = {Microstructure electrostatics, Multiple Instruction Multiple Data (MIMD) model, Parallel computational algorithms, Split-C},
abstract = {We consider parallel computational algorithms for the boundary integral solutions of the Laplace equation for use in the simulation of electrorheological fluids and as a model study of a class of elliptic partial differential equations that appear in basic microscopic descriptions of heterogeneous structured continua. The viewpoint is that of constructing large scale simulations that bridge micro- and macro-length and time scales on state-of-the-art parallel supercomputers. Because of long range interactions, fast communications are the key to scalable N-Body algorithms. The communication scheduling strategies of Fuentes and Kim are examined in two contexts on the Thinking Machines CM-5 parallel computer. First, an N-Body simulation implementation in C using the standard send-receive message passing primitives in the CMMD 2.0 library shows that communication scheduling leads to dramatic improvements in computational performance. Second, an implementation in Split-C, which uses highly efficient activemessages to implement shared memory communication, reduces communication overhead by an order of magnitude. Taken together, these two developments portend great promise for the development of efficient large scale simulations using portable, high level parallel programming languages.}
}
@article{AGARWAL1992251,
title = {Computational fluid dynamics on parallel processors},
journal = {Computing Systems in Engineering},
volume = {3},
number = {1},
pages = {251-259},
year = {1992},
note = {High-Performance Computing for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(92)90110-5},
url = {https://www.sciencedirect.com/science/article/pii/0956052192901105},
author = {R.K. Agarwal and J.C. Lewis},
abstract = {Greater computational power is needed for solving computational fluid dynamics problems of interest in engineering design. Parallel computers offer the promise of providing orders of magnitude increases in computational power compared with current uniprocessor vector supercomputers. This paper is mainly concerned with the implementation of a three-dimensional Navier-Stokes code MDNS3D on concurrent computers with grain sizes ranging from fine to coarse. An overview of commercially available parallel machines and the current state of the art in parallel algorithms is presented. The implementation of MDNS3D on machines such as the CRAY Y-MP/8, IBM 3090S, BBN Butterfly II, Intel iPSC/2, Symult 2010, MASPAR, and the Connection Machine CM-2, is described. Particular attention is paid to differences in implementation on SIMD and MIMD architectures. Factors affecting the performance of the code on different architectures are addressed. In addition, user interface and software portability issues are considered for various machines. Finally, future trends in parallel hardware and software development are assessed, and the factors important in determining the most suitable architecture for performing very large scale calculations are discussed.}
}
@article{SMYE2022105015,
title = {Interdisciplinary approaches to metastasis},
journal = {iScience},
volume = {25},
number = {9},
pages = {105015},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105015},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222012871},
author = {Stephen W. Smye and Robert A. Gatenby},
abstract = {Summary
Interdisciplinary research is making a significant contribution to understanding metastasis - one of the grand challenges in cancer research. Examples drawn from apparently unconnected areas of physics, and described at a recent workshop on metastasis, illustrate the value of interdiscplinary thinking.}
}
@article{SCHUELLER1997197,
title = {A state-of-the-art report on computational stochastic mechanics},
journal = {Probabilistic Engineering Mechanics},
volume = {12},
number = {4},
pages = {197-321},
year = {1997},
note = {A State-of-the-Art Report on Computational Stochastic Mechanics},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(97)00003-9},
url = {https://www.sciencedirect.com/science/article/pii/S0266892097000039},
author = {G.I. Schuëller}
}
@article{GROUT2014680,
title = {Taking Computer Science and Programming into Schools: The Glyndŵr/BCS Turing Project},
journal = {Procedia - Social and Behavioral Sciences},
volume = {141},
pages = {680-685},
year = {2014},
note = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814035435},
author = {Vic Grout and Nigel Houlden},
keywords = {Computer science, programming, computing curriculum, teacher training, British Computer Society (BCS) Academy, Computing At School (CAS), Council of Professors and Heads of Computing (CPHC), Lego NXT Mindstorm, Raspberry Pi, Robot C, Scratch, Picoboards ;},
abstract = {2012 and 2013 have been challenging years for Computer Science (CS) education in the UK. After decades of national neglect, there has been a sudden impetus to reintroduce CS into the 11-16 age school curriculums. Immediate obstacles include a generation of children with no CS background and an estimated need for 20,000 new CS teachers - existing UK IT teachers being insufficiently qualified and experienced. The Computing at School (CAS) movement has been instrumental in this quantum transition from an IT to Computing syllabus, as have the British Computer Society (BCS), leading UK universities and a number of major international technology companies, including Microsoft, Google, IBM, British Telecom and Facebook.This paper discusses the background to this position and the progress being made to address these challenges. It describes, in particular, the work of the BCS-funded Glyndwr University ‘Turing Project’ in introducing Welsh high-school students and staff to high-level programming and ‘computational thinking’. The Turing Project uses an innovative combination of Lego NXT Mindstorm robots, Raspberry Pi computers and PicoBoard hardware together with the Robot C and Scratch programming platforms. The paper discusses initial objectives and the general approach, describes focused delivery across different age groups and ability ranges and presents results and analysis demonstrating the effectiveness of the programme. Lessons learnt and future directions are considered in conclusion.}
}
@article{YANG201451,
title = {Reactivity of concurrent verbal reporting in second language writing},
journal = {Journal of Second Language Writing},
volume = {24},
pages = {51-70},
year = {2014},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2014.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1060374314000113},
author = {Chengsong Yang and Guangwei Hu and Lawrence Jun Zhang},
keywords = {Reactivity, Think-aloud, Second language acquisition (SLA), L2 writing, Argumentative writing, Chinese EFL writers},
abstract = {This paper reports an empirical study designed to explore whether concurrent verbal reporting has a reactive effect on the process of second language writing. Ninety-five Chinese EFL learners were randomly assigned to an argumentative writing task under three conditions: metacognitive thinking aloud (MTA), nonmetacognitive thinking aloud (NMTA), and no thinking aloud (NTA), after they completed a similar baseline writing task. Their essays were analyzed in terms of linguistic fluency, complexity, accuracy, and overall quality to examine if there were any significant between-group differences that could be taken as evidence of reactivity. After controlling for baseline differences, analyses revealed no traces of reactivity left on a majority of measures except that: (a) the two think-aloud conditions significantly increased dysfluencies in participants’ essays; (b) they also tended to reduce syntactic variety of the essays; and (c) MTA significantly prolonged time on task and retarded the speed of written production. These negative effects are interpreted in light of Kellogg's (1996) cognitive model of writing as suggesting no serious interference with L2 writing processes and are taken as cautions for, rather than counterevidence against, the use of the think-aloud method to obtain L2 writing process data.}
}
@article{CAO2024101244,
title = {Explanatory models in neuroscience, Part 1: Taking mechanistic abstraction seriously},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101244},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101244},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400038X},
author = {Rosa Cao and Daniel Yamins},
keywords = {Mechanism, Models, Explanation, Constraints, Similarity, Mapping, Abstraction, Functional abstraction, Neural networks, Computation, Philosophy, Vision, Constraint, Prediction, Transform, Levels of explanation, Mechanistic explanation, Neuroscience, Understanding},
abstract = {Despite the recent success of neural network models in mimicking animal performance on various tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system requires us to characterize its parts, organization, and activities, and how those give rise to behaviors of interest. However, it remains controversial what it takes for a model to be mechanistic, and whether computational models such as neural networks qualify as explanatory on this approach. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when an appropriate notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify an abstract level of description that is still detailed enough to be “runnable”, and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are just those already in use in experimental neuroscience and deployed in the construction of more familiar computational models — just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.}
}
@article{GUPTA20062290,
title = {Towards a new paradigm for innovative training methods for capacity building in remote sensing},
journal = {Advances in Space Research},
volume = {38},
number = {10},
pages = {2290-2298},
year = {2006},
note = {Remote Sensing of Oceanographic Processes and Land Surfaces; Space Science Education and Outreach},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2006.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0273117706004285},
author = {R.K. Gupta and P.M. Bala Manikavelu and D. Vijayan and T.S. Prasad},
keywords = {Thinking curricula, Innovative training methods, Capacity building, Remote sensing},
abstract = {Everybody uses a bulb to illustrate an idea but nobody shows where the current comes from. Majority of remote sensing user community comes from natural and social sciences domain while remote sensing technology evolves from physical and engineering sciences. To ensure inculcation and internalization of remote sensing technology by application/resource scientists, trainer needs to transfer physical and engineering concepts in geometric manner. Here, the steering for the transfer of knowledge (facts, procedures, concepts and principles) and skills (thinking, acting, reacting and interacting) needs to take the trainees from Known to Unknown, Concrete to Abstract, Observation to Theory and Simple to Complex. In the initial stage of training/education, experiential learning by instructor led exploring of thematic details in false colour composite (FCC) as well as in individual black and white spectral band(s) imagery by trainees not only creates interest, confidence build-up and orientation towards purposeful learning but also helps them to overcome their inhibitions towards the physical and engineering basal. The methodology to be adopted has to inculcate productive learning, emphasizing more on thinking and trial and error aspects as opposed to reproductive learning based dominantly on being told and imitation. The delivery by trainer needs to ensure dynamic, stimulating and effective discussions through deluging questions pertaining to analysis, synthesis and evaluation nature. This would ensure proactive participation from trainees. Hands-on module leads to creative concretization of concepts. To keep the trainees inspired to learn in an auto mode during post-training period, they need to consciously swim in the current and emerging knowledge pool during training programme. This is achieved through assignment of seminar delivery task to the trainees. During the delivery of seminar, peers and co-trainees drive the trainee to communicate the seminar content not only in what but also in how and why mode. The interest culminated in this manner keeps the entropy of the trainee minimized even during post-training professional life. So, such germinated trainee would always generate positive induction among colleagues; thus, helping in realizing multiplier effect. Based upon above thought process(es), the paper discusses the concept of “thinking curricula” and associated cares needed in training deliveries.}
}
@article{CORTI19942717,
title = {A computational study of metastability in vapor—liquid equilibrium},
journal = {Chemical Engineering Science},
volume = {49},
number = {17},
pages = {2717-2734},
year = {1994},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(94)E0093-6},
url = {https://www.sciencedirect.com/science/article/pii/0009250994E00936},
author = {David S. Corti and Pablo G. Debenedetti},
abstract = {Computer simulations are ideally suited to study systems under arbitrary constraints; hence they are useful for the investigation of metastability. Different types of constraints were applied to the three-dimensional Lennard—Jones fluid in the vapor—liquid coexistence region. Constraining the magnitude of allowed density fluctuations (restricted ensemble) has little effect on the equation of state and on phase equilibrium predictions for reduced temperatures lower than 0.95. Thermodynamic integrations along constrained and unstable paths are in good agreement with chemical potential calculations, indicating that imposing the density constraint does not violate microscopic reversibility. Restricted ensemble calculations were also used to calculate the width of the transition region where the mechanism of phase separation in the superheated liquid changes from nucleation to spinodal decomposition. The width of this region decreases as the temperature is reduced away from criticality. Free energy barriers to isotropic compression were used to determine the width of the transition region from nucleation to spinodal decomposition in the supercooled vapor. This transition region also becomes narrower as the distance from the critical point increases. The pressure of the deeply superheated liquid was found to be sensitive to the maximum size of voids that are allowed to form.}
}
@article{PUDANE2017517,
title = {Human Emotional Behavior Simulation in Intelligent Agents: Processes and Architecture},
journal = {Procedia Computer Science},
volume = {104},
pages = {517-524},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301680},
author = {Mara Pudane and Egons Lavendelis and Michael A. Radin},
keywords = {Affective agents, Emotive agents, Human behavior simulation, Agent internal architecture},
abstract = {The paper describes and discusses processes needed for human emotional behaviour simulation, in particular, emotion incorporation into rational thinking, as well as presents corresponding agent architecture. Such system would enable various application fields, perhaps one of the most important being enhancing smart devices with emotions. Decreasing frequency of social contact has become an urgent issue, particularly among young people. Emotional and social intelligence are however highly desired set of skills which is impossible to develop without interacting with others. Although this problem has been acknowledged, and there are some efforts to facilitate social contact, e.g., by augmented virtual reality games, that is still not enough. There is a need to develop environment that would allow learning exactly social and emotional skills. This on-going research aims at developing intelligent agents that are able to express and incorporate affects into rational processes.}
}
@article{MIYAMOTO2022231473,
title = {Data-driven optimization of 3D battery design},
journal = {Journal of Power Sources},
volume = {536},
pages = {231473},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231473},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322004803},
author = {Kaito Miyamoto and Scott R. Broderick and Krishna Rajan},
keywords = {Lithium-ion batteries, 3D miniature batteries, Optimization of 3D battery architecture, Machine learning, Multiobjective optimization},
abstract = {To power microelectronics for the internet-of-things applications, high-performance miniature batteries, called microbatteries, are critically important. Given their limited size, the three-dimensional design of microbatteries is key to maximizing their performance. Therefore, a computational strategy to identify the target battery architecture has major implications for performance improvement. In this paper, we propose a data-driven 3D battery optimization system at the full cell level that combines an automatic geometry generator based on Monte Carlo Tree Search and highly accurate machine-learning-based performance simulators. The performance of the proposed method is demonstrated by designing high-performance 3D batteries with more than 5.5 times efficiency compared with the approach based on a randomized algorithm. One of the designed geometries displayed greater power and energy densities due to more than 10% reduced internal resistance than the reported state-of-the-art geometry at the current density of higher than 15.8 mA/cm2. The results demonstrate the effectiveness of the method.}
}
@article{USKOKOVIC2023e15015,
title = {Natural sciences and chess: A romantic relationship missing from higher education curricula},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15015},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15015},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022223},
author = {Vuk Uskoković},
keywords = {Chemistry, Chess, Creativity, Culture, Education, Instruction, Science},
abstract = {Chess is a game that delicately weaves analytical thinking around artistic experience, yet recent conversions of STEM (Science-Technology-Engineering-Mathematics) to STEAM (Science-Technology-Engineering-Art-Mathematics) have omitted adding chess as an elementary coursework to K-12 and higher education curricula. Chess, as per arguments presented in this essay, can be considered as a language and a tool for furthering the development of artistic skills among scientists and analytical, pattern-recognition skills among artists. It can also serve as a missing link between science and art in STEAM curricula thanks to its finding itself halfway between the two. A handful of analogies are drawn here from chess, illustrated sporadically with positions from real-life chess games and converted to lessons in creativity for students in natural sciences. The discussion centered around these analogies is reinforced by a literature review of studies conducted over the past 80 years to assess the effect of exposing students to lessons in chess on their learning in distant domains. Overall, great benefits can emerge from complementing science education with chess and it is hoped that chess will become an integral part of basic education in primary schools and universities worldwide in the near future.}
}
@article{TRAYVICK2024116109,
title = {Speech and language patterns in autism: Towards natural language processing as a research and clinical tool},
journal = {Psychiatry Research},
volume = {340},
pages = {116109},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116109},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124003949},
author = {Jadyn Trayvick and Sarah B. Barkley and Alessia McGowan and Agrima Srivastava and Arabella W. Peters and Guillermo A. Cecchi and Jennifer H. Foss-Feig and Cheryl M. Corcoran},
keywords = {Autism, Speech, Language, Natural language processing, Automated speech analysis, Acoustics, Computational phenotyping},
abstract = {Speech and language differences have long been described as important characteristics of autism spectrum disorder (ASD). Linguistic abnormalities range from prosodic differences in pitch, intensity, and rate of speech, to language idiosyncrasies and difficulties with pragmatics and reciprocal conversation. Heterogeneity of findings and a reliance on qualitative, subjective ratings, however, limit a full understanding of linguistic phenotypes in autism. This review summarizes evidence of both speech and language differences in ASD. We also describe recent advances in linguistic research, aided by automated methods and software like natural language processing (NLP) and speech analytic software. Such approaches allow for objective, quantitative measurement of speech and language patterns that may be more tractable and unbiased. Future research integrating both speech and language features and capturing “natural language” samples may yield a more comprehensive understanding of language differences in autism, offering potential implications for diagnosis, intervention, and research.}
}
@article{SOUSA2015113,
title = {Symmetry-based generative design and fabrication: A teaching experiment},
journal = {Automation in Construction},
volume = {51},
pages = {113-123},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514002283},
author = {José Pedro Sousa and João Pedro Xavier},
keywords = {Architecture, Geometry, Symmetry, Computational design, Digital fabrication, Design education},
abstract = {Throughout history, symmetry has been widely explored as a geometric strategy to conceive architectural forms and spaces. Nonetheless, its concept has changed and expanded overtime. Nowadays, it is understood as an ordering principle resulting from the application of isometric transformations that keep the original object invariant. Departing from this notion, scientists, philosophers and designers have extended it to embrace other geometric scenarios. Following this idea, exploring symmetry does not mean the generation of simple and predictable design solutions. On the contrary, it is a creative window to achieve geometric complexity based on very simple rules. In this context, this paper aims at discussing the relevance of exploring symmetry in architectural design today by means of digital technologies. It argues that the coupled use of computational design and digital fabrication processes allows designers to explore and materialize a higher level of design complexity in a structured and controlled way, especially when non-isometric transformations are involved. As the background for testing and illustrating its arguments, this paper describes a teaching experiment conducted in the Constructive Geometry course at the FAUP, following design-to-fabrication methodologies.}
}
@article{MCGOWEN2010169,
title = {Metaphor or Met-Before? The effects of previouos experience on practice and theory of learning mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {3},
pages = {169-179},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000404},
author = {Mercedes A. McGowen and David O. Tall},
keywords = {Metaphor, Met-before, Epistemological obstacle, Embodiment, Local straightness},
abstract = {While the general notion of ‘metaphor’ may offer a thoughtful analysis of the nature of mathematical thinking, this paper suggests that it is even more important to take into account the particular mental structures available to the individual that have been built from experience that the individual has ‘met-before.’ The notion of ‘met-before’ offers not only a principle to analyse the changing meanings in mathematics and the difficulties faced by the learner—which we illustrate by the problematic case of the minus sign—it can also be used to analyse the met-befores of mathematicians, mathematics educators and those who develop theories of learning to reveal implicit assumptions that support our thinking in some ways and act as impediments in others.}
}
@article{SUN20112118,
title = {How digital scaffolds in games direct problem-solving behaviors},
journal = {Computers & Education},
volume = {57},
number = {3},
pages = {2118-2125},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S036013151100128X},
author = {Chuen-Tsai Sun and Dai-Yi Wang and Hui-Ling Chan},
keywords = {Human–computer interface, Interactive learning environments, Secondary education, Teaching/learning strategies},
abstract = {Digital systems offer computational power and instant feedback. Game designers are using these features to create scaffolding tools to reduce player frustration. However, researchers are finding some unexpected effects of scaffolding on strategy development and problem-solving behaviors. We used a digital Sudoku game named Professor Sudoku to classify built-in critical features, frustration control and demonstration scaffolds, and to investigate their effects on player/learner behaviors. Our data indicate that scaffolding support increased the level at which puzzles could be solved, and decreased frustration resulting from excessive numbers of retries. However, it also reduced the number of unassisted placements (i.e., independently filled cells), and increased reliance on scaffolding tools, both of which are considered disadvantageous for learning. Among the three scaffold types, frustration control reduced the potential for players to feel stuck at certain levels, but also reduced the frequency of use of critical feature-making tools, which are thought to have greater heuristic value. We conclude that the simultaneous provision of critical feature and frustration control scaffolds may increase player reliance on available support, thereby reducing learning opportunities. Providing players with critical features and demonstration scaffolds at the same time increases reliance on available support for some players, but for most it encourages the development of solving strategies.}
}
@incollection{RUNCO200771,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity},
publisher = {Academic Press},
address = {Burlington},
pages = {71-113},
year = {2007},
isbn = {978-0-12-602400-5},
doi = {https://doi.org/10.1016/B978-012602400-5/50003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780126024005500034},
author = {Mark A. Runco},
abstract = {Publisher Summary
This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared to left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with EEG, PET, cerebral blood flow, and MRI techniques. Numerous EEG studies suggest that there are particular brain-wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{GAO2023103794,
title = {Developing virtual acoustic terrain for Urban Air Mobility trajectory planning},
journal = {Transportation Research Part D: Transport and Environment},
volume = {120},
pages = {103794},
year = {2023},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S1361920923001918},
author = {Zhenyu Gao and Alex Porcayo and John-Paul Clarke},
keywords = {Urban Air Mobility, Sustainable aviation, Noise modeling, Trajectory planning, Optimization},
abstract = {Urban Air Mobility (UAM) is a transformative concept that must operate harmoniously within the constraints imposed by societal impacts. Noise-aware flight trajectory planning can address UAM’s community noise concerns. However, the traditional trajectory optimization paradigm requires repetitive computations of a flight’s noise footprints in complex urban environments and is computationally expensive. In this work, we propose virtual acoustic terrain, a novel concept to enable an efficient trajectory optimization paradigm. By applying acoustic ray tracing and the principle of reciprocity in a complex urban environment, we convert different noise constraints into 3D exclusion zones which UAM operations should avoid to maintain limited noise impact. It combines with the physical urban terrain to define an acceptable fly zone for non-repetitive noise-aware trajectory optimization. This framework provides a new angle to future urban area airspace management and can also accommodate other forms of societal constraints.}
}
@incollection{MACHINMASTROMATTEO2025376,
title = {Literacy of the Future},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {376-387},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@article{BAILEY20158,
title = {Metacognitive beliefs moderate the relationship between catastrophic misinterpretation and health anxiety},
journal = {Journal of Anxiety Disorders},
volume = {34},
pages = {8-14},
year = {2015},
issn = {0887-6185},
doi = {https://doi.org/10.1016/j.janxdis.2015.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0887618515000791},
author = {Robin Bailey and Adrian Wells},
keywords = {Health anxiety, Metacognition, Catastrophic misinterpretation, Moderation, S-REF model},
abstract = {Catastrophic misinterpretations of bodily symptoms have a central role in cognitive-behavioural models of health anxiety. However, the metacognitive (S-REF) model postulates that psychological disturbance is linked more to beliefs about thinking i.e., metacognition. Equally the relationship between catastrophic misinterpretation and health anxiety should be moderated by metacognition, in particular negative beliefs about the uncontrollability and danger of thinking (MCQNeg). Participants (N=351) completed measures to examine the relationship between these variables. Results indicated positive relationships between metacognition, catastrophic misinterpretation, and health anxiety. Moderation analysis showed that the effect of catastrophic misinterpretations on health anxiety was explained by the proposed interaction with metacognition. Follow-up regression analysis demonstrated the interaction term explained variance in health anxiety when controlling for other variables, and was a stronger unique predictor of health anxiety than catastrophic misinterpretation. Metacognition appears to be an important factor in the relationship between catastrophic misinterpretation and health anxiety, and would have important implications for existing models and treatment.}
}
@incollection{PAGEL201749,
title = {Chapter Four - Testing for Machine Consciousness},
editor = {J.F. Pagel and Philip Kirshtein},
booktitle = {Machine Dreaming and Consciousness},
publisher = {Academic Press},
address = {San Diego},
pages = {49-65},
year = {2017},
isbn = {978-0-12-803720-1},
doi = {https://doi.org/10.1016/B978-0-12-803720-1.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037201000049},
author = {J.F. Pagel and Philip Kirshtein},
keywords = {Thinking, intelligence, attention, intentionality, volition, self-awareness, artificial intelligence, AI, autonomous entity, Turing Test, Chinese Room Test},
abstract = {Thinking, intelligence, data integration, and attention are aspects of consciousness for which tests have been designed. A short history of the Computer Science field, a description, and an assessment of results obtained to this point for the Turing Test and Chinese Room Test are part of this chapter. Alternative definitions of artificial intelligence are presented. Applied tests for consciousness including those for intelligence, attention, intentionality, volition, and self-awareness are discussed as applied to the assessment of machine systems. Strong AI and the concept of autonomous entities are defined and addressed. The presence of dream-equivalent states is discussed as a potential marker for human-equivalent consciousness.}
}
@article{CASTANEDA2023102391,
title = {A simulation-based approach for assessing the innovation barriers in the manufacturing firms},
journal = {Technology in Society},
volume = {75},
pages = {102391},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001963},
author = {Monica Castaneda and Milton M. Herrera and Alberto Méndez-Morales},
keywords = {Product innovation, Process innovation, Manufacturing sector, System dynamics, Barriers to innovation, Innovation policy},
abstract = {One of the most important challenges organisations’ faces to innovate is dealing with different types of barriers. Particularly, the case of manufacturing firms confronts several barriers, such as demand uncertainty, product imitation, lack of employees, scarcity of government funding, absence of internal and external financing. This paper aims to provide new insights regarding to the innovation barriers faced by the manufacturing firms. To do this, we implemented a computational model for analysing the barriers to innovation in the Colombian case. In this model, product and processes innovation are studied. It was concluded that for the innovation of process, the highly important barrier is the shortcoming of internal financing, while for the innovation of product is the lack of employees. Results show that the government expenditure is scarce compared to private and external investment.}
}
@article{TURNER2024,
title = {Old Strategies, New Environments: Reinforcement Learning on Social Media},
journal = {Biological Psychiatry},
year = {2024},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322324018201},
author = {Georgia Turner and Amanda M. Ferguson and Tanay Katiyar and Stefano Palminteri and Amy Orben},
keywords = {Development, Mental health, Reinforcement learning, Reward learning, Social media, Social reward},
abstract = {The rise of social media has profoundly altered the social world, introducing new behaviors that can satisfy our social needs. However, it is not yet known whether human social strategies, which are well adapted to the offline world we developed in, operate as effectively within this new social environment. Here, we describe how the computational framework of reinforcement learning (RL) can help us to precisely frame this problem and diagnose where behavior-environment mismatches emerge. The RL framework describes a process by which an agent can learn to maximize their long-term reward. RL, which has proven to be successful in characterizing human social behavior, consists of 3 stages: updating expected reward, valuating expected reward by integrating subjective costs such as effort, and selecting an action. Specific social media affordances, such as the quantifiability of social feedback, may interact with the RL process at each of these stages. In some cases, affordances can exploit RL biases that are beneficial offline by violating the environmental conditions under which such biases are optimal, such as when algorithmic personalization of content interacts with confirmation bias. Characterizing the impact of specific aspects of social media through this lens can improve our understanding of how digital environments shape human behavior. Ultimately, this formal framework could help address pressing open questions about social media use, including its changing role across human development and its impact on outcomes such as mental health.}
}
@article{CAO2020118,
title = {Computational parameter identification of strongest influence on the shear resistance of reinforced concrete beams by fiber reinforcement polymer},
journal = {Structures},
volume = {27},
pages = {118-127},
year = {2020},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2020.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352012420302435},
author = {Yan Cao and Qingming Fan and Sadaf {Mahmoudi Azar} and Rayed Alyousef and Salim T. Yousif and Karzan Wakil and Kittisak Jermsittiparsert and Lanh {Si Ho} and Hisham Alabduljabbar and Abdulaziz Alaskar},
keywords = {FRP: reinforced concrete, Shear resistance, Selection procedure, ANFIS},
abstract = {Bars made of fiber reinforcement polymer (FRP) are in common usage for concrete reinforcing instead of steel reinforcing since steel could be affected by corrosion. The concrete beams reinforced by FRP bars have been studied mostly in longitudinal direction without shear reinforcement. The primary objective of this investigation was to design and advance an algorithm for selection procedure of the parameters influence on prediction of shear resistance of reinforced concrete beams by FRP. Six input parameters were used which represent geometric and mechanical properties of the bars as well as shear features. These parameters are: web width, tensile reinforcement depth, ratio of shear and depth, concrete compressive strength, ratio of FRP reinforcement, FRP modulus of elasticity and beam shear resistance. The searching algorithm is based on combination of artificial neural network and fuzzy logic principle or adaptive neuro fuzzy inference system (ANFIS). Based on the obtained results ratio of shear and depth has the strongest influence on the prediction of shear resistance of reinforced concrete beams by FRP. Moreover, combination of tensile reinforcement depth and ratio of shear and depth is the most influential combination of two parameters on the prediction of shear resistance of reinforced concrete beams by FRP. Finally, combination of tensile reinforcement depth, ratio of shear and depth and FRP modulus of elasticity is the most influential combination of three parameters on the prediction of shear resistance of reinforced concrete beams by FRP.}
}
@article{SNEDDON20252898,
title = {Rapid (≤25 °C) cycloisomerization of anhydride-tethered triynes to benzynes – origin of a remarkable anhydride linker-induced rate enhancement††Electronic supplementary information (ESI) available. CCDC 2353613. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc07232d},
journal = {Chemical Science},
volume = {16},
number = {6},
pages = {2898-2906},
year = {2025},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc07232d},
url = {https://www.sciencedirect.com/science/article/pii/S2041652025000392},
author = {Dorian S. Sneddon and Paul V. Kevorkian and Thomas R. Hoye},
abstract = {The hexadehydro-Diels–Alder (HDDA) reaction is a cycloisomerization between a conjugated diyne and a tethered diynophile that generates ortho-benzyne derivatives. Considerable fundamental understanding of aryne reactivity has resulted from this body of research. The multi-yne cycloisomerization substrate is typically pre-formed and the (rate-limiting) closure of this diyne/diynophile pair to produce the isomeric benzyne generally requires thermal input, often requiring reaction temperatures of >100 °C and times of 16–48 h to achieve near-full conversion. We report here that diynoic acids can be dimerized and that the resulting substrate, having a 3-atom anhydride linker (i.e., OCOCO), then undergoes HDDA cyclization within minutes at or below room temperature. This allows for the novel in situ assembly and cyclization of HDDA benzyne precursors in an operationally simple protocol. Experimental kinetic data along with DFT computations are used to identify the source of this surprisingly huge rate acceleration afforded by the anhydride linker: >107 faster than the analogous multi-yne having, instead, a CH2OCH2 ether linker.}
}
@article{LEE1993255,
title = {Interval computation as deduction in chip},
journal = {The Journal of Logic Programming},
volume = {16},
number = {3},
pages = {255-276},
year = {1993},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(93)90045-I},
url = {https://www.sciencedirect.com/science/article/pii/074310669390045I},
author = {J.H.M. Lee and M.H. {Van Emden}},
abstract = {Logic programming realizes the ideal of “computation is deduction,” but not when floating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that floating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the benefits of logic programming to computation involving floating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic that was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artificial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the looking-ahead inference rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when floating-point computations have been used. We believe ours is the first system with this property.}
}
@incollection{VALLERO202151,
title = {Chapter 3 - Transitional and translational sciences},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {51-87},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821953900012X},
author = {Daniel A. Vallero},
keywords = {Translational science, Geodesign, Landfill fires, Tire fires, Coal mine fires, Nuclear accidents, “As low as reasonably practicable” (ALARP), Rational methods, Modularity, Interoperability},
abstract = {This chapter introduces two aspects critical to environmental systems science. The first is attention to ways to move from reductionism to systems thinking. The second is the need to translate the methods, results, and meaning of scientific discoveries from one discipline to all those needed to address an environmental or public health problem. To aid in this discussion, several examples of environmental episodes are discussed with an eye toward root causes. Knowledgebase needs to support the transition to systems thinking are discussed, including modularity and interoperability of models and methods}
}
@article{ADENIJI2023,
title = {Draft genome sequence of active gold mine isolate Pseudomonas iranensis strain ABS_30},
journal = {Microbiology Resource Announcements},
volume = {12},
number = {12},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/MRA.00849-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23009234},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola and Julie C. {Dunning Hotopp}},
keywords = {bioremediation, biosynthetic clusters, genome sequence, gold mine, , secondary metabolites},
abstract = {ABSTRACT
Pseudomonas iranensis ABS_30, isolated from gold mining soil, exhibits metal-resistant properties valuable for heavy metal removal. We report the draft genome sequencing of the P. iranensis ABS_30 strain, which is 5.9 Mb in size.}
}
@article{OXMAN1994141,
title = {Precedents in design: a computational model for the organization of precedent knowledge},
journal = {Design Studies},
volume = {15},
number = {2},
pages = {141-157},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90021-3},
url = {https://www.sciencedirect.com/science/article/pii/0142694X94900213},
author = {Rivka E Oxman},
keywords = {case-based reasoning, design precedents, memory organization},
abstract = {A computational model for the organization of design precedent knowledge is developed. The model is composed of distinct chunks of knowledge called design stories. A formalism for the design story is proposed which represents the linkage between design issue, concept and form in designs. Stories are structured in memory according to a semantic network. The lexicon of the semantic network acts as a memory index. The memory structure and indexing system are demonstrated to enhance search and to support cross-contextual browsing and exploration in the precedent library. The approach is demonstrated in a pilot design aid system in the task domain of early conceptual design in architecture.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of Priestia megaterium AB-S79 strain isolated from active gold mine},
journal = {Microbiology Resource Announcements},
volume = {13},
number = {2},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01055-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23010629},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola},
keywords = {bioremediation, biosynthetic traits, genome analysis, , secondary metabolites, genomics},
abstract = {ABSTRACT
We screened and isolated Priestia megaterium strain AB-S79 from active gold mine soil, then sequenced its genome to unravel its biosynthetic traits. The isolate with a 5.7-Mb genome can be utilized as a reference in genome-guided strain selection for metabolic engineering and other biotechnological operations.}
}
@article{CHUNG201356,
title = {Table-top role playing game and creativity},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {56-71},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000478},
author = {Tsui-shan Chung},
keywords = {Creativity, Role playing game, Divergent thinking test, Priming},
abstract = {The current study aims to observe whether individuals who engaged in table-top role playing game (TRPG) were more creative. Participants total 170 (52 TRPG players, 54 electronic role playing game (ERPG) players and 64 Non-players) aged from 19 to 63. In the current study, an online questionnaire is used, adopting the verbal subtests of Wallach–Kogan Creativity Tests and the McCrae and Costa Big Five Personality Inventory. It is found that TRPG players score higher in divergent thinking tests. Priming and instruction giving methods lower the performance of all participants, in particular, when the instruction is memory provoking. ERPG players score lowest among the three groups. TRPG could be regarded as a form of improvisation. It could also be a preferable activity for the promotion of creativity. It is low cost and no formal setting is required to play. Many ERPGs are originated from TRPGs, therefore, with the popularity of ERPG, there should be advantages in promoting TRPG.}
}
@article{CHEN2010573,
title = {Generating ontologies with basic level concepts from folksonomies},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {573-581},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000621},
author = {Wen-hao Chen and Yi Cai and Ho-fung Leung and Qing Li},
keywords = {Folksonomy, Ontology, Basic level categories, Category utility},
abstract = {This paper deals with the problem of ontology generation. Ontology plays an important role in knowledge representation, and it is an artifact describing a certain reality with specific vocabulary. Recently many researchers have realized that folksonomy is a potential knowledge source for generating ontologies. Although some results have already been reported on generating ontologies from folksonomies, most of them do not consider what a more acceptable and applicable ontology for users should be, nor do they take human thinking into consideration. Cognitive psychologists find that most human knowledge is represented by basic level concepts which is a family of concepts frequently used by people in daily life. Taking cognitive psychology into consideration, we propose a method to generate ontologies with basic level concepts from folksonomies. Using Open Directory Project (ODP) as the benchmark, we demonstrate that the ontology generated by our method is reasonable and consistent with human thinking.}
}
@incollection{GALLISTEL199235,
title = {Classical Conditioning as an Adaptive Specialization: A Computational Model},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {28},
pages = {35-67},
year = {1992},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60487-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604879},
author = {C.R. Gallistel},
abstract = {Publisher Summary
This chapter analyzes the results of some modern classical conditioning experiments from the perspective of a computational model based on the assumption that the underlying learning process is specifically adapted to the domain of multivariate, nonstationary time series. It focuses on the quantitative results from experiments on the effects of partial reinforcement on the rate of acquisition and extinction because the other predictions of the model have been discussed and associative models are conspicuously unsuccessful at making quantitative predictions in this area. The model gives a mathematical characterization of the learning process from which one can derive the results of conditioning experiments. It is unlike these models in the sense that it is not in the associative tradition. The model replaces the associative explanatory framework with a framework that treats the conditioning process as a computational mechanism adapted through evolution to the peculiarities of one domain-a mechanism that solves one and only one of the several fundamentally distinct learning problems that confront mobile, multicellular organisms.}
}
@article{CAMARGOJUNIOR2016190,
title = {Optimal economic result and risk of parallel development of concept options in dynamic markets},
journal = {RAI Revista de Administração e Inovação},
volume = {13},
number = {3},
pages = {190-198},
year = {2016},
issn = {1809-2039},
doi = {https://doi.org/10.1016/j.rai.2016.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1809203916300377},
author = {Alceu Salles {Camargo Júnior} and Abraham Sin Oih Yu},
keywords = {New product development, Economic result and risk of projects, Option thinking},
abstract = {New product development is an essential competence to organizations. Launching success products requires elaborate and precise knowledge about the technological platforms, like the most important market needs and characteristics, and the project team have to employ information systems to support the project decisions, which must be rapid and accurate. However, when the market characteristics are much dynamic and change rapidly or the development project aims at a really new product, the levels of uncertainties are greater, and the project team must employ more robust strategies of risk management. Option thinking is useful to develop several concept alternatives of some crucial subsystems of the new product in order to achieve new technical and market knowledge by repeating cycles of design, built and tested by several and different prototypes in parallel. These different prototypes develop, test and can accumulate knowledge about each one, different technologies, architectures and quality attributes or the usability for potential customers. This study achieves the optimal number of concept options to develop in parallel in order to maximize the economic performance of the development project of a new product constituted of two important subsystems. Mathematical models simulating the sequential decision process are developed to determine the economic result and risk of a two-subsystem product innovation project. Our results point the parallel development of concept options as a robust strategy to manage new product development mostly in adverse conditions, that is, with greater levels of uncertainties.}
}
@article{HENRY2016119,
title = {Hofmeister series: The quantum mechanical viewpoint},
journal = {Current Opinion in Colloid & Interface Science},
volume = {23},
pages = {119-125},
year = {2016},
issn = {1359-0294},
doi = {https://doi.org/10.1016/j.cocis.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1359029416301108},
author = {Marc Henry},
keywords = {Quantum mechanics, Phase coherence, Living cells, Condensed matter, Hofmeister series, Water, Aqueous solutions, Harmonic ratios},
abstract = {It is suggested that electromagnetic quantum vacuum fluctuations are at the very deep root of the so-called “specific ions effects” in concentrated solutions or in living cells. A many-body quantum-mechanical frame of thinking is proposed based on the concept of quantum coherence taking into account explicitly density and excitation frequencies of molecules and/or ionic species. It is also proposed that Hofmeister phenomena could have a natural explanation in the harmonic relationships between sets of characteristic frequencies ruled by quantum mechanical laws. It then follows that physical chemistry of concentrated media and biology should be ruled more by a quantum “symphony” between indistinguishable constituents rather than localized two-body electrical interactions between molecular or ionic species.}
}
@incollection{KUMAR20031,
title = {1 - An introduction to computational development},
editor = {Sanjeev Kumar and Peter J. Bentley},
booktitle = {On Growth, Form and Computers},
publisher = {Academic Press},
address = {London},
pages = {1-43},
year = {2003},
isbn = {978-0-12-428765-5},
doi = {https://doi.org/10.1016/B978-012428765-5/50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124287655500347},
author = {Sanjeev Kumar and Peter J. Bentley}
}
@incollection{ROWLAND2003341,
title = {Chapter 16 - Interpreting Analytical Spectra with Evolutionary Computation},
editor = {Gary B. Fogel and David W. Corne},
booktitle = {Evolutionary Computation in Bioinformatics},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {341-365},
year = {2003},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-797-2},
doi = {https://doi.org/10.1016/B978-155860797-2/50018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607972500184},
author = {Jem J. Rowland},
abstract = {Publisher Summary
This chapter deals with analytical techniques that are used to probe the activity and chemical makeup of cells. Metabolomics, the study of the entire biochemical constituents of a cell at any one time, is found to provide a rich means of monitoring organism activity. It can reveal explanations for different characteristics of seemingly similar organisms and can be used to relate function with gene. Spectroscopies are well suited to the study and interpretation of the metabolome in functional genomics. Another important technique in functional genomics is the measurement of gene expression via transcriptome arrays. This chapter outlines the various ways in which evolutionary computation (EC) can provide the basis for powerful tools for spectral interpretation and thus for functional genomics. It mentions various methods of forming predictive models from multivariate, often quasi-continuous data. It also discusses ways in which the effectiveness of such conventional techniques may be enhanced by combining them with evolutionary techniques.}
}
@article{XU201766,
title = {Emerging Trends for Microbiome Analysis: From Single-Cell Functional Imaging to Microbiome Big Data},
journal = {Engineering},
volume = {3},
number = {1},
pages = {66-70},
year = {2017},
issn = {2095-8099},
doi = {https://doi.org/10.1016/J.ENG.2017.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S2095809917301595},
author = {Jian Xu and Bo Ma and Xiaoquan Su and Shi Huang and Xin Xu and Xuedong Zhou and Wei E. Huang and Rob Knight},
keywords = {Microbiome, Method development, Single-cell analysis, Big data, China Microbiome Initiative},
abstract = {Method development has always been and will continue to be a core driving force of microbiome science. In this perspective, we argue that in the next decade, method development in microbiome analysis will be driven by three key changes in both ways of thinking and technological platforms: ① a shift from dissecting microbiota structure by sequencing to tracking microbiota state, function, and intercellular interaction via imaging; ② a shift from interrogating a consortium or population of cells to probing individual cells; and ③ a shift from microbiome data analysis to microbiome data science. Some of the recent method-development efforts by Chinese microbiome scientists and their international collaborators that underlie these technological trends are highlighted here. It is our belief that the China Microbiome Initiative has the opportunity to deliver outstanding “Made-in-China” tools to the international research community, by building an ambitious, competitive, and collaborative program at the forefront of method development for microbiome science.}
}
@article{GALBUSERA2022103109,
title = {Game-based training in critical infrastructure protection and resilience},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103109},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103109},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003284},
author = {Luca Galbusera and Monica Cardarilli and Marina {Gómez Lara} and Georgios Giannopoulos},
keywords = {Critical infrastructure, Resilience, Preparedness, Training, Exercises, Serious games, Gamification},
abstract = {Several institutions worldwide are reflecting on the relevance of training and exercises to critical infrastructure protection and resilience. This is witnessed, for instance, by Council Directive 2008/114/EC in the EU and the Homeland Security Exercise and Evaluation Program in the US. Contributing to the research actions in the field, the present article discusses methodological approaches, tools, techniques, and technologies relevant to this domain. In particular, we report on a recent training initiative elaborated by the authors and involving a game-based, modelling-and-simulation-backed, computer-assisted exercise for critical infrastructure expert audiences. This was developed taking advantage of JRC's Geospatial Risk and Resilience Assessment Platform (GRRASP) and critical infrastructure analysis methodologies integrated therein. The overarching objective was to enhance system thinking and raise awareness of resilience aspects while familiarizing participants with specific analysis tools and scientific models.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{GARAS2024100885,
title = {A data analytics case study analyzing IRS SOI migration data using no code, low code technologies},
journal = {Journal of Accounting Education},
volume = {66},
pages = {100885},
year = {2024},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100885},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000010},
author = {Samy Garas and Susan L. Wright},
keywords = {Robotic process automation, UiPath, Alteryx, Tableau, Data automation, Data analytics, Data visualizations, Regional migration, Government planning, Business planning},
abstract = {Organizations generate and accumulate vast amounts of structured and unstructured data that have value for formulating and supporting strategic decisions. The advancement of no-code and low-code software has enabled the use of this data to provide significant data insights and business intelligence by employing multiple forms of data analytics. The imperative to cultivate a robust and proficient group of individuals with expertise in data analytics has led to a substantial increase in the number of educational programs focused on data science and analytics. Accounting educators can capitalize on these trends by integrating data analytics and software skills into the accounting curriculum. This case offers essential materials to aid in the development of the curriculum to support accounting and analytics educators. This case serves many objectives by providing a professional setting in which you take on the role of junior data analyst, offering necessary context and motivation for completing the tasks. The case allows you to analyze extensive data sets obtained from the IRS Statistics of Income (SOI) website in order to investigate migration patterns based on state, year, age, and income categories. UiPath-robotic process automation (RPA), Alteryx-based data analysis, and Tableau-based data visualization tools are employed to extract, generate, and present descriptive statistics and to conduct a simple times series analysis. These insights are highly valuable to decision makers in business and government organizations. You are encouraged to engage in critical thinking and to consider the potential impacts of migratory patterns on choices made by firm executives and public policy makers. Migration patterns have a significant impact on firm management decisions, influencing either to expand or reduce current operations and indicating the availability and expansion of new talent pools. Migration patterns have a significant impact on the decision made by public policy makers, particularly in relation to public utilities, infrastructure, and other services and benefits. You analyze temporal data to deduce the influence of changes in the tax code and shifts in the economy. You gain expertise in managing large data sets, exploring features of analytics software, and creating compelling visualizations to effectively communicate important discoveries. Instructors and students are given comprehensive instructions and videos to facilitate the efficient application of these technologies.}
}
@incollection{SUGHRUE2024151,
title = {Chapter 6 - Reimagining neurocognitive functions as emergent phenomena: What resting state is really showing us},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {151-157},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000082},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain hub, Brain landscape, Network control theory, Neurocognitive function, Resting-state fMRI, Structural connectome},
abstract = {In this chapter, we introduce a new way of thinking about neurocognitive functioning and related dysfunction. We discuss how structural wiring patterns, global rhythms in deep structures, and electrochemical gain from neurotransmitters play a key role in the internal dynamics of what the brain is doing. Importantly, together, these elements dictate how the brain can or cannot obtain different brain states. Simultaneously, disruption in intrinsic structures and internal dynamics alters the energetic landscape causing some brain states to become more favorable or less favorable. Importantly, we go on to describe how landscapes arise from structural connectomes, and how these connections can dictate spontaneous behavioral patterns and tendencies in normal as well as pathologic states, such as a depressed patient being stuck in a self-ruminating and negative state. Resting-state fMRI also provides a keyhole into these processes as the entire set of the structural connectome creates the patterns of functional connectivity seen in resting-state brain activity.}
}
@incollection{KUMAR2025185,
title = {Chapter 9 - Future prospective of neuromorphic computing in artificial intelligence: A review, methods, and challenges},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {185-197},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000080},
author = {Vivek Kumar and Kapil Joshi and Rajiv Kumar and Minakshi Memoria and Ashulekha Gupta and F. Ajesh},
keywords = {Neuromorphic computing, Artificial intelligence, Deep learning, Machine learning, Human brain modeling},
abstract = {Neuromorphic computing in the area of artificial intelligence (AI) offers the appeal of human brain modeling. In the Fourth Industrial Revolution era, AI is among the most advanced scientific knowledge that can integrate human behavior and intelligence into machines. Even though neuromorphic computing has been around since the 1980s, it is still a relatively new field. In the last 10 years, in particular, there has been a significant amount of study and the advancement of AI. The next stage of AI is thought to be Neuromorphic Computing. The development of neuromorphic computing technology will be crucial. The most potent computational device in existence, the human brain has long served as an inspiration for AI. This study discusses neuromorphic computing, a new form of sophisticated computing that draws inspiration from brain intelligence. The objective of this paper is to give a summary of the present status of AI and neuromorphic computing to express a viewpoint on the potential and challenges that lie ahead for the main applications of neuromorphic computing. We discuss the prospects for further development of these systems and highlight features of neuromorphic computing that are promising for the field's future.}
}
@article{DO2020110730,
title = {Capturing creative requirements via requirements reuse: A machine learning-based approach},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110730},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301631},
author = {Quoc Anh Do and Tanmay Bhowmik and Gary L. Bradshaw},
keywords = {Requirements reuse, Requirements engineering, Creativity in RE, Boilerplate, Natural language processing, Machine learning},
abstract = {The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework’s ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels.}
}
@article{ROSSITER202017604,
title = {Using interactive tools to facilitate student self-testing of dynamics and PI compensation},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17604-17609},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2677},
url = {https://www.sciencedirect.com/science/article/pii/S240589632033439X},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, staff efficiency, student engagement, independent learning},
abstract = {Virtual laboratories have become a common tool in recent years for supporting student learning and engagement. This paper presents a new tool for helping students self-assess their competence in basic dynamics for 1st and 2nd order systems alongside simple PI compensation techniques. The tools provide a supported environment for helping students work towards the correct answer by providing succinct feedback on incorrect responses and opportunities to try again, while displaying relevant information. A partner interactive tool is also provided which focuses solely on assessment with no feedback, so that students can assess their ability to get correct answers in a scenario that only the first attempt counts. This paper gives the thinking behind the tools, their coding and also accessibility for students.}
}
@incollection{VERSCHAFFEL2010401,
title = {Mathematics Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {401-406},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00517-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947005170},
author = {L. Verschaffel and B. Greer and E. {De Corte}},
keywords = {Adaptive expertise, Assessment, Collaboration, Competence, Constructivism, Design experiment, High-stakes testing, Mathematical, Mathematics education, Mathematics learning, Mathematics teaching, Prior knowledge, Routine expertise, Situated cognition, Standards},
abstract = {This article presents a review of important recent themes and developments in research on the learning and teaching of mathematical knowledge and thinking. As a framework, we use a model for designing a powerful environment for learning and teaching mathematics; this model is structured according to four interrelated components, namely competence, learning, intervention, and assessment (CLIA-model) (De Corte et al., 2004). We argue and illustrate that our empirically based knowledge of each of these four interconnected components has substantially advanced over the past decades, enabling a progressively better understanding of not only the components that constitute a mathematical disposition, but also the nature of the learning and developmental processes that should be induced in students to facilitate the acquisition of competence, the characteristics of learning environments that are powerful in initiating and evoking those processes, and finally, the kind of assessment instruments that are appropriate to help monitor and support learning and teaching.}
}
@article{REISS1967193,
title = {Individual thinking and family interaction—II. A study of pattern recognition and hypothesis testing in families of normals, character disorders and schizophrenics},
journal = {Journal of Psychiatric Research},
volume = {5},
number = {3},
pages = {193-211},
year = {1967},
issn = {0022-3956},
doi = {https://doi.org/10.1016/0022-3956(67)90002-7},
url = {https://www.sciencedirect.com/science/article/pii/0022395667900027},
author = {David Reiss},
abstract = {The present study investigated the relationship between family interaction and individual pattern recognition in five families of normals, five families of character disorders and five families of schizophrenics. Following a period of family interaction, members of normal families showed improvement in pattern recognition; members of families of schizophrenics showed deterioration or no change and members of character disorder families were in between. During the period of family interaction, members of normal families were independent and adventuresome in testing their pattern concepts whereas members of families of schizophrenics were cautios, copied each other's performance but showed little pooling of ideas. These findings support the hypothesis that family interaction can influence perceptual process in its individual members in a short time and points to some particular relationships between family interaction and individual perception.}
}
@article{SHUBBAR2024382,
title = {Bridging Qatar's food demand and self-sufficiency: A system dynamics simulation of the energy–water–food nexus},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {382-399},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000423},
author = {Haya Talib Shubbar and Furqan Tahir and Tareq Al-Ansari},
keywords = {Carbon emissions, Energy-water-food nexus, Food self-sufficiency, Food security, Qatar, System dynamics},
abstract = {The food sector in Qatar is confronted with formidable challenges due to its harsh environmental conditions. Striving for total food self-sufficiency in such an environment would inevitably exert pressure on the energy and water sectors. This heightened demand for energy and water translates into increased costs and escalates environmental impacts. Consequently, this study embarks on an in-depth analysis of food production within the context of Qatar's energy-water-food nexus, aiming to demonstrate how varying degrees of food self-sufficiency may impact the demand on Qatar's water and energy sectors, as well as on greenhouse gas (GHG) emissions. Moreover, this study demonstrates to what extent specific subsystems within the nexus can be modified to enhance sustainability. An energy-water-food nexus is meticulously crafted within the proposed framework to elucidate the intricate interdependencies among these sectors, incorporating pertinent external variables. These interconnections are then transmuted into a system dynamics model (SDM), facilitating a nuanced exploration of potential transformations and their ripple effects. Furthermore, a life-cycle thinking approach explicitly tailored to Qatar was implemented to estimate GHG emissions accurately. Four distinct scenarios are rigorously examined using the SDM, spanning from a status quo perspective to ambitious transitions toward full food self-sufficiency. The findings of the scenarios indicate that scenario 4, which partially provides the country with its food demands locally using desalinated water, treated wastewater, and groundwater and satisfies 20 % of its energy demand from solar energy, is the most ideal with an annual 5.36 × 1010 kWh/year energy consumption, 1.73 × 1012 l/year water demand, and 3.26 × 1010 kg CO2 eq./year emissions. The outcomes underscore the imperative for prioritizing less energy-intensive resources to mitigate overall energy consumption. Additionally, achieving an optimal national scenario necessitates a judicious equilibrium between food imports and domestic production.}
}
@incollection{MARTIGNON2001382,
title = {Algorithms},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {382-385},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00549-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005490},
author = {L. Martignon},
abstract = {The concept of algorithm is central to the modern view of a thinking machine, be it the human mind or the modern computer. An algorithm is a well-defined mathematical recipe for the solution of a well-defined task. It is presented as a finite set of steps or instructions that can be applied to unlimited sets of possibilities. There is a clear-cut rule for the operation to be performed at each step, as well as a clear-cut specification of the conditions under which to terminate the process. An algorithm may contain loops, that is, there may be steps that return to previous steps. Algorithms can be sequential or parallel. An algorithm that produces a ‘yes’ or ‘no’ answer is, decision algorithm. An algorithm that constructs or determines a specific solution to a given problem is a computation algorithm.}
}
@article{KOPPAKA2024,
title = {Mechanism and Selectivity of Bi(V)-Aryl Oxyfunctionalization in Trifluoroacetic Acid Solvents},
journal = {Organometallics},
year = {2024},
issn = {0276-7333},
doi = {https://doi.org/10.1021/acs.organomet.4c00319},
url = {https://www.sciencedirect.com/science/article/pii/S0276733324003509},
author = {Anjaneyulu Koppaka and Dongdong Yang and Sanaz Mohammadzadeh Koumleh and Burjor Captain and Roy A. Periana and Daniel H. Ess},
abstract = {The oxidative functionalization of aromatic sp2 C–H bonds to C–O bonds is a difficult transformation. For main-group metals, the oxyfunctionalization step of a metal-aryl bond is generally slow and potentially problematic if carried out in a relatively strong acid solvent where protonation could prevent oxyfunctionalization. In this work, we experimentally and computationally analyzed the oxyfunctionalization reaction of (Ph)3BiV(TFA)2 (TFA = trifluoroacetate) in a trifluoroacetic acid (TFAH) solvent. Experiments showed a single oxyfunctionalization product phenyl TFA (PhTFA) and two equivalents of benzene. Explicit/continuum solvent density functional theory calculations revealed that a direct intramolecular reductive functionalization pathway is lower in energy than radical or ionic pathways, and surprisingly from (Ph)3BiV(TFA)2, the reductive functionalization pathway is potentially competitive with protonation. In contrast, for (Ph)2BiV(TFA)3 oxyfunctionalization is significantly lower in energy than protonation. For BiIII-phenyl intermediates, redox neutral protonation is significantly lower in energy than a second functionalization. We also examined the oxyfunctionalization versus protonation of BiV-phenyl complexes with a coordinated biphenyl ligand and a coordinated biphenyl sulfone ligand, which both resulted in oxyfunctionalization. For the biphenyl ligand complex, a protonation-first mechanism is proposed, while for the biphenyl sulfone ligand, an oxyfunctionalization first mechanism is consistent with both calculations and experiments.
}
}
@article{NYSTROM201077,
title = {Ontological musings on how nature computes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {77-86},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000116},
author = {J.F. Nystrom},
keywords = {Universe as computation, Quantum vacuum, Computational cosmography},
abstract = {Modern physical theory and modern computational techniques are used to provide conjecture on how nature computes. I utilize time-domain simulation of physical phenomena and build analogies between elements of computation and the “things” of Universe computation, resulting, for example, in the identification of the quantum vacuum as the power source for Universe computation. While reviewing how Universe can be viewed as a computation, we find the need for Negative Universe (which is a part of the quantum vacuum mechanism). This idea is compared with Penrose’s current model which utilizes a separate Platonic world outside of physical Universe. Lastly, in the Discussion, I present an updated version of computational cosmography as a model for Universe as computation.}
}
@incollection{MILLER2023203,
title = {Chapter 7 - The calculated uncertainty of scientific discovery: From Maths to Deep Maths},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {203-226},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016971612300024X},
author = {D. Douglas Miller},
keywords = {Mathematics, Philosophy, Statistics, Null hypothesis, Artificial intelligence, Data dimensionality, Machine learning, Algorithms, Deep learning, Stochastic gradient descent, Model optimization, Bias, Neural networks, Backpropagation, Large language models, Model generalizability},
abstract = {Throughout history, diverse Maths have underpinned numerous important natural and physical science discoveries. In their initial development and application, these Maths were often incompletely or imperfectly understood, with constants and “fudge factors” needed to account for statistical uncertainties to advance a scientific discipline. Some polymaths have acted as philosophers in support of new ways of thinking, based on their novel discoveries about the natural and physical world. Deep Maths integral to artificial intelligence (AI), machine learning and deep learning (DL), are also subject to human imperfections (i.e., computational errors, operator assumptions) and stochastic uncertainties (i.e., modeling biases, convergence optimizers). Mathematicians and domain experts can collaborate to increase AI model accuracy by improving training data quality (i.e., curating, reducing dimensionality), mitigating human and machine biases, and understanding data contexts prior to query. Since the advent of DL and through the design of multilayered feedforward neural networks then large language models, scientists have applied advanced AI computing capabilities to push the limits of this technology trend. Recently, AI's capacity to uncover newly modeled insights has been hyped beyond the proven limits of DL model accuracy. History has witnessed the acceptance of new knowledge (primarily by peers) based on the accuracy and/or reproducibility of empirical observations and on varied interpretations of mathematical proofs. Societal enthusiasm for science or technology insertion is often limited by the general public's understanding of the underlying Maths and Deep Maths, and related human fears and concerns of displacement (i.e., lost jobs, ecological impact, less privacy, etc.). Today's proponents of societal progress based on new discoveries and technologies are motivated by a range of influences (i.e., humanity, control, security, profit, etc.), creating additional uncertainties that can deflect initial scientific enthusiasm and/or delay widespread adoption.}
}
@article{RAI201651,
title = {Fragmentary shape recognition: A BCI study},
journal = {Computer-Aided Design},
volume = {71},
pages = {51-64},
year = {2016},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010448515001542},
author = {Rahul Rai and Akshay V. Deshpande},
keywords = {Brain–Computer Interfaces (BCI), Fragmentary shape recognition, User studies, Cognitive load in shape processing, Natural interactions},
abstract = {Recently, Brain–Computer Interface (BCI) has emerged as a potential modality that utilizes natural and intuitive human mechanisms of thinking process to enable interactions in CAD interfaces. Before BCI could become a mainstream mode of HCI for CAD interfaces; fundamental studies directed towards understanding how humans mentally represent and process the geometry are needed. The outlined work in this paper presents an objective user study to understand shape recognition process in the humans. Specifically, we focus on the fundamental task of fragmentary shape identification. The problem of fragmentary shape recognition can be defined as follows: given a partial and incomplete minimalistic representation of a given shape, can one recognize the actual complete shape or object? In user studies, each subject was progressively (in stages) shown more informative fragmented images of an object to be recognized. During each stage of the experiment, the brain activity of users in the form of electroencephalogram (EEG) signals was recorded with a BCI headset. The recorded signals are then processed to objectively study the fragmentary shape recognition process. The results of user studies conclusively show that the measured brain activities of subjects can serve as a very accurate proxy to estimate subjects fragmentary shape recognition process.}
}
@article{BIRJALI201765,
title = {Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks},
journal = {Procedia Computer Science},
volume = {113},
pages = {65-72},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091731699X},
author = {Marouane Birjali and Abderrahim Beni-Hssane and Mohammed Erritali},
keywords = {Sentiment Analysis, Machine Learning, Suicide, Social Networks, Tweets, Semantic Sentiment Analysis},
abstract = {Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.}
}

@article{BRUNDAGE201532,
title = {Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014)},
journal = {Futures},
volume = {72},
pages = {32-35},
year = {2015},
note = {Confronting Future Catastrophic Threats To Humanity},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2015.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715000932},
author = {Miles Brundage},
keywords = {Existential risk, Artificial intelligence, Superintelligence, Responsible innovation},
abstract = {A new book by Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, is reviewed. Superintelligence explores the future of artificial intelligence and related technologies and the risks they may pose to human civilization. The book ably demonstrates the potential for serious thinking aimed at the long-term future. Bostrom succeeds in arguing that the development of superintelligent machines will, if not properly managed, create catastrophic risks to humanity. The book falls short in some respects, and some sections are more compelling and novel than others. Overall, however, Bostrom’s book succeeds in demolishing the “null hypothesis” according to which the possibility and risks of superintelligence can continue to be ignored, and is a must-read for those interested in the long-term future of humanity.}
}
@article{LOMBARDI2022100601,
title = {Understanding emerging patterns and dynamics through the lenses of the cyber-physical universe},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100601},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100601},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002264},
author = {Mauro Lombardi and Simone Vannuccini},
keywords = {cyber-physical universe, ubiquitous computing, information technology, artificial intelligence, decision making},
abstract = {Summary
The complex interaction among contemporary techno- and socio-economic processes has set the stage for the emergence of a cyber-physical universe, the novel landscape in which agents behave and interact, and which is centered on the fundamental role played by information and computation at all levels. In this paper, we weave into a single analysis the different threads that lead to (and characterize) the cyber-physical universe and outline a map of its building blocks and the complex dynamics at work in the new environment. The resulting description is used to assess how decision-making processes should evolve in order to be able to address the opportunities and challenges of the current era of deep and extended changes. The analysis offers an encompassing interpretative grid to understand and unpack patterns in the contemporary socio-technical systems that experience a fundamental informational turn; this can inform new research trajectories and help open up new areas for scientific inquiry.}
}
@article{DEY2016177,
title = {A probabilistic approach to diagnose faults of air handling units in buildings},
journal = {Energy and Buildings},
volume = {130},
pages = {177-187},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306958},
author = {Debashis Dey and Bing Dong},
keywords = {Air Handling Unit, Bayesian belief network, APAR rules, Fault detection and diagnosis},
abstract = {Air handling unit (AHU) is one of the most extensively used equipment in large commercial buildings. This device is typically customized and lacks quality system integration which can result in hardwire failures and control errors. Air handling unit Performance Assessment Rules (APAR) is a fault detection tool that uses a set of expert rules derived from mass and energy balances to detect faults in air handling units. APAR is computationally simple enough that it can be embedded in commercial building automation and control systems and relies only upon sensor data and control signals that are commonly available in these systems. Although APAR has advantages over other methods, for example no training data required and easy to implement commercially, most of the time it is unable to provide the root diagnosis of the faults. For instance, a fault on temperature sensor could be bias, drifting bias, inappropriate location, or complete failure. In addition a fault in mixing box can be return and/or outdoor damper leak or stuck. In addition, when multiple rules are satisfied, the list of faults increases. There is no proper way to have the correct diagnosis for rule based fault detection system. To overcome this limitation, we proposed Bayesian Belief Network (BBN) as a diagnostic tool. BBN can be used to simulate diagnostic thinking of FDD experts through a probabilistic way. In this study we developed a new way to detect and diagnose faults in AHU through combining APAR rules and Bayesian Belief network. Bayesian Belief Network is used as a decision support tool for rule based expert system. BBN is highly capable to prioritize faults when multiple rules are satisfied simultaneously. Also it can get information from previous AHU operating conditions and maintenance records to provide proper diagnosis. The proposed model is validated with real time measured data of a campus building. The results show that BBN correctly prioritize faults that are verified by manual investigation.}
}
@article{KNYAZEV201817,
title = {Resting state connectivity mediates the relationship between collectivism and social cognition},
journal = {International Journal of Psychophysiology},
volume = {123},
pages = {17-24},
year = {2018},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167876017305470},
author = {Gennady G. Knyazev and Alexander N. Savostyanov and Andrey V. Bocharov and Ekaterina A. Merkulova},
keywords = {Collectivism, Social cognition, Medial prefrontal cortex, Connectivity, Mediation analysis},
abstract = {Humans are intrinsically social beings and it is natural that self-processing is associated with social cognition. The degree to which the self is perceived as a part of social environment is modulated by cultural stereotypes, such as collectivism and individualism. Here, we tested the hypothesis that individuals who endorse collectivist values would spontaneously think more about their relationships with other people and this association would be mediated by connectivity between the medial prefrontal cortex (MPFC) and the rest of the brain. Connectivity was evaluated based on resting state EEG data using the recently developed methods, which combine beamformer spatial filtering with seed based connectivity estimation. The formal mediation analysis revealed that collectivism is associated with an enhanced connectivity of MPFC with a set of cortical regions that are frequently co-activated in moral reasoning, empathy, and theory of mind tasks and with diminished connectivity with the precuneus\posterior cingulate cortex, which is involved in self-centered cognition. The relationship between collectivism and social cognition was mediated by MPFC connectivity with the left middle temporal gyrus implying that in participants with collectivistic attitude, thinking about relationships with other people may be associated with semantic memory retrieval and reasoning on moral issues and others' intentions.}
}
@article{VARGASCARPINTERO2025120104,
title = {Development of an integrated multi-criteria framework to assess the implementation potential of biobased value chains and webs with a territorial approach},
journal = {Industrial Crops and Products},
volume = {223},
pages = {120104},
year = {2025},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2024.120104},
url = {https://www.sciencedirect.com/science/article/pii/S0926669024020818},
author = {Ricardo Vargas-Carpintero},
keywords = {Biobased value chain, Biobased value web, Biorefinery, Territorial bioeconomy system, Multi-criteria assessment, Land-based bioeconomy},
abstract = {Biobased value chains and webs (BVCW) encompass value adding activities and actors from biomass production, its processing into biobased products for manifold sectors, until their commercialization and use. BVCW are part of territorial bioeconomy systems and are shaped by contextual settings. The design and development of BVCW involve strategic decisions towards their sustainable implementation. Throughout the design and development of BVCW, the adoption of an integral approach that links technical aspects of biomass-to-product pathways with non-technical aspects and context factors is necessary to increase the BVCW implementation potential. Accordingly, an active incorporation of the territorial context of BVCW in the design process is required. In view of these requirements, in this study an integrated, multi-criteria framework is developed to assess the implementation potential in BVCW design. For this purpose, key elements from existing biorefinery and biomass supply chain design methodologies are identified and integrated in a multi-criteria framework that allows the consideration of both an internal and external perspective of the BVCW in relation to the context. The conceptualized framework serves as an evaluation approach to check the implementability of biomass-to-product pathways BVCW configurations in form of by means of a multi-criteria catalogue. The set of criteria integrates relevant aspects for the design and development of BVCW from land-based biomass (e.g. crops and crop residues). It entails key criteria related to the functionality of the biomass-to-product pathway in technical-economic terms and the surrounding biophysical, social and economic context. The further operationalization of the multi-criteria catalogue by means of an indicator-based assessment could enable the prioritization and selection of BVCW configurations with best implementation potential. In this way, the framework provides a practical approach for decision-makers, local actors and researchers involved in the design and development of BVCW tailored to the territorial context.}
}
@article{EVANS2003454,
title = {In two minds: dual-process accounts of reasoning},
journal = {Trends in Cognitive Sciences},
volume = {7},
number = {10},
pages = {454-459},
year = {2003},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2003.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661303002250},
author = {Jonathan St.B.T. Evans},
abstract = {Researchers in thinking and reasoning have proposed recently that there are two distinct cognitive systems underlying reasoning. System 1 is old in evolutionary terms and shared with other animals: it comprises a set of autonomous subsystems that include both innate input modules and domain-specific knowledge acquired by a domain-general learning mechanism. System 2 is evolutionarily recent and distinctively human: it permits abstract reasoning and hypothetical thinking, but is constrained by working memory capacity and correlated with measures of general intelligence. These theories essentially posit two minds in one brain with a range of experimental psychological evidence showing that the two systems compete for control of our inferences and actions.}
}
@article{KASHYAPKASHYAP2021395,
title = {The universal language: mathematics or music?},
journal = {Journal for Multicultural Education},
volume = {15},
number = {4},
pages = {395-415},
year = {2021},
issn = {2053-535X},
doi = {https://doi.org/10.1108/JME-05-2021-0064},
url = {https://www.sciencedirect.com/science/article/pii/S2053535X21000197},
author = {RaviRavi KashyapKashyap},
keywords = {Mathematics, Multicultural, Music, Education policy, Artistic encoding of knowledge, Universal language},
abstract = {Purpose
Music could be a challenger for mathematics and a potential candidate for the title “The Universal Language.” This paper aims to discuss the primary objectives of engaging with music, including the therapeutic benefits. Similarities, between mathematics and music and how studying one might enhance one’s abilities of the other are pointed out.
Design/methodology/approach
A formal definition for a universal language is given. A qualitative approach, supplemented with rigorous reasoning, is adopted. The narrative relies on the author’s experiences, teaching mathematical concepts and musical interactions, with students from several countries. A vast amount of literature is reviewed and the corresponding findings are connected toward the arguments made.
Findings
The paper demonstrates that one day, once we understand both mathematics and music better, we might see both of them as the same language. Until then, it is essential to supplement mathematics with music. The educational implications, for all fields, are to ensure that the future creators of knowledge are equally adept at both music and mathematics. The wider policy connotations are to create a blueprint for a society with a vibrant musical and artistic environment.
Originality/value
This study illuminates new ways of thinking about music and mathematics. The possibility that many seemingly complex entities (including our universe, virtual computer worlds, mathematical operations, etc.), are made up of combinations of much simpler building blocks is hinted at. Familiarity with any intricate element of life, without getting flustered, is bound to produce remarkable results in other such endeavors.}
}
@article{EVANS2022281,
title = {The explainability paradox: Challenges for xAI in digital pathology},
journal = {Future Generation Computer Systems},
volume = {133},
pages = {281-296},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000838},
author = {Theodore Evans and Carl Orge Retzlaff and Christian Geißler and Michaela Kargl and Markus Plass and Heimo Müller and Tim-Rasmus Kiehl and Norman Zerbe and Andreas Holzinger},
keywords = {Explainable AI, Digital pathology, Usability, Trust, Artificial intelligence},
abstract = {The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine and proposes empirically-backed principles for their safer and more effective design.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{20213341,
title = {Tim Behrens},
journal = {Neuron},
volume = {109},
number = {21},
pages = {3341-3343},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007200},
abstract = {Summary
Tim Behrens discusses with Neuron creative ways to facilitate virtual meetings, the multiple ways that the pandemic has affected different people, and his advice for the younger generation of neuroscientists in general and computational scientists in particular.}
}
@article{DANAHY2001127,
title = {Technology for dynamic viewing and peripheral vision in landscape visualization},
journal = {Landscape and Urban Planning},
volume = {54},
number = {1},
pages = {127-138},
year = {2001},
note = {Our Visual Landscape: analysis, modeling, visualization and protection},
issn = {0169-2046},
doi = {https://doi.org/10.1016/S0169-2046(01)00131-1},
url = {https://www.sciencedirect.com/science/article/pii/S0169204601001311},
author = {John W. Danahy},
keywords = {Visualization, Real-time immersive virtual reality, Panorama, Peripheral vision, Foveal vision, Dynamic viewing},
abstract = {The dynamic qualities of looking around and moving about, directly sensing spatial queues, using one’s peripheral vision, and focusing with foveal vision on objects of attention are fundamental to a person’s visual experience in landscape. Unfortunately, the visual media commonly used to structure scientific analysis, professional design, decision-making and artistic interpretation of visual landscapes are quite weak at portraying the dynamic and peripheral dimensions of human vision. Also, visual media whether it be manual drawing, photomontage or state-of-the-art computer animation tend to be time consuming and difficult to apply to these dimensions of seeing. The absence of a convenient, cost-effective means for showing all the fundamental visual aspects of landscape in a balanced way is a serious limitation. This deficiency begs the following questions. Is the current state of knowledge in visual landscape management biased by the relative ease with which established media, such as illustration, photography, and photo-realistic rendering can be used? Do the characteristics of these media bias our perception and thinking about landscape toward static foveal aspects of visual experience? Are our ideas about dynamic viewing and computer animation limited by the didactic frame-by-frame approach characteristic of cinematography and video? Can the introduction of equally robust tools and methods for dynamic and peripheral viewing balance any bias caused by current visualization technology? If McLuhan’s insights about media are correct, then we need to do more research on this question. This paper suggests that the field of landscape visualization needs to develop instruments for research that more fully capture the fundamental components of human vision before we can properly study the question or advance practice. It outlines some ways the next generation of visualization technology can be used to balance the disproportionate emphasis on foveal ways of visual thinking commonly used in the past for the study of visual landscapes. The paper explains this deficiency and proposes some area for research and development of visualization instruments more capable of redressing this imbalance. The paper outlines this issue and proposes that as electronic media and computational media become more developed and are applied to the realm of visual concerns, it will become more practical to include peripheral vision and dynamic viewing in deliberations about visual landscapes. This paper reflects on the potential of visualization automation techniques to overcome these shortcomings through illustrations of project work using innovative software tools developed to explore this question at the Centre for Landscape Research (CLR) at the University of Toronto.}
}
@article{ZHOU2025121668,
title = {Sparse loss-aware ternarization for neural networks},
journal = {Information Sciences},
volume = {693},
pages = {121668},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121668},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015822},
author = {Ruizhi Zhou and Lingfeng Niu and Dachuan Xu},
keywords = {Machine learning, Ternary neural networks, Loss-aware quantization, Sparse regularization, ADMM},
abstract = {Deep neural networks (DNNs) have shown great success in machine learning tasks and widely used in many fields. However, the substantial computational and storage requirements inherent to DNNs are usually high, which poses challenges for deploying deep learning models on resource-limited devices and hindering further applications. To address this issue, the lightweight nature of neural networks has garnered significant attention, and quantization has become one of the most popular approaches to compress DNNs. In this paper, we introduce a sparse loss-aware ternarization (SLT) model for training ternary neural networks, which encodes the floating-point parameters into {−1,0,1}. Specifically, we abstract the ternarization process as an optimization problem with discrete constraints, and then modify it by applying sparse regularization to identify insignificant weights. To deal with the challenges brought by the discreteness of the model, we decouple discrete constraints from the objective function and design a new algorithm based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments are conducted on public datasets with popular network architectures. Comparisons with several state-of-the-art baselines demonstrate that SLT always attains comparable accuracy while having better compression performance.}
}
@article{ARMSTRONG20161,
title = {A NIT-picking analysis: Abstractness dependence of subtests correlated to their Flynn effect magnitudes},
journal = {Intelligence},
volume = {57},
pages = {1-6},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300812},
author = {Elijah L. Armstrong and Jan {te Nijenhuis} and Michael A. {Woodley of Menie} and Heitor B.F. Fernandes and Olev Must and Aasa Must},
keywords = {Abstract thinking, Flynn effect, Intelligence, National Intelligence Test, Estonia, g loading},
abstract = {We examine the association between the strength of the Flynn effect in Estonia and highly convergent panel-ratings of the ‘abstractness’ of nine subtests on the National Intelligence Test, in order to test the theory that the Flynn effect results in part from an increase in the use of abstract reference frames in solving cognitive problems. The vectors of abstractness ratings and Flynn effect gains, controlled for guessing) exhibit a near-zero correlation (r=−.02); however, abstractness correlates positively with (and is therefore confounded by) g-loadings (r=.61). A General Linear Model is used to determine the degree to which the abstractness vector predicts the Flynn effect vector, independently of subtest g-loadings and the portion of the secular IQ gain due to guessing (the Brand effect). Consistent with the abstract reasoning model of the Flynn effect, abstractness positively predicts Flynn effect magnitudes, once controlled for confounds (sr=.44), which indicates an increasing tendency to utilize factors external to the items in order to abstract their solutions.}
}
@article{LI20234116,
title = {A call for caution in the era of AI-accelerated materials science},
journal = {Matter},
volume = {6},
number = {12},
pages = {4116-4117},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523005283},
author = {Kangming Li and Edward Kim and Yao Fehlis and Daniel Persaud and Brian DeCost and Michael Greenwood and Jason Hattrick-Simpers},
abstract = {It is safe to state that the field of matter has successfully entered the fourth paradigm, where machine learning and artificial intelligence (AI) are universally seen as useful, if not truly intelligent. AI’s utilization is near-ubiquitous from the prediction of novel materials to reducing computational overhead for material simulations; its value has been demonstrated time and again by both theorists and experimentalists. There is, however, a worrying trend toward large datasets and overparameterized models being all we need to accelerate science through accurate and robust machine learning systems.}
}
@article{TALANOV201641,
title = {Emotional simulations and depression diagnostics},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {41-50},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300676},
author = {Max Talanov and Jordi Vallverdú and Bin Hu and Philip Moore and Alexander Toschev and Diana Shatunova and Anzhela Maganova and Denis Sedlenko and Alexey Leukhin},
keywords = {Dopamine, Serotonin, Fear, Artificial intelligence, Simulation, Rat brain, Affective computing, Emotion modelling, Neuromodulation},
abstract = {In this work we propose the following hypothesis: the neuromodulatory mechanisms that control the emotional states of mammals can be translated and re-implemented in a computer by controlling the computational performance of a hosted computational system. In our specific implementation, we represent the simulation of the ‘fear-like’ state based on the three dimensional neuromodulatory model of affects, in this paper ‘affects’ refer to the basic emotional inborn states, inherited from works of Hugo Lövheim. Whilst dopamine controls attention, serotonin is the key for inhibition, and fear is a elicitator for inhibitory and protective processes. This inhibition can promote [in a cognitive system] to blocking behaviour which can be labelled as ’depression’. Therefore, our interest is how to reimplement biomimetically both action-regulators without the computational system to resulting in a ‘failed’ scenario. We have simulated 1000ms of the dopamine system using NEST Neural Simulation Tool with the rat brain as the model. The results of the simulation experiments are reported with an evaluation to demonstrate the correctness of our hypothesis.}
}
@incollection{VOINOV201733,
title = {Participatory Modeling for Sustainability},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies},
publisher = {Elsevier},
address = {Oxford},
pages = {33-39},
year = {2017},
isbn = {978-0-12-804792-7},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10532-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105329},
author = {Alexey Voinov},
keywords = {Biases, Modeling process, Social media, Stakeholders, Wicked problem},
abstract = {Sustainability is a wicked problem, which is hard to define in a unique way. It cannot be solved and should be treated in a participatory approach involving as many stakeholders in the process as possible. Participatory modeling is an efficient method for dealing with wicked problems. It involves stakeholders in an open-ended process of shared learning and can be essential for developing sustainable technologies. While there may be various levels of participation, the process evolves around a model of the system at stake. The model is built in interaction with the stakeholders; it provides formalism to synchronize stakeholder thinking and knowledge about the system and to move toward consensus about the possible decision making.}
}
@incollection{AKAL202471,
title = {Chapter Four - AI methods in microbial metabolite determination},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {71-85},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000023},
author = {H. Ceren Akal and Rumeysa Nur Kara-Aktaş and Sebnem Ozturkoglu-Budak},
keywords = {Metabolite, Microorganism-derived, Computational, Artificial intelligence},
abstract = {The multitude of microorganism species and the amount of data requiring examination is increasing day by day, which has made it very difficult to make informative determinations and analysis to be conducted by human labour. Artificial intelligence (AI) applications are crucial in mitigating these difficulties. AI is a multidisciplinary field that tries to imitate human-like abilities through learning, analysing, problem-solving and interpretation via digital systems. It can take part in many fields where human labour is required. It is widely used in various scientific disciplines and industries, including biotechnology, microbiology, medicine, etc. Machine learning, a subbranch of AI, is one of the most frequently used auxiliary methods. Critical topics are examined rapidly and meaningfully via machine-learning such as drug production, microbial detection, antimicrobial resistance, vaccine predictions, and disease diagnoses. The aim of this chapter is to highlight the relevance of computational methods for the determination of microbial metabolites which are mainly described in literatures. These computational methods are related with the advanced AI tools of data/genome mining, multivariate data analysis, molecular networking, mathematical modelling, and optimization. These novel methods create new perspectives to the isolation and/or determination of microbial metabolites which are unwanted or essential to human health.}
}
@article{SHIVERSMCNAIR201836,
title = {User-Centered Design In and Beyond the Classroom: Toward an Accountable Practice},
journal = {Computers and Composition},
volume = {49},
pages = {36-47},
year = {2018},
note = {User-Centered Design and Usability in the Composition Classroom},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S8755461518300379},
author = {Ann Shivers-McNair and Joy Phillips and Alyse Campbell and Hanh H. Mai and Alice Yan and John Forrest Macy and James Wenlock and Savannah Fry and Yishan Guan},
keywords = {user-centered design, user experience, usability testing, design thinking},
abstract = {The authors, an instructor and students, describe our practice of user-centered design on three levels: in the design and structure of an advanced undergraduate course in which we all participated, in student projects designed during the course, and in our reflections on the course presented here. We argue that principles of user-centered design can and should be more than course concepts and assignments; they can be core practices of the course that hold both students and teachers accountable for the impacts of their rhetorical choices. We offer a model for other teacher-scholars looking to involve students in the design of their courses and in writing together about their work.}
}
@article{MOEBEHRENS2013e201304003,
title = {THE BIOLOGICAL MICROPROCESSOR, OR HOW TO BUILD A COMPUTER WITH BIOLOGICAL PARTS},
journal = {Computational and Structural Biotechnology Journal},
volume = {7},
number = {8},
pages = {e201304003},
year = {2013},
issn = {2001-0370},
doi = {https://doi.org/10.5936/csbj.201304003},
url = {https://www.sciencedirect.com/science/article/pii/S200103701460026X},
author = {Gerd HG Moe-Behrens},
abstract = {Systemics, a revolutionary paradigm shift in scientific thinking, with applications in systems biology, and synthetic biology, have led to the idea of using silicon computers and their engineering principles as a blueprint for the engineering of a similar machine made from biological parts. Here we describe these building blocks and how they can be assembled to a general purpose computer system, a biological microprocessor. Such a system consists of biological parts building an input / output device, an arithmetic logic unit, a control unit, memory, and wires (busses) to interconnect these components. A biocomputer can be used to monitor and control a biological system.}
}
@article{MEINTZ1994273,
title = {Future directions in computational nursing sciences},
journal = {Mathematical and Computer Modelling},
volume = {19},
number = {6},
pages = {273-288},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90199-6},
url = {https://www.sciencedirect.com/science/article/pii/0895717794901996},
author = {S.L. Meintz and E.A. Yfantis and W.P. Graebel},
keywords = {Computational nursing, Health care data sets, Interdisciplinary, Nurmetrics, Nursing informatics, Nursing science, Supercomputers},
abstract = {The advent of the supercomputer and its capabilities for dealing with terabyte-sized data bases has provided a unique opportunity for nursing sciences to enhance and add to its theories. An interdisciplinary team has formed at the University of Nevada, Las Vegas (UNLV), to provide new tools and methodologies for analyzing large-scale data bases. Their first project is a study of infant mortality. The strategy and goals for this project are presented, along with an assessment of the present state of health care data base analysis.}
}
@article{PADGETT1994185,
title = {Computational intelligence standards: motivation, current activities and progress},
journal = {Computer Standards & Interfaces},
volume = {16},
number = {3},
pages = {185-203},
year = {1994},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(94)90011-6},
url = {https://www.sciencedirect.com/science/article/pii/0920548994900116},
author = {Mary Lou Padgett and Walter J Karplus and Steve Deiss and Robert Shelton},
keywords = {Terminology, Artificial neural networks, Specification, Virtual reality},
abstract = {Computational Intelligence is an emerging technology of keen interest to the developers of computer standards and interfaces. Coherent communications among the diverse set of users of computational AI is necessary for the protection of all parties and can help further the serious development of artificial neural networks, fuzzy systems, evolutionary programming and virtual reality. Current activities of the IEEE Neural Networks Council Standards Committee encompass all these areas, emphasizing the development of glossaries and symbologies, performance measures and interface standards for these interrelated fields. Progress toward these goals is described in this paper.}
}
@article{CORFIELD2011571,
title = {Understanding the infinite II: Coalgebra},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {4},
pages = {571-579},
year = {2011},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S003936811100077X},
author = {David Corfield},
keywords = {Philosophy, Mathematics, Category theory, Coalgebra, Infinite},
abstract = {In this paper we give an account of the rise and development of coalgebraic thinking in mathematics and computer science as an illustration of the way mathematical frameworks may be transformed. Originating in a foundational dispute as to the correct way to characterise sets, logicians and computer scientists came to see maximizing and minimizing extremal axiomatisations as a dual pair, each necessary to represent entities of interest. In particular, many important infinitely large entities can be characterised in terms of such axiomatisations. We consider reasons for the delay in arriving at the coalgebraic framework, despite many unrecognised manifestations occurring years earlier, and discuss an apparent asymmetry in the relationship between algebra and coalgebra.}
}
@article{MILDNER2019763,
title = {Spontaneous Thought as an Unconstrained Memory Process},
journal = {Trends in Neurosciences},
volume = {42},
number = {11},
pages = {763-777},
year = {2019},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166223619301626},
author = {Judith N. Mildner and Diana I. Tamir},
keywords = {spontaneous thought, memory, computational model, mind wandering, default network},
abstract = {The stream of thought can flow freely, without much guidance from attention or cognitive control. What determines what we think about from one moment to the next? Spontaneous thought shares many commonalities with memory processes. We use insights from computational models of memory to explain how the stream of thought flows through the landscape of memory. In this framework of spontaneous thought, semantic memory scaffolds episodic memory to form the content of thought, and drifting context modulated by one's current state – both internal and external – constrains the area of memory to explore. This conceptualization of spontaneous thought can help to answer outstanding questions such as: what is the function of spontaneous thought, and how does the mind select what to think about?}
}
@article{BAICANG2025129857,
title = {Multi-modal Information Fusion for Multi-task End-to-end Behavior Prediction in Autonomous Driving},
journal = {Neurocomputing},
pages = {129857},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129857},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005296},
author = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
keywords = {Autonomous driving, Multi-modal fusion, Vehicle behavior prediction, End-to-End, Attention mechanism},
abstract = {ABSTRACT
Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012rad, RMSE 0.109rad, and MAE 0.074rad, and speed prediction errors of MSE 0.321km/h, RMSE 0.567km/h, and MAE 0.373km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.}
}
@article{KUNZE2024249,
title = {Bioinspired approaches for resource-efficient material flow in production – an innovative actuator concept for peristaltic-based transport},
journal = {Procedia CIRP},
volume = {125},
pages = {249-254},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003974},
author = {Henriette Kunze and Marcel Lorenz},
keywords = {tensegrity, biotensegrity, assembly technologies},
abstract = {In automated material flow, in a wide variety of areas, the primary goal is usually to handle a wide spectrum of components as time- and cost-efficiently as possible. In view of the current and future challenges in industrial production, it is becoming apparent that ecological requirements are becoming increasingly important in automation solutions. For example, in form of resource efficiency, transformability and material efficiency. In this context, especially materials handling technology is subject of various optimization approaches, as no value is added to the part handled. The question: "How does material flow occur in nature?" thus offers biologically inspired approaches to thinking about transport in the industrial sector. This paper first presents a selection of concepts or existing mechanisms that are adaptable in materials- handling technology and have been developed based on a biological model. In the second part of this paper, a new concept is presented that is modeled on peristalsis as a transport mechanism. The approach presented here uses tensegrity-structures for assembly, which are characterized by their high material efficiency and flexibility. The transport movement is achieved by peristaltic typical contraction or relaxation of the respective structure parts.}
}
@article{YUZGEC2025113169,
title = {Accelerated opposition learning based chaotic single candidate optimization algorithm: A new alternative to population-based heuristics},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113169},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113169},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002163},
author = {Ugur Yuzgec},
keywords = {Opposition learning, Chaotic, Single candidate, Engineering design, Benchmark},
abstract = {This study considers the Single Candidate Optimizer (SCO) as an alternative to population-based heuristics, that is faster than them. Although the SCO algorithm is a fast single-candidate-based heuristic, it has certain limitations. To overcome these limitations and enhance the search performance of SCO, several solutions were proposed in this study. First, owing to the single-candidate nature of the SCO, the initial solution position can play a critical role. To compensate for this, an accelerated opposition-learning mechanism was integrated into the SCO. In addition, instead of the equation that is active when the number of unsuccessful improvement attempts is reached in the SCO structure, a mutation operator including chaotic functions (Levy, Gauss, and Cauchy) has been incorporated into the algorithm. Again, equations based on new approaches were added to the SCO algorithm to update the position of the candidate solution during the exploration and exploitation phases. Finally, the standard boundary value control mechanism is replaced with a more effective one. The algorithm developed in this study is named Accelerated Opposition Learning based Chaotic Single Candidate Optimizer (AccOppCSCO), inspired by the accelerated opposition learning mechanism and the mutation operator involving chaotic behaviors. The search capability of the proposed AccOppCSCO algorithm was first analyzed using four different methods: convergence, search history, trajectory, and computational complexity. The effectiveness of the mechanisms used in the AccOppCSCO algorithm for four different two-dimensional benchmark problems from the IEEE Congress on Evolutionary Computation 2014 (CEC2014) package was demonstrated. Subsequently, the performance of the proposed AccOppCSCO algorithm was evaluated on the CEC2014 and IEEE Congress on Evolutionary Computation 2020 (CEC2020) benchmark problems with different dimensions. The results show that the AccOppCSCO algorithm works effectively in the CEC2014 and CEC2020 test sets and offers better optimization results than SCO. The AccOppCSCO algorithm ranked first in the overall evaluation of the 30-dimensional CEC2014 comparison results with State of the Art (SOTA) heuristics from the literature. Finally, for ten different engineering design problems, the AccOppCSCO algorithm was analyzed and compared with the original SCO and other SOTA heuristics. The results show that AccOppCSCO is effective for engineering design problems. This emphasizes that the algorithm can work effectively on a wide range of problems and can be used in various applications. The source code of the AccOppCSCO algorithm for the CEC2014 benchmark suite is publicly available at https://github.com/uguryuzgec/AccOppCSCO.}
}
@article{BESHKOV2024109370,
title = {Topological structure of population activity in mouse visual cortex encodes densely sampled stimulus rotations},
journal = {iScience},
volume = {27},
number = {4},
pages = {109370},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109370},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005911},
author = {Kosio Beshkov and Marianne Fyhn and Torkel Hafting and Gaute T. Einevoll},
keywords = {Neuroscience, Sensory neuroscience, Cognitive neuroscience},
abstract = {Summary
The primary visual cortex is one of the most well understood regions supporting the processing involved in sensory computation. Following the popularization of high-density neural recordings, it has been observed that the activity of large neural populations is often constrained to low dimensional manifolds. In this work, we quantify the structure of such neural manifolds in the visual cortex. We do this by analyzing publicly available two-photon optical recordings of mouse primary visual cortex in response to visual stimuli with a densely sampled rotation angle. Using a geodesic metric along with persistent homology, we discover that population activity in response to such stimuli generates a circular manifold, encoding the angle of rotation. Furthermore, we observe that this circular manifold is expressed differently in subpopulations of neurons with differing orientation and direction selectivity. Finally, we discuss some of the obstacles to reliably retrieving the truthful topology generated by a neural population.}
}
@article{SPENCE2022100433,
title = {Gastrophysics: Getting creative with pairing flavours},
journal = {International Journal of Gastronomy and Food Science},
volume = {27},
pages = {100433},
year = {2022},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2021.100433},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X21001323},
author = {Charles Spence},
keywords = {Food pairing, Flavour pairing hypothesis, Sonic seasoning, Computational gastronomy, Data engineering, Gastrophysics},
abstract = {Traditionally, in the West, the decision about which flavours to pair in a tasting experience has been as much the personal choice of the chef or, more likely, the sommelier, as anything else. However, the last couple of decades have seen a rapid growth of research interest in the pairing of flavours. Nowadays, one can find examples of people pairing everything from beer with food, tea with cheese and chocolate, etc. As interest in the marketing potential of flavour pairing has risen, along with the growing public fascination in the topic, scientists have become increasingly interested in trying to understand the principles (both cognitive/intellectual and perceptual) underlying the successful pairing of flavours. In this narrative review, the relative strengths and weaknesses of the chemical, computational (gastronomy), and perceptual approaches to pairing flavours are highlighted. Thereafter, I show how the various principles of pairing (both perceptual and cognitive/intellectual) can be extended beyond the domain of pairing flavour with flavour to consider the rapidly growing are of sonic seasoning. The latter term refers to those situations in which specific pieces of music or soundscapes are matched, or paired, with particular tastes/flavours based on the crossmodal correspondences. The review ends by considering the future development of pairings flavours, and assessing novel means of establishing connections between flavours and other sensations.}
}
@article{BOSCH2017,
title = {Graduate Biomedical Science Education Needs a New Philosophy},
journal = {mBio},
volume = {8},
number = {6},
year = {2017},
issn = {2150-7511},
doi = {https://doi.org/10.1128/mbio.01539-17},
url = {https://www.sciencedirect.com/science/article/pii/S2161212917003111},
author = {Gundula Bosch and Arturo Casadevall},
keywords = {Ph.D., education, graduate},
abstract = {ABSTRACT
There is a growing realization that graduate education in the biomedical sciences is successful at teaching students how to conduct research but falls short in preparing them for a diverse job market, communicating with the public, and remaining versatile scientists throughout their careers. Major problems with graduate level education today include overspecialization in a narrow area of science without a proper grounding in essential critical thinking skills. Shortcomings in education may also contribute to some of the problems of the biomedical sciences, such as poor reproducibility, shoddy literature, and the rise in retracted publications. The challenge is to modify graduate programs such that they continue to generate individuals capable of conducting deep research while at the same time producing more broadly trained scientists without lengthening the time to a degree. Here we describe our first experiences at Johns Hopkins and propose a manifesto for reforming graduate science education.}
}
@article{XU2025101288,
title = {Multi-criteria feature selection on maritime emission abatement alternatives},
journal = {Research in Transportation Business & Management},
volume = {59},
pages = {101288},
year = {2025},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2025.101288},
url = {https://www.sciencedirect.com/science/article/pii/S2210539525000033},
author = {Kaiqi Xu and Mario P. Brito and Patrick Beullens},
keywords = {Analytic hierarchy process, Multi-criteria decision making, Emission reduction, Technology selection, Sustainability port},
abstract = {To comply with MARPOL Annex VI, stakeholders face multi-criteria decision-making in technology selection. This study provides an Analytic Hierarchy Process (AHP)-based method to support stakeholders in selecting emission abatement technology aligned with their business demands, taking into account a range of sustainability criteria. The analysis reveals that there is no one-size-fits-all solution to technology selection. Low-sulfur fuel oil and LNG are preferable alternative fuels for large-size commercial (long-sea shipping) vessels due to their better capacity storage savings, while a dual-fuel engine offers flexibility in fuel changeover. Electrification offers zero-emission performance, lower noise levels, and peak energy solutions benefiting cruise ships and short-distance or harbor boats, but tugboats need greener diesel to meet performance criteria. From a policy perspective, our model provides insights into the effects of green transition processes in Norway and Singapore on stakeholders' decisions with respect to port infrastructure and land transport at the portside.}
}
@article{MIDGLEY2019181,
title = {Anticipatory practice and the making of surplus food},
journal = {Geoforum},
volume = {99},
pages = {181-189},
year = {2019},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518302720},
author = {Jane L. Midgley},
keywords = {Surplus food, Anticipation, Market devices, Redistribution, United Kingdom},
abstract = {This paper explores the practices that have evolved between a global food retailer and a leading charitable surplus food redistributor to enable the utilization of surplus food in community and charitable meal settings in the UK. I argue that to understand surplus food and its potential futures (consumed or wasted), closer engagement with anticipatory thinking is needed. Drawing on interview data with key stakeholders and observations of the food industry redistribution process the paper explores the anticipatory actions taken by different actors as they attempt to manage the possible futures of foods that become categorized as surplus. The paper shows how different market devices are used to manage market concerns about surplus food and work to assure its future consumption. The devices focus on managing the risks of the food becoming unsafe and the associated legal liabilities. The market concerns, as expressions of anticipatory thinking, inform a series of anticipatory practices throughout the redistribution process to enable all actors, and especially the Retailer, to trust in the process. The paper concludes by noting how reliant the redistribution process is on anticipatory practices, especially pre-emption and improvisation to make the process workable, but also how these work to contain the various concerns within market arrangements. The paper highlights the importance of anticipation as a theoretical basis for exploring surplus food and the concept of surplus more widely.}
}
@article{DELLANNA2022105064,
title = {Evolving Fuzzy logic Systems for creative personalized Socially Assistive Robots},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105064},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105064},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622002251},
author = {Davide Dell’Anna and Anahita Jamshidnejad},
keywords = {Evolving Fuzzy logic Systems, Personalized Socially Assistive Robots, Robot creativity},
abstract = {Socially Assistive Robots (SARs) are increasingly used in dementia and elderly care. In order to provide effective assistance, SARs need to be personalized to individual patients and account for stimulating their divergent thinking in creative ways. Rule-based fuzzy logic systems provide effective methods for automated decision-making of SARs. However, expanding and modifying the rules of fuzzy logic systems to account for the evolving needs, preferences, and medical conditions of patients can be tedious and costly. In this paper, we introduce EFS4SAR, a novel Evolving Fuzzy logic System for Socially Assistive Robots that supports autonomous evolution of the fuzzy rules that steer the behavior of the SAR. EFS4SAR combines traditional rule-based fuzzy logic systems with evolutionary algorithms, which model the process of evolution in nature and have shown to result in creative behaviors. We evaluate EFS4SAR via computer simulations on both synthetic and real-world data. The results show that the fuzzy rules evolved over time are not only personalized with respect to the personal preferences and therapeutic needs of the patients, but they also meet the following criteria for creativity of SARs: originality and effectiveness of the therapeutic tasks proposed to the patients. Compared to existing evolving fuzzy systems, EFS4SAR achieves similar effectiveness with higher degree of originality.}
}
@article{HOSSEINI2019186,
title = {A morphological approach for kinetic façade design process to improve visual and thermal comfort: Review},
journal = {Building and Environment},
volume = {153},
pages = {186-204},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.02.040},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319301416},
author = {Seyed Morteza Hosseini and Masi Mohammadi and Alexander Rosemann and Torsten Schröder and Jos Lichtenberg},
keywords = {Kinetic façade, Biomimicry, Morphological approach, Comfort condition, Parametric design thinking},
abstract = {Visual and thermal comfort for occupants significantly depend on exterior environmental climatic conditions, which are continuously changing. In particular, optimizing visual and thermal comfort simultaneously is a difficult topic due to mutual conflicts between them. This literature review article studies the façade, as a complex interface between inside of buildings and the outside that has a capability to function as a protective or regulatory element against severe fluctuations of external climate. Six interrelated subjects are studied including kinetic façade, biomimicry, building form as a microclimate modifier, energy efficiency, comfort condition, parametric design thinking. The literature review process answers following research questions: (1) what are the interdisciplinary subjects corresponding to kinetic façade design process for creating an innovative architectural process? (2) What is the most important factor in kinetic façade design with the aim to improve occupants’ visual and thermal comfort simultaneously based on multidisciplinary investigation? Many research has been carried out about kinetic façade concepts strategies, principles, and criteria. However, interdisciplinary studies for proposing kinetic façade form is relatively rare. Also, adaptive daylight façade with daily solar geometry variation has been highly required. Therefore, generative-parametric and quick form finding method for responding to different climates would be a solution for providing more adaptability to dynamic daylight. This study aims to propose a kinetic façade design process which have capability to improve occupant visual & thermal comfort simultaneously by controlling on-site renewable energy resources consist of solar radiation and wind. Façade as an only interface between inside and outside of building, far from the literal and historical perceptions, is recognized by intrinsic functional attributes including complexity, heterogeneity and multidisciplinary. Moreover, the interrelated subjects impact façade form individually and aggregately regard to functional scenario that is changed the perception of kinetic façade from elegant and fashionable state to a functional and practical element.}
}
@article{MONRO199293,
title = {Parallel computation of ECG fields},
journal = {Journal of Electrocardiology},
volume = {25},
pages = {93-100},
year = {1992},
note = {Research and Applications in Computerized Electrocardiology},
issn = {0022-0736},
doi = {https://doi.org/10.1016/0022-0736(92)90068-B},
url = {https://www.sciencedirect.com/science/article/pii/002207369290068B},
author = {D.M. Monro and D.M. Budgett},
keywords = {electrocardiogram, modeling, forward solution, magnetic resonance imaging, inverse solution, parallel computers},
abstract = {A parallel implementation of a finite difference model for computing the electric field of cardiac sources is presented. On a relatively inexpensive SIMD parallel computer, a full-forward solution is obtained in minutes, using accurate thoracic detail including anisotropy if required. Because the computation is based on a volume grid with constant size voxels, it readily accepts anatomical data from classified magnetic resonance imaging scans. By using a variation of the colored successive over-relaxation iteration, our finite difference model takes full advantage of the performance of massively parallel computers. Evaluations of the accuracy and performance of the model show the practicality of using specific anatomical models to recover the electrocardiographic field distributions for individual subjects. A relatively modest parallel machine is capable of assembling and computing a specific direct inverse solution from body surface potentials within an hour of measurement, assuming the magnetic resonance imaging classification has been previously completed.}
}
@article{DOOLITTLE2019889,
title = {Making Evolutionary Sense of Gaia},
journal = {Trends in Ecology & Evolution},
volume = {34},
number = {10},
pages = {889-894},
year = {2019},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534719301417},
author = {W. Ford Doolittle},
keywords = {Gaia hypothesis, evolution, differential persistence, clade selection},
abstract = {The Gaia hypothesis in a strong and frequently criticized form assumes that global homeostatic mechanisms have evolved by natural selection favoring the maintenance of conditions suitable for life. Traditional neoDarwinists hold this to be impossible in theory. But the hypothesis does make sense if one treats the clade that comprises the biological component of Gaia as an individual and allows differential persistence – as well as differential reproduction – to be an outcome of evolution by natural selection. Recent developments in theoretical and experimental evolutionary biology may justify both maneuvers.}
}
@article{EARL2019303,
title = {Elusive optima: A process tracing analysis of procedural rationality in mobile phone connection plan choices},
journal = {Journal of Economic Behavior & Organization},
volume = {161},
pages = {303-322},
year = {2019},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167268119300988},
author = {Peter E. Earl and Lana Friesen and Christopher Shadforth},
keywords = {Consumer capabilities, Choice overload, Procedural rationality, Process tracing},
abstract = {This paper reports an experiment in which subjects were rewarded on the basis of how close they came to finding the cheapest mobile phone plan to serve a particular usage remit by searching freely in the Internet. During the task, subjects were required to ‘think aloud’ and recordings were made of what they said and what they did on their computer screens. Analysis of the screen-capture movie recordings revealed major shortfalls in procedural rationality, including poor strategic thinking about how to deal with choice overload, poor conceptual understanding of mobile phone plans and pricing systems, as well as cognitive and calculation errors. Our novel method leads to a very different policy focus from that implied by viewing the problem in terms of excess information per se and irrationality as driven by innate heuristics and biases.}
}
@article{LI2022126546,
title = {Dynamic forecasting performance and liquidity evaluation of financial market by Econophysics and Bayesian methods},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {588},
pages = {126546},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126546},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121008190},
author = {Jiang-Cheng Li and Chen Tao and Hai-Feng Li},
keywords = {Econophysics, Agent-based model, Liquidity risk assessment, Machine learning thinking, Microcosmic evolution models},
abstract = {In a complex financial system, what is the forecasting performance of macro and micro evolution models of Econophysics on asset prices? For this problem, from the perspective of machine learning, we study the dynamic forecasting and liquidity assessment of financial markets, based on econophysics and Bayesian methods. We establish eight dynamic prediction methods, based on our proposed likelihood estimation and Bayesian estimation methods of macro and micro evolution models of econophysics. Combined machine learning thinking and real data, we empirically study and simulate the out-of-sample dynamic forecasting analysis of eight proposed methods and compare with the benchmark GARCH model. A variety of loss functions, superior predictive ability test (SPA), Akaike and Bayesian information criterion (AIC and BIC) methods are introduced to further evaluate the forecasting performance of our proposed methods. The research of out of sample prediction shows that (1) the method of the simplified stochastic model with Bayesian method for only sample return has the best forecasting performance; (2) the method of the stochastic model with Bayesian method for only return samples has the worst forecasting performance. For the liquidity assessment problem, there is a strong correlation between the trading probability evaluated by the proposed eight methods and the real turnover rate, and an increase of liquidity is correspond to the increase of asset risk. In other words, it suggests that all proposed methods can well evaluate market liquidity.}
}
@article{WHITE200337,
title = {Promoting productive mathematical classroom discourse with diverse students},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {1},
pages = {37-53},
year = {2003},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(03)00003-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312303000038},
author = {Dorothy Y. White},
keywords = {Classroom discourse, Questioning techniques, Equity/diversity, Elementary mathematics teaching},
abstract = {Productive mathematical classroom discourse allows students to concentrate on sense making and reasoning; it allows teachers to reflect on students’ understanding and to stimulate mathematical thinking. The focus of the paper is to describe, through classroom vignettes of two teachers, the importance of including all students in classroom discourse and its influence on students’ mathematical thinking. Each classroom vignette illustrates one of four themes that emerged from the classroom discourse: (a) valuing students’ ideas, (b) exploring students’ answers, (c) incorporating students’ background knowledge, and (d) encouraging student-to-student communication. Recommendations for further research on classroom discourse in diverse settings are offered.}
}
@article{HUANG201724,
title = {Energy and carbon performance evaluation for buildings and urban precincts: review and a new modelling concept},
journal = {Journal of Cleaner Production},
volume = {163},
pages = {24-35},
year = {2017},
note = {Achieving Low/no Fossil-carbon Economies based upon the Essential Transformations to Support them},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615018235},
author = {Bin Huang and Ke Xing and Stephen Pullen},
keywords = {Buildings, Integrated modelling, Life cycle energy, Systems thinking, Urban precincts},
abstract = {With the accelerating pace of urbanisation around the world, the planning, development and operation of buildings and precincts have become increasingly important with respect to energy use and the associated carbon footprint of the modern built environment. Over recent decades, much effort, both in research and in practice, has been devoted to building construction and urban planning for the improvement of energy efficiency and greenhouse gas emissions. However, the accuracy of modelling and evaluation of energy and carbon performance for buildings and urban precincts remains limited, affected by inadequate energy intensity data and highly integrated building systems, as well as the complex interactions between buildings and the urban eco-system. This paper presents a critical review of current measures and models for representing and assessing life cycle energy as well as associated emissions profiles at both the building and the precinct levels. It also identifies influential factors and explores interactions among buildings, surrounding environment and user behaviours at the urban precinct level by taking a systems perspective. Based on such a review, this study maps out some key challenges for integrating energy and carbon metrics, and finally proposes a precinct-level system boundary definition and an integrated model following systems thinking. The proposed model can facilitate a critical thinking approach about the evaluations of global energy and emissions, and support the quantification of energy consumption and associated emissions for building precinct systems.}
}
@article{MOSTERT202448,
title = {The Shortfalls of Mental Health Compartment Models: A Call to Improve Mental Health Investment Cases in Developing Countries},
journal = {Value in Health Regional Issues},
volume = {41},
pages = {48-53},
year = {2024},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2212109923001449},
author = {Cyprian M. Mostert and Andrew Aballa and Linda Khakali and Willie Njoroge and Jasmit Shah and Samim Hasham and Zul Merali and Lukoye Atwoli},
keywords = {developing countries, investment cases, mental health compartment model},
abstract = {Objectives
There are irregularities in investment cases generated by the Mental Health Compartment Model. We discuss these irregularities and highlight the costing techniques that may be introduced to improve mental health investment cases.
Methods
This analysis uses data from the World Bank, the World Health Organization Mental Health Compartment Model, the United Nations Development Program, the Kenya Ministry of Health, and Statistics from the Kenyan National Commission of Human Rights.
Results
We demonstrate that the Mental Health Compartment Model produces irrelevant outcomes that are not helpful for clinical settings. The model inflated the productivity gains generated from mental health investment. In some cases, the model underestimated the economic costs of mental health. Such limitation renders the investment cases poor in providing valuable intervention points from the perspectives of both the users and the providers.
Conclusions
There is a need for further calibration and validation of the investment case outcomes. The current estimated results cannot be used to guide service provision, research, and mental health programming comprehensively.}
}
@incollection{HLAVAEEK20041,
title = {Chapter I - Reality, Mathematics, and Computation},
editor = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška},
series = {North-Holland Series in Applied Mathematics and Mechanics},
publisher = {North-Holland},
volume = {46},
pages = {1-49},
year = {2004},
booktitle = {Uncertain Input Data Problems and the Worst Scenario Method},
issn = {0167-5931},
doi = {https://doi.org/10.1016/S0167-5931(04)80005-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167593104800056},
author = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@article{CHAUHAN2023107757,
title = {Personalized optimal room temperature and illuminance for maximizing occupant's mental task performance using physiological data},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107757},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107757},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301937X},
author = {Hardik Chauhan and Youjin Jang and Surakshya Pradhan and Hyosoo Moon},
keywords = {Indoor environment quality, Physiological response, Occupant performance, Machine learning, Particle swarm optimization},
abstract = {Indoor room temperature and illuminance level are critical factors of indoor environment quality (IEQ), affecting human mental task performance. These effects are reflected in their physiological responses such as heart rate, electrodermal activity, and skin temperature. Occupants' individual preferences, sensitivity, and physiological responses to different combinations of room temperature and illuminance level can differ among individuals. Despite previous studies investigating the individual and combined effects of different IEQ parameters, the limited research on the cross-modal relationship between room temperature and illuminance level and its impact on mental task performance highlights its significance. Moreover, to achieve personalized insights, it is essential to incorporate individual physiological responses, and this necessitates the development of an optimization model to comprehensively examine their impact. To address these issues, this study proposes a personalized model that optimizes room temperature and illuminance levels to enhance mental task performance using occupants' physiological data. Having the random forest algorithm, this study first predicted mental task performance, which includes four mental abilities such as attention, perception, working memory, and thinking ability using the occupant's physiological data. Then, the particle swarm optimization algorithm was employed to optimize room temperature and illuminance level to maximize the predicted mental task performance. The results of the proposed model align with observed values of room temperature and illuminance level during experiments, validating the adoption of a personalized approach. The findings contribute to future insights and guidelines for the design and management of indoor environments to maximize occupants' performance.}
}
@incollection{LEACH202221,
title = {Chapter 2 - AI and the limits of human creativity in urban planning and design},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {21-37},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000135},
author = {Neil Leach},
keywords = {AlphaGo, AI, Strategy, Urban planning, Creativity, Perception},
abstract = {What can architects learn from AlphaGo? This chapter explores the lessons to be learnt from the famous match where AlphaGo, a machine-learning system developed by DeepMind, beat leading Korean Go player, Lee Sedol. It explores the ramifications of this victory on a series of different levels, from the global impact of the match on research into AI to the impact on Xkool Technologies and Spacemaker AI, two architectural start-ups developing AI systems for architecture and urban planning. It makes a particular comparison between the operations of AlphaGo and the strategic thinking of urban planning, arguing that AI now puts the future of urban planners—and possibly also architects—at risk. It then goes on to appraise the famous Move 37 made by AlphaGo in Game 2 of this match. It argues that, despite appearances, this move was not actually creative. Finally, it explores how we might view human creativity in the light of comments made about AlphaGo. The chapter concludes by speculating whether the ultimate lesson of AlphaGo is that creativity is simply a question of “perceived creativity.”}
}
@incollection{STAUFFER2006i,
title = {Biology, Sociology, Geology by Computational Physicists},
editor = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins},
series = {Monograph Series on Nonlinear Science and Complexity},
publisher = {Elsevier},
volume = {1},
pages = {i-276},
year = {2006},
booktitle = {Biology, Sociology, Geology by Computational Physicists},
issn = {1574-6917},
doi = {https://doi.org/10.1016/S1574-6917(05)01001-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574691705010019},
author = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins}
}
@article{PARR2025105984,
title = {Inferring when to move},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {169},
pages = {105984},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105984},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004536},
author = {Thomas Parr and Ashwini Oswal and Sanjay G. Manohar},
keywords = {Computational neuroscience, Generative, Bayesian, Active inference, Movement, Dynamical systems, Parkinson’s disease},
abstract = {Most of our movement consists of sequences of discrete actions at regular intervals—including speech, walking, playing music, or even chewing. Despite this, few models of the motor system address how the brain determines the interval at which to trigger actions. This paper offers a theoretical analysis of the problem of timing movements. We consider a scenario in which we must align an alternating movement with a regular external (auditory) stimulus. We assume that our brains employ generative world models that include internal clocks of various speeds. These allow us to associate a temporally regular sensory input with an internal clock, and actions with parts of that clock cycle. We treat this as process of inferring which clock best explains sensory input. This offers a way in which temporally discrete choices might emerge from a continuous process. This is not straightforward, particularly if each of those choices unfolds during a time that has a (possibly unknown) duration. We develop a route for translation to neurology, in the context of Parkinson’s disease—a disorder that characteristically slows down movements. The effects are often elicited in clinic by alternating movements. We find that it is possible to reproduce behavioural and electrophysiological features associated with parkinsonism by disrupting specific parameters—that determine the priors for inferences made by the brain. We observe three core features of Parkinson’s disease: amplitude decrement, festination, and breakdown of repetitive movements. Our simulations provide a mechanistic interpretation of how pathology and therapeutics might influence behaviour and neural activity.}
}
@article{NARIMANI202441,
title = {Intelligent Control for Aerospace Engineers: A Novel Educational Framework},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {16},
pages = {41-46},
year = {2024},
note = {2nd IFAC Workshop on Aerospace Control Education - WACE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.459},
url = {https://www.sciencedirect.com/science/article/pii/S240589632401228X},
author = {Mohammad Narimani and Seyyed Ali Emami and Paolo Castaldi},
keywords = {Aerospace control education, Intelligent control systems, Neural networks, Reinforcement learning, Model-based control, Adaptive control, Expert systems},
abstract = {The integration of intelligent control techniques into aerospace engineering education remains a challenge. This paper presents a novel approach for teaching intelligent control specifically designed for aerospace engineers, bridging the gap between theoretical foundations and practical applications. The proposed framework encompasses a comprehensive curriculum covering model-based and model-free approaches, leveraging neural networks, reinforcement learning, and other computational intelligence techniques. It emphasizes hands-on experiences through simulation-based exercises, hardware-in-the-loop experiments, and design projects tailored to different aerospace vehicle categories, including multi-rotor UAVs, helicopters, fixed-wing aircraft, and Hypersonic Flight Vehicles. The framework also addresses assessment methods, industry collaborations, and case studies to enhance student learning outcomes.}
}
@incollection{NIE2019205,
title = {Two-Stage Land Use Optimization for A Food-Energy-Water Nexus System: A Case Study In Texas Edwards Region},
editor = {Salvador Garcia Muñoz and Carl D. Laird and Matthew J. Realff},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {47},
pages = {205-210},
year = {2019},
booktitle = {Proceedings of the 9th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818597-1.50033-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185971500333},
author = {Yaling Nie and Styliani Avraamidou and Xin Xiao and Efstratios N. Pistikopoulos and Jie Li},
keywords = {Land use optimization, Food-Energy-Water Nexus, multi-period planning},
abstract = {Efficient land use planning and scheduling in Food-Energy-Water Nexus (FEW-N) related systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on FEW-N is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, multi-scale and multi-period problems, and multiple stakeholders. To address these challenges, we developed a generic optimization-based land allocation approach, which provides i) a composite FEW-N metric to help solve the multi-objective optimization problem and carry out assessments, and ii) a two-stage decomposition strategy to solve the multi-scale and multi-period planning and scheduling problem. The developed strategy was applied in a case study within the Texas Edwards Region. Computational results indicate that the approach can provide a comprehensive FEW-N metric to select strategies for optimal land allocation and limit stresses in the FEW-N, and achieve trade-off solutions for the multi-scale and multi-period FEW land use systems.}
}
@incollection{FAVERO2023509,
title = {Chapter 25 - Raster objects},
editor = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
booktitle = {Data Science, Analytics and Machine Learning with R},
publisher = {Academic Press},
pages = {509-519},
year = {2023},
isbn = {978-0-12-824271-1},
doi = {https://doi.org/10.1016/B978-0-12-824271-1.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128242711000111},
author = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
keywords = {Spatial analysis, Maps, Raster objects, R},
abstract = {At the end of this chapter, you will be able to:•Understand what a raster object is;•Load and use raster objects;•Combine raster objects with shapefiles;•Manipulate and cut out raster objects;•Use the raster objects in such a way as to demand less computational time for the execution of tasks;•View raster objects in R language.}
}
@article{KHAN2021104263,
title = {A novel hybrid gravitational search particle swarm optimization algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104263},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104263},
url = {https://www.sciencedirect.com/science/article/pii/S095219762100110X},
author = {Talha Ali Khan and Sai Ho Ling},
keywords = {PSO, GSA, Hybrid, DNA computation},
abstract = {Particle Swarm Optimization (PSO) algorithm is a member of the swarm computational family and widely used for solving nonlinear optimization problems. But, it tends to suffer from premature stagnation, trapped in the local minimum and loses exploration capability as the iteration progresses. On the contrary, Gravitational Search Algorithm (GSA) is proficient for searching global optimum, however, its drawback is its slow searching speed in the final phase. To overcome these problems in this paper a novel Hybrid Gravitational Search Particle Swarm Optimization Algorithm (HGSPSO) is presented. The key concept behind the proposed method is to merge the local search ability of GSA with the capability for social thinking (gbest) of PSO. To examine the effectiveness of these methods in solving the abovementioned issues of slow convergence rate and trapping in local minima five standard and some modern CEC benchmark functions are used to ensure the efficacy of the presented method. Additionally, a DNA sequence problem is also solved to confirm the proficiency of the proposed method. Different parameters such as Hairpin, Continuity, H-measure, and Similarity are employed as objective functions. A hierarchal approach was used to solve this multi-objective problem where a single objective function is first obtained through a weighted sum method and the results were then empirically validated. The proposed algorithm has demonstrated an extraordinary performance per solution stability and convergence.}
}
@article{DAS2023100065,
title = {Informatics on a social view and need of ethical interventions for wellbeing via interference of artificial intelligence},
journal = {Telematics and Informatics Reports},
volume = {11},
pages = {100065},
year = {2023},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2772503023000257},
author = {Kabita Das and Manaswini Pattanaik and Smitimayee Basantia and Radhashyam Mishra and Debashreemayee Das and Kanhucharan Sahoo and Biswaranjan Paital},
keywords = {Artificial intelligence, Ethical enquiry, Ethics in technology, Human conduct, Moral value, Social cognition, Human intelligence},
abstract = {The main focus of this paper was to discuss and appraise the attribution of intelligence and value judgement on Artificial Intelligence (AI) and its regulated use in society. Humans are tool-making creatures and AI is used for civilization via tools. During the time of pre-civilization, tools were simple in the form of crude construction, using hand skills but at present, the achievements are the substitution of machinery to relieve/replace human intellect. AI is the scientific technique of bringing learning, adaptation, and self-organization of machines. It encompasses various concepts and methods, deployed by researchers in many diverse fields of computation and cognition. This is the computational mode of a brain, based on artificial neural networks. The usefulness of AI ethically, initiates a big question i.e. if the human mind is not self-sufficient for any work without harming the moral sentiment of others then, how can people believe in a computational model of the mind, is a machine, morally responsible for any good or bad action. We highlight issues on the use of AI in the replacement of the human mind asking what is the value of humans in this age of AI? Can AI reciprocate and respect human values better than human beings? Can AI replace human intelligence? In the case of ethical enquiry, it is rather a herculean task to consider a machine's action to be moral or immoral, after all, it is just a machinery action devoid of any moral quality.}
}
@article{URECH2022101731,
title = {A simulation-based design framework to iteratively analyze and shape urban landscapes using point cloud modeling},
journal = {Computers, Environment and Urban Systems},
volume = {91},
pages = {101731},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101731},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001381},
author = {Philipp R.W. Urech and Muhammad Omer Mughal and Carlos Bartesaghi-Koc},
keywords = {Point-cloud modeling, Computational fluid dynamics, Laser-scanned data, Urban landscape design, Design performance},
abstract = {The topic of this paper evolves on the discourse of digital modeling in landscape design. Current design methods stagger to address physical forms and dynamics present in the environment. This status quo limits possibilities to integrate scientific evidence when developing spatial and aesthetic configurations in urban landscapes. Remote sensing technology such as laser scanning measures physical forms to reproduce them as geo-specific digital 3D models, while dynamic simulation is widely used to predict how scenarios will perform under given conditions. However, there is still a need for a holistic design process that is capable of integrating both the measured physical forms and physical dynamics. This paper presents a novel framework using point cloud modeling to shape design scenarios that are iteratively evaluated for their performance. The proposed framework is demonstrated through a case study in Singapore. New spatial configurations are tested for the site through an iterative and comparative analysis of the design performance. The case study exposes (1) a site-specific design approach by iteratively modeling a laser-scanned point cloud model, (2) a workflow to convert the geometric data from the point cloud models into voxels and meshes, (3) an integration of computational fluid dynamics (CFD) simulation during design development as per-point attributes, and (4) a comparison of the configurations to identify best performing scenarios. This design framework can support city managers, planners, urban and landscape designers to better inform their decision-making process by relying on accurate scientific feedback. By guiding the design process with the consideration of the built environment as a complex adaptive system, it will be possible to improve how open spaces and ecosystem services perform in cities, and to design landscapes that can mitigate dynamic events such as urban heat islands.}
}
@article{KROGER2013189,
title = {An ERP study of passive creative conceptual expansion using a modified alternate uses task},
journal = {Brain Research},
volume = {1527},
pages = {189-198},
year = {2013},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0006899313009566},
author = {Sören Kröger and Barbara Rutter and Holger Hill and Sabine Windmann and Christiane Hermann and Anna Abraham},
keywords = {Creativity, ERP, N400, Conceptual expansion, Alternate uses task, Divergent thinking, Semantic cognition},
abstract = {A novel ERP paradigm was employed to investigate conceptual expansion, a central component of creative thinking. Participants were presented with word pairs, consisting of everyday objects and uses for these objects, which had to be judged based on the two defining criteria of creative products: unusualness and appropriateness. Three subject-determined trial types resulted from this judgement: high unusual and low appropriate (nonsensical uses), low unusual and high appropriate (common uses), and high unusual and high appropriate (creative uses). Word pairs of the creative uses type are held to passively induce conceptual expansion. The N400 component was not specifically modulated by conceptual expansion but was, instead, generally responsive as a function of unusualness or novelty of the stimuli (nonsense=creative>common). Explorative analyses in a later time window (500–900ms) revealed that ERP activity in this phase indexes appropriateness (nonsense>creative=common). In the discussion of these findings with reference to the literature on semantic cognition, both components are proposed as indexing processes relevant to conceptual expansion as they are selectively involved in the encoding and integration of a newly established semantic connection between two previously unrelated concepts.}
}
@article{WHITACRE2020100816,
title = {The roles of tools and models in a prospective elementary teachers’ developing understanding of multidigit multiplication},
journal = {The Journal of Mathematical Behavior},
volume = {60},
pages = {100816},
year = {2020},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100816},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320300808},
author = {Ian Whitacre and Chepina Rumsey},
keywords = {Prospective teachers, Mental computation, Multiplication, Tools, Models},
abstract = {It is important for prospective elementary teachers to understand multidigit multiplication deeply; however, the development of such understanding presents challenges. We document the development of a prospective elementary teacher’s reasoning about multidigit multiplication during a Number and Operations course. We present evidence of profound progress in Valerie’s understanding of multidigit multiplication, and we highlight the roles of particular tools and models in her developing reasoning. In this way, we contribute an illuminating case study that can inform the work of mathematics teacher educators. We discuss specific instructional implications that derive from this case.}
}
@article{HASELI2023184,
title = {HECON: Weight assessment of the product loyalty criteria considering the customer decision's halo effect using the convolutional neural networks},
journal = {Information Sciences},
volume = {623},
pages = {184-205},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015201},
author = {Gholamreza Haseli and Ramin Ranjbarzadeh and Mostafa Hajiaghaei-Keshteli and Saeid {Jafarzadeh Ghoushchi} and Aliakbar Hasani and Muhammet Deveci and Weiping Ding},
keywords = {Customer loyalty, Deep learning, Convolutional neural networks, Multi-criteria decision-making, Halo effect},
abstract = {The economic pressures and increasing competition in markets have led to the CEOs of companies being forced to make the right strategic decisions in the development of products for selling the right products to the right customers. To achieve this goal, companies need to know which criteria of their products lead to customer loyalty to that product. In the past, various methods have been introduced to obtain the importance (weight) of criteria that use the opinions of experts or customers. There is a halo effect in human decisions that leads to biases in evaluating the criteria by influencing human emotions. This study introduces a new method for weight assessment of the product loyalty criteria by considering the customer's decisions halo effect using the convolutional neural network (CNN) called the halo effect using the convolutional neural networks (HECON) method. In the HECON method to consider the halo effect of the customer decisions, a CNN model is proposed as the deep learning pipeline to obtain more accurate weights of the criteria. The HECON method to obtain the weight of the criteria and identify criteria that lead to product loyalty needs to collect the feedback of a large number of customers based on the net promoter score (NPS) scale. The innovation of the HECON method is to obtain the effect level of each product criterion on selection and loyalty to the product through the feedback of a large number of customers by considering the halo effect on the customers' thinking. To date, the analyzing methods have often not been able to identify the halo effect in evaluating the reasons for customer loyalty to the product. The halo effect indicates sometimes some of the product criteria secretly affect the customers' opinions that require deep neural networks to analyze them. By using the deep CNN model of the HECON method to evaluate product criteria for understanding customer behavior, companies will be able to identify customers' behavior and develop their products exactly following the customer's desires. To evaluate the performance and demonstrate the applicability of the HECON method, presented two case studies. It is presented that there are challenging differences between the results of the HECON method with the other methods because the HECON method considers the halo effect on the customer decisions and demonstrates better performance.}
}
@article{HUANG2012250,
title = {The effectiveness of using procedural scaffoldings in a paper-plus-smartphone collaborative learning context},
journal = {Computers & Education},
volume = {59},
number = {2},
pages = {250-259},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000310},
author = {Hui-Wen Huang and Chih-Wei Wu and Nian-Shing Chen},
keywords = {Cooperative/collaborative learning, Improving classroom teaching, Teaching/learning strategies},
abstract = {The purpose of this study was to evaluate the effectiveness of using procedural scaffoldings in fostering students’ group discourse levels and learning outcomes in a paper-plus-smartphone collaborative learning context. All participants used built-in camera smartphones to learn new knowledge by scanning Quick Response (QR) codes, a type of two-dimensional barcode, embedded in paper-based learning materials in this study. Sixty undergraduate and graduate students enrolled at a four-year university in southern Taiwan participated in this study. Participants were randomly assigned into two different groups, using procedural scaffoldings learning and non-procedural scaffoldings learning. The learning unit about the Long Tail, an important concept used in products sales, was the learning task that participants were expected to complete. During the experiment, pretest–posttest and the completed group worksheets were used to collect data. The researchers applied content analyses, chi-square test, t-test, and ANCOVA to answer research questions. The findings indicated that participants in the experimental group using procedural scaffoldings achieved better learning outcomes than their counterparts in the control group in terms of group discourse levels, group learning, and individual learning.}
}
@article{KAMPPINEN1998481,
title = {Evolution and culture: the Darwinian view on infosphere},
journal = {Futures},
volume = {30},
number = {5},
pages = {481-484},
year = {1998},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(98)00050-0},
url = {https://www.sciencedirect.com/science/article/pii/S0016328798000500},
author = {Matti Kamppinen},
abstract = {This essay looks at the idea that human culture is an evolving system, a complex entity that undergoes evolutionary processes. This idea can also be expressed as follows: the cultural infosphere has the same mode of operation as the organic biosphere. There are three parts to the essay: it begins with some highlights from the history of evolutionary thinking; second, it explains the mechanisms of cultural selection; and third, it discusses the vision of the future provided by evolutionary thinking. The kind of evolutionary thinking focused upon is one that takes Charles Darwin seriously. The depth, reach and relevance of Darwinian thinking has been aptly exposed by Daniel C. Dennett,[1]and this essay assesses its worth in futures research.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{ENDICOTT2003403,
title = {Moral reasoning, intercultural development, and multicultural experiences: relations and cognitive underpinnings},
journal = {International Journal of Intercultural Relations},
volume = {27},
number = {4},
pages = {403-419},
year = {2003},
note = {Special Training Issue},
issn = {0147-1767},
doi = {https://doi.org/10.1016/S0147-1767(03)00030-0},
url = {https://www.sciencedirect.com/science/article/pii/S0147176703000300},
author = {Leilani Endicott and Tonia Bock and Darcia Narvaez},
keywords = {Moral development, Intercultural development, Flexible thinking, Cognitive complexity, Multicultural experience, Schema theory},
abstract = {The relation between moral reasoning and intercultural sensitivity is discussed. We hypothesize that multicultural experiences are related to both types of development, describe the cognitive processes through which multicultural experiences theoretically facilitate development, and present empirical data supporting the association. Though the underlying developmental constructs were initially conceptualized as stage theories, we borrow from cognitive science and contemporary theories of human learning (Derry, 1996) to think of moral and intercultural development in terms of increasing sociocognitive flexibility. Intercultural and moral development share the common element of a critical shift from rigid to flexible thinking. In moral reasoning, this is characterized by the shift from conventional to post-conventional thinking. In intercultural development, a similar movement occurs between the ethnocentric and ethnorelative orientations of intercultural sensitivity. In order to test our hypothesis, college students (n=70) took measures of intercultural development (Intercultural Development Inventory), moral judgment (Defining Issues Test), and multicultural experience (Multicultural Experience Questionnaire). The results indicate that moral judgment and intercultural development are significantly related to one another. Both are related to multicultural experiences, particularly depth of the experiences, as opposed to breadth.}
}
@article{WANG1996579,
title = {The IDS model of intelligent design system},
journal = {Computers & Structures},
volume = {61},
number = {3},
pages = {579-586},
year = {1996},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(96)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0045794996000545},
author = {Xiaotong Wang},
abstract = {Existing models of intelligent design system nowadays are generally logic-based, which solve only simple and small-scale design problems. In the author's opinion, these models which concentrate only on far-fetched use of logical inference and abstract knowledge deviate from designer's thinking and decision process; the crux of the deviation is the lack of imitating thinking with mental imagery ability. Considering the nature of design problems and imitating rational thinking with alternate use of pattern association and symbolic operation, a new intelligent design system (IDS) model and its implementation techniques are presented. Imitation of thinking with mental imagery which is also called pattern association in the IDS model is considered by applying artificial neural network (ANN) techniques. The pattern association in the IDS model imitates the rule of human thinking, “comprehending by analogy”, to some extent. Because of the robustness of the pattern-type knowledge used in pattern association, IDS provides a practical way in producing a design scheme using incomplete and/or undeterminate input data, which is very difficult to achieve in general expert design systems. According to the IDS model, an intelligent structural layout design system of wing (ISDW) is developed. ISDW realizes mapping from key parameters of design requirements and the environment of the wing to the layout design of wing structure in not only graphic form, but also in readable data form. After getting a layout of wing structure, the user will modify it interactively by Auto-CAD, and then return to the ISDW environment to produce FEM meshes by an intelligent meshing interface in order to do the preliminary static and dynamic structural analysis. The design schemes created by the system proved to be proper and usable, and this concludes that IDS model is practicable and practical.}
}
@incollection{MARKOVA2015443,
title = {Representations, Social Psychology of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {443-449},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.24084-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868240841},
author = {Ivana Marková},
keywords = {Anchoring, Cognitive polyphasia, Common sense, Communication, Dialogicality, Ego–Alter–Object, Ethics, Figurative scheme, Imagination, Interactional epistemology, Intervention strategies, Language, Objectification, Social representations, Themata},
abstract = {The theory of social representations studies the formation and transformation of meanings and activities of complex social phenomena like health and illness, political problems or environmental issues in and through language and communication, history and culture. There are two mutually interdependent meanings of social representations. The first meaning concerns the theory of social representations as an interactional theory of knowledge. It refers to networks of concepts and figurative schemes that are generated in and through tradition, common sense, daily knowledge, and communication; these are shared by particular groups and communities. The main features of this theory are the Ego–Alter–Object, the field, the interdependence of asymmetries and symmetries, ethics, figurative scheme, and cognitive polyphasia. Second, social representations refer to concrete social phenomena and to forms of apprehending and creating social realities in and through communication, experience, social practices, and interventions. Human thinking is characterized by the capacity to make distinctions and understand phenomena as dyadic antinomies or themata. Thematization of dyadic antinomies is linked with anchoring and objectification, through which social representations are formed and transformed.}
}
@article{SHU2025111052,
title = {Optimal power flow in hybrid AC-DC systems considering N-k security constraints in the preventive-corrective control stage},
journal = {Electric Power Systems Research},
volume = {238},
pages = {111052},
year = {2025},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624009349},
author = {Hongchun Shu and Hongfang Zhao and Mengli Liao},
keywords = {Flexible DC transmission, AC-DC hybrid system, -, Safety constraints, Optimal power flow},
abstract = {The optimal power flow methods for AC-DC systems containing VSC-HVDC generally only consider the economy during normal operation, overlooking the distribution of line transmission power in fault conditions. As a result, lines that continue to operate after a fault may experience overloading or operate at full capacity. Thus, a method for optimal power flow calculation is proposed that incorporates N-k security constraints in the preventive-corrective control stage for secure and economic operation of hybrid AC-DC systems. This method ensures that the line transmission power in the system meets the limits in the normal, short-term fault, and long-term fault states. In addition to the optimal power flow in the normal state, the method incorporates the system's imbalance as an indicator to evaluate system resilience. It combines this indicator with the economic, network loss, and performance metrics of the system, forming a two-stage bi-level multi-objective optimization model. Furthermore, to address the curse of dimensionality in anticipating system fault sets, a method for generating the anticipated fault set using non-sequential Monte Carlo simulation is proposed, along with a fault scenario search approach based on robust thinking to identify the most severe faults. Finally, the traditional IEEE 30-bus system was improved, and simulation verification was conducted using examples of an AC/DC system with a three-terminal DC network and a wind-solar-storage hybrid AC/DC system with a three-terminal DC network. The simulation results indicate that the proposed optimal power flow method considering the preventive-corrective control stage with N-k security constraints can effectively enhance system resilience. Furthermore, it improves the economic efficiency while ensuring the secure operation of the system.}
}
@article{FUKAI2021145,
title = {Neural mechanisms for learning hierarchical structures of information},
journal = {Current Opinion in Neurobiology},
volume = {70},
pages = {145-153},
year = {2021},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2021.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001252},
author = {Tomoki Fukai and Toshitake Asabuki and Tatsuya Haga},
abstract = {Spatial and temporal information from the environment is often hierarchically organized, so is our knowledge formed about the environment. Identifying the meaningful segments embedded in hierarchically structured information is crucial for cognitive functions, including visual, auditory, motor, memory, and language processing. Segmentation enables the grasping of the links between isolated entities, offering the basis for reasoning and thinking. Importantly, the brain learns such segmentation without external instructions. Here, we review the underlying computational mechanisms implemented at the single-cell and network levels. The network-level mechanism has an interesting similarity to machine-learning methods for graph segmentation. The brain possibly implements methods for the analysis of the hierarchical structures of the environment at multiple levels of its processing hierarchy.}
}
@article{WILKINSON2013394,
title = {The past and the future of business marketing theory},
journal = {Industrial Marketing Management},
volume = {42},
number = {3},
pages = {394-404},
year = {2013},
note = {Theoretical Perspectives in Industrial Marketing Management},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850113000266},
author = {Ian F. Wilkinson and Louise C. Young},
keywords = {Complex adaptive systems, Business relations and networks, Dynamics and evolution, Agent based models, Mechanisms},
abstract = {A complex systems approach to understanding and modelling business marketing systems is described. The focus is on the dynamics and evolution of such systems and the processes and mechanisms driving this, rather than the more usual comparative static, variables based statistical models. Order emerges in a self-organising, bottom up way from the local or micro actions and interactions of those involved. We describe the development of our thinking regarding this approach and its main features, including the development of agent based simulation models and the identification and modelling of underlying mechanisms and processes. We conclude by discussing the implications of this approach for business marketing theory and research.}
}
@article{THANKACHAN2024101283,
title = {A mathematical formulation of learner cognition for personalised learning experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101283},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000779},
author = {Jeena A. Thankachan and Bama Srinivasan},
keywords = {Virtual Learning Environment (VLE), Cognitive Evaluation Metrics (CEM), Multimode evaluation, Cognitive abilities, Learning tasks, Reinforcement learning},
abstract = {The paper focuses on the assessment of cognitive skills within Virtual Learning Environments (VLEs). In response to the global shift to remote learning amid the COVID-19 pandemic, VLEs, which include learning management systems (LMS) and online collaboration platforms, gained prominence. The proposed work leverages an established Cattell–Horn–Carroll (CHC) theory to propose eight metrics, which collectively form a part of Cognitive Evaluation Metrics (CEM). The proposed metrics introduce a novel computational approach for multimode evaluation of learners’ cognitive abilities for each learning task within a learning environment. The paper details the formalism for the evaluation of the metrics and makes a contribution towards the potential of the proposed methodology to evaluate cognitive abilities. Additionally, the work implements CEM integration into the learner module of a Game-Based Learning (GBL) environment. Analysis of simulations in the GBL environment, along with statistical analysis, provides insights into the normal distribution of cognitive metrics. This reveals diverse ranges in various abilities such as long or short term memory, working memory, reasoning, attention, and processing speed. The paper also explores the impact of virtual assistants, which highlights their limited relevance to enhance cognitive abilities but serve as valuable on-demand support resources.}
}
@article{SUN2024103771,
title = {Supply chain planning with free trade zone and uncertain demand},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {192},
pages = {103771},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524003624},
author = {Haoying Sun and Manoj Vanajakumari and Chelliah Sriskandarajah and Subodha Kumar},
keywords = {Supply chain management, Robust optimization, Dynamic lot sizing},
abstract = {Our research is inspired by the subcontracting problem at a major oil field services company in North America. The company’s supply chain consists of suppliers bringing raw materials to a Free Trade Zone (FTZ). The FTZ receives raw materials in full containers from various suppliers, and then the company ships them to various plants (e.g. oil excavation sites) frequently via subcontractors. This allows the company to focus on managing only the inbound transportation and inventory at the FTZ. The demand for each raw material is stochastic. We derive an algorithm running at polynomial time for the stochastic programming formulation and perform μ− regret Robust Optimization to handle the demand uncertainty. We also use a Sample Average Approximation method to alleviate the high computational requirement of the robust optimization model. The modeling approach demonstrated by this paper not only meets the needs of this specific company and industry but also can be applied to other industries with similar supply chain structures.}
}
@article{YIM2014144,
title = {A development of a quantitative situation awareness measurement tool: Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE)},
journal = {Annals of Nuclear Energy},
volume = {65},
pages = {144-157},
year = {2014},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2013.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0306454913005598},
author = {Ho Bin Yim and Seung Min Lee and Poong Hyun Seong},
keywords = {Quantitative measure, Situation awareness, Graphical expression, NPP MCR operators},
abstract = {Operator performance measures are used for multiple purposes, such as control room design, human system interface (HSI) evaluation, training, and so on. Performance measures are often focused on results; however, especially for a training purpose – at least in a nuclear industry, more detailed descriptions about processes are required. Situation awareness (SA) measurements have directly/indirectly played as a complimentary measure and provided descriptive insights on how to improve performance of operators for the next training. Unfortunately, most of the well-developed SA measurement techniques, such as Situation Awareness Global Assessment Technique (SAGAT) need an expert opinion which sometimes troubles easy spread of measurement’s application or usage. A quantitative SA measurement tool named Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE) is introduced to resolve some of these concerns. CoRSAGE is based on production rules to represent a human operator’s cognitive process of problem solving, and Bayesian inference to quantify it. Petri Net concept is also used for graphical expressions of SA flow. Three components – inference transition, volatile/non-volatile memory tokens – were newly developed to achieve required functions. Training data of a Loss of Coolant Accident (LOCA) scenario for an emergency condition and an earthquake scenario for an abnormal condition by real plant operators were used to validate the tool. The validation result showed that CoRSAGE performed a reasonable match to other performance results.}
}
@article{BRENT19961,
title = {Advances in the computational study of language acquisition},
journal = {Cognition},
volume = {61},
number = {1},
pages = {1-38},
year = {1996},
note = {Compositional Language Acquisition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(96)00779-2},
url = {https://www.sciencedirect.com/science/article/pii/S0010027796007792},
author = {Michael R. Brent},
abstract = {This paper provides a tutorial introduction to computational studies of how children learn their native languages. Its aim is to make recent advances accessible to the broader research community, and to place them in the context of current theoretical issues. The first section locates computational studies and behavioral studies within a common theoretical framework. The next two sections review two papers that appear in this volume: one on learning the meanings of words and one on learning the sounds of words. The following section highlights an idea which emerges independently in these two papers and which I have dubbed autonomous bootstrapping. Classical bootstrapping hypotheses propose that children begin to get a toe-hold in a particular linguistic domain, such as syntax, by exploiting information from another domain, such as semantics. Autonomous bootstrapping complements the cross-domain acquisition strategies of classical bootstrapping with strategies that apply within a single domain. Autonomous bootstrapping strategies work by representing partial and/or uncertain linguistic knowledge and using it to analyze the input. The next two sections review two more more contributions to this special issue: one on learning word meanings via selectional preferences and one on algorithms for setting grammatical parameters. The final section suggests directions for future research.}
}
@article{BEATY201922,
title = {Network neuroscience of creative cognition: mapping cognitive mechanisms and individual differences in the creative brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {27},
pages = {22-30},
year = {2019},
note = {Creativity},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301219},
author = {Roger E Beaty and Paul Seli and Daniel L Schacter},
abstract = {Network neuroscience research is providing increasing specificity on the contribution of large-scale brain networks to creative cognition. Here, we summarize recent experimental work examining cognitive mechanisms of network interactions and correlational studies assessing network dynamics associated with individual creative abilities. Our review identifies three cognitive processes related to network interactions during creative performance: goal-directed memory retrieval, prepotent-response inhibition, and internally-focused attention. Correlational work using prediction modeling indicates that functional connectivity between networks — particularly the executive control and default networks — can reliably predict an individual’s creative thinking ability. We discuss potential directions for future network neuroscience, including assessing creative performance in specific domains and using brain stimulation to test causal hypotheses regarding network interactions and cognitive mechanisms of creative thought.}
}
@article{FOWLER20245,
title = {Will variants of uncertain significance still exist in 2030?},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {1},
pages = {5-10},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2023.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929723004007},
author = {Douglas M. Fowler and Heidi L. Rehm},
abstract = {Summary
In 2020, the National Human Genome Research Institute (NHGRI) made ten “bold predictions,” including that “the clinical relevance of all encountered genomic variants will be readily predictable, rendering the diagnostic designation ‘variant of uncertain significance (VUS)’ obsolete.” We discuss the prospects for this prediction, arguing that many, if not most, VUS in coding regions will be resolved by 2030. We outline a confluence of recent changes making this possible, especially advances in the standards for variant classification that better leverage diverse types of evidence, improvements in computational variant effect predictor performance, scalable multiplexed assays of variant effect capable of saturating the genome, and data-sharing efforts that will maximize the information gained from each new individual sequenced and variant interpreted. We suggest that clinicians and researchers can realize a future where VUSs have largely been eliminated, in line with the NHGRI’s bold prediction. The length of time taken to reach this future, and thus whether we are able to achieve the goal of largely eliminating VUSs by 2030, is largely a consequence of the choices made now and in the next few years. We believe that investing in eliminating VUSs is worthwhile, since their predominance remains one of the biggest challenges to precision genomic medicine.}
}
@article{SFARD20121,
title = {Introduction: Developing mathematical discourse—Some insights from communicational research},
journal = {International Journal of Educational Research},
volume = {51-52},
pages = {1-9},
year = {2012},
note = {Developing mathematical discourse–Some insights from communicational research},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511001327},
author = {Anna Sfard},
keywords = {Mathematics, Discourse, Learning, Development, Cognition, Emotions, Interactions},
abstract = {Quite diverse in their foci and specific themes, the seven articles collected in this special issue are unified by their common conceptual framework. Grounded in the premise that thinking can be usefully defined as self-communicating and that mathematics can thus be viewed as a discourse, the communicational framework provides a unified set of conceptual tools with which to investigate cognitive, affective and social aspects of mathematics learning. The communicational tools are employed by the authors as they investigate diverse aspects of mathematical discourse and explore its development in the classroom and beyond. The seven studies combine together to produce a set of insights, some of which go against widespread beliefs about teaching and learning mathematics.}
}
@article{BOUDIN2016448,
title = {Opinion dynamics: Kinetic modelling with mass media, application to the Scottish independence referendum},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {448-457},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008602},
author = {Laurent Boudin and Francesco Salvarani},
keywords = {Opinion formation, Mass media, Kinetic equations},
abstract = {We consider a kinetic model describing some mechanisms of opinion formation in the framework of referendums, where the individuals, who can interact between themselves and modify their opinion by means of spontaneous self-thinking, are moreover under the influence of mass media. We study, at the numerical level, both the transient and the asymptotic regimes. In particular, we point out that a plurality of media, with different orientations, is a key ingredient to allow pluralism and prevent consensus. The forecasts of the model are compared to some surveys related to the Scottish independence referendum of 2014.}
}
@article{PIOLOPEZ2023103585,
title = {Morphoceuticals: Perspectives for discovery of drugs targeting anatomical control mechanisms in regenerative medicine, cancer and aging},
journal = {Drug Discovery Today},
volume = {28},
number = {6},
pages = {103585},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103585},
url = {https://www.sciencedirect.com/science/article/pii/S1359644623001010},
author = {Léo Pio-Lopez and Michael Levin},
keywords = {Biomedicine, Drug discovery, Morphogenesis},
abstract = {Morphoceuticals are a new class of interventions that target the setpoints of anatomical homeostasis for efficient, modular control of growth and form. Here, we focus on a subclass: electroceuticals, which specifically target the cellular bioelectrical interface. Cellular collectives in all tissues form bioelectrical networks via ion channels and gap junctions that process morphogenetic information, controlling gene expression and allowing cell networks to adaptively and dynamically control growth and pattern formation. Recent progress in understanding this physiological control system, including predictive computational models, suggests that targeting bioelectrical interfaces can control embryogenesis and maintain shape against injury, senescence and tumorigenesis. We propose a roadmap for drug discovery focused on manipulating endogenous bioelectric signaling for regenerative medicine, cancer suppression and antiaging therapeutics.}
}
@article{MOSKOWITZ200387,
title = {The intertwining of psychophysics and sensory analysis: historical perspectives and future opportunities—a personal view},
journal = {Food Quality and Preference},
volume = {14},
number = {2},
pages = {87-98},
year = {2003},
issn = {0950-3293},
doi = {https://doi.org/10.1016/S0950-3293(02)00072-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950329302000721},
author = {Howard R. Moskowitz},
keywords = {History, Psychology, Psychophysics},
abstract = {From today’s point of view, psychophysics and sensory analysis appear conjoined, at least from the vantage point of sensory analysis. This paper shows how psychophysical thinking has not only entered sensory analysis, but also shaped some of the ways that modern day sensory analysts conceptualize their problems and go about solving them. The paper also shows how this was not always the case. The rapprochement of the two fields has only gradually developed as sensory analysis has come to accept psychophysical thinking. The paper concludes by listing a series of trends that may bring the two fields even closer in the future.}
}
@article{MININA2022104684,
title = {Neuron quantum computers and a way to unification of science: A compendium of Efim Liberman's scientific work},
journal = {Biosystems},
volume = {217},
pages = {104684},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104684},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000727},
author = {Svetlana V. Minina and Nikita E. Shklovskiy-Kordi},
keywords = {Efim liberman, cAMP, Biological computation, Biophysics, Chaimatics, Quantum biology, Unity of science, Quantum computation, Molecular cell computer, Quantum regulator},
abstract = {In 1972, Efim Liberman, a Soviet biophysicist, pioneered a brand-new approach to studying the operation of the brain, the live cell and the human mind by publishing a paper titled “Cell as a molecular computer” (1972). In this paper, Liberman posited that a consecutive/parallel stochastic molecular computer (MCC) controls a living cell. An MCC operates with molecule-words (DNA, RNA, proteins) according to the program recorded in DNA and RNA. Computational operations are implemented by molecular operators acting as enzymes. An MCC is present in each live cell. A neuron cell MCC can be involved in solving tasks for the entire organism. Neuron MCC investigation was started with studying an impact of an intracellular injection of cyclic AMP on electric activity of a neuron. Cyclic nucleotides were considered as input words for an MCC, which are generated inside a neuron as a result of synaptic activity. This led Efim Liberman to the idea that, in order to solve complex physical problems, which are encountered by a neuron and require rapid solutions, the molecular computer adjusts the operation of the quantum molecular regulator, which uses the “computational environment” of the cytoskeleton and quantum properties of the elementary hypersound quasiparticles for completing mathematical operations for the minimum price of action. Efim Liberman suggested that the human self-consciousness is a quantum computer of even a higher level and designated it as an extreme quantum regulator. In order to describe such systems, he suggested to join biology, physics and mathematics into a unified science, and formulated its four fundamental principles. Results of Efim Liberman’s theoretical and experimental studies on the topic of biological computation are summarized in this review.}
}
@article{CHATRABHUJ2024101045,
title = {Design of an iterative method for environmental-sustainable development: Integrating bioinspired computing techniques},
journal = {Environmental Development},
volume = {51},
pages = {101045},
year = {2024},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2024.101045},
url = {https://www.sciencedirect.com/science/article/pii/S2211464524000836},
author = { Chatrabhuj and Kundan Meshram},
keywords = {Sustainable development, Bioinspired computing, Hybrid algorithms, Agent-based modelling, High-performance computing},
abstract = {The need for sustainable development has grown in response to global environmental, social, and economic challenges. Conventional computational methods frequently struggle to address the complex nature of the Sustainable Development Goals (SDGs), lacking the ability to balance global search with local optimization and failing to prioritize goals related to sustainability. To address these restrictions, this work introduces the Integrated Bioinspired Computing Model for Sustainable Development (IBCMSD). By combining Genetic Algorithms (GAs), Artificial Neural Networks (ANNs), and Ant Colony Optimization (ACO), a cohesive hybrid model is developed that improves exploration and exploitation, balance for increased efficiency, and solution quality. It is implemented on High-Performance Computing (HPC) clusters to ensure scalability and resilience when dealing with complicated optimization challenges. Furthermore, using a multidisciplinary co-design method completes the model with multiple views, increasing its relevance and applicability in real-world circumstances. IBCMSD makes a significant contribution to computational sustainability by leveraging bioinspired computing, potentially enabling informed decision-making and SDG accomplishment across multiple domains.}
}
@article{LI2024112467,
title = {Study on correlation between perioperative cognitive function and nutritional status in elderly patients with gastric cancer},
journal = {Experimental Gerontology},
volume = {193},
pages = {112467},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2024.112467},
url = {https://www.sciencedirect.com/science/article/pii/S0531556524001098},
author = {Rong Li and Yuping Liu and Yingtao Meng and Xianlin Qu and Meimei Shang and Lihui Yang and Jie Chai},
keywords = {Elderly, Gastric cancer, Perioperative period, Cognitive function, Nutritional status, Correlation, Analysis},
abstract = {Objective: To investigate the cognitive function and nutritional status of elderly patients with gastric cancer during perioperative period, and to analyze their correlation. Methods: Aged patients undergoing gastric cancer surgery in The Affiliated Cancer Hospital of Shandong First Medical University from March to October 2021 were selected as the subjects of this study. The monitoring data of cognitive function and nutritional status were retrospectively analyzed from 1 to 3 days before surgery, 1 and 3 days after surgery, 7 days after surgery (before discharge) and 30 days after surgery to analyze the correlation between cognitive function and nutritional status in elderly patients with gastric cancer. Results: the incidence of mild cognitive impairment in elderly patients with gastric cancer was 52.43 %, the visual space of the two groups' (mild cognitive impairment) ability of execution, name, attention, language, abstract thinking, delayed memory and cognitive function scores were lower than 1 set of directional force (cognitive function in normal group), statistically significant difference (P < 0.05). The nutritional status of elderly patients with gastric cancer was lower than that of healthy elderly group at the same period (P < 0.05). The scores of visual spatial executive function, name, attention, delayed memory, orientation and total score of cognitive function in elderly gastric cancer patients were positively correlated with nutritional status (P < 0.05). Conclusions: The cognitive function and nutritional status of elderly patients with gastric cancer are both in a low state during treatment and a higher level of cognitive function can help patients maintain a more correct nutritional cognition, and the nutritional status of patients will be relatively better. There is a positive correlation between cognitive function and nutritional status in elderly patients with gastric cancer, which should be paid attention to in the treatment.}
}
@article{KIM201828,
title = {Degree of satisfaction-difference (DOSD) method for measuring consumer acceptance: A signal detection measurement with higher reliability than hedonic scaling},
journal = {Food Quality and Preference},
volume = {63},
pages = {28-37},
year = {2018},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950329317301738},
author = {Min-A Kim and Danielle {van Hout} and Jean-Marc Dessirier and Hye-Seong Lee},
keywords = {Acceptance test, Affective product discrimination, Range effects, Indirect scaling, Satisfaction, Reference framing},
abstract = {Predictions of consumer acceptance are often based on hedonic scores, but these are determined not only by the consumer level of product liking, but also by consumer scale usage, which in turn is affected by thinking style and experimental contexts. To improve the validity and reliability of consumer acceptance measurement, a new indirect scaling method, the ‘Degree of Satisfaction-Difference (DOSD)’, was developed using a reminder design and signal detection theory (SDT). In DOSD, a product-specified ‘cognitive warm-up’ was used to evoke the consumer personal context and the internal evaluative criteria prior to product evaluation. In DOSD, each test product was presented together with a fixed-reference (identified as such) and consumers were asked to evaluate their satisfaction with the reference first with a sureness rating, and then to evaluate the test product for both absolute satisfaction and comparative satisfaction to the reference. The reliability of DOSD was tested against traditional hedonic scaling using an independent samples design of two consumer groups with equivalent cognitive reflection test profiles, each including High Reflection Thinkers (HRTs) and Low Reflection Thinkers (LRTs) in equal proportion. Each group tested two sets of skin lotions differing in product range, either using DOSD or hedonic scaling. When examining the affective discriminations of the two common products in terms of d′ values between product sets, the LRT subjects generated inconsistent responses with hedonic scaling, but reproducible responses with DOSD. The HRT subjects performed consistently using both scaling methods. These results validate DOSD’s superior reliability in affective tests and demonstrate its potential as an alternative consumer acceptance measurement to hedonic scaling.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@incollection{BROWN201589,
title = {Space, Linguistic Expression of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {89-93},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570172},
author = {Penelope Brown},
keywords = {Adpositions, Language and cognition, Language universals, Locative constructions, Motion verbs, Space, Spatial frames of reference, Topological language},
abstract = {Spatial cognition is central to human thinking, and spatial language is thus an important area of study, as it may reveal fundamental properties of human thought. Recent research has shown that spatial language is much more divergent across languages than had previously been thought, suggesting significant cultural patterning of spatial conceptualization. This article reviews spatial language cross-linguistically, sets out a typological framework for the language of space, and considers the relationship of spatial language to spatial cognition, in the context of extensive linguistic diversity in the spatial domain.}
}
@article{ADRIAENSEN2023106294,
title = {Systems-theoretic interdependence analysis in robot-assisted warehouse management},
journal = {Safety Science},
volume = {168},
pages = {106294},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523002369},
author = {Arie Adriaensen and Liliane Pintelon and Francesco Costantino and Giulio {Di Gravio} and Riccardo Patriarca},
keywords = {FRAM, Human-machine interaction, Industry 4.0, Industry 5.0, Cobots},
abstract = {The safe and efficient application of collaborative robots requires an understanding of actual work practices transformation, emerging from the adoption of new technological instruments. Functional systems-thinking is largely absent in literature about collaborative robot applications. In this context, this study proposes a framework that combines two safety analysis methods, being the Functional Resonance Analysis Method and Interdependence Analysis. Both safety and efficiency are examined by selected case study highlights to gain an in-depth understanding of human operators’ role as the central driver of human–machine (eco)systems in a warehouse distribution system, in which warehouse robot assistance is provided. Whereas the Functional Resonance Analysis Method first maps the work system interactions as a whole, Interdependence Analysis is subsequently applied to investigate individual inter-agent exchanges by the principles of Observability, Predictability, and Directability as a core principle for goal coordination between multiple agents, including warehouse robot agents. The case study examples reveal the combined effects of the working system environment and the robot application but also demonstrate possible operational solutions to deal with socio-technical complexity.}
}
@article{NAGOEV2020615,
title = {Model of the reasoning process in a multiagent cognitive system},
journal = {Procedia Computer Science},
volume = {169},
pages = {615-619},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303252},
author = {Zalimhan Nagoev and Inna Pshenokova and Murat Anchekov},
keywords = {Multi-Agent Systems, Neurocognitive Architecture, Simulation Model, Artificial Intelligence Systems, Reasoning Models},
abstract = {A model of the reasoning process in a multiagent cognitive system for the synthesis of intelligent solutions of the problem is presented. The approach based on the computational abstraction of multi-agent neurocognitive systems that illustrates architectural conformity to self-organizing neurocognitive networks of the brain. The model represents the process of reasoning in the form of cognitive blocks that synthesize intelligent solutions and allow the user to effectively solve the tasks.}
}
@article{SON2015120,
title = {The history of Western futures studies: An exploration of the intellectual traditions and three-phase periodization},
journal = {Futures},
volume = {66},
pages = {120-137},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002079},
author = {Hyeonju Son},
keywords = {History of Western futures studies, Intellectual tradition, Periodization, Rationalization of futures, Industrialization of futures, Fragmentation of futures.},
abstract = {The main purpose of this paper is to present a three-phase periodization of modern Western futures studies to construct historical classification. In order to reach this goal, the following intellectual traditions are introduced to review the philosophical and historical contexts that affect the very foundations of futures studies: (a) religions, (b) utopias, (c) historicism, (d) science fiction, and (e) systems thinking. The first phase (beginning in 1945 to the 1960s) was the era of scientific inquiry and rationalization of the futures characterized by the prevalence of technological forecasting, the rise of alternative futures in systematic ways, and the growth of professionalization of futures studies. In the first phase, futures had become objects of rationalization removed from the traditional approaches such as utopia, grandiose evolutionary ideas, naive prophecies, science fiction, religious attitudes, and mystical orientation. The second phase (the 1970s and the 1980s) saw the creation the global institution and industrialization of the futures. This era was marked by the rise of worldwide discourse on global futures, the development of normative futures, and the deep involvement of the business community in futures thinking. In the second phase, futures studies-industry ties were growing and the future-oriented thoughts extensively permeated the business decision-making process. The third phase (the 1990s – the present) reflects the current era of the neoliberal view and fragmentation of the futures. This phase is taking place in the time of neoliberal globalization and risk society discourses and is characterized by the dominance of foresight, the advance of critical futures studies, and the intensification of fragmentation. In the third phase, futures practice tends to be confined to the support of strategic planning, and hence is experiencing an identity crisis and loss of its earlier status of humanity-oriented futures.}
}
@article{LEE2021101596,
title = {Measuring Mohr social capital},
journal = {Poetics},
volume = {88},
pages = {101596},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101596},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000863},
author = {Monica Lee and Amaç Herdağdelen and Minsu Park and John Levi Martin},
abstract = {We here bring together two different traditions of thinking about social capital. One, the Tocquevillian, looks to associations and group memberships as the core of social capital. The other, the Colemanian, looks to interpersonal networks as the core of social capital. We argue that the most common way of articulating how humans use these types of relationships in different ways—the distinction between “bridging” and “bonding” social capital—is epistemically unstable. What might be possible, however, is to use the insights developed by Ronald Burt regarding tie non-redundancy to study associational social capital. We do this by drawing on the insights of the approach consistently adopted and developed by John Mohr, which emphasizes duality and diversity, to develop measures of group affiliation-based social capital. We accordingly, for both Tocquevillian and Colemanian social capital, distinguish measures that focus on the mass of social capital from those that focus on its diversity. To illustrate, we use de-identified data from 77 Million U.S. Facebook Groups users to measure their degree of all resulting types of social capital. We show that our understanding of who has the most social capital varies greatly by whether we are considering Tocquevillian or Colemanian capital, and whether we are focusing on mass or diversity.}
}
@article{BATTISTELLI2022,
title = {Online Strategies To Improve Quantitative Skills in Microbiology Laboratory Classes},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {1},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00333-21},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000995},
author = {Joseph M. Battistelli and Rima B. Franklin},
keywords = {quantitative literacy, quantitative biology, problem solving, word problems, math skills, formula question, Canvas, spreadsheets, algebra, formula questions},
abstract = {Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills.
ABSTRACT
Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills. This can create a substantial grading burden for faculty teaching online and/or large enrollment courses, but the “formula question” feature present in many learning management systems (LMS) offers a solution. Using this feature, faculty set up a basic scaffold for an algebraic word problem, and the LMS can then automatically generate and grade many different versions of the question. In this paper, we describe the use of “formula questions” in an undergraduate microbiology course and specifically focus on how the strategic use of algebraic word problems at multiple points throughout the semester can help build quantitative literacy. Key to the success of this approach is that faculty provide a review of foundational mathematical skills early in the semester, even in upper-level classes. This should include reacquainting students with formatting conventions (e.g., rounding and scientific notation), familiarizing them with any idiosyncrasies of the technology platforms, and demonstrating how to solve math problems using spreadsheets. This initial effort increases student success when more complex problems are introduced later in the semester. Though the tips summarized in this paper focus on undergraduate microbiology teaching laboratories using Canvas, the approach can easily be modified to help students develop their critical thinking and quantitative reasoning skills at other levels and in other disciplines.}
}
@article{COUVELAS2020326,
title = {Bioclimatic building design theory and application},
journal = {Procedia Manufacturing},
volume = {44},
pages = {326-333},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.238},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308258},
author = {Agnes Couvelas},
keywords = {Modern architecture, Cultural heritage, Sustainable architecture design, Bioclimatic Performance Optimization, Inter-locality},
abstract = {Ecological thinking is the recognition of the dialectic unity between natural and man-made environment, the respect to what exists around us, and the concomitant “openness” toward others. Here, I present examples from my own work to describe a number of passive bioclimatic approaches focused on the above principles. First, the use of the wind as an expressive element in building design, including the enhancement of air flow in the interior space, the moderation of wind and sand accumulation, the moderation of the sound carried by prevailing winds, and the conversion of the wind into a means of protection against its own force. Second, the use of adaptive building envelopes and shading systems to achieve control of natural light, ventilation and temperature of the inner space through their own transformability, surface openings and materials, including planting as a building material; in a sense, treating buildings as living organisms. Three of these examples have been included in the H2020-MSCA-RISE OptArch project, in which I am scientifically responsible for the work package WP5 entitled “Improvement of bioclimatic design through optimization of performance”.}
}
@article{LOCKWOOD2019100688,
title = {Computing as a mathematical disciplinary practice},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100688},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318300282},
author = {Elise Lockwood and Anna F. DeJarnette and Matthew Thomas},
keywords = {Computation, Mathematical disciplinary practices, Mathematicians},
abstract = {In this paper, we make a case for computing as a mathematical disciplinary practice. We present results from interviews with research mathematicians in which they reflected on the use of computing in their professional work. We draw on their responses to present evidence that computing is an inherent part of doing mathematics and is a practice they want their students to develop. We also discuss the mathematicians’ perspectives on how they learned and teach computing, and we suggest that much needs to be explored about how to teach computing effectively. Our overarching goal is to draw attention to the importance of the teaching and learning of computing, and we argue that it is an imperative topic of study in mathematics education research.}
}
@article{SCHWARZ201359,
title = {Business wargaming for teaching strategy making},
journal = {Futures},
volume = {51},
pages = {59-66},
year = {2013},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328713000864},
author = {Jan Oliver Schwarz},
keywords = {Business wargaming, Teaching, Simulation, Management education, Strategy making, Strategic thinking},
abstract = {An increasingly complex and dynamic business environment requires new approaches to teaching strategy to management students. Business wargaming, a dynamic strategic simulation, is discussed as a management simulation which can respond to the contemporary challenges in management education. Reflecting on the practical use of business wargaming in the classroom, it is described how such simulations prepare management students for making strategic decisions in complex and dynamic environments characterised by high uncertainty concerning the future.}
}
@article{LIN20162176,
title = {New statistical analysis in marketing research with fuzzy data},
journal = {Journal of Business Research},
volume = {69},
number = {6},
pages = {2176-2181},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315006517},
author = {Hsin-Cheng Lin and Chen-Song Wang and Juei Chao Chen and Berlin Wu},
keywords = {Decision making, Fuzzy statistics, Fuzzy data, Marketing research},
abstract = {This research proposes new statistical methods for marketing research and decision making. The study employs a soft computing technique and a new statistical tool to evaluate people's thinking. Because the classical measurement system has difficulties in dealing with the non-real valued information, the study aims to find an appropriate measurement system to overcome this problem. The main idea is to decompose the data into a two-dimensional type, centroid and its length (area). The two-dimensional questionnaires this study proposes help reaching market information.}
}
@article{SATTARI2021104981,
title = {Application of Bayesian network and artificial intelligence to reduce accident/incident rates in oil & gas companies},
journal = {Safety Science},
volume = {133},
pages = {104981},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104981},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520303787},
author = {Fereshteh Sattari and Renato Macciotta and Daniel Kurian and Lianne Lefsrud},
keywords = {Artificial intelligence, Bayesian network, Machine learning, Keyword analysis, Incident data, Process safety management, Latent causes},
abstract = {Process safety management (PSM) is a framework that demonstrates a company’s commitment to process safety, a better understanding of hazards and risks, a comprehensive assessment and management of risks, and enhanced learning from experience to improve overall safety and operational performance. Companies often use an incident data reporting system to execute PSM. While companies keep incident data in thousands of reports, rarely do they glean full value in learning from these to prevent and reduce future incidents. To overcome this challenge, this research applied machine learning and keyword analysis to label and classify 8199 incident reports from an oil and gas company into nine groups identified in the latest version of PSM guidelines published by the Center for Chemical Process Safety (CCPS). To converge on an optimal solution, two different Bayesian network techniques (Tabu and hill climbing) were applied. Both methods resulted in the same map, showing that the Total Number of Incidents has the maximum dependency (50%) on Asset Integrity & Reliability; this means focusing resources on this aspect could reduce the total number of incidents by half. Cross correlation analysis (CCA) was also applied, which validated and confirmed this result. This analysis identifies which measures enhance the company’s safety management strategy to reduce these latent causes, but also supports critical thinking, enhanced communication, and learning culture to improve organizational safety.}
}
@article{HAJELA20021,
title = {Soft computing in multidisciplinary aerospace design—new directions for research},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {1},
pages = {1-21},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(01)00015-X},
url = {https://www.sciencedirect.com/science/article/pii/S037604210100015X},
author = {Prabhat Hajela},
abstract = {There has been increased activity in the study of methods for multidisciplinary analysis and design. This field of research has been a busy one over the past decade, driven by advances in computational methods and significant new developments in computer hardware. There is a concern, however, that while new computers will derive their computational speed through parallel processing, current algorithmic procedures that have roots in serial thinking are poor candidates for use on such machines—a paradigm shift is required! Among new advances in computational methods, soft computing techniques have enjoyed a remarkable period of development and growth. Of these, methods of neural computing, evolutionary search, and fuzzy logic have been the most extensively explored in problems of multidisciplinary analysis and design. The paper will summarize important accomplishments to-date, of neurocomputing, fuzzy logic, and evolutionary search, including immune network modeling, in the field of multidisciplinary aerospace design.}
}
@article{1995462,
title = {95/06537 Historical rates of atmospheric Pb deposition using 210Pb dated peat cores: Corroboration, computation, and interpretation},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {6},
pages = {462},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)98112-5},
url = {https://www.sciencedirect.com/science/article/pii/0140670195981125}
}
@article{GORMONG20231988,
title = {Neighboring Group Effects on the Rates of Cleavage of Si–O–Si-Containing Compounds},
journal = {The Journal of Organic Chemistry},
volume = {88},
number = {4},
pages = {1988-1995},
year = {2023},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c02126},
url = {https://www.sciencedirect.com/science/article/pii/S002232632300107X},
author = {Ethan A. Gormong and Dorian S. Sneddon and Theresa M. Reineke and Thomas R. Hoye},
abstract = {ABSTRACT
The presence of a nearby tethered functional group (G, G = tertiary amide or amine) can significantly impact the rate of cleavage of an Si–O bond. We report here an in situ1H NMR spectroscopic investigation of the relative rates of cleavage of model substrates containing two different Si–O substructures, namely alkoxydisiloxanes [GRO–Si­(Me2)–O–SiMe3] and carbodisiloxanes [GR–Si­(Me2)–O–SiMe3]. The trends in the relative rates (which slowed with increasing chain length, with a notable exception) of alkoxydisiloxane hydrolyses were probed via computation. The results correlated well with the experimental data. In contrast to the hydrolysis of the alkoxydisiloxanes, the carbodisiloxanes were not fully hydrolyzed, but rather formed an equilibrium mixture of starting asymmetric disiloxane, two silanols, and a new symmetrical disiloxane. We also uncovered a facile siloxy-metathesis reaction of an incoming silanol with the carbodisiloxane substrate [e.g., Me2NR–Si­(Me2)–O–SiMe3 + HOSiEt3 ⇋ Me2NR–Si­(Me2)–O–SiEt3 + HOSiMe3] facilitated by the pendant dimethylamino group, a process that was also probed by computation.}
}
@article{CHEN2025121691,
title = {A novel attribute reduction algorithm based on granular sequential three-way decision},
journal = {Information Sciences},
volume = {694},
pages = {121691},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016050},
author = {Yuliang Chen and Yunlong Cheng and Binbin Luo and Yabin Shao and Mingfu Zhao and Qinghua Zhang},
keywords = {Granular computing, Sequential three-way decision, Granular rough sets, Attribute reduction},
abstract = {Attribute reduction plays a crucial role in knowledge discovery, and sequential three-way decision (S3WD) provides a new method for attribute reduction. However, the three regions of the S3WD model are usually represented as three sets, which leads to two disadvantages. On one hand, it is difficult to obtain the condition of a decision rule when multiple equivalence classes are merged into a set because different equivalence classes have different descriptions. On the other hand, if the boundary region of the upper level of S3WD is a set, one has to partition the upper level with all the acquired attributes rather than the newly added attribute. That is, there is double counting. Therefore, this paper focuses on how to retain the topology of equivalence classes in S3WD, and how to use this topology to enhance semantic interpretation and improve computational efficiency. To this end, a granular version of S3WD, called granular sequential three-way decision (GS3WD), is first developed to retain the information structure of equivalence classes. And then, three acceleration strategies and an efficient granular sequential three-way reduction (GS3WR) are proposed. Finally, a concept tree can be generated simultaneously in the process of GS3WR, and the decision rules with multi-granularity can be extracted from this concept tree directly. Experimental results show that GS3WR can obtain the same core attributes and reducts as the representative attribute reduction algorithms in rough sets and the computational efficiency is improved by hundreds of times.}
}
@incollection{SUGHRUE2024205,
title = {Chapter 12 - Connectomic approaches to neurosurgical planning},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {205-214},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000112},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain tumor surgery, Cerebral cortex, Cognitive deficits, fMRI, Graph theory, Neuro-Oncology, Onco-functional balance},
abstract = {In this chapter, we introduce how connectomics can provide an improved understanding of the structural and functional organization of the human brain which can be applied for intracerebral brain surgery. In particular, such connectomic thinking expands our ability to improve patient functional outcomes after surgery beyond mere motor and language functions by also considering the anatomy responsible for complex cognitive functions. We introduce the concept of “disconnection surgery,” where the surgical decisions when removing a tumor can be thought of a series of specific cuts that we plan to perform on the periphery of the tumor such that we can disconnect the tumor from the surrounding networks. Connectomics allows us to define the risks associated with specific tumors and surgical decisions, which can subsequently guide the operation but also tailor preoperative patient discussion. Novel mathematical concepts from the field of network neuroscience on graph theory are also introduced so as to better define truly eloquent brain regions on an individualized basis.}
}
@article{SCOLOZZI2017957,
title = {The anthroposphere as an anticipatory system: Open questions on steering the climate},
journal = {Science of The Total Environment},
volume = {579},
pages = {957-965},
year = {2017},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2016.10.086},
url = {https://www.sciencedirect.com/science/article/pii/S0048969716322604},
author = {Rocco Scolozzi and Davide Geneletti},
keywords = {System thinking, Future studies, Climate change, System dynamics, Anticipatory system, Transdisciplinary},
abstract = {Climate change research and action counteracting it affect everyone and would involve cross-societal transformations reshaping the anthroposphere in its entirety. Scrutinizing climate-related science and policies, we recognize attempts to steer the evolution of climate according to expected (or modelled) futures. Such attempts would turn the anthroposphere into a large “anticipatory system”, in which human society seeks to anticipate and, possibly, to govern climate dynamics. The chief aim of this discussion paper is to open a critical debate on the climate change paradigm (CCP) drawing on a strategic and systemic framework grounded in the concept of anticipatory system sensu Rosen (1991). The proposed scheme is ambitiously intended to turn an intricate issue into a complex but structured problem that is to say, to make such complexity clear and manageable. This framework emerges from concepts borrowed from different scientific fields (including future studies and system dynamics) and its background lies in a simple quantitative literature overview, relying upon a broad level of analysis. The proposed framework will assist researchers and policy makers in thinking of CCP in terms of an anticipatory system, and in disentangling its interrelated (and sometimes intricate) aspects. In point of fact, several strategic questions related to CCP were not subjected to an adequate transdisciplinary discussion: what are the interplays between physical processes and social-political interventions, who is the observer (what he/she is looking for), and which paradigm is being used (or who defines the desirable future). The proposed scheme allows to structure such various topics in an arrangement which is easier to communicate, highlighting the linkages in between, and making them intelligible and open to verification and discussion. Furthermore, ideally developments will help scientists and policy makers address the strategic gaps between the evidence-based climatological assessments and the plurality of possible answers as applied to the geopolitical contingencies.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@article{GLASSMEYER2021100873,
title = {Identifying and supporting teachers’ robust understanding of proportional reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100873},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100873},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000341},
author = {David Glassmeyer and Aaron Brakoniecki and Julie M. Amador},
keywords = {Content knowledge, Knowledge resource, Proportional reasoning, Proportions, Ratio, Teachers},
abstract = {This case study uses the Framework for Teachers’ Robust Understanding of Proportional Reasoning for Teaching (Weiland et al., 2020) to characterize how 51 mathematics teachers solved a comparison proportional problem. We found 50 of the 51 teachers productively drew upon four knowledge resources: (1) proportional situation, (2) ratios as part: part or part: whole, (3) unit rates, and (4) ratio as measure. This study details these and teachers’ less commonly used knowledge resources, as well as counterproductive statements related to the knowledge resources. We analyze the structure of the comparison proportion problem and suggest why teachers drew on particular knowledge resources. Lastly, we highlight how counterproductive statements highlight areas of focus for mathematics teacher educators and extends the operationalizing of the robust proportional reasoning framework for mathematics education researchers.}
}
@article{KIM201716,
title = {A study on metadata structure and recommenders of biological systems to support bio-inspired design},
journal = {Engineering Applications of Artificial Intelligence},
volume = {57},
pages = {16-41},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616301786},
author = {Sun-Joong Kim and Ji-Hyun Lee},
keywords = {Bio-inspired design, Biological system metadata modeling, Knowledge-based system, Recommendation system, Ontology},
abstract = {Bio-inspired design was introduced as an alternative method to encourage breakthrough innovations during design projects by stimulating analogical reasoning and thinking of designers. However, the method did not perform as well as researchers expected because most designers, who are novices in the fields of biology and ecology, cannot infer the proper analogue (i.e. biological system) from nature. To resolve this fundamental problem, a causal model based representation framework for ‘analogical reasoning’ – searching and selecting the biological systems to apply – have been developed. In addition, ontology based repository structures and retrieval systems have been proposed to support ‘analogical thinking’ of designers. Nevertheless, these systematic approaches still restrict the candidates and inevitably lose potential biological systems relevant to the design project, due to the ‘physical relation’ biased problem and the ambiguity of the indexing mechanism of both current representation frameworks and retrieval systems. For example, the causality based support system known as a robust representation framework for a single biological system, stores information of a biological system only by its internal ‘physical relations’ and retrieves biological systetabms only by the physical relevance. However, from the perspective of ecological thinking, the further relatedness of ‘physical, biological, and ecological relations’ composes the holistic concept used to identify an organism in the flow of evolution because the ‘biological and ecological relations’ are also involved in the traits that designers may be interested in. Therefore, the supplementary information for ‘biological and ecological relations’ must be added to index the biological and environmental interactions, and to use the connectivity among entire organisms in the retrieval process. In this research, a causality based holistic representation framework for biological systems and an ‘all-connected’ ontology based repository and retrieval system are developed as a knowledge-based recommendation system to support bio-inspired design. The knowledge-based system we developed allows engineering designers to search and select a particular biological system and extract design strategy without much biological knowledge. This effort provides more opportunities in a bio-inspired design process by adding potential biological systems that might previously not have been considered.}
}

@article{LEHRER200039,
title = {Developing Model-Based Reasoning in Mathematics and Science},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {39-48},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00049-0},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000490},
author = {Richard Lehrer and Leona Schauble},
abstract = {It is essential to base instruction on a foundation of understanding of children's thinking, but it is equally important to adopt the longer-term view that is needed to stretch these early competencies into forms of thinking that are complex, multifaceted, and subject to development over years, rather than weeks or months. We pursue this topic through our studies of model-based reasoning. We have identified four forms of models and related modeling practices that show promise for developing model-based reasoning. Models have the fortuitous feature of making forms of student reasoning public and inspectable—not only among the community of modelers, but also to teachers. Modeling provides feedback about student thinking that can guide teaching decisions, an important dividend for improving professional practice.}
}
@incollection{PERKINS2002187,
title = {Standard logic as a model of reasoning: The empirical critique},
editor = {Dov M. Gabbay and Ralph H. Johnson and Hans Jürgen Ohlbach and John Woods},
series = {Studies in Logic and Practical Reasoning},
publisher = {Elsevier},
volume = {1},
pages = {187-223},
year = {2002},
booktitle = {Handbook of the Logic of Argument and Inference},
issn = {1570-2464},
doi = {https://doi.org/10.1016/S1570-2464(02)80007-6},
url = {https://www.sciencedirect.com/science/article/pii/S1570246402800076},
author = {David N. Perkins},
abstract = {Publisher Summary
This chapter describes standard logic as a model of reasoning. The notion of formal logic has figured centrally in conceptions of human reasoning, rationality, and adaptiveness. The chapter reviews the evidence, appraises its weight, and offers a summative judgment of the place of logic in human thinking. "Standard logic," includes the canons of formal deduction, the special case of disconfirming hypotheses by finding counterevidence for their implications, and also the principles of probabilistic and statistical inference developed by mathematicians over the past couple of hundred years. It also examines deliberate or reflexive reasoning. The chapter argues that standard logic or subsets of it can be implemented in quite different ways and that human cognition incorporates more than one implementation. In addition, almost all the research on the role of standard logic in human thinking concerns deliberate rather than reflexive reasoning. Accordingly, the present analysis focuses on deliberate reasoning and the place of standard logic in it.}
}
@article{SOSA201656,
title = {Visual divergence in humans and computers},
journal = {Design Studies},
volume = {42},
pages = {56-85},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000836},
author = {Ricardo Sosa and Nicolas Rojas and John S. Gero and Qinqi Xu},
keywords = {creativity, sketching, computer models, solution space},
abstract = {Studies of design creativity have underlined the importance of divergent reasoning and visual reasoning in idea generation. Connecting these two key design skills, this paper presents a model of divergent visual reasoning for the study of creativity. A visual divergence task called ShapeStorm is demonstrated for the study of creative ideation that can be applied to humans as well as computational systems. The model is examined in a study with human subjects, a computational stochastic generator, and a geometrical analysis of the solution space. The main significance of this task is that it offers a straightforward means to define a simple design task that can be used across research studies. Several scenarios for the application of ShapeStorm for the study of creativity are advanced.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@article{COOK201895,
title = {An investigation of an undergraduate student’s reasoning with zero-divisors and the zero-product property},
journal = {The Journal of Mathematical Behavior},
volume = {49},
pages = {95-115},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301748},
author = {John Paul Cook},
keywords = {Abstract algebra, Zero-product property, Zero-divisors, Equation solving, Student thinking, Realistic Mathematics Education},
abstract = {The zero-product property (ZPP), often stated as ‘if ab = 0, then a = 0 or b = 0,’ is an important concept in secondary algebra (as a tool for solving equations) and abstract algebra (as a property of integral domains). This study analyzes results from a teaching experiment to investigate how an undergraduate mathematics major might intuitively reason with zero-divisors and the ZPP. There are two primary findings. First, a procedurally embodied view of equation solving might preclude students’ attention to the algebraic properties (including the ZPP) that justify the equivalence of two equations. Second, students might not carefully attend to zero-divisors because they are employing the converse of the ZPP instead of the ZPP itself. These findings advance a hypothesis about why students might view abstract algebra as a different subject than school algebra and also affirm the utility of the student-centered theoretical perspective that guided the instructional design and analysis of student activity.}
}
@article{KESBERG2021110458,
title = {Personal values as motivational basis of psychological essentialism: An exploration of the value profile underlying essentialist beliefs},
journal = {Personality and Individual Differences},
volume = {171},
pages = {110458},
year = {2021},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2020.110458},
url = {https://www.sciencedirect.com/science/article/pii/S0191886920306498},
author = {Rebekka Kesberg and Johannes Keller},
keywords = {Essentialist beliefs, Human values, Psychological essentialism},
abstract = {Essentialist lay-theories can reflect a belief in genetic, social, and metaphysical determinism. These three types of essentialist beliefs are similar as they can be linked to a set of motives and each of those beliefs is related to stereotyping and prejudice. Nevertheless, the available evidence indicates that the three types of essentialist thinking are largely unrelated and it is unclear why some individuals endorse one type of essentialism and reject another. Examining the association between the endorsement of essentialist beliefs and personal values, our results based on N = 348 respondents indicate that specific value profiles build the motivational basis for specific essentialist beliefs. Specifically, conservation values are associated with belief in genetic and metaphysical determinism, while self-transcendence and self-enhancement are associated with belief in social determinism.}
}
@article{YU2025101303,
title = {AI as a co-creator and a design material: Transforming the design process},
journal = {Design Studies},
volume = {97},
pages = {101303},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101303},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000158},
author = {Wendy Fangwen Yu},
keywords = {, , , , },
abstract = {Recent advancements in Artificial Intelligence (AI) have created new opportunities for incorporating AI into creative activities. Consequently, AI has become an increasingly significant tool in the design process, changing traditional workflows. This paper explores AI's role as both a co-creator and a design material, focusing on its impact on the ideation and evaluation stages of the design process. Through a transdisciplinary literature review, this study reveals that AI enhances creativity by providing inspirational stimuli, and supports evaluation and decision-making. However, it also introduces complexities such as cognitive overload and dependency. This paper emphasizes the need for design education reform, and training students as Designer Arbiters and Integrators to effectively collaborate with AI in a rapidly evolving technological landscape.}
}
@article{VONRICHTHOFEN2018573,
title = {The ‘Urban Elements’ method for teaching parametric urban design to professionals},
journal = {Frontiers of Architectural Research},
volume = {7},
number = {4},
pages = {573-587},
year = {2018},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S209526351830044X},
author = {Aurel {von Richthofen} and Katja Knecht and Yufan Miao and Reinhard König},
keywords = {Urban design education, Parametric urban design, Singapore, Urban Elements},
abstract = {The article proposes a method for teaching advanced urban design to working professionals in Singapore. The article aims to expand the discourse on parametric urban design education by introducing ‘Urban Elements’ as conceptual urban design instruments with an inherent rule-based logic, which can help to bridge gaps in teaching parametric urban design thinking. As case study we present a course developed for and delivered to the Urban Redevelopment Authority (URA) in Singapore in 2017 by the Future Cities Laboratory at the Singapore-ETH Centre. The article reports on the pedagogical method, course results and course feedback. The main difficulties of teaching professionals in parametric urban design are described and possible reasons and improvements are discussed. The results show that participants using the ‘Urban Elements’ method successfully linked theoretical input to urban design problems, applied evidence-based urban design strategies to these problems, and developed parametric definitions to explore the solution spaces of these urban design challenges. The teaching methodology presented opens up a new research field for urban design pedagogy at the intersection of explicating urban design intent, integrating multidisciplinary knowledge and exploring new software driven tools.}
}
@article{BIALEK19901227,
title = {Temporal filtering in retinal bipolar cells. Elements of an optimal computation?},
journal = {Biophysical Journal},
volume = {58},
number = {5},
pages = {1227-1233},
year = {1990},
issn = {0006-3495},
doi = {https://doi.org/10.1016/S0006-3495(90)82463-2},
url = {https://www.sciencedirect.com/science/article/pii/S0006349590824632},
author = {W. Bialek and W.G. Owen},
abstract = {Recent experiments indicate that the dark-adapted vertebrate visual system can count photons with a reliability limited by dark noise in the rod photoreceptors themselves. This suggests that subsequent layers of the retina, responsible for signal processing, add little if any excess noise and extract all the available information. Given the signal and noise characteristics of the photoreceptors, what is the structure of such an optimal processor? We show that optimal estimates of time-varying light intensity can be accomplished by a two-stage filter, and we suggest that the first stage should be identified with the filtering which occurs at the first anatomical stage in retinal signal processing, signal transfer from the rod photoreceptor to the bipolar cell. This leads to parameter-free predictions of the bipolar cell response, which are in excellent agreement with experiments comparing rod and bipolar cell dynamics in the same retina. As far as we know this is the first case in which the computationally significant dynamics of a neuron could be predicted rather than modeled.}
}
@article{INIGUEZLOMELI2024105106,
title = {A hardware architecture for single and multiple ellipse detection using genetic algorithms and high-level synthesis tools},
journal = {Microprocessors and Microsystems},
volume = {111},
pages = {105106},
year = {2024},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2024.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0141933124001017},
author = {Francisco J. Iñiguez-Lomeli and Carlos H. Garcia-Capulin and Horacio Rostro-Gonzalez},
keywords = {Ellipse detection, Genetic algorithm, System-on-a-Chip (SoC), High-level synthesis (HLS), Hardware implementation, FPGA},
abstract = {Ellipse detection techniques are often developed and validated in software environments, neglecting the critical consideration of computational efficiency and resource constraints prevalent in embedded systems. Furthermore, programmable logic devices, notably Field Programmable Gate Arrays (FPGAs), have emerged as indispensable assets for enhancing performance and expediting various processing applications. In the realm of computational efficiency, hardware implementations have the flexibility to tailor the required arithmetic for various applications using fixed-point representation. This approach enables faster computations while upholding adequate accuracy, resulting in reduced resource and energy consumption compared to software applications that rely on higher clock speeds, which often lead to increased resource and energy consumption. Additionally, hardware solutions provide portability and are suitable for resource-constrained and battery-powered applications. This study introduces a novel hardware architecture in the form of an intellectual property core that harnesses the capabilities of a genetic algorithm to detect single and multi ellipses in digital images. In general, genetic algorithms have been demonstrated to be an alternative that shows better results than those based on traditional methods such as the Hough Transform and Random Sample Consensus, particularly in terms of accuracy, flexibility, and robustness. Our genetic algorithm randomly takes five edge points as parameters from the image tested, creating an individual treated as a potential candidate ellipse. The fitness evaluation function determines whether the candidate ellipse truly exists in the image space. The core is designed using Vitis High-Level Synthesis (HLS), a powerful tool that converts C or C＋＋functions into Register-Transfer Level (RTL) code, including VHDL and Verilog. The implementation and testing of the ellipse detection system were carried out on the PYNQ-Z1, a cost-effective development board housing the Xilinx Zynq-7000 System-on-Chip (SoC). PYNQ, an open-source framework, seamlessly integrates programmable logic with a dual-core ARM Cortex-A9 processor, offering the flexibility of Python programming for the onboard SoC processor. The experimental results, based on synthetic and real images, some of them with the presence of noise processed by the developed ellipse detection system, highlight the intellectual property core’s exceptional suitability for resource-constrained embedded systems. Notably, it achieves remarkable performance and accuracy rates, consistently exceeding 99% in most cases. This research aims to contribute to the advancement of hardware-accelerated ellipse detection, catering to the demanding requirements of real-time applications while minimizing resource consumption.}
}
@article{AKPOLAT2025,
title = {Enhancing operational reliability for high penetration of green hydrogen production in energy islands: A power-to-X case study},
journal = {International Journal of Hydrogen Energy},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.02.131},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925006962},
author = {Alper Nabi Akpolat},
keywords = {Distributed energy resources, Energy islands, Green hydrogen production, Power electronic converters, Machine learning, Power-to-X, Reliability},
abstract = {The production, storage, and conversion of hydrogen into energy, as well as its use in areas such as green ammonia production for agriculture or the catalysis of natural gas, are of significant interest due to their stable structure and source diversity. For this purpose, energy islands (EIs) have been established near consumption points, and renewable energy (RE) obtained from photovoltaic (PV) panels and wind turbines (WTs) is used for green hydrogen production (GHP). In these EIs, hydrogen production from renewable sources shows significant growth, contributing to the power-to-X (P2X) system in terms of storage and flexibility. GHP through renewables will likely become a prominent solution for EIs within the next ten years. One of the bottlenecks here is not to reflect the adverse effects of the variable nature of renewables while transferring sustainable energy. Herein, to enhance operation sustainability, stability, and reliability of renewable-based distributed energy resources (DERs), machine learning (ML)-based techniques can be neat and auxiliary solutions. Power electronic converters (PECs) have such a duty as being the backbone of transferring energy in renewables. The crucial matter is here to keep the inputs and outputs of the converters as stable as possible. In this context, this paper outlines an ML approach to reduce the computational burden and enhance reliability. Therefore, this paper proposes the utilization of an EI to strengthen the stability and reliability of the general scheme. This work is a preliminary attempt and effective solution to establish these EIs, including GHP, considering feasibility criteria and improving reliability. Furthermore, this study examines the essential components, design criteria, challenges, and future issues for system establishment. It aims to facilitate the work of researchers in this field and further enhance the development of EIs.}
}
@article{YOSHIOKA2024114985,
title = {An escort replicator dynamic with a continuous action space and its application to resource management},
journal = {Chaos, Solitons & Fractals},
volume = {185},
pages = {114985},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.114985},
url = {https://www.sciencedirect.com/science/article/pii/S096007792400537X},
author = {Hidekazu Yoshioka},
keywords = {Evolutionary game, Escort replicator dynamic, Kaniadakis escort function, Numerical computation, Application to sustainable resource management},
abstract = {The escort replicator dynamic (ERD) is a version of the replicator dynamic in evolutionary games where the utility-driven decision-making process is modulated due to the information costs to be paid by players. The escort function as a coefficient to distort the decision-making determines the behavior of solutions to the ERD, whereas its investigations are still not sufficient. Particularly, the ERD was investigated in finite-action settings in the previous studies, while that with a continuum of actions has not been studied well. In this paper, we formulate and analyze the ERD with a continuum of actions represented by a bounded interval. Our ERD is a partial integro-differential equation whose well-posedness is nontrivial because of specific nonlocal terms arising from the escort function. The Kaniadakis escort function is chosen as a major example of the escort function, with which we obtain the unique existence of solutions to the ERD. We also discuss cases with the other escort functions, such as the power and constant ones, and suggest that the growth and regularity behaviors of the escort function are crucial. Finally, we computationally apply the ERD to problems related to sustainable environmental and resource management.}
}
@article{GAULD2024116255,
title = {Exploring the interplay of clinical reasoning and artificial intelligence in psychiatry: Current insights and future directions},
journal = {Psychiatry Research},
volume = {342},
pages = {116255},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116255},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124005407},
author = {Christophe Gauld and Vincent P. Martin and Hugo Bottemanne and Pierre Fourneret and Jean-Arthur Micoulaud-Franchi and Guillaume Dumas},
keywords = {Artificial intelligence, Statistical prediction, Psychiatry, Computational sciences, Explainability, Deep learning},
abstract = {For many years, it has been widely accepted in the psychiatric field that clinical practice cannot be reduced to finely tuned statistical prediction systems utilizing diverse clinical data. Clinicians are recognized for their unique and irreplaceable roles. In this brief historical overview, viewed through the lens of artificial intelligence (AI), we propose that comprehending the reasoning behind AI can enhance our understanding of clinical reasoning. Our objective is to systematically identify the factors that shape clinical reasoning in medicine, based on six factors that were historically considered beyond the reach of statistical methods: open-endedness, unanalyzed stimulus-equivalences, empty cells, theory mediation, insufficient time, and highly configured functions. Nevertheless, a pertinent consideration in the age of AI is whether these once-considered insurmountable specific factors of clinicians are now subject to scrutiny or not. Through example in AI, we demonstrate that a deeper understanding of these factors not only sheds light on clinical decision-making and its heuristic processes but also underscores the significance of collaboration between AI experts and healthcare professionals. This comparison between AI and clinical reasoning contributes to a better grasp of the current challenges AI faces in the realm of clinical medicine.}
}
@article{BOURDAKOU2023102881,
title = {Drug repurposing on Alzheimer's disease through modulation of NRF2 neighborhood},
journal = {Redox Biology},
volume = {67},
pages = {102881},
year = {2023},
issn = {2213-2317},
doi = {https://doi.org/10.1016/j.redox.2023.102881},
url = {https://www.sciencedirect.com/science/article/pii/S2213231723002823},
author = {Marilena M. Bourdakou and Raquel Fernández-Ginés and Antonio Cuadrado and George M. Spyrou},
keywords = {Alzheimer's disease, NRF2, , Association network,  drug repurposing, Differentially expressed genes},
abstract = {Alzheimer's disease (AD) is an age-dependent neurodegenerative disorder and the most common cause of cognitive decline. The alarming epidemiological features of Alzheimer's disease, combined with the high failure rate of candidate drugs tested in the preclinical phase, impose more intense investigations for new curative treatments. NRF2 (Nuclear factor-erythroid factor 2-related factor 2) plays a critical role in the inflammatory response and in the cellular redox homeostasis and provides cytoprotection in several diseases including those in the neurodegeneration spectrum. These roles suggest that NRF2 and its directly associated proteins may be novel attractive therapeutic targets in the fight against AD. In this study, through a systemics perspective, we propose an in silico drug repurposing approach for AD, based on the NRF2 interactome and regulome, with the aim of highlighting possible repurposed drugs for AD. Using publicly available information based on differential expressions of the NRF2-neighborhood in AD and through a computational drug repurposing pipeline, we derived to a short list of candidate repurposed drugs and small molecules that affect the expression levels of the majority of NRF2-partners. The relevance of these findings was assessed in a four-step computational meta-analysis including i) structural similarity comparisons with currently ongoing NRF2-related drugs in clinical trials ii) evaluation based on the NRF2-diseasome iii) comparison of relevance between targeted pathways of shortlisted drugs and NRF2-related drugs in clinical trials and iv) further comparison with existing knowledge on AD and NRF2-related drugs in clinical trials based on their known modes of action. Overall, our analysis yielded in 5 candidate repurposed drugs for AD. In cell culture, these 5 candidates activated a luciferase reporter for NRF2 activity and in hippocampus derived TH22 cells they increased NRF2 protein levels and the NRF2 transcriptional signatures as determined by increased expression of its downstream target heme oxygenase 1. We expect that our proposed candidate repurposed drugs will be useful for further research and clinical translation for AD.}
}
@article{LEUKHIN2018166,
title = {Bio-plausible simulation of three monoamine systems to replicate emotional phenomena in a machine},
journal = {Biologically Inspired Cognitive Architectures},
volume = {26},
pages = {166-173},
year = {2018},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X1830152X},
author = {Alexey Leukhin and Max Talanov and Jordi Vallverdú and Fail Gafarov},
keywords = {Affective computing, Affective computation, Spiking neural networks, Bio-inspired cognitive architecture},
abstract = {In this paper we present the validation of the three-dimensional model of emotions by Hugo Lövheim the “cube of emotion” via neurosimulation in the NEST. We also present the extension of original “cube of emotion” with the bridge to computational processes parameters. The neurosimulation is done via re-implementation of DA, 5-HT and NA subsystems of a rat brain to replicate 8 basic psycho-emotional states according to the “cube of emotion”. Results of neurosimulations indicate the incremental influence of DA and NA over computational resources of a psycho-emotional state while 5-HT decreases the computational resources used to calculate a psycho-emotional state. This way we indicate the feasibility of the bio-plausible re-implementation of psycho-emotional states in a computational system. This approach could be useful extension of decision making and load balancing components of modern artificial agents as well as intelligent robotic systems.}
}
@article{TONKS2021102036,
title = {How situational competence beliefs and task value relate to inference strategies and comprehension during reading},
journal = {Learning and Individual Differences},
volume = {90},
pages = {102036},
year = {2021},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102036},
url = {https://www.sciencedirect.com/science/article/pii/S104160802100073X},
author = {Stephen M. Tonks and Joseph P. Magliano and John Schwartz and Ryan D. Kopatich},
keywords = {Reading motivation, Inference generation, Reading comprehension, College students},
abstract = {In two studies, we explored the associations among situational reading-related competence beliefs and task value, inference strategies, comprehension during reading, and foundational skills in college age students. In Study 1, 93 participants from a community college completed assessments of comprehension and two types of inference strategies (elaboration and bridging), each immediately followed by a survey of their competence beliefs and task value regarding the task. Results showed that competence beliefs and task value related positively to reading comprehension. In addition, task value was positively associated with both elaborating and bridging inferences, and competence beliefs correlated positively with bridging inferences. In Study 2, we investigated these associations further in a group of 418 students studying at three different colleges. Participants completed the same assessments for competence beliefs, task value, and inference strategies, as well as assessments of comprehension and foundational reading skills. Study 2 analyses revealed that foundational reading skills were a strong predictor of both types of inferencing and also comprehension. Further, when controlling for foundational reading skills, task value predicted elaboration and bridging inferences, whereas competence beliefs did not predict inferencing, but were trending as a predictor of comprehension. Finally, we created a path model to explore mediational effects, and found that task value positively predicted comprehension performance through increased elaborations while thinking aloud.}
}
@article{PAMPLONA2020100189,
title = {An overview of air delay: A case study of the Brazilian scenario},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100189},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100189},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301007},
author = {Daniel Alberto Pamplona and Claudio Jorge Pinto Alves},
keywords = {Air delay, Air traffic flow, Problem-structuring method, Value-focused thinking},
abstract = {Delay is a key point in air transportation activity. As a performance metric, it affects common policy concerns. Delay impacts passenger satisfaction and imposes costs. The complexity that sets in for the air traffic manager is how to mitigate delay, especially in an environment with several stakeholders. The present article applied a problem-structuring method (PSM), named value-focused thinking (VFT), to structure the problem of the air traffic flow management arrival delay. The inflexibility of incorporating a flight operator's specific needs is considered one of the reasons for the limited success of air traffic flow management (ATFM) programs. PSM allows participants to clarify their dilemmas, converge on a mutually liable problem, or agree to the proposed solutions and compromise on what partially solves the issue. The problem is that most papers focus only on the applied solution for air delay mitigation. Before implementing operational research techniques, we investigated the nature and characteristics of air delay. Results showed that there were several stakeholders with distinctive requirements for their business and many of their objectives are interconnected. The use of VFT provided an objective map that can be used as a guide for future solutions.}
}
@article{LIU2022121968,
title = {Comparison of coal-to-ethanol product separation strategies},
journal = {Separation and Purification Technology},
volume = {301},
pages = {121968},
year = {2022},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2022.121968},
url = {https://www.sciencedirect.com/science/article/pii/S1383586622015234},
author = {Daoyan Liu and Hao Lyu and Jiahao Wang and Chengtian Cui and Jinsheng Sun},
keywords = {Coal-to-ethanol, Separation strategy, Differential evolution algorithm, Parallel computation, Heat integration},
abstract = {Given China's energy structure and the limitations of bioethanol, the coal-to-ethanol (CTE) pathway, from dimethyl ether to ethanol (DMTE) via carbonylation and hydrogenation, is highly anticipated. Ethanol, methanol, methyl acetate, and ethyl acetate are the crude hydrogenation products that need to be purified, requiring at least an eight-column scheme. However, the optimization of the existing separation strategy with ethanol as the priority is unfavorable in the following aspects: it is usually plagued by tedious rules of thumb and, due to the large scale of the process, is prone to falling into local minima; pre-designed heat integration inevitably neglects the interaction of parameter optimization and heat integration; reports on alternative feasible distillation sequences are scarce in publications, let alone comparisons amongst these counterparts. Therefore, four viable separation strategies are proposed in this paper to compare with this faulted separation strategy. A self-adapting dynamic differential evolution (SADDE) algorithm, which is accelerated by parallel computation, is used to search for optimal column parameters of all the configuration options and facilitates simultaneous heat integration structure synthesis. Two strategies stand out after 3000 generations of evolution. Splitting methanol outperforms in specific steam consumption (SSC) of ethanol (1.8177), much better than the benchmark (2.4840), and splitting ethyl acetate with ethyl acetate priority has the most competitive total annual cost (TAC), 23.98% lower than the benchmark. In summary, this paper provides a reference for optimizing complex distillation systems like CTE product separation, or more specifically, the DMTE route, before the appearance of the most suitable separation strategy in demand. Furthermore, it will also serve for the CTE superstructure to further explore the optimal distillation sequence.}
}
@article{KANCHANATAWAN2018168,
title = {Affective symptoms in schizophrenia are strongly associated with neurocognitive deficits indicating disorders in executive functions, visual memory, attention and social cognition},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {80},
pages = {168-176},
year = {2018},
note = {Peripheral markers of inflammation, oxidative & nitrosative stress pathways and memory functions as a new target of pharmacotherapy in depression},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2017.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S027858461730129X},
author = {Buranee Kanchanatawan and Supaksorn Thika and George Anderson and Piotr Galecki and Michael Maes},
keywords = {Major depression, Bipolar, Anxiety, Schizophrenia, CANTAB, Cognition},
abstract = {The aim of this study was to assess the neurocognitive correlates of affective symptoms in schizophrenia. Towards this end, 40 healthy controls and 80 schizophrenia patients were investigated with six tests of the Cambridge Neuropsychological Test Automated Battery (CANTAB), assessing spatial working memory, paired-association learning, one touch stocking, rapid visual information (RVP), emotional recognition test and intra/extradimensional set shifting. The Hamilton Depression (HDRS) and Anxiety (HAMA) Rating Scales and the Calgary Depression Scale for Schizophrenia (CDSS) as well as the Positive and Negative Syndrome Scale (PANSS) were also used. There were highly significant associations between all 6 CANTAB tests and HDRS, HAMA and CDSS (except RVP) scores. The most significant items associating with neurocognitive impairments in schizophrenia were self-depreciation (CDSS), fatigue, psychomotor retardation and agitation, psychic and somatic anxiety (HDRS), fears, cognitive symptoms, somatic-muscular, genito-urinary and autonomic symptoms and anxious behavior (HAMA). The selected HDRS and HAMA symptoms indicate fatigue, fears, anxiety, agitation, retardation, somatization and subjective cognitive complaints (SCC) and are therefore labeled “FAARS”. Up to 28.8% of the variance in the 6 CANTAB measurements was explained by FAARS, which are better predictors of neurocognitive impairments than the PANSS negative subscale score. Neurocognitive deficits in schizophrenia are best predicted by FAARS combined with difficulties in abstract thinking. In conclusion, depression and anxiety symptoms accompanying the negative and positive symptoms of schizophrenia are associated with neurocognitive deficits indicating disorders in executive functions, attention, visual memory, and social cognition. Neurocognitive deficits in schizophrenia reflect difficulties in abstract thinking and FAARS, including subjective cognitive complaints.}
}
@article{CHILMON2020106870,
title = {Modelling and simulation considerations for an end-to-end supply chain system},
journal = {Computers & Industrial Engineering},
volume = {150},
pages = {106870},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106870},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305659},
author = {Barbara Chilmon and Nicoleta S. Tipi},
keywords = {Simulation, End-to-end supply chain, Systematic literature review},
abstract = {The efforts of this review paper are twofold: to provide an insightful examination of various contributions to knowledge surrounding simulation methods within an end-to-end supply chain and to guide research agenda by indicating generic elements required to model such systems using simulation. The authors examined 255 publications from 21 peer-reviewed journals in the field of an end-to-end supply chain and simulation using a systematic literature review approach. Each publication was thoroughly reviewed to capture best practices and key characteristics relative to simulation modelling techniques used in the context of complex end-to-end supply chain systems. This allowed for identification of generic elements required to model such systems, which were grouped into Structural, Computational and System Organization pillars. This research contributes to the body of knowledge by defining generic aspects of simulation modelling techniques used to study properties and attributes of complex end-to-end supply chains. The paper advances the theoretical understanding of the simulation methods used and applicability of simulation methodology in modelling end-to-end supply chain systems. The research presents the key findings from the use of simulation in modelling end-to-end supply chains and the main ways in which this modelling technique has informed research and practise.}
}
@article{MUNEEPEERAKUL2012123,
title = {The effect of scaling and connection on the sustainability of a socio-economic resource system},
journal = {Ecological Economics},
volume = {77},
pages = {123-128},
year = {2012},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2012.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S092180091200081X},
author = {Rachata Muneepeerakul and Murad R. Qubbaj},
keywords = {Sustainability, Scaling, Connection, Bifurcation, Population dynamics},
abstract = {Policy makers dealing with complex systems oftentimes rely on “linear thinking.” This is understandable due to the ease and convenience offered by the simplicity of such conceptualization. Although this line of thinking may help facilitate decision making processes, it is only as defensible as the degree at which the system under consideration behaves linearly. Recent work shows that diverse properties of cities exhibit power-law relationships with population size. Such relationships may invalidate the reliance on linear thinking. Furthermore, in the era of globalization, resources and people move virtually freely through bounds of any confines used to define a system. We incorporate into a simple resource-population model the power-law scaling behavior and the influence of import and immigration, and investigate their effects on sustainable growth of communities. We explore through bifurcation analysis the different scenarios of how an unsustainable system could be sustained. Import can be effective if: the import exceeds a critical level and a critical mass of people populates the system. In contrast, increasing immigration alone can rescue the intrinsically unsustainable system, both directly through people entering the system and indirectly by increasing its harvesting ability, although critical values exist that cause the population to sharply rise or shrink.}
}
@article{RODRIGUEZMENDEZ2024103804,
title = {UK net-zero policy design – from optimisation to robustness},
journal = {Environmental Science & Policy},
volume = {158},
pages = {103804},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001382},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {Robust Decision Making, Deep Uncertainty, Greenhouse Gas Removal, Climate modelling, United Kingdom Net-Zero target},
abstract = {The need to deal with the deep uncertainty and system complexity associated to Net-Zero pathways, especially those relying on emergent greenhouse gas removal (GGR) technologies, has resulted in a growing body of literature on alternative decision-support approaches. Exploratory modelling, and specifically Robust Decision Making (RDM), are potential approaches capable of addressing these challenges: by exploring a wide range of conceivable futures, they explicitly embrace deep uncertainties while seeking to reduce system vulnerabilities. However, though RDM methods have been well documented, there is little insight as to how such approach might be integrated into Net-Zero policy design processes. By means of a workshop (n=17) and interviews (n=13) with the UK climate policy and energy modelling communities, this contribution provides insights into the role and potential of RDM in explicitly dealing with the deep uncertainties that pervade in the establishment of a 60–100 MtCO2 UK GGR sector within three decades. The consultation process revealed that there is an appetite from the decision-making and analytical communities in integrating exploratory modelling concepts into UK policy design processes. It is recommended that to bridge the gap between theoretical RDM constructs and their broader adoption, the analytical process should include a broader set of disciplines and expertise. Specifically for the modelling community, this work suggests that in-use computational models should be adapted, rather than new tools developed. Key challenges also arise from the time and resources required, suggesting small scale place-based pilots could promote the acceptability and foster the adoption of the RDM methodology.}
}
@article{HARWOOD201610,
title = {Locking up passwords – for good},
journal = {Network Security},
volume = {2016},
number = {4},
pages = {10-13},
year = {2016},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(16)30037-X},
url = {https://www.sciencedirect.com/science/article/pii/S135348581630037X},
author = {Will Harwood},
abstract = {It's clear that bulk identity thefts – that is, the mass stealing of passwords or other personally identifiable information (PII) – are among the most harmful types of cyber-attack faced by businesses. They're a huge problem, not only in terms of the damage each attack causes, but also the volume of attacks overall. A cursory glance over the business headlines for the past few years announces huge password or PII thefts from organisations ranging from Sony PlayStation to eBay and Facebook to JP Morgan. We were barely a week into 2016 when it was revealed that email passwords for up to 320,000 users had been stolen from Time Warner. Bulk identity thefts are among the most harmful types of cyber-attack faced by businesses today and part of the problem is that businesses, security firms and cyber-criminals all share the same playing field. Thinking beyond standard computing architectures is the only solution to the ongoing arms race between hackers and security vendors. In a battle against cyber-criminality, in which businesses are always playing catch-up, this is a way of getting on the front foot and beginning to operate in a world beyond the attackers' reach, says Dr Will Harwood of Silicon:SAFE.}
}
@article{ZANUY2006330,
title = {Computational Study of the Fibril Organization of Polyglutamine Repeats Reveals a Common Motif Identified in β-Helices},
journal = {Journal of Molecular Biology},
volume = {358},
number = {1},
pages = {330-345},
year = {2006},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2006.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0022283606001112},
author = {David Zanuy and Kannan Gunasekaran and Arthur M. Lesk and Ruth Nussinov},
keywords = {protofibril conformation, polyglutamine repeats, β-helices, structural analysis, huntingtin protein},
abstract = {The formation of fibril aggregates by long polyglutamine sequences is assumed to play a major role in neurodegenerative diseases such as Huntington. Here, we model peptides rich in glutamine, through a series of molecular dynamics simulations. Starting from a rigid nanotube-like conformation, we have obtained a new conformational template that shares structural features of a tubular helix and of a β-helix conformational organization. Our new model can be described as a super-helical arrangement of flat β-sheet segments linked by planar turns or bends. Interestingly, our comprehensive analysis of the Protein Data Bank reveals that this is a common motif in β-helices (termed β-bend), although it has not been identified so far. The motif is based on the alternation of β-sheet and helical conformation as the protein sequence is followed from the N to the C termini (β-αR-β-polyPro-β). We further identify this motif in the ssNMR structure of the protofibril of the amyloidogenic peptide Aβ1-40. The recurrence of the β-bend suggests a general mode of connecting long parallel β-sheet segments that would allow the growth of partially ordered fibril structures. The design allows the peptide backbone to change direction with a minimal loss of main chain hydrogen bonds. The identification of a coherent organization beyond that of the β-sheet segments in different folds rich in parallel β-sheets suggests a higher degree of ordered structure in protein fibrils, in agreement with their low solubility and dense molecular packing.}
}
@article{STEFIK1989241,
title = {Computation and cognition: Toward a foundation of cognitive science: Z.W. Pylyshyn, (MIT Press, Cambridge, MA, 1986); 292 pages, $33.75 (hardcover), $9.95 (paperback)},
journal = {Artificial Intelligence},
volume = {38},
number = {2},
pages = {241-247},
year = {1989},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(89)90061-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370289900611},
author = {Mark Stefik}
}
@article{THAGARD1986301,
title = {Parallel computation and the mind-body problem},
journal = {Cognitive Science},
volume = {10},
number = {3},
pages = {301-318},
year = {1986},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(86)80020-9},
url = {https://www.sciencedirect.com/science/article/pii/S0364021386800209},
author = {Paul Thagard},
abstract = {The position in the philosophy of mind called functionalism claims that mental states are to be understood in terms of their functional relationships to other mental states, not in terms of their material instantiation in any particular kind of hardware. But the argument that material instantiation is irrelevant to functional relationships is computationally naive. This paper uses recent work on parallel computation to argue that software and hardware are much more intertwined than the functionalists allow. Parallelism offers qualitative as well as quantitative advantages, leading to different styles of programming as well as increased speed. Hence hardware may well matter to the mental: only by further empirical investigations of the relation between the mind and brain and between artificial intelligence software and underlying hardware will we be able to achieve a defensible solution to the mind-body problem. The major disadvantage of parallel systems is the need to coordinate their subprocesses, but recent proposals that consciousness provides a serial control for parallel computation are implausible.}
}
@article{BARNES2021,
title = {Gene Expression and Data Analysis Pipeline Using Cancer BioPortal in the Classroom},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {1},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v22i1.2315},
url = {https://www.sciencedirect.com/science/article/pii/S1935787721000277},
author = {Chassidy N. Barnes and Blake P. Johnson and Stefanie W. Leacock and Ruben M. Ceballos and Lori L. Hensley and Nathan S. Reyna},
abstract = {At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis.
ABSTRACT
At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis. Cancer BioPortal (cBioPortal) is an open-access collective cancer research resource for storing and exploring clinical, genomic, proteomic, and transcriptomic data. cBioPortal eliminates the computational barrier of interpreting complex genomic data by providing easily understandable visualization that can be interpreted and translated into relevant biological insights. Because no prior computational knowledge is required, cBioPortal is an ideal educational tool for either in-person or distance learning environments. We developed a pedagogical approach, video tutorials, and data analysis workflows centered on using cBioPortal. Pedagogically, students develop an initial research outline that is continually updated and graded throughout the project. Progress during the project or course is assessed by a series of student presentations that are 5 to 15 min in length and are aimed at explaining the approach used in data acquisition, interpretation of the data, and relevance to the initial hypothesis. While cancer-specific, this analysis platform appeals to a wide range of classes and student interests. Further, the project has been successfully done both as an independent research experience and as part of a virtual class-based research project.}
}
@article{CASTROSCHEZ201465,
title = {Experience applying language processing techniques to develop educational software that allow active learning methodologies by advising students},
journal = {Journal of Network and Computer Applications},
volume = {41},
pages = {65-79},
year = {2014},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2013.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804513002166},
author = {J.J. Castro-Schez and M.A. Redondo and F. Jurado and J. Albusac},
keywords = {Environment for active learning, Formal languages techniques, Automatic assessment},
abstract = {This paper is focused on those systems that allow students to build their own knowledge by providing them with feedback regarding their actions while performing a problem based learning activity or while making changes to problem statements, so that a higher order thinking skill can be achieved. This feedback is the consequence of an automatic assessment. Particularly, we propose a method that makes use of Language Processor techniques for developing these kinds of systems. This method could be applied in subjects in which problem statements and solutions can be formalized by mean of a formal language and the problems can be solved in an algorithmic way. The method has been used to develop a number of tools that are partially described in this paper. Thus, we show that our approach is applicable in addressing the development of the aforementioned systems. One of these tools (a virtual laboratory for language processing) has been in use for several years in order to support home assignments. The data collected for these years are presented and analyzed in this paper. The results of the analysis confirm that this tool is effective in facilitating the achievement of learning outcomes.}
}
@article{UBAN2021480,
title = {An emotion and cognitive based analysis of mental health disorders from social media data},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {480-494},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001825},
author = {Ana-Sabina Uban and Berta Chulvi and Paolo Rosso},
keywords = {Mental health disorders, Early risk prediction, Emotions, Cognitive styles, Deep learning, Social media},
abstract = {Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.}
}
@article{DUBEY2020118,
title = {Understanding exploration in humans and machines by formalizing the function of curiosity},
journal = {Current Opinion in Behavioral Sciences},
volume = {35},
pages = {118-124},
year = {2020},
note = {Curiosity (Explore vs Exploit)},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301108},
author = {Rachit Dubey and Thomas L Griffiths},
abstract = {Recent work in machine learning has demonstrated the benefits of providing artificial agents with a sense of curiosity—a form of intrinsic reward that supports exploration. Two strategies have emerged for defining these rewards: favoring novelty and pursuing prediction errors. Psychological theories of curiosity have also emphasized these two factors. We show how these two literatures can be connected by understanding the function of curiosity, which requires thinking about the abstract computational problem that both humans and machines face as they explore their world.}
}
@article{CAMARGO2022496,
title = {Existence, Hypotheses and Categories in Knowledge Representation},
journal = {Procedia Computer Science},
volume = {213},
pages = {496-503},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.096},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017872},
author = {Eduardo Camargo and Eduardo Yuji Sakabe and Ricardo Gudwin},
keywords = {Knowledge Representation, Cognitive Architecture, Cognitive Semiotics, Artificial Intelligent Agent},
abstract = {Cognitive architectures employ different means for knowledge representation. In this work, we describe how the Cognitive Systems Toolkit (CST), a toolkit for the construction of cognitive architectures addresses the issue of knowledge representation, by introducing the notion of a computational idea, as being an abstract and generic building block for representing multiple different pieces of knowledge. We particularly address how computational ideas can be used to represent both facts that really happened at an environment and just hypothesis that are not to be considered as being a part of existence, explaining how these are instances of general categories. At the end, we provide different examples to illustrate the subtle differences that are possible to be represented using this knowledge representation scheme.}
}
@article{LIN2024200448,
title = {Maximizing the spread of information through content optimization},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200448},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200448},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001224},
author = {Lei Lin and Yihua Du and Shibo Zhao and Wenkang Jiang and Qirui Tang and Li Xu},
keywords = {Computational social science, Human-in-the-loop, System simulation and optimization, Computational journalism, Computational advertisement},
abstract = {As data-driven prediction models advance, an increasing number of people are enjoying news personalized to their interests. The primary problem such recommendation models solve is to precisely match information with users and, in so doing, ensure that news spreads with greater efficiency. However, these techniques only help the media platform; they do not help those who produce the news. Hence, we devised a propagation framework based on a human-in-the-loop simulation that helps content authors maximize the spread of their messages through social networks. The framework works by acting on feedback provided by the simulation model. Additionally, the spread of information is formulated as a multi-objective optimization problem in which propagation is data-driven and simulated with machine learning techniques that leverage data on the historical behaviors of users. We additionally describe an implementation for this framework as an example of how the framework might be used in real life. On the practical side, the implementation uses text data from a blog to simulate the message's propagation, while, from a technical point of view, the multi-objective optimization problem is divided into an information retrieval problem and an integer programming problem, the results of which are fed back into the content editor as content operation strategies. A case study with the Sina Weibo microblog site not only validates the framework but also provides practitioners with insights into how to maximize the spread of information through social networking platforms. The results show that the proposed propagation framework is capable of increasing retweets by 7.9575 %. As an interesting aside, our experiments also show that the Weibo retweet lottery is both popular and a highly effective mechanism for increasing reposts.}
}
@article{MEICHENBAUM1969101,
title = {The effects of instructions and reinforcement on thinking and language behavior of schizophrenics},
journal = {Behaviour Research and Therapy},
volume = {7},
number = {1},
pages = {101-114},
year = {1969},
issn = {0005-7967},
doi = {https://doi.org/10.1016/0005-7967(69)90054-0},
url = {https://www.sciencedirect.com/science/article/pii/0005796769900540},
author = {Donald H. Meichenbaum},
abstract = {Six experimental groups and 2 control groups (N=48) were used to investigate the relative effectiveness of prolonged training of schizophrenics with contingent social and token reinforcement on (a) the level of abstraction as measured on a proverbs task, (b) the percentage of “sick talk” (% ST) emitted in a structured interview, (c) both verbal response classes of proverb abstraction and % ST. Prior to treatment, schizophrenic Ss compared with 20 nonpsychiatric hospitalized medical patients were significantly inferior on the proverbs task and emitted five times more ST in a structured interview. The results indicated that the experimental treatments were effective in decreasing % ST and increasing abstraction to proverbs with token reinforcement being most effective. Evidence for response and stimulus generalization was obtained.}
}
@article{VEGA2008255,
title = {The catwalk task: Reflections and synthesis: Part 2},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {4},
pages = {255-263},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2009.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312309000042},
author = {Emiliano Vega and Shawn Hicks},
keywords = {Modeling, Representation, Teacher learning, Task design},
abstract = {In this article we recount our experiences with a series of encounters with the catwalk task and reflect on the professional growth that these opportunities afforded. First, we individually reflect on our own mathematical work on the catwalk task. Second, we reflect on our experiences working with a group of community college students on the catwalk task and our interpretations of their mathematical thinking. In so doing we also detail a number of innovative and novel student-generated representations of the catwalk photos. Finally, we each individually reflect on the entire experience with the catwalk problem, as mathematics learners, as teachers, and as professionals.}
}
@article{1995146,
title = {95/02164 Sequential pressure-based Navier-Stokes algorithms on SIMD computers: Computational issues},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {2},
pages = {146},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)93829-X},
url = {https://www.sciencedirect.com/science/article/pii/014067019593829X}
}
@incollection{STEEDMAN2011925,
title = {21 - Temporality},
editor = {Johan {van Benthem} and Alice {ter Meulen}},
booktitle = {Handbook of Logic and Language (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {London},
pages = {925-969},
year = {2011},
isbn = {978-0-444-53726-3},
doi = {https://doi.org/10.1016/B978-0-444-53726-3.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444537263000219},
author = {Mark Steedman},
keywords = {tense, aspect, natural language semantics, computational semantics, temporal semantics, aktionsarten, causality, evidentiality},
abstract = {Publisher Summary
In thinking about the logical and computational semantics of temporal categories in natural languages, issues of temporal ontology, or metaphysics, must be distinguished from issues of temporal relation. The first thing to observe about the temporal ontology implicit in natural languages is that it is not purely temporal. To take a simple example, the English perfect, when predicated of an event like losing a watch, says that some contextually retrievable consequences of the event in question hold at the time under discussion. Thus, conjoining such a perfect with a further clause denying those consequences is infelicitous. The claim that the semantics depends directly on the conceptual representation of action and contingency suggests that this semantics might be universal, despite considerable differences in its syntactic and morphological encoding across languages. The work described in this chapter suggests that such differences across languages are superficial. Ironically, the English tense/aspect system seems to be based on semantic primitives remarkably like those, which Whorf ascribed to Hopi. Matters of temporal sequence and temporal locality seem to be quite secondary to matters of perspective and contingency. This observation in turn suggests that the semantics of tense and aspect is profoundly shaped by concerns with goals, actions, and consequences, and that temporality in the narrow sense of the term is merely one facet of this system among many.}
}
@article{ZIA2022108066,
title = {SoFTNet: A concept-controlled deep learning architecture for interpretable image classification},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {108066},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108066},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101145X},
author = {Tehseen Zia and Nauman Bashir and Mirza Ahsan Ullah and Shakeeb Murtaza},
keywords = {Interpretability, Concepts, KNN, Explanation satisfaction},
abstract = {Interpreting deep learning (DL)-based computer vision models is challenging due to the complexity of internal representations. Most recent techniques for rendering DL learning outcomes interpretable operate on low-level features rather than high-level concepts. Methods that explicitly incorporate high-level concepts do so through a determination of the relevancy of user-defined concepts or else concepts extracted directly from the data. However, they do not leverage the potential of concepts to explain model predictions. To overcome this challenge, we introduce a novel DL architecture – the Slow/Fast Thinking Network (SoFTNet) – enabling users to define/control high-level features and utilize them to perform image classification predicatively. We draw inspiration from the dual-process theory of human thought processes, decoupling low-level, fast & non-transparent processing from high-level, slow & transparent processing. SoFTNet hence uses a shallow convolutional neural network for low-level processing in conjunction with a memory network for high-level concept-based reasoning. We conduct experiments on the CUB-200-2011 and STL-10 datasets and also present a novel concept-based deep K-nearest neighbor approach for baseline comparisons. Our experiments show that SoFTNet achieves comparable performance to state-of-art non-interpretable models and outperforms comparable interpretative methods.}
}
@article{SCHINCKUS20094415,
title = {Economic uncertainty and econophysics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {388},
number = {20},
pages = {4415-4423},
year = {2009},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2009.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437109005494},
author = {Christophe Schinckus},
keywords = {Econophysics, Uncertainty, Economics, Keynes, Knight, Hayek},
abstract = {The objective of this paper is to provide a methodological link between econophysics and economics. I will study a key notion of both fields: uncertainty and the ways of thinking about it developed by the two disciplines. After having presented the main economic theories of uncertainty (provided by Knight, Keynes and Hayek), I show how this notion is paradoxically excluded from the economic field. In economics, uncertainty is totally reduced by an a priori Gaussian framework—in contrast to econophysics, which does not use a priori models because it works directly on data. Uncertainty is then not shaped by a specific model, and is partially and temporally reduced as models improve. This way of thinking about uncertainty has echoes in the economic literature. By presenting econophysics as a Knightian method, and a complementary approach to a Hayekian framework, this paper shows that econophysics can be methodologically justified from an economic point of view.}
}
@article{SEYMOUR2020117212,
title = {Hierarchical models of pain: Inference, information-seeking, and adaptive control.},
journal = {NeuroImage},
volume = {222},
pages = {117212},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117212},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306984},
author = {Ben Seymour and Flavia Mancini},
keywords = {Pain, Nociception, Information theory, Reinforcement learning, Optimal control, Predictive coding, Epistemic value, Free energy principle, Endogenous modulation},
abstract = {Computational models of pain consider how the brain processes nociceptive information and allow mapping neural circuits and networks to cognition and behaviour. To date, they have generally have assumed two largely independent processes: perceptual inference, typically modelled as an approximate Bayesian process, and action control, typically modelled as a reinforcement learning process. However, inference and control are intertwined in complex ways, challenging the clarity of this distinction. Here, we consider how they may comprise a parallel hierarchical architecture that combines inference, information-seeking, and adaptive value-based control. This sheds light on the complex neural architecture of the pain system, and takes us closer to understanding from where pain ’arises’ in the brain.}
}
@article{RAMOS202335,
title = {An institutional modernization project in chemical engineering education in Brazil: Developing broader competencies for societal challenges},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {35-44},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000167},
author = {Bruno Ramos and Moisés Teles dos Santos and Ardson S. Vianna and Luiz Kulay},
keywords = {Process safety, Chemical reaction engineering, Education, Active-based learning},
abstract = {Contemporary societal challenges put in evidence the need to improve the hard and soft skills of chemical engineering students. To promote a more student-centered approach, active-based learning, and improved assessment strategies, the Brazilian government approved the so-called New National Curriculum Guidelines (NCG) for engineering courses. To comply with those guidelines, the Department of Chemical Engineering of the Polytechnic School of the University of São Paulo (USP) is currently developing an educational modernization process sponsored by the Fulbright Commission in Brazil, called Special Program for Modernization of Undergraduate Education (PMG). The project is based on three pillars of modernization: content (what), form (how), and infrastructure (where). This paper describes initiatives in each of those pillars: content and format changes in Chemical Reaction Engineering and Process Safety courses and the creation of new spaces for a student-centered approach (an innovative classroom layout and a makerspace). By gathering two concrete classroom experiences guided by a broader institutional educational policies (the PMG project and the NCG), this paper highlights that slight changes can lead to great improvements in the learning process, leading to more engagement in the development of hard skills while favoring improvements in soft skills, such as communication, team-based work, and critical thinking.}
}
@article{COON1995787,
title = {Generalized block-tridiagonal matrix orderings for parallel computation in process flowsheeting},
journal = {Computers & Chemical Engineering},
volume = {19},
number = {6},
pages = {787-805},
year = {1995},
note = {Applications of Parallel Computing},
issn = {0098-1354},
doi = {https://doi.org/10.1016/0098-1354(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0098135494000816},
author = {A.B. Coon and M.A. Stadtherr},
abstract = {A new graph partitioning algorithm for use on structurally unsymmetric systems is presented. Unlike other partitioning algorithms that have been used to provide reorderings for structurally symmetric matrices, this algorithm employs a bipartite graph model, and hence, can be used to consider unsymmetric permutations of structurally unsymmetric matrices. It is shown that the algorithm can be used in identifying coarse-granular, balanced tasks in the direct solution of flowsheeting matrices by parallel techniques based on generalized block-tridiagonal and nested-block-tridiagonal matrix structures. It is also shown that such reorderings can be obtained inexpensively, in worst-case running times that increase linearly with the order of the matrix.}
}
@article{JOO2024226,
title = {Teaching and Learning Model for Artificial Intelligence Education},
journal = {Procedia Computer Science},
volume = {239},
pages = {226-233},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.166},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014091},
author = {Kil Hong Joo and Nam Hun Park},
keywords = {Artificial intelligence education, Lower grades in elementary school},
abstract = {AI education aims to nurture convergence talents equipped with various knowledge and AI capabilities. However, the educational programs developed so far are designed for less than 10 sessions. This is insufficient time for students to understand artificial intelligence algorithms, utilize the learned principles of artificial intelligence, and expand by converging with various knowledge. Since the developmental characteristics of students in the lower grades of elementary school are different, it is difficult to apply them as they are. Therefore, it is necessary to study the following AI education teaching and learning methods suitable for lower grade students. In this study, we develop a teaching system design model for artificial intelligence-based subject convergence education for elementary school students in the lower grades, and based on this, design and apply an artificial intelligence-based subject convergence education program. Experiments were conducted with 47 second-year elementary school students, and students’ responses were better in the AI-based convergence education program than in general subject classes in terms of interest, understanding, and expectations for classes.}
}
@article{GARG2024101391,
title = {Molecular Mechanics Demonstrate S-COMT as promising therapeutic receptor when analyzed with secondary plant metabolites},
journal = {Journal of the Indian Chemical Society},
volume = {101},
number = {11},
pages = {101391},
year = {2024},
issn = {0019-4522},
doi = {https://doi.org/10.1016/j.jics.2024.101391},
url = {https://www.sciencedirect.com/science/article/pii/S0019452224002711},
author = {Deepanshu Garg and Aarya Vashishth and Maharsh Jayadeep Jayawant and Virupaksha A. Bastikar},
keywords = {S-COMT receptor, Depression, Plant secondary metabolites, Molecular docking, Molecular dynamic simulation},
abstract = {Major depressive disorder (MDD) and other psychiatric conditions are debilitating illnesses affecting millions globally. Catechol-O-methyltransferase (COMT), an enzyme that regulates dopamine and norepinephrine breakdown in the brain, has emerged as a potential therapeutic target for these disorders. This study explores the inhibitory potential of plant secondary metabolites against S-COMT using computational techniques. COMT exists in two isoforms: membrane-bound COMT (MB-COMT), primarily found in brain neurons, and soluble COMT (S-COMT), present in peripheral tissues. S-COMT, particularly in the prefrontal cortex, is crucial for regulating neurotransmitters and maintaining cognitive function. Studies suggest S-COMT variants might be linked to the development of depression, schizophrenia, and other psychiatric disorders. Current COMT inhibitors often suffer from limitations, necessitating the exploration of novel therapeutic strategies. This study employed in-silico methods to investigate plant secondary metabolites as potential S-COMT inhibitors. Here, we describe the S-COMT protein structure retrieval and validation, followed by molecular docking simulations to identify plant compounds with the strongest binding affinity to the receptor's active site. Key amino acid residues involved in these interactions were also analyzed. Furthermore, molecular dynamics simulations were conducted to assess the stability of the top-scoring protein-ligand complexes over a 100-ns timeframe. The results explored the stability of ligand binding within the active site and its impact on the overall conformation of the S-COMT receptor. Our findings highlight promising therapeutic potential for these plant-derived compounds. Further in vitro and in vivo studies are warranted to validate their efficacy and safety for potential clinical applications in treating S-COMT-related disorders.
Subjects
Bioinformatics and Computational Biology, Proteomics, Neurogenerative Diseases.}
}
@article{MU2025109748,
title = {Adaptive model-agnostic meta-learning network for cross-machine fault diagnosis with limited samples},
journal = {Engineering Applications of Artificial Intelligence},
volume = {141},
pages = {109748},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109748},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624019079},
author = {Mingzhe Mu and Hongkai Jiang and Xin Wang and Yutong Dong},
keywords = {Fault diagnosis, Rotating machine, Adaptive model-agnostic meta-learning network, Cross-machine, Limited samples},
abstract = {Deep learning-based methods have been extensively studied in rotating machinery defect diagnosis. However, training an accurate and robust diagnostic model is still a challenge under severe domain bias and limited samples. For this reason, a new adaptive model-agnostic meta-learning (AMAML) is proposed for cross-machine fault diagnosis with limited samples. First, a novel adaptive feature encode network is built, incorporating lightweight spatial-bilateral channel attention. This enables the network to extract critical fault information in multiple dimensions adaptively within limited samples, which improves the learning efficiency of generalized diagnostic knowledge. Then, an adaptive loss computation (ALC) method is devised, which inventively realizes the interaction between loss computation and model performance. The underfitting and overfitting dilemmas under few-shot conditions are tackled by ALC. Finally, an adaptive meta-optimization strategy is proposed for dynamically adapting the update strategy of the base learner, so that the model is always optimized in the direction of strong generalizability while obtaining high performance. Six cross-machine diagnosis tasks are conducted to verify the effectiveness of AMAML. The average diagnostic accuracy of the AMAML under the 5-shot setting reached 97.42%. Experiments confirm that AMAML is superior to other prevailing methods and is potentially promising for engineering applications.}
}
@article{ALEXIOU2009623,
title = {Exploring the neurological basis of design cognition using brain imaging: some preliminary results},
journal = {Design Studies},
volume = {30},
number = {6},
pages = {623-647},
year = {2009},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000313},
author = {K. Alexiou and T. Zamenopoulos and J.H. Johnson and S.J. Gilbert},
keywords = {design cognition, problem solving, design problems, research methods, cognitive neuroscience},
abstract = {The paper presents a pilot interdisciplinary research study carried out as a step towards understanding the neurological basis of design thinking. The study involved functional magnetic resonance imaging (fMRI) of volunteers while performing design and problem-solving tasks. The findings suggest that design and problem solving involve distinct cognitive functions associated with distinct brain networks. The paper introduces the methodology, presents the findings, and discusses the potential role of brain imaging in design research.}
}
@article{KAMARI2017330,
title = {Sustainability focused decision-making in building renovation},
journal = {International Journal of Sustainable Built Environment},
volume = {6},
number = {2},
pages = {330-350},
year = {2017},
issn = {2212-6090},
doi = {https://doi.org/10.1016/j.ijsbe.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S221260901730064X},
author = {Aliakbar Kamari and Rossella Corrao and Poul Henning Kirkegaard},
keywords = {Sustainability, Building renovation, Decision support, Knowledge management, Soft Systems Methodology (SSM), Value Focused Thinking (VFT)},
abstract = {An overview of recent research related to building renovation has revealed that efforts to date do not address sustainability issues comprehensively. The question then arises in regard to the holistic sustainability objectives within building renovation context. In order to deal with this question, the research adopts a multi-dimensional approach involving literature review, exploration of existing assessment methods and methodologies, individual and focus group interviews, and application of Soft Systems Methodologies (SSM) with Value Focused Thinking (VFT). In doing so, appropriate data about sustainability objectives have been collected and structured, and subsequently verified using a Delphi study. A sustainability framework was developed in cooperation with University of Palermo and Aarhus University to audit, develop and assess building renovation performance, and support decision-making during the project’s lifecycle. The paper represents the results of research aiming at addressing sustainability of the entire renovation effort including new categories, criteria, and indicators. The developed framework can be applied during different project stages and to assist in the consideration of the sustainability issues through support of decision-making and communication with relevant stakeholders. Early in a project, it can be used to identify key performance criteria, and later to evaluate/compare the pros and cons of alternative retrofitting solutions either during the design stage or upon the project completion. According to the procedure of the consensus-based process for the development of an effective sustainability decision-making framework which was employed in this study, the outcome can also be considered as an outset step intended for the establishment of a Decision Support Systems (DSS) and assessment tool suited to building renovation context.}
}
@article{DONALDSON1995301,
title = {Building object-oriented systems: An introduction from concepts to implementation in c++: R. E. Callan, Computational Mechanics Publications, Southampton, UK, 1994. ISBN 1-85312-340-4. 304 pp. £47.00},
journal = {Artificial Intelligence in Engineering},
volume = {9},
number = {4},
pages = {301},
year = {1995},
note = {Selected Papers from the 1994 Japan/Korea Joint Conference on Expert Systems},
issn = {0954-1810},
doi = {https://doi.org/10.1016/0954-1810(95)90016-0},
url = {https://www.sciencedirect.com/science/article/pii/0954181095900160},
author = {Iain Donaldson}
}
@article{ENE2016973,
title = {A genetic algorithm for minimizing energy consumption in warehouses},
journal = {Energy},
volume = {114},
pages = {973-980},
year = {2016},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2016.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0360544216311586},
author = {Seval Ene and İlker Küçükoğlu and Aslı Aksoy and Nursel Öztürk},
keywords = {Genetic algorithm, Green supply chain, Minimization of energy consumption, Warehouse management},
abstract = {Green supply chain management is generally defined as integration of green thinking and environmental issues into the whole supply chain operations like product design, manufacturing process, warehousing, distribution etc. Within this context green principles should be adopted in warehouse management to minimize negative impact on the environment. In warehouse operations, picking must be analyzed attentively which is widely studied in literature for minimizing service time levels because of its close relation to the higher costs. The efficiency of picking in warehouses mainly depends on storage assignment policy that directly affects picking performance in warehouses. In this paper, picking operation in warehouses is studied to minimize energy consumption with proper storage policy other than service time. Genetic algorithm (GA) is proposed to solve the problem and numerical examples are presented to demonstrate the performance of the GA. Results show that, the GA gives efficient solutions to the problem.}
}
@article{PAILLARD2025101182,
title = {GREEN: A lightweight architecture using learnable wavelets and Riemannian geometry for biomarker exploration with EEG signals},
journal = {Patterns},
pages = {101182},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101182},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000303},
author = {Joseph Paillard and Jörg F. Hipp and Denis A. Engemann},
keywords = {electroencephalography, EEG, biomarkers, deep learning, wavelets, Riemannian geometry, brain-computer interface, BCI},
abstract = {Summary
Spectral analysis using wavelets is widely used for identifying biomarkers in EEG signals. Recently, Riemannian geometry has provided an effective mathematical framework for predicting biomedical outcomes from multichannel electroencephalography (EEG) recordings while showing concord with neuroscientific domain knowledge. However, these methods rely on handcrafted rules and sequential optimization. In contrast, deep learning (DL) offers end-to-end trainable models achieving state-of-the-art performance on various prediction tasks but lacks interpretability and interoperability with established neuroscience concepts. We introduce Gabor Riemann EEGNet (GREEN), a lightweight neural network that integrates wavelet transforms and Riemannian geometry for processing raw EEG data. Benchmarking on six prediction tasks across four datasets with over 5,000 participants, GREEN outperformed non-deep state-of-the-art models and performed favorably against large DL models while using orders-of-magnitude fewer parameters. Computational experiments showed that GREEN facilitates learning sparse representations without compromising performance. By integrating domain knowledge, GREEN combines a desirable complexity-performance trade-off with interpretable representations.}
}
@article{KIM2024110348,
title = {Hierarchical aerial offload computing algorithm based on the Stackelberg-evolutionary game model},
journal = {Computer Networks},
volume = {245},
pages = {110348},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110348},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624001804},
author = {Sungwook Kim},
keywords = {Aerial access networks, High-altitude platforms, Unmanned aerial vehicles, Stackelberg-evolutionary game, Multi-objective bargaining solution},
abstract = {Aerial access networks have been envisioned as a promising 6 G technology to enhance the service experience in underserved areas where terrestrial base stations do not exist. In such scenarios, a hierarchical model of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs) is considered to provide aerial computing services for ground Internet of Things (IoT) devices. In this study, we investigate a hierarchical aerial computing system to optimally orchestrate the limited computation resources in both HAPs and UAVs. For offloading services, we formulate a joint resource allocation problem to maximize service satisfaction for terrestrial IoT devices. To solve this problem, we employ the ideas of game theory with centralized decision and decentralized execution. Through the Stackelberg-evolutionary game model, the HAP works as a leader, and selects its price strategy based on the evolutionary learning process. As followers, individual UAVs make decisions to partially offload their computing tasks by considering different objectives. According to the interactive control paradigm, our proposed method can get reciprocal advantages for HAPs, UAVs, and ground IoT devices while adaptively handling dynamic aerial network conditions. Finally, extensive simulation results verify the efficiency of our proposed algorithm to increase the usability of edge servers’ computational resources. Compared with other existing state-of-the-art aerial network offloading protocols, we can improve the profits of system throughput, resource usability and UAV fairness up to 10 %, 10 %, and 15 %, respectively.}
}
@article{MOLNAR20152667,
title = {Three Dimensional Applications in Teaching and Learning Processes},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {2667-2673},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.600},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815028608},
author = {György Molnár and András Benedek},
keywords = {ICT, 3D interactive system, new learning potential, Leonar3Do ;},
abstract = {In the world of today's information society the torrent of information we are dailyfaced with has to be appropriately transformed and translated in order to yield representationswe are somehow capable of understanding. By extending 2D representations to three-dimensional ones, pictorialcontents become more lifelike, getting closer to practice, creating the basis for a new view ofpictorial thinking, giving rise to the emergence to a very effective method of dealing withinformation overload. To depict three-dimensionalreality onto a two-dimensional plane of course constitutes an age-old scientific problem, theprincipal aim of the technique sought after being the exact representation.We also present a general review of Hungarian and international experienceson ICT application and its environment that comply with current practice.}
}
@article{PEYRACHE2024255,
title = {A homothetic data generated technology},
journal = {European Journal of Operational Research},
volume = {316},
number = {1},
pages = {255-267},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S037722172400050X},
author = {Antonio Peyrache},
keywords = {Data envelopment analysis, Input homotheticity, Free disposal hull, Efficiency},
abstract = {I propose a method for constructing an enlargement of a variable returns to scale production technology that will satisfy homotheticity. The method can be used both with DEA and FDH single output (or single input) technologies and it is computationally fast. The method is constructed by adding a restriction to the axiomatically delineated homothetic reference technologies which requires these reference technologies to be subsets of the minimal reference technology that satisfies constant returns to scale. Within this set it is possible to identify a homothetic technology that satisfies the property of minimum extrapolation.}
}
@incollection{KARALIS2024215,
title = {Chapter 6 - Artificial intelligence in drug discovery and clinical practice},
editor = {Natassa Pippa and Costas Demetzos and Maria Chountoulesi},
booktitle = {From Current to Future Trends in Pharmaceutical Technology},
publisher = {Academic Press},
pages = {215-255},
year = {2024},
isbn = {978-0-323-91111-5},
doi = {https://doi.org/10.1016/B978-0-323-91111-5.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911115000068},
author = {Vangelis D. Karalis},
keywords = {Artificial intelligence, Drug discovery, Clinical practice, Machine learning},
abstract = {Artificial intelligence (AI) is the imitation of human intelligence by computers. It is the science of creating intelligent machines capable of performing tasks equivalent or superior to those performed by humans. The process involves collecting data, formulating rules for its use, making approximate or definitive determinations, and self-correcting. In particular, artificial intelligence and machine learning (ML) have attracted considerable attention in a variety of sectors, including pharmaceutical sciences, and have led to a rapid rise in new applications for machine learning in numerous areas of pharmaceutical sciences. In computational chemistry, deep learning models have been used to predict drug-target interactions, develop new compounds, and predict pharmacokinetics. AI, robotics, and advanced computing have applications in drug repurposing, quality-by-design, 3D printing, and nanomedicine. When used properly, AI techniques can improve patient treatment, detection and reduction of risk factors, and identification of complications.}
}
@article{GRUNAU2025107014,
title = {General Polyhedral Approximation of two-stage robust linear programming for budgeted uncertainty},
journal = {Computers & Operations Research},
volume = {179},
pages = {107014},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.107014},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000425},
author = {Lukas Grunau and Tim Niemann and Sebastian Stiller},
keywords = {Robust optimization, Two-stage robust optimization, Linear programming, Approximation algorithm, Transportation location problem},
abstract = {We consider two-stage robust linear programs with uncertain righthand side. We develop a General Polyhedral Approximation (GPA), in which the uncertainty set U is substituted by a finite set of polytopes derived from the vertex set of an arbitrary polytope that dominates U. The union of the polytopes need not contain U. We analyze and computationally test the performance of GPA for the frequently used budgeted uncertainty set U (with m rows). For budgeted uncertainty affine policies are known to be best possible approximations (if coefficients in the constraints are nonnegative for the second-stage decision). In practice calculating affine policies typically requires inhibitive running times. Therefore an approximation of U by a single simplex has been proposed in the literature. GPA maintains the low practical running times of the simplex based approach while improving the quality of approximation by a constant factor. The generality of our method allows to use any polytope dominating U (including the simplex). We provide a family of polytopes that allows for a trade-off between running time and approximation factor. The previous simplex based approach reaches a threshold at Γ>m after which it is not better than a quasi nominal solution. Before this threshold, GPA significantly improves the approximation factor. After the threshold, it is the first fast method to outperform the quasi nominal solution. We exemplify the superiority of our method by a fundamental logistics problem, namely, the Transportation Location Problem, for which we also specifically adapt the method and show stronger results.}
}
@article{YECKEL19971379,
title = {Parallel computation of incompressible flows in materials processing: Numerical experiments in diagonal preconditioning},
journal = {Parallel Computing},
volume = {23},
number = {9},
pages = {1379-1400},
year = {1997},
note = {Parallel computing methods in applied fluid mechanics},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(97)00059-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819197000598},
author = {Andrew Yeckel and Jeffrey J. Derby},
keywords = {Incompressible flow, Finite element method, Preconditioning, Iterative solution, Linear systems},
abstract = {Massively parallel computing is enabling dramatic advances in the simulation of three-dimensional flows in materials processing systems. This study focuses on the efficiency and robustness of parallel algorithms applied to such systems. Specifically, various diagonal preconditioning schemes are tested for the iterative solution of the linear equations arising from Newton's method applied to finite element discretizations. Two finite element discretizations are considered — the classical Galerkin and the Galerkin/least-squares method. Results show that the choice of preconditioning method can greatly influence the rate of convergence, but that no type worked uniformly well in all cases.}
}
@article{LOWENSTEIN20191237,
title = {Visual perception, cognition, and error in dermatologic diagnosis: Diagnosis and error},
journal = {Journal of the American Academy of Dermatology},
volume = {81},
number = {6},
pages = {1237-1245},
year = {2019},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0190962219303251},
author = {Eve J. Lowenstein and Richard Sidlow and Christine J. Ko},
keywords = {cognitive error, diagnostic error, heuristic, metacognition, patient safety, visual intelligence},
abstract = {Diagnostic error in dermatology is a large practice gap that has received little attention. Diagnosis in dermatology relies heavily on a heuristic approach that is responsible for our perception of clinical findings. To improve our diagnostic accuracy, a better understanding of the strengths and limitations of heuristics (cognitive shortcuts) used in dermatology is essential. Numerous methods have been proposed to improve diagnostic accuracy, including brain training, reducing cognitive load, and getting feedback and second opinions. Becoming comfortable with the uncertainty intrinsic to medicine is essential. Ultimately, the practice of metacognition, or thinking about how we think, can offer corrective insights to improve accuracy in diagnosis.}
}
@article{SCHIFERL1997249,
title = {Evolution of plastic anisotropy for high-strain-rate computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {143},
number = {3},
pages = {249-270},
year = {1997},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(96)01159-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782596011590},
author = {Sheila K. Schiferl and Paul J. Maudlin},
abstract = {A model for anisotropic material strength, and for changes in the anisotropy due to plastic strain, is described. This model has been developed for use in high-rate, explicit, Lagrangian multidimensional continuum-mechanics codes. The model handles anisotropies, in single-phase materials, in particular the anisotropies due to crystallographic texture—preferred orientations of the single-crystal grains. Textural anisotropies, and the changes in these anisotropies, depend overwhelmingly on the crystal structure of the material and on the deformation history. The changes, particularly for complex deformations, are not amenable to simple analytical forms. To handle this problem, the material model described here includes a texture code, or micromechanical calculation, coupled to a continuum code. The texture code updates grain orientations as a function of tensor plastic strain, and calculates the yield strength in different directions. A yield function is fitted to these yield ‘points’. For each computational cell in the continuum simulation, the texture code tracks a particular set of grain orientations. The orientations will change due to the tensor strain history, and the yield function will change accordingly. Hence, the continuum code supplies a tensor strain to the texture code, and the texture code supplies an updated yield function to the continuum code. Since significant texture changes require relatively large strains—typically, a few percent or more—the texture code is not called very often, and the increase in computer time is not excessive. The model was implemented, using a finite-element continuum code and a texture code specialized for hexagonal-close-packed crystal structures. The results for several uniaxial stress problems and an explosive-forming problem are shown.}
}
@article{FATAHI2016272,
title = {A fuzzy cognitive map model to calculate a user's desirability based on personality in e-learning environments},
journal = {Computers in Human Behavior},
volume = {63},
pages = {272-281},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216303685},
author = {Somayeh Fatahi and Hadi Moradi},
keywords = {Personality, Emotion, User's status, Desirability, E-learning},
abstract = {The recent research in artificial intelligence shows an increasing interest in the modeling of human behavior factors such as personality, mood, and emotion for developing human-friendly systems. That is why there is an interest in developing models and algorithms to determine a human's emotions while interacting with a system to improve the quality of the interaction. In this paper, we propose a computational model to calculate a user's desirability based on personality in e-learning environments. The desirability is one of the most important variables in determining a user's emotions. The model receives several e-learning environmental events and predicts the desirability of the events based on the user's personality and his/her goals. The proposed model has been evaluated in a simulated and real e-learning environment. The results show that the model formulates the relationship between personality and emotions with high accuracy.}
}
@incollection{COHEN20253,
title = {Chapter 1 - The evolution of machine learning: Past, present, and future},
editor = {Chhavi Chauhan and Stanley Cohen},
booktitle = {Artificial Intelligence in Pathology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {3-14},
year = {2025},
isbn = {978-0-323-95359-7},
doi = {https://doi.org/10.1016/B978-0-323-95359-7.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323953597000017},
author = {Stanley Cohen},
keywords = {Capsule, Core memory, Graphical, Instruction set, Neural networks, Neuromorphic computing, Probability, Statistics support vectors},
abstract = {The earliest computers were designed to perform complex calculations, and their architecture allowed for the storage of not only data but also instructions as to how to manipulate that data. This evolved to the point where the computer-processed data according to a structure model of the real world, expressible in mathematical terms. The computer did not learn but was merely following instructions. The next step was to create a set of instructions that would allow the computer to learn from experience, i.e., to extract its own rules from large amounts of data and use those rules for classification and prediction. This was the beginning of machine learning and has led to the field collectively defined as artificial intelligence (AI). A major breakthrough came with the implementation of algorithms that were loosely modeled on brain architecture, with multiple interconnecting units sharing weighted puts among them, organized in computational layers (deep learning). AI has already revolutionized many aspects of modern life and is finding application in biomedical research and clinical practice at an accelerating rate.}
}
@article{ZUO2010268,
title = {Integrating performance-based design in beginning interior design education: an interactive dialog between the built environment and its context},
journal = {Design Studies},
volume = {31},
number = {3},
pages = {268-287},
year = {2010},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000969},
author = {Qun Zuo and Wesley Leonard and Eileen E. MaloneBeach},
keywords = {performance-based design, interior design, design education, computer aided design, design process},
abstract = {This paper presents a new paradigm in interior design education in which building performance simulation was employed for decision making and design generation. Digital technology was intermixed with conventional paper-based media in the design process to explore formal, spatial and passive solar energy solutions. The intention of the study was to re-discover the value of computers in assisting design thinking and improving effective learning. The results indicated the Performance-Based Design approach resulted in an early awareness of sustainable energy for beginning interior design students. Further, it enhanced understanding of the mutual relationship between interior and exterior and between the built and natural environment. This paper acknowledged the achievements as well as limitations and future directions for the integration of Performance-Based Design into interior design curriculum.}
}
@article{WU2023100739,
title = {The development of teacher feedback literacy in situ: EFL writing teachers’ endeavor to human-computer-AWE integral feedback innovation},
journal = {Assessing Writing},
volume = {57},
pages = {100739},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100739},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000478},
author = {Peisha Wu and Shulin Yu and Yanqi Luo},
keywords = {Teacher feedback literacy, Second language writing, Teacher feedback, Computer-mediated feedback, Writing assessment},
abstract = {While recent years have witnessed increasing theoretical and empirical elaboration on the construct of teacher feedback literacy in higher education and second language education, little research has investigated the development of teacher feedback literacy, especially when teachers collaborate in an attempt to improve feedback strategies with technology. To fill this gap, the present study examined two L2 writing teachers taking the initiative to create, update, and implement a human-computer-automatic writing evaluation (AWE) integral feedback platform, and how such a feedback innovation process impacted their feedback literacy development. The analysis of multiple sources of data, including semi-structured interviews, stimulated recalls, class observation, and artifacts, revealed that the two teachers approached the innovation by orchestrating mediating tools, interacting dialogically with social agents, reflecting critically, and crossing boundaries. Through this process, the development of teacher feedback literacy occurred at varying rates across different aspects. Specifically, positive changes were effected in the teachers’ feedback thinking as well as feedback giving and sharing practices. However, the teachers’ feedback literacy in classroom practice did not seem to have generated as salient a positive outcome. Possible reasons are discussed regarding the scope of the feedback innovation and contextual constraints, and implications are offered. The study underscored L2 writing teacher feedback literacy as a developmental phenomenon molded by situated social practice.}
}
@article{GARBEY1990293,
title = {Massively parallel computation of conservation laws},
journal = {Parallel Computing},
volume = {16},
number = {2},
pages = {293-304},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90067-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090067J},
author = {Marc Garbey and David Levine},
keywords = {Cellular automata, Partial differential equations, Method of characteristics, Parallel algorithms, Conservation laws},
abstract = {We present a new method for computing solutions of conservation laws based on the use of cellular automata with the method of characteristics. The method exploits the high degree of parallelism available with cellular automata and retains important features of the method of characteristics. It yields high numerical accuracy and extends naturally to adaptive meshes and domain decomposition methods for perturbed conservation laws. We describe the method and its implementation for a Dirichlet problem with a single conservation law for the one-dimensional case. Numerical results for the one-dimensional law with the classical Burgers nonlinearity or the Buckley-Leverett equation show good numerical accuracy outside the neighborhood of the shocks. The error in the area of the shocks is of the order of the mesh size. The algorithm is well suited for execution on both massively parallel computers and vector machines. We present timing results for an Alliant FX/8, Connection Machine Model 2, and CRAY X-MP.}
}
@incollection{TSOTSOS1993261,
title = {The Role of Computational Complexity in Perceptual Theory},
editor = {Sergio C. Masin},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {99},
pages = {261-296},
year = {1993},
booktitle = {Foundations of Perceptual Theory},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)62776-4},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508627764},
author = {John K. Tsotsos},
abstract = {The validity of perceptual theories cannot be considered only in terms of how well the explanations fit experimental observations. Rather, it is argued that sufficient consideration must also be given to the physical realizability of the explanation. Experimental scientists attempt to explain their data and not just describe it, in essence, providing an algorithm whose behavior leads to the observed data. Thus, computational plausibility is not only an appropriate but a necessary consideration. One dimension of plausibility is satisfaction of the constraints imposed by the computational complexity of the problem, the resources available for the solution of the problem, and the specific algorithm proposed. It is shown that such constraints play critical roles in the explanations of perception, intelligent behavior, and evolution.}
}
@incollection{HUTCHINS20012068,
title = {Cognition, Distributed},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2068-2072},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01636-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016363},
author = {E. Hutchins},
abstract = {Distributed cognition is a framework for thinking about cognition which seeks to understand how the cognitive properties of aggregates emerge from the interactions of component parts. It can be applied to cognitive systems at many levels of complexity, from areas of an individual brain to communities of interacting persons. Distributed cognition is sometimes construed as a special kind of cognition that occurs when people are in interaction with one another or with material artifacts. This is only partly correct. Rather than being a kind of cognition, distributed cognition is a manner of thinking about cognition that permits one to examine the relationships between what is in the mind and the world the mind is in. When applied to groups of persons, distributed cognition provides a language for cognitive processes that are distributed across the members of a social group, between people and their material environments, and through time. It attempts to use an understanding of the social, cultural, and material context of cognitive practices to constrain models of cognitive processes within and among individual minds.}
}
@article{RUBIOFERNANDEZ2024,
title = {Tracking minds in communication},
journal = {Trends in Cognitive Sciences},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324003127},
author = {Paula Rubio-Fernandez and Marlene D. Berke and Julian Jara-Ettinger},
keywords = {social cognition, Theory of Mind, language, communication},
abstract = {How does social cognition help us communicate through language? At what levels does this interaction occur? In classical views, social cognition is independent of language, and integrating the two can be slow, effortful, and error-prone. But new research into word level processes reveals that communication is brimming with social micro-processes that happen in real time, guiding even the simplest choices like how we use adjectives, articles, and demonstratives. We interpret these findings in the context of advances in theoretical models of social cognition and propose a communicative mind-tracking framework, where social micro-processes are not a secondary process in how we use language – they are fundamental to how communication works.}
}
@article{BRANICKY199567,
title = {Universal computation and other capabilities of hybrid and continuous dynamical systems},
journal = {Theoretical Computer Science},
volume = {138},
number = {1},
pages = {67-100},
year = {1995},
note = {Hybrid Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(94)00147-B},
url = {https://www.sciencedirect.com/science/article/pii/030439759400147B},
author = {Michael S. Branicky},
abstract = {We explore the simulation and computational capabilities of hybrid and continuous dynamical systems. The continuous dynamical systems considered are ordinary differential equations (ODEs). For hybrid systems we concentrate on models that combine ODEs and discrete dynamics (e.g., finite automata). We review and compare four such models from the literature. Notions of simulation of a discrete dynamical system by a continuous one are developed. We show that hybrid systems whose equations can describe a precise binary timing pulse (exact clock) can simulate arbitrary reversible discrete dynamical systems defined on closed subsets of Rn. The simulations require continuous ODEs in R2n with the exact clock as input. All four hybrid systems models studied here can implement exact clocks. We also prove that any discrete dynamical system in Zn can be simulated by continuous ODEs in R2n + 1. We use this to show that smooth ODEs in R3 can simulate arbitrary Turing machines, and hence possess the power of universal computation. We use the famous asynchronous arbiter problem to distinguish between hybrid and continuous dynamical systems. We prove that one cannot build an arbiter with devices described by a system of Lipschitz ODEs. On the other hand, all four hybrid systems models considered can implement arbiters even if their ODEs are Lipschitz.}
}
@article{RASTEIRO2009e9,
title = {LABVIRTUAL—A virtual platform to teach chemical processes},
journal = {Education for Chemical Engineers},
volume = {4},
number = {1},
pages = {e9-e19},
year = {2009},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772809000025},
author = {M.G. Rasteiro and L. Ferreira and J. Teixeira and F.P. Bernardo and M.G. Carvalho and A. Ferreira and R.Q. Ferreira and F. Garcia and C.M.S.G. Baptista and N. Oliveira and M. Quina and L. Santos and P.A. Saraiva and A. Mendes and F. Magalhães and A.S. Almeida and J. Granjo and M. Ascenso and R.M. Bastos and R. Borges},
keywords = {Chemical processes, E-learning, Virtual laboratories, Computational platform},
abstract = {The need to develop the capacity for autonomous and critical thinking in students and introduce practical approaches that complement the scientific background, have been acting as driving-forces that motivate engineering educators to develop new teaching methodologies. The Chemical Engineering Departments of both the Universities of Coimbra and Porto have been experimenting in this area and addressing these concerns. Recently, they have been engaged in a broader project, involving a large group of academics with complementary competencies. This project is aimed at developing a virtual platform directed towards the learning of Chemical Processes with a wide scope. From the functional point of view the platform is organized into four main areas: Chemical Engineering, Chemical Processes, Virtual Experiments and Simulators. The Chemical Processes area is further divided into four different sections: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications and case studies to better understand the chemical/biochemical processes. The Virtual Experiments area considers both the laboratory visualization of the basic phenomena related to the processes in the other four sections, and the remote monitoring of laboratory experiments. This platform, constructed around a dynamic Web Portal, allows discussion forums and is also aimed at sharing experiences with other schools. This paper describes the different subjects included in the web platform, as well as the simulation strategies and the web methodologies used for its construction, and also presents examples of application in the classroom.}
}
@article{LEI2025110929,
title = {Fusion of heterogeneous industrial wireless networks: A survey},
journal = {Computer Networks},
volume = {257},
pages = {110929},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110929},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624007618},
author = {Jiale Lei and Piao Jiang and Linghe Kong and Chi Xu and Chenren Xu and Kai Lin and Yueping Cai and Yanzhao Su and Weiping Ding and Zhen Wang and Bangyu Li and Xiaoguang Chen and Feng Gao and Weibo Wang and Jiadi Yu},
keywords = {Industrial wireless networks, 5G-U, Network fusion, Industrial internet of things},
abstract = {With the surge of wireless communication technology in smart factories, competition between different signals for limited unauthorized spectrum has led to heavy network conflicts and congestion. New technologies such as 5G unlicensed (5G-U) join industrial wireless networks (IWN) to provide advanced network access properties, making the issue more troublesome. This paper explores the advantages of fusion thinking from a new perspective of integrating heterogeneous IWNs, and systematically analyzes the key technologies that support IWN fusion, filling the gap in existing literature on IWN fusion systems. The main contribution of this paper includes proposing a technical framework based on the classic IWN architecture, fully studying the technologies and extensive research work that contribute to achieving IWN fusion from a bottom-up perspective. Moreover, open issues and prospects are enumerated to inspire valuable research works. Our research not only provides substantial contributions to the integration of the latest technologies, but also has important potential impacts on the future development of smart factory network infrastructure.}
}
@incollection{MILLER2017103,
title = {6 - Graduate and postgraduate education at a crossroads},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {103-128},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000064},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Postdoctoral, Postgraduate, Master's degree, Doctorate.},
abstract = {In this chapter we introduce the proverbial crossroads we have reached in graduate and postgraduate education and jobs. Many factors are at play, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has been called “the gathering storm.” Core drug discovery disciplines are discussed, such as medicinal and organic chemistry, especially in the context of academia–industry symbiosis. Challenges in making sure we continue to assemble the best and the brightest to tackle important biomedical problems are considered. Finally, we scratch the surface of how to navigate employers, employment, and careers.}
}
@article{HICKMAN1995153,
title = {Advanced computational methods for spatial information extraction},
journal = {Computers & Geosciences},
volume = {21},
number = {1},
pages = {153-173},
year = {1995},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(94)00063-Z},
url = {https://www.sciencedirect.com/science/article/pii/009830049400063Z},
author = {Betty L. Hickman and Michael P. Bishop and Michael V. Rescigno},
keywords = {Spatial feature extraction, Texture features, Parallel processing, Spatial task partitioning},
abstract = {A variety of mathematical approaches for spatial information extraction using digitized aerial photography and satellite imagery have been developed and implemented on serial computers. However, because of data volume and scale, the computational demands of spatial analysis procedures frequently exceed the capacity of available serial processing technologies. One way of addressing this problem is through parallel processing in which the power of multiple computing units can be used on a single problem. In this study we investigate the utility of parallel processing for spatial feature extraction. Our testing in the situation of texture feature extraction using a cooccurrence matrix indicates that dramatic reductions in execution time are possible—an image that required about 34 min to process using one processor was solved in under 2 min using nineteen processors. The availability of additional processors could result in smaller execution times. This speedup potential is a critical element in future studies focusing on more complex spatial analysis procedures.}
}
@article{NAKAKOJI2000451,
title = {Computational support for collective creativity},
journal = {Knowledge-Based Systems},
volume = {13},
number = {7},
pages = {451-458},
year = {2000},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(00)00069-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950705100000691},
author = {K Nakakoji and Y Yamamoto and M Ohira},
keywords = {Computer support for collective creativity, Human–computer interaction, Visual images in creative insight, Knowledge-based approaches, Visualization},
abstract = {The goal of our research is to develop computer systems that support designers’ collective creativity; such systems support individual creative aspects in design through the use of representations created by others in the community. We have developed two systems, IAM-eMMa and EVIDII, that both aim at supporting designers in finding visual images that would be useful for their creative design task. IAM-eMMa uses knowledge-based rules, which are constructed by other designers, to retrieve images related to a design task, and infers the underlying “rationale” when a designer chooses one of the images. EVIDII allows designers to associate affective words and images, and then shows several visual representations of the relationships among designers, images and words. By observing designers interacting with the two systems, we have identified that systems for supporting collective creativity need to be based on design knowledge that: (1) is contextualized; (2) is respectable and trustful; and (3) enables “appropriation” of a design task.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{VAITESSWAR2024210,
title = {Machine learning based feature engineering for thermoelectric materials by design††Electronic supplementary information (ESI) available: Details on methodology, Box–Cox transformations, machine learning models, and inverse design. See DOI: https://doi.org/10.1039/d3dd00131h},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {210-220},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00131h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000305},
author = {U. S. Vaitesswar and Daniil Bash and Tan Huang and Jose Recatala-Gomez and Tianqi Deng and Shuo-Wang Yang and Xiaonan Wang and Kedar Hippalgaonkar},
abstract = {Availability of material datasets through high performance computing has enabled the use of machine learning to not only discover correlations and employ materials informatics to perform screening, but also to take the first steps towards materials by design. Computational materials databases are well-labelled and provide a fertile ground for predicting both ground-state and functional properties of materials. However, a clear design approach that allows prediction of materials with the desired functional performance does not yet exist. In this work, we train various machine learning models on a dataset curated from a combination of Materials Project as well as computationally calculated thermoelectric electronic power factor using a constant relaxation time Boltzmann transport equation (BoltzTrap). We show that simple random forest-based machine learning models outperform more complex neural network-based approaches on the moderately sized dataset and also allow for interpretability. In addition, when trained on only cubic material systems, the best performing machine learning model employs a perturbative scanning approach to find new candidates in Materials Project that it has never seen before, and automatically converges upon half-Heusler alloys as promising thermoelectric materials. We validate this prediction by performing density functional theory and BoltzTrap calculations to reveal accurate matching. One of those predicted to be a good material, NbFeSb, has been studied recently by the thermoelectric community; from this study, we propose four new half-Heusler compounds as promising thermoelectric materials – TiGePt, ZrInAu, ZrSiPd and ZrSiPt. Our approach is generalizable to extrapolate into previously unexplored material spaces and establishes an automated pipeline for the development of high-throughput functional materials.}
}
@incollection{MARKOVA2015443,
title = {Representations, Social Psychology of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {443-449},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.24084-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868240841},
author = {Ivana Marková},
keywords = {Anchoring, Cognitive polyphasia, Common sense, Communication, Dialogicality, Ego–Alter–Object, Ethics, Figurative scheme, Imagination, Interactional epistemology, Intervention strategies, Language, Objectification, Social representations, Themata},
abstract = {The theory of social representations studies the formation and transformation of meanings and activities of complex social phenomena like health and illness, political problems or environmental issues in and through language and communication, history and culture. There are two mutually interdependent meanings of social representations. The first meaning concerns the theory of social representations as an interactional theory of knowledge. It refers to networks of concepts and figurative schemes that are generated in and through tradition, common sense, daily knowledge, and communication; these are shared by particular groups and communities. The main features of this theory are the Ego–Alter–Object, the field, the interdependence of asymmetries and symmetries, ethics, figurative scheme, and cognitive polyphasia. Second, social representations refer to concrete social phenomena and to forms of apprehending and creating social realities in and through communication, experience, social practices, and interventions. Human thinking is characterized by the capacity to make distinctions and understand phenomena as dyadic antinomies or themata. Thematization of dyadic antinomies is linked with anchoring and objectification, through which social representations are formed and transformed.}
}
@incollection{HALFORD2020327,
title = {Cognitive Developmental Theories☆},
editor = {Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {327-336},
year = {2020},
isbn = {978-0-12-816511-9},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05787-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245057874},
author = {G.S. Halford},
keywords = {Analogy, Cognitive complexity, Conceptual chunking, Dynamic systems, Information processing, Mental models, Neural net, Object permanence, Relational knowledge, Symbolic processes, Theory of mind, Working memory},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next. Conceptual coherence is achieved by categorizing cognitive processes according to their core properties.}
}
@article{BRENTDANIEL2023105630,
title = {Extremely rapid, Lagrangian modeling of 2D flooding: A rivulet-based approach},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105630},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105630},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000166},
author = {W. {Brent Daniel} and Corinne Roth and Xue Li and Cindy Rakowski and Tim McPherson and David Judi},
keywords = {Modeling, Fluid flow, Flood modeling, Hydrodynamic model, Rivulet, Lagrangian},
abstract = {Estimates of potential flood inundation areas and depths are critical to informing the preparedness, response, and investment decisions of many government agencies and private sector organizations, especially under a changing climate. The standard modeling approaches, however, are often either computationally intensive or constrained in their accuracy or applicability. A novel, rivulet-based, 2D model of flooding is described in this article that is 10,000 to 10 million times less computationally complex than the full solution of the shallow water equations, yet achieves inundation area hit rates of between 0.8 and 0.9, relative absolute mean errors of 10%–20% across a wide range of flow depths, and comparable accuracy at forecasting empirical high-water marks. This combination of accuracy and efficiency will significantly enhance real-time depth estimates during flood events, support detailed sensitivity analyses, and allow for the generation of large ensembles to enable complex uncertainty analyses.}
}
@article{YANG2020119,
title = {A multilevel neighborhood sequential decision approach of three-way granular computing},
journal = {Information Sciences},
volume = {538},
pages = {119-141},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304734},
author = {Xin Yang and Tianrui Li and Dun Liu and Hamido Fujita},
keywords = {Three-way granular computing, Sequential three-way decision, Neighborhood, Multilevel},
abstract = {The fusion of three-way decision and granular computing provides powerful ideas and methods to understand and solve the problems of cognitive science by thinking and information processing in threes. As a typical representation of three-way granular computing, sequential three-way decision focuses on making a multiple stages of decisions by a sequence of trisecting-acting-outcome (TAO) models. To construct more general granules, levels, and hierarchies, we investigate an integrative multi-granularity approach to sequential three-way decision in a neighborhood system by the evolution mechanism of data and parameters. We employ the γ-cut similarity neighborhood relation based on Gaussian kernel function to the hierarchical granulation of universe. Subsequently, we propose the multilevel neighborhood granular structures by the combinations of horizontal granularity and vertical granularity, and discuss the monotonicity of level measurements associated with the uncertainty of decision. Based on such a neighborhood structured approach, a multilevel framework of sequential three-way decision is examined from coarser to finer concerning the granularity of neighborhood information. Finally, we report a series of experiments to demonstrate the performance of proposed models and algorithms.}
}
@article{GUPTA19941,
title = {On the principles of fuzzy neural networks},
journal = {Fuzzy Sets and Systems},
volume = {61},
number = {1},
pages = {1-18},
year = {1994},
issn = {0165-0114},
doi = {https://doi.org/10.1016/0165-0114(94)90279-8},
url = {https://www.sciencedirect.com/science/article/pii/0165011494902798},
author = {M.M. Gupta and D.H. Rao},
keywords = {Fuzzy logic, neural networks, fuzzy neural networks, confluence operation, synpatic and somatic operations},
abstract = {Over the last decade or so, significant advances have been made in two distinct technological areas: fuzzy logic and computational neutral networks. The theory of fuzzy logic provides a mathematical framework to capture the uncertainties associated with human cognitive processes, such as thinking and reasoning. Also, it provides a mathematical morphology to emulate certain perceptual and linguistic attributes associated with human cognition. On the other hand, the computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. Computational neural networks replicate, on a small scale, some of the computational operations observed in biological learning and adaptation. The integration of these two fields, fuzzy logic and neural networks; has given birth to an emerging technological field — the fuzzy neural networks. The fuzzy neural networks have the potential to capture the benefits of the two fascinating fields, fuzzy logic and neural networks, into a single capsule. The intent of this tutorial paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks. Towards this goal, we develop a fuzzy neural architecture based upon the notion of T-norm and T-conorm connectives. An error-based learning scheme is described for this neural structure.}
}
@article{DECOUGNY1994157,
title = {Load balancing for the parallel adaptive solution of partial differential equations},
journal = {Applied Numerical Mathematics},
volume = {16},
number = {1},
pages = {157-182},
year = {1994},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(94)00039-5},
url = {https://www.sciencedirect.com/science/article/pii/0168927494000395},
author = {H.L. deCougny and K.D. Devine and J.E. Flaherty and R.M. Loy and C. Özturan and M.S. Shephard},
abstract = {An adaptive technique for a partial differential system automatically adjusts a computational mesh or varies the order of a numerical procedure with a goal of obtaining a solution satisfying prescribed accuracy criteria in an optimal fashion. Processor load imbalances will, therefore, be introduced at adaptive enrichment steps during the course of a parallel computation. We develop and describe three procedures for retaining and restoring load balance that have low unit cost and are appropriate for use in an adaptive solution environment. Tiling balances load by using local optimality criteria within overlapping processor neighborhoods. Elemental data are migrated between processors within the same neighborhoods to restore balance. Tiling is restricted to uniform two-dimensional meshes and provides limited control of communications volume by priority-based element selection criteria. These shortcomings can potentially be overcome by creating a dynamic partition graph connecting processors and their neighboring regions. After coloring the edges of the graph, elemental data are iteratively transferred between processors by pairwise exchange to permit a more global migration. Octree decomposition of a spatial domain is a successful three-dimensional mesh generation strategy. The octree structure facilities a rapid load balancing procedure by performing tree traversals that (i) appraise subtree costs and (ii) partition spatial regions accordingly. Computational results are reported for two- and three-dimensional problems using nCUBE/2 hypercube, MasPar MP-2, and Thinking Machines CM-5 computers.}
}
@article{ELLIOT2022112418,
title = {An expanded framing of ecosystem services is needed for a sustainable urban future},
journal = {Renewable and Sustainable Energy Reviews},
volume = {162},
pages = {112418},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112418},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003264},
author = {T. Elliot and J.A. Torres-Matallana and B. Goldstein and J. {Babí Almenar} and E. Gómez-Baggethun and V. Proença and B. Rugani},
keywords = {Urban ecosystem services, Urban metabolism, Life cycle thinking, Land cover change, System dynamics modelling, Urban land teleconnections},
abstract = {Urban activities are an important driver of ecosystem services decline. Sustainable urbanisation necessitates anticipating and mitigating these negative socio-ecological impacts, both within and beyond city boundaries. There is a lack of scalable, dynamic models of changes to ecosystems wrought by urban processes. We developed a system dynamics model, ESTIMUM, to predict locations, types, and magnitude of changes in ecosystem services. We tested the model in Lisbon (Portugal) under four specific urban development scenarios – a base case scenario and three local sustainability-driven scenarios – to the year 2050. Our results show that urban sustainability policies focused on reducing impacts within Lisbon can be undermined by increased impacts in the extended regions that supply resources to the city. In particular, carbon sequestration from urban greening pales in comparison to growing greenhouse gases from the consumption of food, energy and construction materials. We also find that policies targeted at these extended environmental impacts can be much more effective than those with a limited focus on the urban form. For example, dietary shifts could support positive changes outside that city to increase global climate regulation by 54% compared to a mere 1% increase through intensive urban greening. This highlights the urgent need for a reframing of urban sustainability in policy and scholarly circles from city-centric focus towards an expanded multi-scalar conceptualisation of urban sustainability that accounts for urban impacts beyond the city boundaries.}
}
@article{SCHAEFER198897,
title = {A history of ab initio computational quantum chemistry: 1950–1960},
journal = {Tetrahedron Computer Methodology},
volume = {1},
number = {2},
pages = {97-102},
year = {1988},
issn = {0898-5529},
doi = {https://doi.org/10.1016/0898-5529(88)90014-0},
url = {https://www.sciencedirect.com/science/article/pii/0898552988900140},
author = {Henry F. Schaefer},
keywords = {Quantum chemistry, Ab initio, Electronic structure theory, Molecular quantum mechanics, Computations},
abstract = {Although ab initio computational quantum chemistry produced virtually no predictions of chemical interest during the 1950's, an important foundation for future work was laid during this decade. Much of this fundamental computational research was carried out in the laboratories of Frank Boys in Cambridge (England) and Clemens Roothaan and Robert Mulliken in Chicago. Other senior contributors to ab initio chemical theory during this period include Klaus Ruedenberg, Robert Parr, John Pople, Robert Nesbet, Harrison Shull, Per-Olov Löwdin, Isaiah Shavitt, Albert Matsen, Douglas McLean, and Bernard Ransil.}
}
@incollection{TIRAPELLE20233465,
title = {Practical learning activities to increase the interest of university applicants in STEM careers in the era of Industry 4.0},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3465-3470},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50553-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740505539},
author = {Monica Tirapelle and Dian Ning Chia and Fanyi Duanmu and Konstantinos Katsoulas and Alberto Marchetto and Eva Sorensen},
keywords = {Engineering Education, Hands-on activities, Active learning, Orientation to university, PSE computational tools},
abstract = {Inspiring young students, especially young girls, about STEM disciplines is crucial to address the current shortage of engineers. Since the engineering skills that are required by graduates are evolving in line with technological progress, there is now an even stronger need for graduates with strong Process Systems Engineering skills. In this work, we describe an effective way to promote the chemical engineering curriculum, with particular emphasis on computational tools, to a group of Year 12 high school students during a one-week course in our department. The course was designed to engage students in active learning through interactive sessions and practical hands-on activities. Through the course, the students gained a better understanding of the importance of STEM subjects and, in particular, of the challenges and opportunities that engineers encounter in the era of Industry 4.0 with ever-increasing use of digitalization in process design and operation.}
}
@article{LIGABO2023102155,
title = {Practical way to apply fourth-generation assessment tools integrated into creating meaningful learning experiences in biology at high school},
journal = {Evaluation and Program Planning},
volume = {96},
pages = {102155},
year = {2023},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922001094},
author = {Mateus Ligabo and Fabiana Carvalho Silva and Ana Carolina da S.A. Carvalho and Durval Rodrigues and Rita C.L.B. Rodrigues},
keywords = {Concept maps, Meaningful learning, Hermeneutic-dialectic circle, Hofstede's cultural dimensions, Fourth-generation assessment},
abstract = {The learning process for a Biology topic regarding organisms and animal kingdom diversity was investigated through an innovative Interactive Didactic Sequence (IDS) which integrated the idea of “concept maps” with the Hermeneutic-Dialectic Circle (HDC). HDC is a tool for data collection and a reference for pluralist-constructivist thinking, considered a form of fourth-generation evaluation. Hofstede's cultural dimensions were also integrated into the investigation in order to facilitate mediation in an evaluative context. Students' performances (N = 25) from a São Paulo-Brazil public school were statistically evaluated. Their cultural profile was determined via the Hofstede Value Survey Model 1994 questionnaire. The elaborative process of arranging concept maps was individual (CM-individual) and integrated with HDC in groups (CM-HDC). Concept map assessment methods were based off existing literature. An improvement in students' performances (p < 0.05) that presented concept maps integrated to HDC in a more complex structure when compared to individually-built maps was observed. Employment of HDC helped form motivational/interactive dialogues between students and teachers, which, in turn, assisted in achieving greater learning through the use of concept maps. The application of the fourth-generation evaluation was improved via knowledge regarding students' cultural profiles.}
}
@article{STREVENS202192,
title = {Permissible idealizations for the purpose of prediction},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {92-100},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301813},
author = {Michael Strevens},
keywords = {Prediction, Idealization, Modeling, Difference-making, Causal relevance},
abstract = {Every model leaves out or distorts some factors that are causally connected to its target phenomenon—the phenomenon that it seeks to predict or explain. If we want to make predictions, and we want to base decisions on those predictions, what is it safe to omit or to simplify, and what ought a causal model to describe fully and correctly? A schematic answer: the factors that matter are those that make a difference to the target phenomenon. There are several ways to understand differencemaking. This paper advances a view as to which is the most relevant to the forecaster and the decision-maker. It turns out that the right notion of differencemaking for thinking about idealization in prediction is also the right notion for thinking about idealization in explanation; this suggests a carefully circumscribed version of Hempel’s famous thesis that there is a symmetry between explanation and prediction.}
}
@article{WOLLMANN2019278,
title = {Proposal for a model to hierarchize strategic decisions according to criteria of value innovation, sustainability and budgetary constraint},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {278-289},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.190},
url = {https://www.sciencedirect.com/science/article/pii/S095965261931724X},
author = {Dewey Wollmann and Ubiratã Tortato},
keywords = {Complex adaptive systems, Analytic network process, BOCR analysis, Linear programming},
abstract = {Organizations need management models, which will enable their executives to develop systemic thinking. In addition, executives should keep in mind that: some decision-making processes should be shared; impose political influence according to their preferences; value innovation strategies may be present; it is essential to consider the environmental, economic and social dimensions of sustainability. In this context, this study describes a model to hierarchize strategic decisions according to criteria innovation value, sustainability and budgetary constraint, developed according to the methodology proposed by Mitroff et al. (1974). In addition to hierarchizing, the model allows identifying the degree of importance of each of the strategic decisions in the performance indicators defined as evaluation criteria and sub-criteria. In the conceptualization phase, the model is influenced by concepts that describe complex adaptive systems. Next, the Analytic Network Process with Benefits, Opportunities, Costs and Risks Analysis and Linear Programming techniques are used in order to define the mathematical structure that operationalizes the model. The use of a hypothetical example demonstrates the capacity of the model proposed in this work to support the decision-making process of an organization in the selection of its decision alternatives. Thus, the model can help the academic and business communities concerned with the progress of sustainable societies, insofar as it subsidizes decision-making for the development and implementation of new products and processes related to cleaner production.}
}
@article{MILLNER2020704,
title = {Advancing the Understanding of Suicide: The Need for Formal Theory and Rigorous Descriptive Research},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {9},
pages = {704-716},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301480},
author = {Alexander J. Millner and Donald J. Robinaugh and Matthew K. Nock},
keywords = {suicide, suicide theory, formal models},
abstract = {Suicide is a leading cause of death worldwide and perhaps the most puzzling and devastating of all human behaviors. Suicide research has primarily been guided by verbal theories containing vague constructs and poorly specified relationships. We propose two fundamental changes required to move toward a mechanistic understanding of suicide. First, we must formalize theories of suicide, expressing them as mathematical or computational models. Second, we must conduct rigorous descriptive research, prioritizing direct observation and precise measurement of suicidal thoughts and behaviors and of the factors posited to cause them. Together, theory formalization and rigorous descriptive research will facilitate abductive theory construction and strong theory testing, thereby improving the understanding and prevention of suicide and related behaviors.}
}
@article{GOECKE2020101470,
title = {Testing competing claims about overclaiming},
journal = {Intelligence},
volume = {81},
pages = {101470},
year = {2020},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2020.101470},
url = {https://www.sciencedirect.com/science/article/pii/S0160289620300489},
author = {B. Goecke and S. Weiss and D. Steger and U. Schroeders and O. Wilhelm},
keywords = {Overclaiming, Declarative knowledge, Self-reported knowledge, Creativity, Intelligence, Faking},
abstract = {Overclaiming has been described as people's tendency to overestimate their cognitive abilities in general and their knowledge in particular. We discuss four different perspectives on the phenomenon of overclaiming that have been proposed in the research literature: Overclaiming as a result of a) self-enhancement tendencies, b) as a cognitive bias (e.g., hindsight bias, memory bias), c) as proxy for cognitive abilities, and d) as sign of creative engagement. Moreover, we discuss two different scoring methods for an OCQ (signal detection theory vs. familiarity ratings). To distinguish between the different viewpoints of what overclaiming is, we juxtaposed overclaiming, as indicated by claiming familiarity with non-existent terms, with fluid and crystallized intelligence, self-reported knowledge, creativity, faking ability, and personality. Overclaiming was measured with a newly comprised overclaiming questionnaire. Results of several latent variable analyses based upon a multivariate study with 298 participants were: First, overclaiming is neither predicted by honesty-humility nor faking ability and therefore reflects something different than mere self-enhancement tendencies. Second, overclaiming is not predicted by crystallized intelligence, but is highly predictive of self-reported knowledge and, thus, not suitable as an index or a proxy for cognitive abilities. Finally, overclaiming is neither related to divergent thinking and originality, and only moderately predicted by self-reported openness creativity from the HEXACO which means that overclaiming does not reflect creative ability. In sum, our results favor an interpretation of overclaiming as a phenomenon that requires more than self-enhancement motivation, in contrast to the claim that was initially proposed in the literature.}
}
@article{OSORIOMORA2025837,
title = {A risk-averse latency location-routing problem with stochastic travel times},
journal = {European Journal of Operational Research},
volume = {321},
number = {3},
pages = {837-850},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724008440},
author = {Alan Osorio-Mora and Francisco Saldanha-da-Gama and Paolo Toth},
keywords = {Routing, Cumulative routing, Sampling, Variable neighborhood search},
abstract = {In this paper, a latency location-routing problem with stochastic travel times is investigated. The problem is cast as a two-stage stochastic program. The ex-ante decision comprises the location of the depots. The ex-post decision regards the routing, which adapts to the observed travel times. A risk-averse decision-maker is assumed, which is conveyed by adopting the latency CVaRα as the objective function. The problem is formulated mathematically. An efficient multi-start variable neighborhood search algorithm is proposed for tackling the problem when uncertainty is captured by a finite set of scenarios. This procedure is then embedded into a sampling mechanism so that realistic instances of the problem can be tackled, namely when the travel times are represented by random vectors with an infinite support. An extensive computational analysis is conducted to assess the methodological developments proposed and the relevance of capturing uncertainty in the problem. Additional insights include the impact of the risk level in the solutions.}
}
@article{KNOWLTON2012373,
title = {A neurocomputational system for relational reasoning},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {7},
pages = {373-381},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312001283},
author = {Barbara J. Knowlton and Robert G. Morrison and John E. Hummel and Keith J. Holyoak},
abstract = {The representation and manipulation of structured relations is central to human reasoning. Recent work in computational modeling and neuroscience has set the stage for developing more detailed neurocomputational models of these abilities. Several key neural findings appear to dovetail with computational constraints derived from a model of analogical processing, ‘Learning and Inference with Schemas and Analogies’ (LISA). These include evidence that (i) coherent oscillatory activity in the gamma and theta bands enables long-distance communication between the prefrontal cortex and posterior brain regions where information is stored; (ii) neurons in prefrontal cortex can rapidly learn to represent abstract concepts; (iii) a rostral-caudal abstraction gradient exists in the PFC; and (iv) the inferior frontal gyrus exerts inhibitory control over task-irrelevant information.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{BATTISTELLI2022,
title = {Online Strategies To Improve Quantitative Skills in Microbiology Laboratory Classes},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {1},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00333-21},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000995},
author = {Joseph M. Battistelli and Rima B. Franklin},
keywords = {quantitative literacy, quantitative biology, problem solving, word problems, math skills, formula question, Canvas, spreadsheets, algebra, formula questions},
abstract = {Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills.
ABSTRACT
Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills. This can create a substantial grading burden for faculty teaching online and/or large enrollment courses, but the “formula question” feature present in many learning management systems (LMS) offers a solution. Using this feature, faculty set up a basic scaffold for an algebraic word problem, and the LMS can then automatically generate and grade many different versions of the question. In this paper, we describe the use of “formula questions” in an undergraduate microbiology course and specifically focus on how the strategic use of algebraic word problems at multiple points throughout the semester can help build quantitative literacy. Key to the success of this approach is that faculty provide a review of foundational mathematical skills early in the semester, even in upper-level classes. This should include reacquainting students with formatting conventions (e.g., rounding and scientific notation), familiarizing them with any idiosyncrasies of the technology platforms, and demonstrating how to solve math problems using spreadsheets. This initial effort increases student success when more complex problems are introduced later in the semester. Though the tips summarized in this paper focus on undergraduate microbiology teaching laboratories using Canvas, the approach can easily be modified to help students develop their critical thinking and quantitative reasoning skills at other levels and in other disciplines.}
}
@incollection{HEGARTY2010265,
title = {Chapter 7 - Components of Spatial Intelligence},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {52},
pages = {265-297},
year = {2010},
booktitle = {The Psychology of Learning and Motivation},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(10)52007-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742110520073},
author = {Mary Hegarty},
abstract = {This chapter identifies two basic components of spatial intelligence, based on analyses of performance on tests of spatial ability and on complex spatial thinking tasks in domains such as mechanics, chemistry, medicine, and meteorology. The first component is flexible strategy choice between mental imagery (or mental simulation more generally) and more analytic forms of thinking. Research reviewed here suggests that mental simulation is an important strategy in spatial thinking, but that it is augmented by more analytic strategies such as task decomposition and rule-based reasoning. The second is meta-representational competence [diSessa, A. A. (2004). Metarepresentation: Native competence and targets for instruction. Cognition and Instruction, 22, 293–331], which encompasses ability to choose the optimal external representation for a task and to use novel external representations productively. Research on this aspect of spatial intelligence reveals large individual differences in ability to adaptively choose and use external visual–spatial representations for a task. This research suggests that we should not just think of interactive external visualizations as ways of augmenting spatial intelligence, but also consider the types of intelligence that are required for their use.}
}
@article{WANG2025113691,
title = {Effect of interlayer spacing on the mechanical properties of the graphene oxide/thermoplastic polyurethane nanocomposite},
journal = {Computational Materials Science},
volume = {250},
pages = {113691},
year = {2025},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2025.113691},
url = {https://www.sciencedirect.com/science/article/pii/S0927025625000345},
author = {Yuyang Wang and Guofu Yin and Junpeng Liu and Jiao Li and Chunqiang Wei and Minjie Li},
keywords = {Thermoplastic Polyurethane, Graphene Oxide, Interlayer Spacing, Mechanical Properties, Molecular Dynamics},
abstract = {In this paper, in order to investigate the effect of graphene oxide (GO) interlayer spacing on the overall mechanical properties of GO/thermoplastic polyurethane (TPU) nanocomposite, the uniaxial tensile simulation on the computational model of GO/TPU nanocomposite with monolayer and bilayer graphene sheets were carried out. During the tensile process, the variation of potential energy, void percentage, tensile strain contour, and density distribution with tensile strain were applied to analyze the underlying microscopic mechanism of stress change of monolayer GO/TPU system. The results show that when the system was within the stress yield region, the micro rearrangement motion and relative sliding of TPU chains play a major role. Furthermore, when the system enters into the stress softening region until stress failure, the void formation and nuclear within the system dominates the stress change, which is attributed to the interfacial structure of GO/TPU. Subsequently, based on the above-mentioned results in the case of monolayer GO/TPU result, the effects of different layer spacings on the overall mechanical properties of GO/TPU nanocomposite system were discussed by varying the spacing of GO layers. The results show that with the increase of GO layer spacing, the elastic modulus and yield strength of the system show a tendency of increasing and then decreasing, and the failure strain is the opposite, and when the GO dispersion is better, it can also play the role of delayed damage failure of the system.}
}
@article{OLIVIER2024103956,
title = {DynBioSketch: A tool for sketching dynamic visual summaries in biology, and its application to infection phenomena},
journal = {Computers & Graphics},
volume = {122},
pages = {103956},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000918},
